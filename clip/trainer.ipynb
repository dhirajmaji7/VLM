{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import CLIPDataset\n",
    "from model import CLIP\n",
    "from tokenizer import CLIPTextTokenizer\n",
    "from loss import InfoNCECriterion\n",
    "from utils import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import model, dataset, tokenizer, loss, utils\n",
    "\n",
    "# Reload all modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, train_dataloader,test_dataloader, optimizer, device, tokenizer, num_epochs):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        #self.criterion = criterion.to(self.device)\n",
    "        self.criterion = F.cross_entropy\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Initialize OpenAI's GPT-2 BPE tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def train_step(self, images, token_ids):\n",
    "        images = images.to(self.device)\n",
    "        token_ids = token_ids.to(self.device)\n",
    "        #print(\"token id\",token_ids)\n",
    "        logits = self.model(images, token_ids)\n",
    "        target = torch.arange(logits.size(0), device=self.device)  # B\n",
    "        loss = self.criterion(logits, target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    @timeit\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        for i, (images, token_ids) in enumerate(self.train_dataloader):\n",
    "            loss = self.train_step(images, token_ids)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{self.num_epochs}], Step [{i+1}/{len(self.train_dataloader)}], Loss: {loss:.4f}\")\n",
    "        return loss\n",
    "\n",
    "    @timeit\n",
    "    def run(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss = self.train_epoch(epoch)\n",
    "            print(f\"Epoch [{epoch+1}/{self.num_epochs}], Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer: gpt2 with vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer_ = CLIPTextTokenizer(context_length=25)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "            CLIPDataset(image_dir='../Images', captions_filepath='../captions.txt', tokenizer=tokenizer.tokenize_text),\n",
    "            batch_size=32, shuffle=True, num_workers=4\n",
    "        )\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "            CLIPDataset(image_dir='../Images', captions_filepath='../captions.txt', tokenizer=tokenizer.tokenize_text),\n",
    "            batch_size=32, shuffle=False, num_workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import model, dataset, tokenizer, loss, utils\n",
    "importlib.reload(model)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "importlib.reload(loss)\n",
    "importlib.reload(utils)\n",
    "from dataset import CLIPDataset\n",
    "from model import CLIP\n",
    "from tokenizer import CLIPTextTokenizer\n",
    "from loss import InfoNCECriterion\n",
    "from utils import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using tokenizer: gpt2 with vocab size: 50257\n",
      "Starting training...\n",
      "Epoch [1/10], Step [10/253], Loss: 5.1160\n",
      "Epoch [1/10], Step [20/253], Loss: 4.4140\n",
      "Epoch [1/10], Step [30/253], Loss: 3.9029\n",
      "Epoch [1/10], Step [40/253], Loss: 3.5686\n",
      "Epoch [1/10], Step [50/253], Loss: 3.7846\n",
      "Epoch [1/10], Step [60/253], Loss: 3.7231\n",
      "Epoch [1/10], Step [70/253], Loss: 3.6017\n",
      "Epoch [1/10], Step [80/253], Loss: 3.7355\n",
      "Epoch [1/10], Step [90/253], Loss: 3.6202\n",
      "Epoch [1/10], Step [100/253], Loss: 3.6485\n",
      "Epoch [1/10], Step [110/253], Loss: 3.5051\n",
      "Epoch [1/10], Step [120/253], Loss: 3.7207\n",
      "Epoch [1/10], Step [130/253], Loss: 3.6085\n",
      "Epoch [1/10], Step [140/253], Loss: 3.7181\n",
      "Epoch [1/10], Step [150/253], Loss: 3.5126\n",
      "Epoch [1/10], Step [160/253], Loss: 3.4836\n",
      "Epoch [1/10], Step [170/253], Loss: 3.5998\n",
      "Epoch [1/10], Step [180/253], Loss: 3.5560\n",
      "Epoch [1/10], Step [190/253], Loss: 3.5891\n",
      "Epoch [1/10], Step [200/253], Loss: 3.5263\n",
      "Epoch [1/10], Step [210/253], Loss: 3.4621\n",
      "Epoch [1/10], Step [220/253], Loss: 3.5352\n",
      "Epoch [1/10], Step [230/253], Loss: 3.5322\n",
      "Epoch [1/10], Step [240/253], Loss: 3.4925\n",
      "Epoch [1/10], Step [250/253], Loss: 3.4589\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [1/10], Loss: 3.3301\n",
      "Epoch [2/10], Step [10/253], Loss: 3.4387\n",
      "Epoch [2/10], Step [20/253], Loss: 3.4936\n",
      "Epoch [2/10], Step [30/253], Loss: 3.5093\n",
      "Epoch [2/10], Step [40/253], Loss: 3.5070\n",
      "Epoch [2/10], Step [50/253], Loss: 3.5035\n",
      "Epoch [2/10], Step [60/253], Loss: 3.4588\n",
      "Epoch [2/10], Step [70/253], Loss: 3.5198\n",
      "Epoch [2/10], Step [80/253], Loss: 3.5066\n",
      "Epoch [2/10], Step [90/253], Loss: 3.4803\n",
      "Epoch [2/10], Step [100/253], Loss: 3.4786\n",
      "Epoch [2/10], Step [110/253], Loss: 3.4470\n",
      "Epoch [2/10], Step [120/253], Loss: 3.4547\n",
      "Epoch [2/10], Step [130/253], Loss: 3.4669\n",
      "Epoch [2/10], Step [140/253], Loss: 3.5061\n",
      "Epoch [2/10], Step [150/253], Loss: 3.4895\n",
      "Epoch [2/10], Step [160/253], Loss: 3.4861\n",
      "Epoch [2/10], Step [170/253], Loss: 3.4831\n",
      "Epoch [2/10], Step [180/253], Loss: 3.5242\n",
      "Epoch [2/10], Step [190/253], Loss: 3.5310\n",
      "Epoch [2/10], Step [200/253], Loss: 3.4905\n",
      "Epoch [2/10], Step [210/253], Loss: 3.5261\n",
      "Epoch [2/10], Step [220/253], Loss: 3.4824\n",
      "Epoch [2/10], Step [230/253], Loss: 3.4949\n",
      "Epoch [2/10], Step [240/253], Loss: 3.5014\n",
      "Epoch [2/10], Step [250/253], Loss: 3.4585\n",
      "[train_epoch] Execution time: 0:00:41\n",
      "Epoch [2/10], Loss: 3.3442\n",
      "Epoch [3/10], Step [10/253], Loss: 3.5270\n",
      "Epoch [3/10], Step [20/253], Loss: 3.4855\n",
      "Epoch [3/10], Step [30/253], Loss: 3.4760\n",
      "Epoch [3/10], Step [40/253], Loss: 3.5253\n",
      "Epoch [3/10], Step [50/253], Loss: 3.4788\n",
      "Epoch [3/10], Step [60/253], Loss: 3.4925\n",
      "Epoch [3/10], Step [70/253], Loss: 3.4816\n",
      "Epoch [3/10], Step [80/253], Loss: 3.4675\n",
      "Epoch [3/10], Step [90/253], Loss: 3.4327\n",
      "Epoch [3/10], Step [100/253], Loss: 3.4604\n",
      "Epoch [3/10], Step [110/253], Loss: 3.4795\n",
      "Epoch [3/10], Step [120/253], Loss: 3.4798\n",
      "Epoch [3/10], Step [130/253], Loss: 3.4961\n",
      "Epoch [3/10], Step [140/253], Loss: 3.5973\n",
      "Epoch [3/10], Step [150/253], Loss: 3.5340\n",
      "Epoch [3/10], Step [160/253], Loss: 3.5542\n",
      "Epoch [3/10], Step [170/253], Loss: 3.7106\n",
      "Epoch [3/10], Step [180/253], Loss: 3.9622\n",
      "Epoch [3/10], Step [190/253], Loss: 3.6709\n",
      "Epoch [3/10], Step [200/253], Loss: 3.7079\n",
      "Epoch [3/10], Step [210/253], Loss: 3.7643\n",
      "Epoch [3/10], Step [220/253], Loss: 3.5164\n",
      "Epoch [3/10], Step [230/253], Loss: 3.5418\n",
      "Epoch [3/10], Step [240/253], Loss: 3.7313\n",
      "Epoch [3/10], Step [250/253], Loss: 3.6705\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [3/10], Loss: 3.4355\n",
      "Epoch [4/10], Step [10/253], Loss: 3.5682\n",
      "Epoch [4/10], Step [20/253], Loss: 3.5227\n",
      "Epoch [4/10], Step [30/253], Loss: 3.5331\n",
      "Epoch [4/10], Step [40/253], Loss: 3.4941\n",
      "Epoch [4/10], Step [50/253], Loss: 3.4816\n",
      "Epoch [4/10], Step [60/253], Loss: 3.4520\n",
      "Epoch [4/10], Step [70/253], Loss: 3.5113\n",
      "Epoch [4/10], Step [80/253], Loss: 3.4717\n",
      "Epoch [4/10], Step [90/253], Loss: 3.4345\n",
      "Epoch [4/10], Step [100/253], Loss: 3.3859\n",
      "Epoch [4/10], Step [110/253], Loss: 3.4323\n",
      "Epoch [4/10], Step [120/253], Loss: 3.3983\n",
      "Epoch [4/10], Step [130/253], Loss: 3.4396\n",
      "Epoch [4/10], Step [140/253], Loss: 3.3442\n",
      "Epoch [4/10], Step [150/253], Loss: 3.6840\n",
      "Epoch [4/10], Step [160/253], Loss: 3.4894\n",
      "Epoch [4/10], Step [170/253], Loss: 3.4932\n",
      "Epoch [4/10], Step [180/253], Loss: 3.4587\n",
      "Epoch [4/10], Step [190/253], Loss: 3.2952\n",
      "Epoch [4/10], Step [200/253], Loss: 3.4544\n",
      "Epoch [4/10], Step [210/253], Loss: 3.3930\n",
      "Epoch [4/10], Step [220/253], Loss: 3.4707\n",
      "Epoch [4/10], Step [230/253], Loss: 3.4620\n",
      "Epoch [4/10], Step [240/253], Loss: 3.3685\n",
      "Epoch [4/10], Step [250/253], Loss: 3.5255\n",
      "[train_epoch] Execution time: 0:00:41\n",
      "Epoch [4/10], Loss: 3.1466\n",
      "Epoch [5/10], Step [10/253], Loss: 3.2123\n",
      "Epoch [5/10], Step [20/253], Loss: 3.4053\n",
      "Epoch [5/10], Step [30/253], Loss: 3.3171\n",
      "Epoch [5/10], Step [40/253], Loss: 3.3636\n",
      "Epoch [5/10], Step [50/253], Loss: 3.3711\n",
      "Epoch [5/10], Step [60/253], Loss: 3.4548\n",
      "Epoch [5/10], Step [70/253], Loss: 3.4427\n",
      "Epoch [5/10], Step [80/253], Loss: 3.3068\n",
      "Epoch [5/10], Step [90/253], Loss: 3.4277\n",
      "Epoch [5/10], Step [100/253], Loss: 3.2752\n",
      "Epoch [5/10], Step [110/253], Loss: 3.3835\n",
      "Epoch [5/10], Step [120/253], Loss: 3.2605\n",
      "Epoch [5/10], Step [130/253], Loss: 3.5256\n",
      "Epoch [5/10], Step [140/253], Loss: 3.5337\n",
      "Epoch [5/10], Step [150/253], Loss: 3.6187\n",
      "Epoch [5/10], Step [160/253], Loss: 3.4901\n",
      "Epoch [5/10], Step [170/253], Loss: 3.4539\n",
      "Epoch [5/10], Step [180/253], Loss: 3.4328\n",
      "Epoch [5/10], Step [190/253], Loss: 3.3566\n",
      "Epoch [5/10], Step [200/253], Loss: 3.3093\n",
      "Epoch [5/10], Step [210/253], Loss: 3.5058\n",
      "Epoch [5/10], Step [220/253], Loss: 3.3629\n",
      "Epoch [5/10], Step [230/253], Loss: 3.1603\n",
      "Epoch [5/10], Step [240/253], Loss: 3.2830\n",
      "Epoch [5/10], Step [250/253], Loss: 3.2886\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [5/10], Loss: 2.9862\n",
      "Epoch [6/10], Step [10/253], Loss: 3.1508\n",
      "Epoch [6/10], Step [20/253], Loss: 3.0601\n",
      "Epoch [6/10], Step [30/253], Loss: 3.1715\n",
      "Epoch [6/10], Step [40/253], Loss: 3.2884\n",
      "Epoch [6/10], Step [50/253], Loss: 3.1452\n",
      "Epoch [6/10], Step [60/253], Loss: 3.0004\n",
      "Epoch [6/10], Step [70/253], Loss: 3.0830\n",
      "Epoch [6/10], Step [80/253], Loss: 2.9039\n",
      "Epoch [6/10], Step [90/253], Loss: 3.5691\n",
      "Epoch [6/10], Step [100/253], Loss: 3.0808\n",
      "Epoch [6/10], Step [110/253], Loss: 3.3542\n",
      "Epoch [6/10], Step [120/253], Loss: 3.0234\n",
      "Epoch [6/10], Step [130/253], Loss: 3.1553\n",
      "Epoch [6/10], Step [140/253], Loss: 3.1852\n",
      "Epoch [6/10], Step [150/253], Loss: 2.9002\n",
      "Epoch [6/10], Step [160/253], Loss: 3.2292\n",
      "Epoch [6/10], Step [170/253], Loss: 3.0245\n",
      "Epoch [6/10], Step [180/253], Loss: 3.3617\n",
      "Epoch [6/10], Step [190/253], Loss: 3.2472\n",
      "Epoch [6/10], Step [200/253], Loss: 3.4149\n",
      "Epoch [6/10], Step [210/253], Loss: 3.2079\n",
      "Epoch [6/10], Step [220/253], Loss: 3.1779\n",
      "Epoch [6/10], Step [230/253], Loss: 3.2684\n",
      "Epoch [6/10], Step [240/253], Loss: 3.4702\n",
      "Epoch [6/10], Step [250/253], Loss: 3.2938\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [6/10], Loss: 3.3890\n",
      "Epoch [7/10], Step [10/253], Loss: 2.8816\n",
      "Epoch [7/10], Step [20/253], Loss: 3.1822\n",
      "Epoch [7/10], Step [30/253], Loss: 2.9205\n",
      "Epoch [7/10], Step [40/253], Loss: 3.1532\n",
      "Epoch [7/10], Step [50/253], Loss: 3.2715\n",
      "Epoch [7/10], Step [60/253], Loss: 3.1688\n",
      "Epoch [7/10], Step [70/253], Loss: 3.1265\n",
      "Epoch [7/10], Step [80/253], Loss: 2.9245\n",
      "Epoch [7/10], Step [90/253], Loss: 3.0420\n",
      "Epoch [7/10], Step [100/253], Loss: 3.1167\n",
      "Epoch [7/10], Step [110/253], Loss: 3.2025\n",
      "Epoch [7/10], Step [120/253], Loss: 3.1174\n",
      "Epoch [7/10], Step [130/253], Loss: 3.2179\n",
      "Epoch [7/10], Step [140/253], Loss: 3.1647\n",
      "Epoch [7/10], Step [150/253], Loss: 3.0758\n",
      "Epoch [7/10], Step [160/253], Loss: 3.4517\n",
      "Epoch [7/10], Step [170/253], Loss: 3.1173\n",
      "Epoch [7/10], Step [180/253], Loss: 3.2750\n",
      "Epoch [7/10], Step [190/253], Loss: 3.3798\n",
      "Epoch [7/10], Step [200/253], Loss: 3.0081\n",
      "Epoch [7/10], Step [210/253], Loss: 3.0689\n",
      "Epoch [7/10], Step [220/253], Loss: 3.0866\n",
      "Epoch [7/10], Step [230/253], Loss: 3.2097\n",
      "Epoch [7/10], Step [240/253], Loss: 2.9373\n",
      "Epoch [7/10], Step [250/253], Loss: 2.9033\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [7/10], Loss: 3.0427\n",
      "Epoch [8/10], Step [10/253], Loss: 3.1527\n",
      "Epoch [8/10], Step [20/253], Loss: 3.0292\n",
      "Epoch [8/10], Step [30/253], Loss: 3.1057\n",
      "Epoch [8/10], Step [40/253], Loss: 3.4968\n",
      "Epoch [8/10], Step [50/253], Loss: 2.8230\n",
      "Epoch [8/10], Step [60/253], Loss: 2.8776\n",
      "Epoch [8/10], Step [70/253], Loss: 2.8140\n",
      "Epoch [8/10], Step [80/253], Loss: 3.1342\n",
      "Epoch [8/10], Step [90/253], Loss: 3.0891\n",
      "Epoch [8/10], Step [100/253], Loss: 3.1068\n",
      "Epoch [8/10], Step [110/253], Loss: 3.0394\n",
      "Epoch [8/10], Step [120/253], Loss: 2.9124\n",
      "Epoch [8/10], Step [130/253], Loss: 3.0526\n",
      "Epoch [8/10], Step [140/253], Loss: 2.8909\n",
      "Epoch [8/10], Step [150/253], Loss: 3.0050\n",
      "Epoch [8/10], Step [160/253], Loss: 3.1473\n",
      "Epoch [8/10], Step [170/253], Loss: 3.2821\n",
      "Epoch [8/10], Step [180/253], Loss: 2.8324\n",
      "Epoch [8/10], Step [190/253], Loss: 3.1091\n",
      "Epoch [8/10], Step [200/253], Loss: 2.8970\n",
      "Epoch [8/10], Step [210/253], Loss: 2.9603\n",
      "Epoch [8/10], Step [220/253], Loss: 3.1137\n",
      "Epoch [8/10], Step [230/253], Loss: 2.9631\n",
      "Epoch [8/10], Step [240/253], Loss: 2.8393\n",
      "Epoch [8/10], Step [250/253], Loss: 2.8868\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [8/10], Loss: 2.6803\n",
      "Epoch [9/10], Step [10/253], Loss: 2.6452\n",
      "Epoch [9/10], Step [20/253], Loss: 2.8618\n",
      "Epoch [9/10], Step [30/253], Loss: 2.5161\n",
      "Epoch [9/10], Step [40/253], Loss: 2.6929\n",
      "Epoch [9/10], Step [50/253], Loss: 2.7102\n",
      "Epoch [9/10], Step [60/253], Loss: 2.9125\n",
      "Epoch [9/10], Step [70/253], Loss: 2.5878\n",
      "Epoch [9/10], Step [80/253], Loss: 2.8837\n",
      "Epoch [9/10], Step [90/253], Loss: 3.0121\n",
      "Epoch [9/10], Step [100/253], Loss: 2.4975\n",
      "Epoch [9/10], Step [110/253], Loss: 3.2404\n",
      "Epoch [9/10], Step [120/253], Loss: 2.4543\n",
      "Epoch [9/10], Step [130/253], Loss: 2.8882\n",
      "Epoch [9/10], Step [140/253], Loss: 2.6698\n",
      "Epoch [9/10], Step [150/253], Loss: 2.5994\n",
      "Epoch [9/10], Step [160/253], Loss: 2.8061\n",
      "Epoch [9/10], Step [170/253], Loss: 2.6633\n",
      "Epoch [9/10], Step [180/253], Loss: 3.5349\n",
      "Epoch [9/10], Step [190/253], Loss: 2.8454\n",
      "Epoch [9/10], Step [200/253], Loss: 3.1217\n",
      "Epoch [9/10], Step [210/253], Loss: 2.7185\n",
      "Epoch [9/10], Step [220/253], Loss: 3.2471\n",
      "Epoch [9/10], Step [230/253], Loss: 2.8734\n",
      "Epoch [9/10], Step [240/253], Loss: 2.7795\n",
      "Epoch [9/10], Step [250/253], Loss: 2.7555\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [9/10], Loss: 2.7397\n",
      "Epoch [10/10], Step [10/253], Loss: 2.7841\n",
      "Epoch [10/10], Step [20/253], Loss: 2.7527\n",
      "Epoch [10/10], Step [30/253], Loss: 2.5538\n",
      "Epoch [10/10], Step [40/253], Loss: 2.4715\n",
      "Epoch [10/10], Step [50/253], Loss: 2.6079\n",
      "Epoch [10/10], Step [60/253], Loss: 2.6643\n",
      "Epoch [10/10], Step [70/253], Loss: 2.6546\n",
      "Epoch [10/10], Step [80/253], Loss: 2.6892\n",
      "Epoch [10/10], Step [90/253], Loss: 3.0323\n",
      "Epoch [10/10], Step [100/253], Loss: 2.6614\n",
      "Epoch [10/10], Step [110/253], Loss: 2.7787\n",
      "Epoch [10/10], Step [120/253], Loss: 2.3373\n",
      "Epoch [10/10], Step [130/253], Loss: 2.7335\n",
      "Epoch [10/10], Step [140/253], Loss: 3.0876\n",
      "Epoch [10/10], Step [150/253], Loss: 2.8179\n",
      "Epoch [10/10], Step [160/253], Loss: 2.9154\n",
      "Epoch [10/10], Step [170/253], Loss: 2.6507\n",
      "Epoch [10/10], Step [180/253], Loss: 2.4073\n",
      "Epoch [10/10], Step [190/253], Loss: 2.4825\n",
      "Epoch [10/10], Step [200/253], Loss: 3.1216\n",
      "Epoch [10/10], Step [210/253], Loss: 3.3726\n",
      "Epoch [10/10], Step [220/253], Loss: 3.1113\n",
      "Epoch [10/10], Step [230/253], Loss: 2.5744\n",
      "Epoch [10/10], Step [240/253], Loss: 2.6109\n",
      "Epoch [10/10], Step [250/253], Loss: 2.8391\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [10/10], Loss: 2.5305\n",
      "[run] Execution time: 0:07:01\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "tokenizer_ = CLIPTextTokenizer(context_length=25)\n",
    "\n",
    "# Initialize model components\n",
    "model = CLIP(vocab_size=tokenizer_.n_vocab,image_dim=192, caption_dim=512, embedding_dim=512)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = InfoNCECriterion()\n",
    "\n",
    "trainer = Trainer(model, criterion, train_dataloader,test_dataloader, optimizer, device, tokenizer_, num_epochs=10)\n",
    "print(\"Starting training...\")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [10/253], Loss: 0.6999\n",
      "Epoch [1/10], Step [20/253], Loss: 1.0488\n",
      "Epoch [1/10], Step [30/253], Loss: 1.2342\n",
      "Epoch [1/10], Step [40/253], Loss: 1.6791\n",
      "Epoch [1/10], Step [50/253], Loss: 1.0836\n",
      "Epoch [1/10], Step [60/253], Loss: 1.1651\n",
      "Epoch [1/10], Step [70/253], Loss: 1.3230\n",
      "Epoch [1/10], Step [80/253], Loss: 1.2022\n",
      "Epoch [1/10], Step [90/253], Loss: 1.7410\n",
      "Epoch [1/10], Step [100/253], Loss: 0.8990\n",
      "Epoch [1/10], Step [110/253], Loss: 1.2030\n",
      "Epoch [1/10], Step [120/253], Loss: 1.0674\n",
      "Epoch [1/10], Step [130/253], Loss: 1.1343\n",
      "Epoch [1/10], Step [140/253], Loss: 1.2541\n",
      "Epoch [1/10], Step [150/253], Loss: 1.2865\n",
      "Epoch [1/10], Step [160/253], Loss: 1.3873\n",
      "Epoch [1/10], Step [170/253], Loss: 1.2811\n",
      "Epoch [1/10], Step [180/253], Loss: 1.2079\n",
      "Epoch [1/10], Step [190/253], Loss: 0.9278\n",
      "Epoch [1/10], Step [200/253], Loss: 1.1761\n",
      "Epoch [1/10], Step [210/253], Loss: 1.1748\n",
      "Epoch [1/10], Step [220/253], Loss: 1.2018\n",
      "Epoch [1/10], Step [230/253], Loss: 1.2082\n",
      "Epoch [1/10], Step [240/253], Loss: 1.0502\n",
      "Epoch [1/10], Step [250/253], Loss: 1.1971\n",
      "[train_epoch] Execution time: 0:00:41\n",
      "Epoch [1/10], Loss: 1.0495\n",
      "Epoch [2/10], Step [10/253], Loss: 0.7434\n",
      "Epoch [2/10], Step [20/253], Loss: 1.2360\n",
      "Epoch [2/10], Step [30/253], Loss: 1.1478\n",
      "Epoch [2/10], Step [40/253], Loss: 0.8226\n",
      "Epoch [2/10], Step [50/253], Loss: 0.9608\n",
      "Epoch [2/10], Step [60/253], Loss: 1.4878\n",
      "Epoch [2/10], Step [70/253], Loss: 1.2474\n",
      "Epoch [2/10], Step [80/253], Loss: 1.1345\n",
      "Epoch [2/10], Step [90/253], Loss: 1.0650\n",
      "Epoch [2/10], Step [100/253], Loss: 0.9128\n",
      "Epoch [2/10], Step [110/253], Loss: 0.7411\n",
      "Epoch [2/10], Step [120/253], Loss: 1.0017\n",
      "Epoch [2/10], Step [130/253], Loss: 1.2393\n",
      "Epoch [2/10], Step [140/253], Loss: 1.2441\n",
      "Epoch [2/10], Step [150/253], Loss: 1.0579\n",
      "Epoch [2/10], Step [160/253], Loss: 1.2691\n",
      "Epoch [2/10], Step [170/253], Loss: 1.0963\n",
      "Epoch [2/10], Step [180/253], Loss: 1.0625\n",
      "Epoch [2/10], Step [190/253], Loss: 0.8643\n",
      "Epoch [2/10], Step [200/253], Loss: 1.1954\n",
      "Epoch [2/10], Step [210/253], Loss: 1.1661\n",
      "Epoch [2/10], Step [220/253], Loss: 0.9476\n",
      "Epoch [2/10], Step [230/253], Loss: 0.9503\n",
      "Epoch [2/10], Step [240/253], Loss: 1.8210\n",
      "Epoch [2/10], Step [250/253], Loss: 1.1875\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [2/10], Loss: 0.9376\n",
      "Epoch [3/10], Step [10/253], Loss: 1.0726\n",
      "Epoch [3/10], Step [20/253], Loss: 0.9967\n",
      "Epoch [3/10], Step [30/253], Loss: 0.8619\n",
      "Epoch [3/10], Step [40/253], Loss: 1.1799\n",
      "Epoch [3/10], Step [50/253], Loss: 1.0138\n",
      "Epoch [3/10], Step [60/253], Loss: 1.2861\n",
      "Epoch [3/10], Step [70/253], Loss: 0.8838\n",
      "Epoch [3/10], Step [80/253], Loss: 0.9951\n",
      "Epoch [3/10], Step [90/253], Loss: 1.4450\n",
      "Epoch [3/10], Step [100/253], Loss: 0.9536\n",
      "Epoch [3/10], Step [110/253], Loss: 1.1574\n",
      "Epoch [3/10], Step [120/253], Loss: 0.9290\n",
      "Epoch [3/10], Step [130/253], Loss: 0.9484\n",
      "Epoch [3/10], Step [140/253], Loss: 0.9022\n",
      "Epoch [3/10], Step [150/253], Loss: 1.0252\n",
      "Epoch [3/10], Step [160/253], Loss: 1.4368\n",
      "Epoch [3/10], Step [170/253], Loss: 1.0789\n",
      "Epoch [3/10], Step [180/253], Loss: 0.7780\n",
      "Epoch [3/10], Step [190/253], Loss: 1.2243\n",
      "Epoch [3/10], Step [200/253], Loss: 0.8757\n",
      "Epoch [3/10], Step [210/253], Loss: 1.6094\n",
      "Epoch [3/10], Step [220/253], Loss: 1.0018\n",
      "Epoch [3/10], Step [230/253], Loss: 1.0519\n",
      "Epoch [3/10], Step [240/253], Loss: 1.0171\n",
      "Epoch [3/10], Step [250/253], Loss: 1.4047\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [3/10], Loss: 1.3096\n",
      "Epoch [4/10], Step [10/253], Loss: 0.8207\n",
      "Epoch [4/10], Step [20/253], Loss: 0.6934\n",
      "Epoch [4/10], Step [30/253], Loss: 1.1740\n",
      "Epoch [4/10], Step [40/253], Loss: 0.9899\n",
      "Epoch [4/10], Step [50/253], Loss: 0.8020\n",
      "Epoch [4/10], Step [60/253], Loss: 1.2119\n",
      "Epoch [4/10], Step [70/253], Loss: 1.0447\n",
      "Epoch [4/10], Step [80/253], Loss: 0.8232\n",
      "Epoch [4/10], Step [90/253], Loss: 0.8603\n",
      "Epoch [4/10], Step [100/253], Loss: 0.8220\n",
      "Epoch [4/10], Step [110/253], Loss: 1.2117\n",
      "Epoch [4/10], Step [120/253], Loss: 0.6242\n",
      "Epoch [4/10], Step [130/253], Loss: 1.0177\n",
      "Epoch [4/10], Step [140/253], Loss: 1.3733\n",
      "Epoch [4/10], Step [150/253], Loss: 1.2928\n",
      "Epoch [4/10], Step [160/253], Loss: 0.6647\n",
      "Epoch [4/10], Step [170/253], Loss: 0.9040\n",
      "Epoch [4/10], Step [180/253], Loss: 1.0780\n",
      "Epoch [4/10], Step [190/253], Loss: 0.7378\n",
      "Epoch [4/10], Step [200/253], Loss: 1.0730\n",
      "Epoch [4/10], Step [210/253], Loss: 0.8629\n",
      "Epoch [4/10], Step [220/253], Loss: 0.8675\n",
      "Epoch [4/10], Step [230/253], Loss: 1.1012\n",
      "Epoch [4/10], Step [240/253], Loss: 1.2823\n",
      "Epoch [4/10], Step [250/253], Loss: 1.1787\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [4/10], Loss: 1.4853\n",
      "Epoch [5/10], Step [10/253], Loss: 0.8918\n",
      "Epoch [5/10], Step [20/253], Loss: 0.8243\n",
      "Epoch [5/10], Step [30/253], Loss: 0.8605\n",
      "Epoch [5/10], Step [40/253], Loss: 0.6143\n",
      "Epoch [5/10], Step [50/253], Loss: 1.3134\n",
      "Epoch [5/10], Step [60/253], Loss: 1.0115\n",
      "Epoch [5/10], Step [70/253], Loss: 1.3583\n",
      "Epoch [5/10], Step [80/253], Loss: 1.1162\n",
      "Epoch [5/10], Step [90/253], Loss: 0.9915\n",
      "Epoch [5/10], Step [100/253], Loss: 1.1304\n",
      "Epoch [5/10], Step [110/253], Loss: 0.9167\n",
      "Epoch [5/10], Step [120/253], Loss: 0.9520\n",
      "Epoch [5/10], Step [130/253], Loss: 1.0881\n",
      "Epoch [5/10], Step [140/253], Loss: 0.9499\n",
      "Epoch [5/10], Step [150/253], Loss: 0.8766\n",
      "Epoch [5/10], Step [160/253], Loss: 1.2555\n",
      "Epoch [5/10], Step [170/253], Loss: 0.8675\n",
      "Epoch [5/10], Step [180/253], Loss: 0.8566\n",
      "Epoch [5/10], Step [190/253], Loss: 0.5535\n",
      "Epoch [5/10], Step [200/253], Loss: 1.1850\n",
      "Epoch [5/10], Step [210/253], Loss: 1.2323\n",
      "Epoch [5/10], Step [220/253], Loss: 0.8351\n",
      "Epoch [5/10], Step [230/253], Loss: 0.9152\n",
      "Epoch [5/10], Step [240/253], Loss: 1.0798\n",
      "Epoch [5/10], Step [250/253], Loss: 0.9578\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [5/10], Loss: 0.6375\n",
      "Epoch [6/10], Step [10/253], Loss: 0.6759\n",
      "Epoch [6/10], Step [20/253], Loss: 0.8315\n",
      "Epoch [6/10], Step [30/253], Loss: 0.6335\n",
      "Epoch [6/10], Step [40/253], Loss: 0.8419\n",
      "Epoch [6/10], Step [50/253], Loss: 0.6858\n",
      "Epoch [6/10], Step [60/253], Loss: 1.1359\n",
      "Epoch [6/10], Step [70/253], Loss: 0.9569\n",
      "Epoch [6/10], Step [80/253], Loss: 0.6581\n",
      "Epoch [6/10], Step [90/253], Loss: 1.1454\n",
      "Epoch [6/10], Step [100/253], Loss: 0.7050\n",
      "Epoch [6/10], Step [110/253], Loss: 1.0038\n",
      "Epoch [6/10], Step [120/253], Loss: 0.7490\n",
      "Epoch [6/10], Step [130/253], Loss: 0.8575\n",
      "Epoch [6/10], Step [140/253], Loss: 0.8340\n",
      "Epoch [6/10], Step [150/253], Loss: 1.3426\n",
      "Epoch [6/10], Step [160/253], Loss: 0.7050\n",
      "Epoch [6/10], Step [170/253], Loss: 0.9727\n",
      "Epoch [6/10], Step [180/253], Loss: 0.7963\n",
      "Epoch [6/10], Step [190/253], Loss: 0.8295\n",
      "Epoch [6/10], Step [200/253], Loss: 0.8935\n",
      "Epoch [6/10], Step [210/253], Loss: 1.4183\n",
      "Epoch [6/10], Step [220/253], Loss: 0.8941\n",
      "Epoch [6/10], Step [230/253], Loss: 0.7240\n",
      "Epoch [6/10], Step [240/253], Loss: 1.1678\n",
      "Epoch [6/10], Step [250/253], Loss: 0.9793\n",
      "[train_epoch] Execution time: 0:00:42\n",
      "Epoch [6/10], Loss: 1.0081\n",
      "Epoch [7/10], Step [10/253], Loss: 0.7989\n",
      "Epoch [7/10], Step [20/253], Loss: 1.0045\n",
      "Epoch [7/10], Step [30/253], Loss: 0.8025\n",
      "Epoch [7/10], Step [40/253], Loss: 0.4941\n",
      "Epoch [7/10], Step [50/253], Loss: 1.3067\n",
      "Epoch [7/10], Step [60/253], Loss: 0.8898\n",
      "Epoch [7/10], Step [70/253], Loss: 1.1604\n",
      "Epoch [7/10], Step [80/253], Loss: 0.4959\n",
      "Epoch [7/10], Step [90/253], Loss: 0.7009\n",
      "Epoch [7/10], Step [100/253], Loss: 0.8031\n",
      "Epoch [7/10], Step [110/253], Loss: 0.9260\n",
      "Epoch [7/10], Step [120/253], Loss: 0.7806\n",
      "Epoch [7/10], Step [130/253], Loss: 0.6565\n",
      "Epoch [7/10], Step [140/253], Loss: 0.9413\n",
      "Epoch [7/10], Step [150/253], Loss: 0.4351\n",
      "Epoch [7/10], Step [160/253], Loss: 0.7273\n",
      "Epoch [7/10], Step [170/253], Loss: 1.2188\n",
      "Epoch [7/10], Step [180/253], Loss: 0.6486\n",
      "Epoch [7/10], Step [190/253], Loss: 1.0891\n",
      "Epoch [7/10], Step [200/253], Loss: 0.5441\n",
      "Epoch [7/10], Step [210/253], Loss: 1.1002\n",
      "Epoch [7/10], Step [220/253], Loss: 1.2227\n",
      "Epoch [7/10], Step [230/253], Loss: 1.0381\n",
      "Epoch [7/10], Step [240/253], Loss: 0.5970\n",
      "Epoch [7/10], Step [250/253], Loss: 0.8835\n",
      "[train_epoch] Execution time: 0:01:05\n",
      "Epoch [7/10], Loss: 1.1505\n",
      "Epoch [8/10], Step [10/253], Loss: 0.6809\n",
      "Epoch [8/10], Step [20/253], Loss: 0.6169\n",
      "Epoch [8/10], Step [30/253], Loss: 0.4958\n",
      "Epoch [8/10], Step [40/253], Loss: 0.5267\n",
      "Epoch [8/10], Step [50/253], Loss: 0.7708\n",
      "Epoch [8/10], Step [60/253], Loss: 0.9856\n",
      "Epoch [8/10], Step [70/253], Loss: 0.8742\n",
      "Epoch [8/10], Step [80/253], Loss: 1.0090\n",
      "Epoch [8/10], Step [90/253], Loss: 0.8857\n",
      "Epoch [8/10], Step [100/253], Loss: 0.7430\n",
      "Epoch [8/10], Step [110/253], Loss: 0.8226\n",
      "Epoch [8/10], Step [120/253], Loss: 0.7304\n",
      "Epoch [8/10], Step [130/253], Loss: 0.7386\n",
      "Epoch [8/10], Step [140/253], Loss: 0.7396\n",
      "Epoch [8/10], Step [150/253], Loss: 0.7114\n",
      "Epoch [8/10], Step [160/253], Loss: 0.5936\n",
      "Epoch [8/10], Step [170/253], Loss: 1.0549\n",
      "Epoch [8/10], Step [180/253], Loss: 0.5666\n",
      "Epoch [8/10], Step [190/253], Loss: 0.6177\n",
      "Epoch [8/10], Step [200/253], Loss: 0.6086\n",
      "Epoch [8/10], Step [210/253], Loss: 0.8201\n",
      "Epoch [8/10], Step [220/253], Loss: 0.9060\n",
      "Epoch [8/10], Step [230/253], Loss: 0.9241\n",
      "Epoch [8/10], Step [240/253], Loss: 0.3769\n",
      "Epoch [8/10], Step [250/253], Loss: 0.4939\n",
      "[train_epoch] Execution time: 0:01:08\n",
      "Epoch [8/10], Loss: 0.7294\n",
      "Epoch [9/10], Step [10/253], Loss: 0.6186\n",
      "Epoch [9/10], Step [20/253], Loss: 0.4766\n",
      "Epoch [9/10], Step [30/253], Loss: 0.8464\n",
      "Epoch [9/10], Step [40/253], Loss: 0.7799\n",
      "Epoch [9/10], Step [50/253], Loss: 0.8160\n",
      "Epoch [9/10], Step [60/253], Loss: 1.3641\n",
      "Epoch [9/10], Step [70/253], Loss: 1.0792\n",
      "Epoch [9/10], Step [80/253], Loss: 0.8410\n",
      "Epoch [9/10], Step [90/253], Loss: 1.1861\n",
      "Epoch [9/10], Step [100/253], Loss: 0.7469\n",
      "Epoch [9/10], Step [110/253], Loss: 0.4747\n",
      "Epoch [9/10], Step [120/253], Loss: 1.6265\n",
      "Epoch [9/10], Step [130/253], Loss: 0.8043\n",
      "Epoch [9/10], Step [140/253], Loss: 0.7457\n",
      "Epoch [9/10], Step [150/253], Loss: 0.4251\n",
      "Epoch [9/10], Step [160/253], Loss: 0.7174\n",
      "Epoch [9/10], Step [170/253], Loss: 0.5410\n",
      "Epoch [9/10], Step [180/253], Loss: 0.7540\n",
      "Epoch [9/10], Step [190/253], Loss: 0.4999\n",
      "Epoch [9/10], Step [200/253], Loss: 0.6306\n",
      "Epoch [9/10], Step [210/253], Loss: 1.0104\n",
      "Epoch [9/10], Step [220/253], Loss: 0.5416\n",
      "Epoch [9/10], Step [230/253], Loss: 0.8077\n",
      "Epoch [9/10], Step [240/253], Loss: 0.9036\n",
      "Epoch [9/10], Step [250/253], Loss: 0.7723\n",
      "[train_epoch] Execution time: 0:01:08\n",
      "Epoch [9/10], Loss: 1.4232\n",
      "Epoch [10/10], Step [10/253], Loss: 0.4636\n",
      "Epoch [10/10], Step [20/253], Loss: 0.9110\n",
      "Epoch [10/10], Step [30/253], Loss: 0.6923\n",
      "Epoch [10/10], Step [40/253], Loss: 0.5983\n",
      "Epoch [10/10], Step [50/253], Loss: 0.7079\n",
      "Epoch [10/10], Step [60/253], Loss: 0.5302\n",
      "Epoch [10/10], Step [70/253], Loss: 0.7686\n",
      "Epoch [10/10], Step [80/253], Loss: 0.5806\n",
      "Epoch [10/10], Step [90/253], Loss: 0.7504\n",
      "Epoch [10/10], Step [100/253], Loss: 0.8419\n",
      "Epoch [10/10], Step [110/253], Loss: 0.7677\n",
      "Epoch [10/10], Step [120/253], Loss: 0.5197\n",
      "Epoch [10/10], Step [130/253], Loss: 0.8498\n",
      "Epoch [10/10], Step [140/253], Loss: 0.4058\n",
      "Epoch [10/10], Step [150/253], Loss: 0.4111\n",
      "Epoch [10/10], Step [160/253], Loss: 0.5106\n",
      "Epoch [10/10], Step [170/253], Loss: 0.7242\n",
      "Epoch [10/10], Step [180/253], Loss: 0.7117\n",
      "Epoch [10/10], Step [190/253], Loss: 1.1037\n",
      "Epoch [10/10], Step [200/253], Loss: 0.8173\n",
      "Epoch [10/10], Step [210/253], Loss: 0.7004\n",
      "Epoch [10/10], Step [220/253], Loss: 1.1218\n",
      "Epoch [10/10], Step [230/253], Loss: 0.6363\n",
      "Epoch [10/10], Step [240/253], Loss: 0.7765\n",
      "Epoch [10/10], Step [250/253], Loss: 0.7135\n",
      "[train_epoch] Execution time: 0:01:08\n",
      "Epoch [10/10], Loss: 0.6607\n",
      "[run] Execution time: 0:08:45\n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to 'clip_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'clip_model.pth')\n",
    "print(\"Model saved to 'clip_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (image vs 5 captions):\n",
      "tensor([[31.8644, 39.7345, 52.7754, 43.4383, 71.9336],\n",
      "        [31.8644, 39.7345, 52.7754, 43.4383, 71.9336],\n",
      "        [31.8644, 39.7345, 52.7754, 43.4383, 71.9336],\n",
      "        [31.8644, 39.7345, 52.7754, 43.4383, 71.9336],\n",
      "        [31.8644, 39.7345, 52.7754, 43.4383, 71.9336]], device='cuda:0')\n",
      "Similarity scores: tensor([3.9644e-18, 1.0378e-14, 4.7831e-09, 4.2138e-13, 1.0000e+00],\n",
      "       device='cuda:0')\n",
      "Captions:\n",
      "Caption 1: perorsA female softball player making a pitch . fielded!!!!!!!!!!!!!!\n",
      "Caption 2: perorsA woman in a dress is crossing a suspended bridge . fielded!!!!!!!!!!!!\n",
      "Caption 3: perors\"A girl dressed in a red top  fielded!!!!!!!!!!!!!!\n",
      "Caption 4: perorsThe two children make a funny pose in front of some bushes . fielded!!!!!!!!!!\n",
      "Caption 5: perorsa woman at a desk signing paperwork in front of another fielded!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, caption \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(captions_text):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaption \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backend_bases.py:3620\u001b[0m, in \u001b[0;36m_Backend.show\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3618\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m ipython_pylab \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive()\n\u001b[1;32m   3619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m-> 3620\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/_backend_tk.py:523\u001b[0m, in \u001b[0;36mFigureManagerTk.start_main_loop\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    521\u001b[0m manager_class\u001b[38;5;241m.\u001b[39m_owns_mainloop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     \u001b[43mfirst_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     manager_class\u001b[38;5;241m.\u001b[39m_owns_mainloop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/tkinter/__init__.py:1429\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "images, token_ids = next(test_iter)  # first batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Select one image and 5 captions\n",
    "image = images[0].unsqueeze(0)  # Shape: (1, 3, H, W)\n",
    "captions = token_ids[:5]        # Shape: (5, T)\n",
    "\n",
    "# Step 3: Repeat the image 5 times to pair with 5 captions\n",
    "image_batch = image.repeat(5, 1, 1, 1)  # Shape: (5, 3, H, W)\n",
    "\n",
    "# Step 4: Move to device\n",
    "image_batch = image_batch.to(trainer.device)\n",
    "captions = captions.to(trainer.device)\n",
    "\n",
    "#suffle captions\n",
    "captions = captions[torch.randperm(captions.size(0))]  # Shuffle captions\n",
    "\n",
    "# Step 5: Run inference\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = trainer.model(image_batch, captions)  # Shape: (5, 5)\n",
    "\n",
    "# Step 6: Print logits\n",
    "print(\"Logits (image vs 5 captions):\")\n",
    "print(logits)\n",
    "\n",
    "# Optional: Compute similarity scores (e.g., softmax)\n",
    "import torch.nn.functional as F\n",
    "scores = F.softmax(logits[0], dim=0)  # Similarity of image to 5 captions\n",
    "print(\"Similarity scores:\", scores)\n",
    "\n",
    "plt.imshow(images[0].permute(1, 2, 0).cpu())\n",
    "plt.axis('off')\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Assume `captions` is a tensor of shape (5, T)\n",
    "captions_text = [tokenizer.decode(caption.tolist()) for caption in captions]\n",
    "\n",
    "print(\"Captions:\")\n",
    "for i, caption in enumerate(captions_text):\n",
    "    print(f\"Caption {i+1}: {caption}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
