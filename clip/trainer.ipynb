{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from model import CLIPModel\n",
    "from tokenizer import CLIPTextTokenizer\n",
    "from loss import InfoNCECriterion\n",
    "from train import Trainer\n",
    "from config import CLIPConfig\n",
    "from dataset import CLIPDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Reload all modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: image\n",
      "Skipping non-image file: image\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = CLIPConfig()\n",
    "tokenizer = CLIPTextTokenizer(config)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    CLIPDataset(config, 'train', tokenizer=tokenizer.tokenize_text),\n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=config.num_workers\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "        CLIPDataset(config, 'val', tokenizer=tokenizer.tokenize_text),\n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=config.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb onnx -Uq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import model, dataset, tokenizer, loss, utils, train,config\n",
    "importlib.reload(model)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "importlib.reload(loss)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(train)\n",
    "importlib.reload(config)\n",
    "from model import CLIPModel\n",
    "from tokenizer import CLIPTextTokenizer\n",
    "from loss import InfoNCECriterion\n",
    "from train import Trainer\n",
    "from config import CLIPConfig\n",
    "from dataset import CLIPDataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marpan12pallar\u001b[0m (\u001b[33marpan12pallar-arcbest\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/VLM/clip/wandb/run-20250721_183626-fwn5v6qe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arpan12pallar-arcbest/clip-training/runs/fwn5v6qe' target=\"_blank\">eager-flower-5</a></strong> to <a href='https://wandb.ai/arpan12pallar-arcbest/clip-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arpan12pallar-arcbest/clip-training' target=\"_blank\">https://wandb.ai/arpan12pallar-arcbest/clip-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arpan12pallar-arcbest/clip-training/runs/fwn5v6qe' target=\"_blank\">https://wandb.ai/arpan12pallar-arcbest/clip-training/runs/fwn5v6qe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "run = wandb.init(project=\"clip-training\", entity=\"arpan12pallar-arcbest\",reinit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training run directory created at: runs/run_2025-07-21_18-37-01\n",
      "Checkpoint directory created at: runs/run_2025-07-21_18-37-01/checkpoints\n",
      "Starting training...\n",
      "Using device: cuda\n",
      "*********  Epoch 1/2  *********\n",
      "[train_epoch] Execution time: 0:02:39637939453125\n",
      "Epoch 1 - Train Loss: 4.1317\n",
      "Epoch 1 - Validation Loss: 0.0000\n",
      "Best model saved at epoch 1 with validation loss: 0.0000\n",
      "*********  Epoch 2/2  *********\n",
      "[train_epoch] Execution time: 0:02:34105300903324\n",
      "Epoch 2 - Train Loss: 3.4872\n",
      "Epoch 2 - Validation Loss: 0.0000\n",
      "********* Training complete ********\n",
      "Final model saved to runs/run_2025-07-21_18-37-01/final_model.pth\n",
      "Training results saved to: runs/run_2025-07-21_18-37-01/results.json\n",
      "********* Training Summary *********\n",
      "Total epochs: 2\n",
      "Final training loss: 3.4872\n",
      "Final validation loss: 0.0000\n",
      "Final model: runs/run_2025-07-21_18-37-01/final_model.pth\n",
      "----------\n",
      "Best epoch: 1\n",
      "Best validation loss: 0.0000\n",
      "Best model: runs/run_2025-07-21_18-37-01/best_model.pth\n",
      "[run] Execution time: 0:05:14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_ = CLIPConfig()\n",
    "\n",
    "model = CLIPModel(config_)\n",
    "criterion = InfoNCECriterion()\n",
    "\n",
    "\n",
    "run.watch(model, criterion, log=\"all\", log_freq=1)\n",
    "\n",
    "trainer = Trainer(config_, model,train_dataloader,val_dataloader, criterion, device, tokenizer)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Using device: cuda\n",
      "*********  Epoch 1/2  *********\n",
      "[train_epoch] Execution time: 0:02:45949626922607\n",
      "Epoch 1 - Train Loss: 3.9266\n",
      "Validation Batch [22/253]: Loss 3.4665906429290777\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VLM/clip/utils.py:12\u001b[0m, in \u001b[0;36mtimeit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m     duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/VLM/clip/train.py:88\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_epoch()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss)\n",
      "File \u001b[0;32m~/VLM/clip/utils.py:12\u001b[0m, in \u001b[0;36mtimeit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m     duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/VLM/clip/train.py:71\u001b[0m, in \u001b[0;36mTrainer.validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     69\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m token_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 71\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits)\n\u001b[1;32m     73\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/integration/torch/wandb_torch.py:113\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.<lambda>\u001b[0;34m(mod, inp, outp)\u001b[0m\n\u001b[1;32m    110\u001b[0m log_track_params \u001b[38;5;241m=\u001b[39m log_track_init(log_freq)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     hook \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mregister_forward_hook(\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m mod, inp, outp: \u001b[43mparameter_log_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_track_params\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix] \u001b[38;5;241m=\u001b[39m hook\n\u001b[1;32m    118\u001b[0m     module\u001b[38;5;241m.\u001b[39m_wandb_hook_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/integration/torch/wandb_torch.py:108\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.parameter_log_hook\u001b[0;34m(module, input_, output, log_track)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     data \u001b[38;5;241m=\u001b[39m parameter\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_tensor_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/integration/torch/wandb_torch.py:229\u001b[0m, in \u001b[0;36mTorchHistory.log_tensor_stats\u001b[0;34m(self, tensor, name)\u001b[0m\n\u001b[1;32m    227\u001b[0m     bins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([tmin, tmax])\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mflat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    231\u001b[0m     bins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(tmin, tmax, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bins \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, token_ids = next(test_iter)  # first batch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Select one image and 5 captions\n",
    "image = images[0].unsqueeze(0)  # Shape: (1, 3, H, W)\n",
    "captions = token_ids[:5]        # Shape: (5, T)\n",
    "\n",
    "# Step 3: Repeat the image 5 times to pair with 5 captions\n",
    "image_batch = image.repeat(5, 1, 1, 1)  # Shape: (5, 3, H, W)\n",
    "\n",
    "# Step 4: Move to device\n",
    "image_batch = image_batch.to(trainer.device)\n",
    "captions = captions.to(trainer.device)\n",
    "\n",
    "#suffle captions\n",
    "captions = captions[torch.randperm(captions.size(0))]  # Shuffle captions\n",
    "\n",
    "# Step 5: Run inference\n",
    "trainer.model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = trainer.model(image_batch, captions)  # Shape: (5, 5)\n",
    "\n",
    "# Step 6: Print logits\n",
    "print(\"Logits (image vs 5 captions):\")\n",
    "print(logits)\n",
    "\n",
    "# Optional: Compute similarity scores (e.g., softmax)\n",
    "import torch.nn.functional as F\n",
    "scores = F.softmax(logits[0], dim=0)  # Similarity of image to 5 captions\n",
    "print(\"Similarity scores:\", scores)\n",
    "\n",
    "plt.imshow(images[0].permute(1, 2, 0).cpu())\n",
    "plt.axis('off')\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Assume `captions` is a tensor of shape (5, T)\n",
    "captions_text = [tokenizer.decode(caption.tolist()) for caption in captions]\n",
    "\n",
    "print(\"Captions:\")\n",
    "for i, caption in enumerate(captions_text):\n",
    "    print(f\"Caption {i+1}: {caption}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
