{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' BLIP 2:\n",
    " submodule:\n",
    "        LLM : flanT5\n",
    "        Image encoder:VIT\n",
    "        Q_former:\n",
    "            N tranformer: with cross attention :- image transformer\n",
    "                1. learned query embedding as input : 32x768\n",
    "                2. cross attention from image encoder every other layer\n",
    "                3. different mask for different pretraining mask\n",
    "            \n",
    "                \n",
    "            N transformer without cross attention :- text decoder and encdoer\n",
    "\n",
    "                1. self attention weights are shared \n",
    "\n",
    "\n",
    "\n",
    "                self.bert =  BERTbase(pretrained_true)\n",
    "                self.image_transformer = []\n",
    "                \n",
    "\n",
    "                for i,transformer_block in enumerate(self.bert):\n",
    "                    self.image_transformer.append(transformer.attn_block)\n",
    "                    if(i%2):\n",
    "                        self.image_transformer.append(new CrossAttention())\n",
    "                    self.image_transformer.append(transformer.MLP.copy())\n",
    "                \n",
    "        \n",
    "        LOSS:\n",
    "        1. ITC :\n",
    "        image transfomer : query embedding -q: 32x768  text encoder : t: 768x1\n",
    "        max cosine similarity = max(q@t)\n",
    "\n",
    "        2.  ITG : text : B,S -> B,S,E->softmax target: B,S[1:]\n",
    "\n",
    "        3. ITM : query : q:32x768 ->32,2 -> sum (2,)-> cross entropy loss\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
