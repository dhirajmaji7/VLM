{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.local/lib/python3.8/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3; platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\" in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (1.1.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (4.0.3)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch) (18.1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate torch pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "SAVE_DIR = \"./blip2-opt-2.7b\" \n",
    "\n",
    "if os.path.isdir(SAVE_DIR):\n",
    "    blip2_processor = Blip2Processor.from_pretrained(SAVE_DIR, local_files_only=True)\n",
    "    blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        SAVE_DIR,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        local_files_only=True,\n",
    "    )\n",
    "else:\n",
    "    # First run: download from Hub\n",
    "    blip2_processor = Blip2Processor.from_pretrained(MODEL_ID)\n",
    "    blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # Save for future runs\n",
    "    blip2_processor.save_pretrained(SAVE_DIR)\n",
    "    blip2_model.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model : Blip2VisionModel\n",
      "  Public methods: ['_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_slow_forward', 'forward', 'register_forward_hook', 'register_forward_pre_hook']\n",
      "qformer : Blip2QFormerModel\n",
      "  Public methods: ['_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_slow_forward', 'forward', 'register_forward_hook', 'register_forward_pre_hook']\n",
      "language_projection : Linear\n",
      "  Public methods: ['_slow_forward', 'forward', 'register_forward_hook', 'register_forward_pre_hook']\n",
      "language_model : OPTForCausalLM\n",
      "  Public methods: ['_hook_rss_memory_post_forward', '_hook_rss_memory_pre_forward', '_slow_forward', 'forward', 'register_forward_hook', 'register_forward_pre_hook']\n"
     ]
    }
   ],
   "source": [
    "for name, module in blip2_model.named_children():\n",
    "    print(name, \":\", module._get_name())\n",
    "    # Get public methods\n",
    "    public_methods = [m for m in dir(module) if callable(getattr(module, m)) and m.__contains__(\"forward\") ]\n",
    "    print(\"  Public methods:\", public_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2ForConditionalGeneration(\n",
      "  (vision_model): Blip2VisionModel(\n",
      "    (embeddings): Blip2VisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
      "    )\n",
      "    (encoder): Blip2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-38): 39 x Blip2EncoderLayer(\n",
      "          (self_attn): Blip2Attention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
      "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Blip2MLP(\n",
      "            (activation_fn): GELUActivation()\n",
      "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
      "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (qformer): Blip2QFormerModel(\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (encoder): Blip2QFormerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (crossattention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): Blip2QFormerLayer(\n",
      "          (attention): Blip2QFormerAttention(\n",
      "            (attention): Blip2QFormerMultiHeadAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): Blip2QFormerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate_query): Blip2QFormerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output_query): Blip2QFormerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (language_projection): Linear(in_features=768, out_features=2560, bias=True)\n",
      "  (language_model): OPTForCausalLM(\n",
      "    (model): OPTModel(\n",
      "      (decoder): OPTDecoder(\n",
      "        (embed_tokens): Embedding(50304, 2560, padding_idx=1)\n",
      "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2560)\n",
      "        (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x OPTDecoderLayer(\n",
      "            (self_attn): OPTAttention(\n",
      "              (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "              (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "            (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(blip2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0057, -0.0701, -0.1501,  ..., -0.4714, -0.8687,  0.2786],\n",
      "         [-0.0491, -0.0917,  0.1030,  ...,  0.3206, -0.1873, -0.9541],\n",
      "         [-0.0351,  1.1143, -0.3308,  ...,  0.0599, -1.0059, -0.4644],\n",
      "         ...,\n",
      "         [ 0.0833, -0.2878, -0.0047,  ...,  0.3586,  0.2903,  0.1124],\n",
      "         [ 0.3037,  0.7974, -0.8682,  ...,  0.4258, -0.2030, -1.2402],\n",
      "         [ 0.4199,  0.0694,  0.2830,  ...,  0.2053, -0.1896,  0.5386]],\n",
      "\n",
      "        [[ 0.1459, -0.0856, -0.0608,  ..., -0.3530, -0.8271,  0.3594],\n",
      "         [-0.0386,  0.8530, -0.0566,  ...,  0.0690, -0.4939, -0.1065],\n",
      "         [ 0.4946,  0.3811,  1.4014,  ..., -0.3660, -1.4658,  0.5591],\n",
      "         ...,\n",
      "         [ 0.5913,  0.9463, -0.3752,  ...,  0.6357, -0.1910, -1.0908],\n",
      "         [ 0.4905,  1.2158,  0.1620,  ...,  0.3718, -0.3167, -0.3457],\n",
      "         [ 0.4304,  0.8438, -0.1110,  ...,  0.0414, -0.8647,  0.0812]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.0778, -0.1635, -0.4446,  ..., -1.2344, -1.6064,  0.4961],\n",
      "        [ 0.2076, -0.1906, -0.2632,  ..., -1.0176, -1.5010,  0.6548]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "Teacher dimension: 1408\n",
      "Teacher total parameters: 985952256\n",
      "Blip2VisionConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_size\": 1408,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 1e-10,\n",
      "  \"intermediate_size\": 6144,\n",
      "  \"layer_norm_eps\": 1e-06,\n",
      "  \"model_type\": \"blip_2_vision_model\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 39,\n",
      "  \"patch_size\": 14,\n",
      "  \"projection_dim\": 512,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.3\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher = blip2_model.vision_model\n",
    "img = torch.randn((2,3,224,224),device='cuda')\n",
    "print(teacher(img))\n",
    "teacher.eval().requires_grad_(False)\n",
    "teacher_dim = teacher.config.hidden_size\n",
    "print(\"Teacher dimension:\", teacher_dim)\n",
    "print(f\"Teacher total parameters: {sum(p.numel() for p in teacher.parameters())}\")\n",
    "\n",
    "print(teacher.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMobileNet(nn.Module):\n",
    "    def __init__(self, teacher_dim):\n",
    "        super(StudentMobileNet, self).__init__()\n",
    "        self.mobilenet = mobilenet_v2(weights=None) \n",
    "        self.backbone = self.mobilenet.features       \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inp = torch.randn(1, 3, 224, 224)\n",
    "            out = self.backbone(inp)\n",
    "        \n",
    "        self.proj = nn.Conv2d(out.shape[1], teacher_dim, kernel_size=1, stride=1, padding=0)   #(B, 1408 , H/32, W/32) -> B, 1408, 7, 7\n",
    "\n",
    "        self.fc = nn.Linear(teacher_dim, teacher_dim)\n",
    "\n",
    "    def forward(self, x , size_hw = (16, 16)):\n",
    "        x = self.backbone(x)\n",
    "        x = self.proj(x)  # Project to teacher dimension\n",
    "        x = F.interpolate(x, size=size_hw, mode=\"bilinear\", align_corners=False) # B, teacher_dim, G*G\n",
    "        x = x.flatten(2).permute(0, 2, 1)  # B, G*G, teacher_dim\n",
    "\n",
    "        # CLS made from global mean of patch tokens (then a small head)\n",
    "        cls = self.fc(x.mean(dim=1, keepdim=True))  # [B, 1, D]\n",
    "        tokens = torch.cat([cls, x], dim=1)               # [B, 1+G*G, D]\n",
    "        return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_dir = \"../datasets/flickr8k/Images\"\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "        self.images_dict = self.read_images(self.image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_fname = list(self.images_dict.keys())[idx]\n",
    "        img = self.images_dict[image_fname]\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def read_images(self, image_dir):\n",
    "        images_dict = {}\n",
    "        for fname in os.listdir(image_dir):\n",
    "            if fname.lower().endswith('.jpg') or fname.lower().endswith('.png'):\n",
    "                image = Image.open(os.path.join(image_dir, fname)).convert('RGB')\n",
    "                images_dict[fname] = image\n",
    "        return images_dict\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(CLIPDataset(),16,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2_ = StudentMobileNet(teacher_dim)\n",
    "mobilenet_v2_ = mobilenet_v2_.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KdLoss(s, t, temp=2):\n",
    "    #t -> (B,Q,D)\n",
    "    #s -> (B,Q,D)\n",
    "    B,Q,D = t.shape\n",
    "\n",
    "    t = t.reshape(-1,D)  #B*Q,D\n",
    "    s = t.reshape(-1,D)  #B*Q,D\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pt = (t @ t.t()) / temp     # B,B\n",
    "        pt = F.softmax(pt, dim=1)   # B,B\n",
    "    ps = (s @ t.t()) / temp         # B,B\n",
    "    log_ps = F.log_softmax(ps, dim=1)       # B,B \n",
    "    loss = (-pt * log_ps).sum(dim=1).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7292392\n",
      "*************** epoch 1/3*************\n",
      "loss 0.8455662131309509\n",
      "loss 0.7561948299407959\n",
      "loss 0.7198495864868164\n",
      "loss 0.6873339414596558\n",
      "loss 0.6595894694328308\n",
      "loss 0.648616373538971\n",
      "loss 0.6387004852294922\n",
      "loss 0.6393941044807434\n",
      "loss 0.6564106345176697\n",
      "loss 0.6380351781845093\n",
      "loss 0.6525426506996155\n",
      "loss 0.6174314618110657\n",
      "loss 0.6357091069221497\n",
      "loss 0.6622722148895264\n",
      "loss 0.6336650848388672\n",
      "loss 0.6200361251831055\n",
      "loss 0.6191397905349731\n",
      "loss 0.6259984970092773\n",
      "loss 0.6121312975883484\n",
      "loss 0.6197715997695923\n",
      "loss 0.6041571497917175\n",
      "loss 0.6224290132522583\n",
      "loss 0.6127133369445801\n",
      "loss 0.6178023815155029\n",
      "loss 0.6422786712646484\n",
      "loss 0.6116185188293457\n",
      "loss 0.61407071352005\n",
      "loss 0.6205826997756958\n",
      "loss 0.6146852970123291\n",
      "loss 0.6058721542358398\n",
      "loss 0.6092562675476074\n",
      "loss 0.6226474642753601\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(mobilenet_v2_.parameters(),lr = 1e-5)\n",
    "\n",
    "print(sum(p.numel() for p in mobilenet_v2_.parameters()))\n",
    "epoch = 3\n",
    "\n",
    "teacher = teacher.to('cuda')\n",
    "teacher.eval()\n",
    "\n",
    "for e in range(epoch):\n",
    "    print(f\"*************** epoch {e+1}/{epoch}*************\")\n",
    "    for i,img in enumerate(train_dataloader):\n",
    "        mobilenet_v2_.train()\n",
    "\n",
    "        img = img.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            teacher_y = teacher(img).last_hidden_state.float()\n",
    "        student_y = mobilenet_v2_(img)\n",
    "\n",
    "        mse_loss = F.mse_loss(student_y,teacher_y)\n",
    "        # kd_loss = KdLoss(student_y, teacher_y) * 10\n",
    "\n",
    "        loss = mse_loss \n",
    "\n",
    "        if (i)%10==0:\n",
    "            print(f\"loss {loss.item()}\")\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "\n",
    "class StudentVisionAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for BLIP-2's vision_model.\n",
    "    Produces last_hidden_state [B, 1+G*G, D] and pooler_output [B, D].\n",
    "    \"\"\"\n",
    "    def __init__(self, student_model, teacher_dim=1408, image_size=224, patch_size=14):\n",
    "        super().__init__()\n",
    "        self.student = student_model\n",
    "        # Minimal config attributes that BLIP-2 code looks at\n",
    "        self.config = SimpleNamespace(\n",
    "            hidden_size=teacher_dim,\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, **kwargs):\n",
    "        # Derive token grid from inputs to stay in sync with the processor\n",
    "        H = pixel_values.shape[-2]\n",
    "        W = pixel_values.shape[-1]\n",
    "        assert H == W, \"Expected square images\"\n",
    "        G = H // self.config.patch_size\n",
    "\n",
    "        tok = self.student(pixel_values, size_hw=(G, G))  # [B, 1+G*G, D]\n",
    "        # Keep dtype consistent with inputs (handles fp16 runs)\n",
    "        tok = tok.to(dtype=torch.float16)\n",
    "\n",
    "        print(\"Student output dim:\",tok.dtype)\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=tok,          # [B, 1+G*G, D]\n",
    "            pooler_output=tok[:, 0, :],     # use CLS as pooled vector\n",
    "            hidden_states=None, \n",
    "            attentions=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_width = blip2_model.config.qformer_config.encoder_hidden_size  # e.g., 1408\n",
    "student_vision = StudentVisionAdapter(\n",
    "    student_model=mobilenet_v2_,\n",
    "    teacher_dim=vision_width,\n",
    "    image_size=blip2_processor.image_processor.size[\"shortest_edge\"] if \"shortest_edge\" in blip2_processor.image_processor.size else 224,\n",
    "    patch_size=14,  # BLIP-2 vision backbones use 14; keep it consistent with G=H//14\n",
    ")\n",
    "\n",
    "blip2_model.vision_model = student_vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_model.vision_model = teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student output dim: torch.float16\n",
      " a man in a blue shirt and black pants is standing in front of a white building\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"/home/ubuntu/VLM/datasets/flickr8k/Images/36422830_55c844bc2d.jpg\").convert(\"RGB\")\n",
    "prompt = \"Question: Describe the image Answer:\"\n",
    "\n",
    "# Preprocess input\n",
    "inputs = blip2_processor(images=image, text=prompt, return_tensors=\"pt\").to(blip2_model.device)\n",
    "\n",
    "# Generate output\n",
    "generated_ids = blip2_model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "generated_text = blip2_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "cifar_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    iter = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to('cuda')\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            print(f\"Iteration [{iter}/{len(data_loader)}]\", end=\"\\r\")\n",
    "            iter += 1\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def accuracy(all_preds, all_labels):\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    return (all_preds == all_labels).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
