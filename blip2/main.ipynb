{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d20a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import FlanT5Tokenizer, BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4b2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, dataset, tokenizer\n",
    "importlib.reload(config)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import FlanT5Tokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ccd714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "t5_model_name = \"google/flan-t5-small\"\n",
    "bert_autotokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "t5_autotokenizer = AutoTokenizer.from_pretrained(t5_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aff7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Blip2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6d6f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: image\n",
      "Skipping non-image file: image\n"
     ]
    }
   ],
   "source": [
    "flan_t5_tokenizer = FlanT5Tokenizer(config, t5_autotokenizer)\n",
    "bert_tokenizer = BertTokenizer(config, bert_autotokenizer)\n",
    "config.bert_vocab_size = bert_tokenizer.n_vocab\n",
    "config.t5_vocab_size = flan_t5_tokenizer.n_vocab\n",
    "\n",
    "stage1_train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=bert_tokenizer.tokenize_text, type=\"bert\")\n",
    "stage1_train_dataloader = DataLoader(stage1_train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "stage2_train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=flan_t5_tokenizer.tokenize_text, type=\"flan_t5\")\n",
    "stage2_train_dataloader = DataLoader(stage2_train_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40420224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import timm\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class BertMLPBlock(nn.Module):\n",
    "    def __init__(self, intermediate, output):\n",
    "        super().__init__()\n",
    "        self.intermediate = intermediate\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_output = self.intermediate(x)\n",
    "        return self.output(intermediate_output, x)\n",
    "    \n",
    "\n",
    "class BertEncoderBlock(nn.Module):\n",
    "    def __init__(self, bert_layer, bert_config, is_cross_attn=False):\n",
    "        super().__init__()\n",
    "        self.bert_config = bert_config\n",
    "        self.is_cross_attn = is_cross_attn\n",
    "        self.self_attn = bert_layer.attention\n",
    "        self.mlp_img_transformer = BertMLPBlock(bert_layer.intermediate, bert_layer.output)\n",
    "        self.mlp_text_transformer = BertMLPBlock(\n",
    "                    copy.deepcopy(bert_layer.intermediate), \n",
    "                    copy.deepcopy(bert_layer.output)\n",
    "                    )\n",
    "        if is_cross_attn:\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim=self.bert_config.hidden_size, \n",
    "                                                    num_heads=self.bert_config.num_attention_heads, \n",
    "                                                    batch_first=True)\n",
    "            self.cross_layer_norm = nn.LayerNorm(self.bert_config.hidden_size)\n",
    "        \n",
    "    def forward(self, query_embds, img_embds, text_embds, attn_mask):\n",
    "        _, Qs, _ = query_embds.shape\n",
    "        _, Ts, _ = text_embds.shape\n",
    "\n",
    "        combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
    "\n",
    "        self_attn_output = self.self_attn(combined_embds, attention_mask=attn_mask)[0]\n",
    "        query_embds = combined_embds[:, :Qs]\n",
    "        text_embds= combined_embds[:, Qs:]\n",
    "        \n",
    "        if self.is_cross_attn:\n",
    "            hidden_states = self.cross_attn(query_embds, img_embds, img_embds)[0]\n",
    "            query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
    "\n",
    "        query_embds = self.mlp_img_transformer(query_embds)\n",
    "        text_embds = self.mlp_text_transformer(text_embds)\n",
    "        return query_embds, text_embds\n",
    "\n",
    "\n",
    "class QTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert_cfg  = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\", config = self.bert_cfg)\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, bert_layer in enumerate(self.bert_model.encoder.layer):\n",
    "            self.encoder.append(BertEncoderBlock(bert_layer, self.bert_cfg, i % 2 == 0))\n",
    "        \n",
    "        qs = config.num_queries\n",
    "        ts = config.context_length\n",
    "        combined_seq_len = qs + ts\n",
    "\n",
    "        ####  STAGE 1: ITC, ITM, ITG Loss Masks ####\n",
    "        # ITC Loss Mask\n",
    "        itc_attn_mask = torch.zeros((combined_seq_len, combined_seq_len))\n",
    "        itc_attn_mask[:qs, :qs] = 1\n",
    "        itc_attn_mask[qs:, qs:] = 1\n",
    "\n",
    "        # ITM Loss Mask\n",
    "        itm_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "\n",
    "        # ITG Loss Mask\n",
    "        itg_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "        itg_attn_mask[:qs, qs:] = 0\n",
    "        itg_attn_mask[qs:, qs:] = torch.tril(itg_attn_mask[qs:, qs:], diagonal=0)\n",
    "\n",
    "        self.register_buffer(\"itc_attn_mask\", itc_attn_mask)\n",
    "        self.register_buffer(\"itm_attn_mask\", itm_attn_mask)\n",
    "        self.register_buffer(\"itg_attn_mask\", itg_attn_mask)\n",
    "\n",
    "        ####  STAGE 2: ####\n",
    "        # ITC Loss Mask will be same as stage 1 and reused for stage 2\n",
    "\n",
    "    def forward(self, query_embds, img_embds, cls_text_embds, dec_text_embds, stage):\n",
    "\n",
    "        itc_query_embds = query_embds.clone()\n",
    "        itm_query_embds = query_embds.clone()\n",
    "        itg_query_embds = query_embds.clone()\n",
    "\n",
    "        itc_text_embds = cls_text_embds.clone()\n",
    "        itm_text_embds = cls_text_embds.clone()\n",
    "        itg_text_embds = dec_text_embds.clone()\n",
    "\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            itc_query_embds, itc_text_embds = encoder(itc_query_embds, img_embds, itc_text_embds, self.itc_attn_mask)\n",
    "            if stage == 1:\n",
    "                itm_query_embds, itm_text_embds = encoder(itm_query_embds, img_embds, itm_text_embds, self.itm_attn_mask)\n",
    "                itg_query_embds, itg_text_embds = encoder(itg_query_embds, img_embds, itg_text_embds, self.itg_attn_mask)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds\n",
    "    \n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q_transformer = QTransformer(config)\n",
    "        self.learned_query = nn.Parameter(torch.randn(config.num_queries, config.embedding_dim))\n",
    "        self.output_embedding  = nn.Embedding(config.bert_vocab_size, config.embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(config.context_length, config.embedding_dim)\n",
    "\n",
    "        position_ids = torch.arange(self.config.context_length).unsqueeze(0)\n",
    "        self.register_buffer(\"position_ids\", position_ids)\n",
    "\n",
    "    def forward(self, image_embedding: torch.tensor, cls_tokens: torch.tensor, dec_tokens: torch.tensor, stage:int):\n",
    "        B, S, E = image_embedding.shape\n",
    "        learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        cls_text_embeddings = self.output_embedding(cls_tokens) #(S,768)\n",
    "        cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "        dec_text_embeddings = self.output_embedding(dec_tokens) #(S,768)\n",
    "        dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds = self.q_transformer(\n",
    "            learned_query, image_embedding, cls_text_embeddings, dec_text_embeddings, stage)\n",
    "\n",
    "        if itg_text_embds is not None:\n",
    "            itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
    "        else:\n",
    "            itg_logits = None\n",
    "\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "\n",
    "\n",
    "class FlanT5Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlanT5Model, self).__init__()\n",
    "        self.lm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "        for param in self.lm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, query_embedding, input_token, label, enc_mask):\n",
    "        #query_embd : (B,32,512)\n",
    "        # input_token : (B,L)\n",
    "        B, Q, d = query_embedding.shape\n",
    "        device = query_embedding.device\n",
    "        with torch.no_grad():\n",
    "            input_embd = self.lm_model.encoder.embed_tokens(input_token)  #(B,L,512)\n",
    "\n",
    "        encoder_input = torch.concat((query_embedding, input_embd) , dim = 1).contiguous()\n",
    "\n",
    "        prefix_mask = torch.ones((B, Q ), dtype= enc_mask.dtype, device=device)\n",
    "        attention_mask = torch.concat((prefix_mask, enc_mask) , dim=1).contiguous()  # [B, 32+L]\n",
    "        label = label.contiguous()  # [B, L]\n",
    "        out = self.lm_model(inputs_embeds=encoder_input,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=label,\n",
    "                                return_dict=True)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def predict(self, query_embedding, input_token, enc_mask):\n",
    "        B, Q, d = query_embedding.shape\n",
    "        device = query_embedding.device\n",
    "        with torch.no_grad():\n",
    "            input_embd = self.lm_model.encoder.embed_tokens(input_token)  #(B,L,512)\n",
    "\n",
    "        encoder_input = torch.concat((query_embedding, input_embd) , dim = 1)\n",
    "\n",
    "        prefix_mask = torch.ones((B, Q ), dtype= enc_mask.dtype, device=device)\n",
    "        attention_mask = torch.concat((prefix_mask, enc_mask) , dim=1)  # [B, 32+L]\n",
    "        \n",
    "        enc_out = self.lm_model.encoder(\n",
    "            inputs_embeds=encoder_input,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "            )\n",
    "\n",
    "        gen_ids = self.lm_model.generate(\n",
    "            encoder_outputs=enc_out,\n",
    "            max_new_tokens=30,\n",
    "            decoder_start_token_id=self.lm_model.config.decoder_start_token_id,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        return gen_ids\n",
    "\n",
    "\n",
    "class Blip2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.image_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "        self.image_encoder.reset_classifier(0)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.image_proj = nn.Linear(config.img_embd_dim, config.embedding_dim)\n",
    "\n",
    "        self.q_former = QFormer(config)\n",
    "        self.z_proj = nn.Linear(config.embedding_dim, config.lm_embedding_dim)\n",
    "\n",
    "        self.lm_model = FlanT5Model()\n",
    "    \n",
    "    \n",
    "    def stage1(self, image:torch.tensor, cls_caption:torch.tensor, dec_caption:torch.tensor):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, cls_caption, dec_caption, 1)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "    \n",
    "    \n",
    "    def stage2(self, image, input_token, label, enc_mask, dummy_input_size):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "        \n",
    "        cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
    "        dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, \n",
    "                                                            cls_caption_dummy, dec_caption_dummy, 2)\n",
    "        \n",
    "        z = self.z_proj(itc_query_embds)  # [B, Qs, D]\n",
    "\n",
    "        out = self.lm_model(z, input_token, label, enc_mask)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b708e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2Model(\n",
      "  (image_encoder): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (image_proj): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (q_former): QFormer(\n",
      "    (q_transformer): QTransformer(\n",
      "      (bert_model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (encoder): ModuleList(\n",
      "        (0): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_embedding): Embedding(30526, 768)\n",
      "    (position_embedding): Embedding(77, 768)\n",
      "  )\n",
      "  (z_proj): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (lm_model): FlanT5Model(\n",
      "    (lm_model): T5ForConditionalGeneration(\n",
      "      (shared): Embedding(32128, 512)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerCrossAttention(\n",
      "                (EncDecAttention): T5Attention(\n",
      "                  (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Blip2Model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6bffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query_embds, text_embds):\n",
    "        # query_embds: B, 32, d\n",
    "        # text_embds: B, 77, d\n",
    "        text_logit = text_embds[:, :1] # B, 1, d\n",
    "        B, _, _ = text_logit.shape\n",
    "        B, Qs, d = query_embds.shape \n",
    "        query_embds = query_embds.reshape(B * Qs, d)\n",
    "        text_embds = text_logit.squeeze()\n",
    "        logits = query_embds @ text_embds.T   # B*Qs,B\n",
    "        logits = torch.max(logits.reshape(B,Qs,B),dim=1)[0] # B,B\n",
    "        label = torch.arange(B,device=query_embds.device)\n",
    "        return (F.cross_entropy(logits,label)+ F.cross_entropy(logits.T,label)) / 2, logits\n",
    "\n",
    "\n",
    "class ITMLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d = config.embedding_dim\n",
    "        self.classification_layer = nn.Linear(d,2)\n",
    "    \n",
    "    def forward(self, query_embd, label):\n",
    "        # query_embd --> (B,32,768)\n",
    "        #label ->(B,1) B x [0/1]\n",
    "\n",
    "        match_logit = self.classification_layer(query_embd) #(B,32,2)\n",
    "        match_logit = match_logit.mean(dim=1)\n",
    "        return F.cross_entropy(match_logit,label)\n",
    "\n",
    "\n",
    "class ITGLoss(nn.Module):\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def forward(self, itg_logits, label_token):\n",
    "        #itg_logits -> B,S,vocab size\n",
    "        #label_token -> B,S\n",
    "        B, S, V = itg_logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            itg_logits.view(B * S, V),\n",
    "            label_token.view(B * S),\n",
    "            ignore_index=self.pad_token_id\n",
    "        )\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73734f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################   STAGE 1    #####################################\n",
    "\n",
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "num_epochs = 30\n",
    "\n",
    "itc_loss_func = ITCLoss()\n",
    "itm_loss_func = ITMLoss(config)\n",
    "itg_loss_func = ITGLoss(bert_tokenizer.pad_token_id)\n",
    "\n",
    "itc_loss_func = itc_loss_func.to(device)\n",
    "itm_loss_func = itm_loss_func.to(device)\n",
    "itg_loss_func = itg_loss_func.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iteration = 0\n",
    "    print(f\"***************   Epoch {epoch + 1}  ***************\")\n",
    "    for img, cls_caption, dec_caption in stage1_train_dataloader:\n",
    "        img = img.to(device)\n",
    "        cls_caption = cls_caption.to(device)\n",
    "        dec_caption = dec_caption.to(device)\n",
    "        B, _, _, _ = img.shape\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = model.stage1(img, cls_caption, dec_caption)\n",
    "\n",
    "        # ITC Loss\n",
    "        itc_loss, itc_logits = itc_loss_func(itc_query_embds, itc_text_embds)\n",
    "\n",
    "        # ITM Loss\n",
    "        idx = torch.arange(B,device = device)\n",
    "        itc_logits[idx,idx] = -1e9\n",
    "        next_best_text_value , next_best_text_idx = torch.max(itc_logits,dim=1)\n",
    "        mismatched_cls_caption = cls_caption[next_best_text_idx]\n",
    "        mismatched_dec_caption = dec_caption[next_best_text_idx]\n",
    "\n",
    "        _,_,mismatched_itm_query_embeds,_,_ = model.stage1(img, mismatched_cls_caption, mismatched_dec_caption)\n",
    "\n",
    "        itm_query_embed_concatenated = torch.concat((itm_query_embds, mismatched_itm_query_embeds) ,dim=0 )\n",
    "        itm_labels = torch.zeros(2 * B, dtype=torch.long).to(device)\n",
    "        itm_labels[B:] = 1\n",
    "        itm_loss = itm_loss_func(itm_query_embed_concatenated, itm_labels)\n",
    "\n",
    "        # ITG Loss\n",
    "        itg_labels = torch.concat((dec_caption[:, 1:], dec_caption[:, -1].unsqueeze(1)), dim=1)\n",
    "        itg_loss = itg_loss_func(itg_logits, itg_labels)\n",
    "\n",
    "\n",
    "        total_loss = itc_loss + itm_loss + itg_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} : Iter [{iteration} / {len(stage1_train_dataloader)}]\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\")\n",
    "        print(f\"ITC Loss: {itc_loss}, ITM Loss: {itm_loss}, ITG Loss: {itg_loss}\")\n",
    "        print(\"\" + \"*\" * 50)\n",
    "        iteration += 1\n",
    "\n",
    "torch.save(model.state_dict, \"q_former.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6353080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Epoch 1  ***************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Iter [0 / 506]\n",
      "Total Loss: 46.26622772216797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [1 / 506]\n",
      "Total Loss: 41.48877716064453\n",
      "**************************************************\n",
      "Epoch 1 : Iter [2 / 506]\n",
      "Total Loss: 46.40373229980469\n",
      "**************************************************\n",
      "Epoch 1 : Iter [3 / 506]\n",
      "Total Loss: 45.10622024536133\n",
      "**************************************************\n",
      "Epoch 1 : Iter [4 / 506]\n",
      "Total Loss: 43.381980895996094\n",
      "**************************************************\n",
      "Epoch 1 : Iter [5 / 506]\n",
      "Total Loss: 42.2867317199707\n",
      "**************************************************\n",
      "Epoch 1 : Iter [6 / 506]\n",
      "Total Loss: 40.3843994140625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [7 / 506]\n",
      "Total Loss: 40.10426330566406\n",
      "**************************************************\n",
      "Epoch 1 : Iter [8 / 506]\n",
      "Total Loss: 38.460594177246094\n",
      "**************************************************\n",
      "Epoch 1 : Iter [9 / 506]\n",
      "Total Loss: 39.225372314453125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [10 / 506]\n",
      "Total Loss: 39.71262741088867\n",
      "**************************************************\n",
      "Epoch 1 : Iter [11 / 506]\n",
      "Total Loss: 37.091365814208984\n",
      "**************************************************\n",
      "Epoch 1 : Iter [12 / 506]\n",
      "Total Loss: 34.090797424316406\n",
      "**************************************************\n",
      "Epoch 1 : Iter [13 / 506]\n",
      "Total Loss: 35.44383239746094\n",
      "**************************************************\n",
      "Epoch 1 : Iter [14 / 506]\n",
      "Total Loss: 34.5186767578125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [15 / 506]\n",
      "Total Loss: 35.382667541503906\n",
      "**************************************************\n",
      "Epoch 1 : Iter [16 / 506]\n",
      "Total Loss: 32.489933013916016\n",
      "**************************************************\n",
      "Epoch 1 : Iter [17 / 506]\n",
      "Total Loss: 32.32962417602539\n",
      "**************************************************\n",
      "Epoch 1 : Iter [18 / 506]\n",
      "Total Loss: 32.136314392089844\n",
      "**************************************************\n",
      "Epoch 1 : Iter [19 / 506]\n",
      "Total Loss: 31.96662139892578\n",
      "**************************************************\n",
      "Epoch 1 : Iter [20 / 506]\n",
      "Total Loss: 30.698379516601562\n",
      "**************************************************\n",
      "Epoch 1 : Iter [21 / 506]\n",
      "Total Loss: 30.050010681152344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [22 / 506]\n",
      "Total Loss: 30.868587493896484\n",
      "**************************************************\n",
      "Epoch 1 : Iter [23 / 506]\n",
      "Total Loss: 29.99742889404297\n",
      "**************************************************\n",
      "Epoch 1 : Iter [24 / 506]\n",
      "Total Loss: 28.901201248168945\n",
      "**************************************************\n",
      "Epoch 1 : Iter [25 / 506]\n",
      "Total Loss: 29.644651412963867\n",
      "**************************************************\n",
      "Epoch 1 : Iter [26 / 506]\n",
      "Total Loss: 29.255250930786133\n",
      "**************************************************\n",
      "Epoch 1 : Iter [27 / 506]\n",
      "Total Loss: 28.190935134887695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [28 / 506]\n",
      "Total Loss: 28.68630599975586\n",
      "**************************************************\n",
      "Epoch 1 : Iter [29 / 506]\n",
      "Total Loss: 28.352067947387695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [30 / 506]\n",
      "Total Loss: 28.129209518432617\n",
      "**************************************************\n",
      "Epoch 1 : Iter [31 / 506]\n",
      "Total Loss: 27.087146759033203\n",
      "**************************************************\n",
      "Epoch 1 : Iter [32 / 506]\n",
      "Total Loss: 27.62272071838379\n",
      "**************************************************\n",
      "Epoch 1 : Iter [33 / 506]\n",
      "Total Loss: 28.21671485900879\n",
      "**************************************************\n",
      "Epoch 1 : Iter [34 / 506]\n",
      "Total Loss: 27.850858688354492\n",
      "**************************************************\n",
      "Epoch 1 : Iter [35 / 506]\n",
      "Total Loss: 26.813631057739258\n",
      "**************************************************\n",
      "Epoch 1 : Iter [36 / 506]\n",
      "Total Loss: 26.939292907714844\n",
      "**************************************************\n",
      "Epoch 1 : Iter [37 / 506]\n",
      "Total Loss: 25.511356353759766\n",
      "**************************************************\n",
      "Epoch 1 : Iter [38 / 506]\n",
      "Total Loss: 25.66862678527832\n",
      "**************************************************\n",
      "Epoch 1 : Iter [39 / 506]\n",
      "Total Loss: 25.624202728271484\n",
      "**************************************************\n",
      "Epoch 1 : Iter [40 / 506]\n",
      "Total Loss: 25.346698760986328\n",
      "**************************************************\n",
      "Epoch 1 : Iter [41 / 506]\n",
      "Total Loss: 26.449689865112305\n",
      "**************************************************\n",
      "Epoch 1 : Iter [42 / 506]\n",
      "Total Loss: 25.538522720336914\n",
      "**************************************************\n",
      "Epoch 1 : Iter [43 / 506]\n",
      "Total Loss: 24.197948455810547\n",
      "**************************************************\n",
      "Epoch 1 : Iter [44 / 506]\n",
      "Total Loss: 24.027570724487305\n",
      "**************************************************\n",
      "Epoch 1 : Iter [45 / 506]\n",
      "Total Loss: 24.36355209350586\n",
      "**************************************************\n",
      "Epoch 1 : Iter [46 / 506]\n",
      "Total Loss: 23.820514678955078\n",
      "**************************************************\n",
      "Epoch 1 : Iter [47 / 506]\n",
      "Total Loss: 24.149385452270508\n",
      "**************************************************\n",
      "Epoch 1 : Iter [48 / 506]\n",
      "Total Loss: 23.61341667175293\n",
      "**************************************************\n",
      "Epoch 1 : Iter [49 / 506]\n",
      "Total Loss: 24.18754768371582\n",
      "**************************************************\n",
      "Epoch 1 : Iter [50 / 506]\n",
      "Total Loss: 25.23114013671875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [51 / 506]\n",
      "Total Loss: 24.13907241821289\n",
      "**************************************************\n",
      "Epoch 1 : Iter [52 / 506]\n",
      "Total Loss: 23.3617000579834\n",
      "**************************************************\n",
      "Epoch 1 : Iter [53 / 506]\n",
      "Total Loss: 23.318490982055664\n",
      "**************************************************\n",
      "Epoch 1 : Iter [54 / 506]\n",
      "Total Loss: 22.32904052734375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [55 / 506]\n",
      "Total Loss: 22.994823455810547\n",
      "**************************************************\n",
      "Epoch 1 : Iter [56 / 506]\n",
      "Total Loss: 23.331554412841797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [57 / 506]\n",
      "Total Loss: 23.246681213378906\n",
      "**************************************************\n",
      "Epoch 1 : Iter [58 / 506]\n",
      "Total Loss: 22.50607681274414\n",
      "**************************************************\n",
      "Epoch 1 : Iter [59 / 506]\n",
      "Total Loss: 23.03190040588379\n",
      "**************************************************\n",
      "Epoch 1 : Iter [60 / 506]\n",
      "Total Loss: 23.738731384277344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [61 / 506]\n",
      "Total Loss: 26.21001434326172\n",
      "**************************************************\n",
      "Epoch 1 : Iter [62 / 506]\n",
      "Total Loss: 23.560218811035156\n",
      "**************************************************\n",
      "Epoch 1 : Iter [63 / 506]\n",
      "Total Loss: 22.17875099182129\n",
      "**************************************************\n",
      "Epoch 1 : Iter [64 / 506]\n",
      "Total Loss: 24.460954666137695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [65 / 506]\n",
      "Total Loss: 23.031496047973633\n",
      "**************************************************\n",
      "Epoch 1 : Iter [66 / 506]\n",
      "Total Loss: 22.885488510131836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [67 / 506]\n",
      "Total Loss: 22.670948028564453\n",
      "**************************************************\n",
      "Epoch 1 : Iter [68 / 506]\n",
      "Total Loss: 22.501911163330078\n",
      "**************************************************\n",
      "Epoch 1 : Iter [69 / 506]\n",
      "Total Loss: 22.784809112548828\n",
      "**************************************************\n",
      "Epoch 1 : Iter [70 / 506]\n",
      "Total Loss: 21.917123794555664\n",
      "**************************************************\n",
      "Epoch 1 : Iter [71 / 506]\n",
      "Total Loss: 22.41939353942871\n",
      "**************************************************\n",
      "Epoch 1 : Iter [72 / 506]\n",
      "Total Loss: 21.632131576538086\n",
      "**************************************************\n",
      "Epoch 1 : Iter [73 / 506]\n",
      "Total Loss: 21.92937660217285\n",
      "**************************************************\n",
      "Epoch 1 : Iter [74 / 506]\n",
      "Total Loss: 20.974876403808594\n",
      "**************************************************\n",
      "Epoch 1 : Iter [75 / 506]\n",
      "Total Loss: 21.48881721496582\n",
      "**************************************************\n",
      "Epoch 1 : Iter [76 / 506]\n",
      "Total Loss: 20.439464569091797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [77 / 506]\n",
      "Total Loss: 21.256507873535156\n",
      "**************************************************\n",
      "Epoch 1 : Iter [78 / 506]\n",
      "Total Loss: 20.93060874938965\n",
      "**************************************************\n",
      "Epoch 1 : Iter [79 / 506]\n",
      "Total Loss: 19.652231216430664\n",
      "**************************************************\n",
      "Epoch 1 : Iter [80 / 506]\n",
      "Total Loss: 20.33988380432129\n",
      "**************************************************\n",
      "Epoch 1 : Iter [81 / 506]\n",
      "Total Loss: 20.656291961669922\n",
      "**************************************************\n",
      "Epoch 1 : Iter [82 / 506]\n",
      "Total Loss: 19.449844360351562\n",
      "**************************************************\n",
      "Epoch 1 : Iter [83 / 506]\n",
      "Total Loss: 19.590547561645508\n",
      "**************************************************\n",
      "Epoch 1 : Iter [84 / 506]\n",
      "Total Loss: 21.945463180541992\n",
      "**************************************************\n",
      "Epoch 1 : Iter [85 / 506]\n",
      "Total Loss: 19.03792953491211\n",
      "**************************************************\n",
      "Epoch 1 : Iter [86 / 506]\n",
      "Total Loss: 19.815832138061523\n",
      "**************************************************\n",
      "Epoch 1 : Iter [87 / 506]\n",
      "Total Loss: 20.510482788085938\n",
      "**************************************************\n",
      "Epoch 1 : Iter [88 / 506]\n",
      "Total Loss: 21.548805236816406\n",
      "**************************************************\n",
      "Epoch 1 : Iter [89 / 506]\n",
      "Total Loss: 20.319110870361328\n",
      "**************************************************\n",
      "Epoch 1 : Iter [90 / 506]\n",
      "Total Loss: 21.0755558013916\n",
      "**************************************************\n",
      "Epoch 1 : Iter [91 / 506]\n",
      "Total Loss: 20.8521785736084\n",
      "**************************************************\n",
      "Epoch 1 : Iter [92 / 506]\n",
      "Total Loss: 20.206104278564453\n",
      "**************************************************\n",
      "Epoch 1 : Iter [93 / 506]\n",
      "Total Loss: 19.53752899169922\n",
      "**************************************************\n",
      "Epoch 1 : Iter [94 / 506]\n",
      "Total Loss: 19.768857955932617\n",
      "**************************************************\n",
      "Epoch 1 : Iter [95 / 506]\n",
      "Total Loss: 19.300796508789062\n",
      "**************************************************\n",
      "Epoch 1 : Iter [96 / 506]\n",
      "Total Loss: 19.03252601623535\n",
      "**************************************************\n",
      "Epoch 1 : Iter [97 / 506]\n",
      "Total Loss: 17.39176368713379\n",
      "**************************************************\n",
      "Epoch 1 : Iter [98 / 506]\n",
      "Total Loss: 16.701377868652344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [99 / 506]\n",
      "Total Loss: 17.69024658203125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [100 / 506]\n",
      "Total Loss: 15.722234725952148\n",
      "**************************************************\n",
      "Epoch 1 : Iter [101 / 506]\n",
      "Total Loss: 18.0615291595459\n",
      "**************************************************\n",
      "Epoch 1 : Iter [102 / 506]\n",
      "Total Loss: 15.117530822753906\n",
      "**************************************************\n",
      "Epoch 1 : Iter [103 / 506]\n",
      "Total Loss: 15.447744369506836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [104 / 506]\n",
      "Total Loss: 17.429826736450195\n",
      "**************************************************\n",
      "Epoch 1 : Iter [105 / 506]\n",
      "Total Loss: 15.08251667022705\n",
      "**************************************************\n",
      "Epoch 1 : Iter [106 / 506]\n",
      "Total Loss: 13.78787612915039\n",
      "**************************************************\n",
      "Epoch 1 : Iter [107 / 506]\n",
      "Total Loss: 12.38198471069336\n",
      "**************************************************\n",
      "Epoch 1 : Iter [108 / 506]\n",
      "Total Loss: 12.419564247131348\n",
      "**************************************************\n",
      "Epoch 1 : Iter [109 / 506]\n",
      "Total Loss: 12.964681625366211\n",
      "**************************************************\n",
      "Epoch 1 : Iter [110 / 506]\n",
      "Total Loss: 10.73391056060791\n",
      "**************************************************\n",
      "Epoch 1 : Iter [111 / 506]\n",
      "Total Loss: 9.110527992248535\n",
      "**************************************************\n",
      "Epoch 1 : Iter [112 / 506]\n",
      "Total Loss: 8.730682373046875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [113 / 506]\n",
      "Total Loss: 11.50241756439209\n",
      "**************************************************\n",
      "Epoch 1 : Iter [114 / 506]\n",
      "Total Loss: 7.962955951690674\n",
      "**************************************************\n",
      "Epoch 1 : Iter [115 / 506]\n",
      "Total Loss: 8.333970069885254\n",
      "**************************************************\n",
      "Epoch 1 : Iter [116 / 506]\n",
      "Total Loss: 10.987112998962402\n",
      "**************************************************\n",
      "Epoch 1 : Iter [117 / 506]\n",
      "Total Loss: 9.153390884399414\n",
      "**************************************************\n",
      "Epoch 1 : Iter [118 / 506]\n",
      "Total Loss: 9.629639625549316\n",
      "**************************************************\n",
      "Epoch 1 : Iter [119 / 506]\n",
      "Total Loss: 9.415263175964355\n",
      "**************************************************\n",
      "Epoch 1 : Iter [120 / 506]\n",
      "Total Loss: 11.071704864501953\n",
      "**************************************************\n",
      "Epoch 1 : Iter [121 / 506]\n",
      "Total Loss: 7.844751834869385\n",
      "**************************************************\n",
      "Epoch 1 : Iter [122 / 506]\n",
      "Total Loss: 9.627713203430176\n",
      "**************************************************\n",
      "Epoch 1 : Iter [123 / 506]\n",
      "Total Loss: 9.562760353088379\n",
      "**************************************************\n",
      "Epoch 1 : Iter [124 / 506]\n",
      "Total Loss: 8.853849411010742\n",
      "**************************************************\n",
      "Epoch 1 : Iter [125 / 506]\n",
      "Total Loss: 10.891039848327637\n",
      "**************************************************\n",
      "Epoch 1 : Iter [126 / 506]\n",
      "Total Loss: 7.926799297332764\n",
      "**************************************************\n",
      "Epoch 1 : Iter [127 / 506]\n",
      "Total Loss: 7.922842502593994\n",
      "**************************************************\n",
      "Epoch 1 : Iter [128 / 506]\n",
      "Total Loss: 7.5722174644470215\n",
      "**************************************************\n",
      "Epoch 1 : Iter [129 / 506]\n",
      "Total Loss: 7.621377944946289\n",
      "**************************************************\n",
      "Epoch 1 : Iter [130 / 506]\n",
      "Total Loss: 8.909399032592773\n",
      "**************************************************\n",
      "Epoch 1 : Iter [131 / 506]\n",
      "Total Loss: 7.740788459777832\n",
      "**************************************************\n",
      "Epoch 1 : Iter [132 / 506]\n",
      "Total Loss: 8.147170066833496\n",
      "**************************************************\n",
      "Epoch 1 : Iter [133 / 506]\n",
      "Total Loss: 7.491255760192871\n",
      "**************************************************\n",
      "Epoch 1 : Iter [134 / 506]\n",
      "Total Loss: 7.332973003387451\n",
      "**************************************************\n",
      "Epoch 1 : Iter [135 / 506]\n",
      "Total Loss: 7.618260860443115\n",
      "**************************************************\n",
      "Epoch 1 : Iter [136 / 506]\n",
      "Total Loss: 7.068141937255859\n",
      "**************************************************\n",
      "Epoch 1 : Iter [137 / 506]\n",
      "Total Loss: 8.355582237243652\n",
      "**************************************************\n",
      "Epoch 1 : Iter [138 / 506]\n",
      "Total Loss: 8.083645820617676\n",
      "**************************************************\n",
      "Epoch 1 : Iter [139 / 506]\n",
      "Total Loss: 8.070402145385742\n",
      "**************************************************\n",
      "Epoch 1 : Iter [140 / 506]\n",
      "Total Loss: 7.206203460693359\n",
      "**************************************************\n",
      "Epoch 1 : Iter [141 / 506]\n",
      "Total Loss: 7.64349365234375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [142 / 506]\n",
      "Total Loss: 7.862764358520508\n",
      "**************************************************\n",
      "Epoch 1 : Iter [143 / 506]\n",
      "Total Loss: 7.4667253494262695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [144 / 506]\n",
      "Total Loss: 7.259673595428467\n",
      "**************************************************\n",
      "Epoch 1 : Iter [145 / 506]\n",
      "Total Loss: 6.8171868324279785\n",
      "**************************************************\n",
      "Epoch 1 : Iter [146 / 506]\n",
      "Total Loss: 7.557045936584473\n",
      "**************************************************\n",
      "Epoch 1 : Iter [147 / 506]\n",
      "Total Loss: 6.850377559661865\n",
      "**************************************************\n",
      "Epoch 1 : Iter [148 / 506]\n",
      "Total Loss: 6.857377052307129\n",
      "**************************************************\n",
      "Epoch 1 : Iter [149 / 506]\n",
      "Total Loss: 6.707551956176758\n",
      "**************************************************\n",
      "Epoch 1 : Iter [150 / 506]\n",
      "Total Loss: 6.6703314781188965\n",
      "**************************************************\n",
      "Epoch 1 : Iter [151 / 506]\n",
      "Total Loss: 7.040557384490967\n",
      "**************************************************\n",
      "Epoch 1 : Iter [152 / 506]\n",
      "Total Loss: 7.1774420738220215\n",
      "**************************************************\n",
      "Epoch 1 : Iter [153 / 506]\n",
      "Total Loss: 6.672938346862793\n",
      "**************************************************\n",
      "Epoch 1 : Iter [154 / 506]\n",
      "Total Loss: 6.6819586753845215\n",
      "**************************************************\n",
      "Epoch 1 : Iter [155 / 506]\n",
      "Total Loss: 6.879727363586426\n",
      "**************************************************\n",
      "Epoch 1 : Iter [156 / 506]\n",
      "Total Loss: 6.824173450469971\n",
      "**************************************************\n",
      "Epoch 1 : Iter [157 / 506]\n",
      "Total Loss: 6.844869613647461\n",
      "**************************************************\n",
      "Epoch 1 : Iter [158 / 506]\n",
      "Total Loss: 6.9118733406066895\n",
      "**************************************************\n",
      "Epoch 1 : Iter [159 / 506]\n",
      "Total Loss: 6.534462928771973\n",
      "**************************************************\n",
      "Epoch 1 : Iter [160 / 506]\n",
      "Total Loss: 6.358997821807861\n",
      "**************************************************\n",
      "Epoch 1 : Iter [161 / 506]\n",
      "Total Loss: 6.732999324798584\n",
      "**************************************************\n",
      "Epoch 1 : Iter [162 / 506]\n",
      "Total Loss: 6.431430339813232\n",
      "**************************************************\n",
      "Epoch 1 : Iter [163 / 506]\n",
      "Total Loss: 6.53535270690918\n",
      "**************************************************\n",
      "Epoch 1 : Iter [164 / 506]\n",
      "Total Loss: 6.514644145965576\n",
      "**************************************************\n",
      "Epoch 1 : Iter [165 / 506]\n",
      "Total Loss: 6.909306526184082\n",
      "**************************************************\n",
      "Epoch 1 : Iter [166 / 506]\n",
      "Total Loss: 6.4933552742004395\n",
      "**************************************************\n",
      "Epoch 1 : Iter [167 / 506]\n",
      "Total Loss: 6.371494770050049\n",
      "**************************************************\n",
      "Epoch 1 : Iter [168 / 506]\n",
      "Total Loss: 6.532333850860596\n",
      "**************************************************\n",
      "Epoch 1 : Iter [169 / 506]\n",
      "Total Loss: 6.760372161865234\n",
      "**************************************************\n",
      "Epoch 1 : Iter [170 / 506]\n",
      "Total Loss: 6.865698337554932\n",
      "**************************************************\n",
      "Epoch 1 : Iter [171 / 506]\n",
      "Total Loss: 6.551207065582275\n",
      "**************************************************\n",
      "Epoch 1 : Iter [172 / 506]\n",
      "Total Loss: 7.403672695159912\n",
      "**************************************************\n",
      "Epoch 1 : Iter [173 / 506]\n",
      "Total Loss: 6.705375671386719\n",
      "**************************************************\n",
      "Epoch 1 : Iter [174 / 506]\n",
      "Total Loss: 7.23748779296875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [175 / 506]\n",
      "Total Loss: 6.762948989868164\n",
      "**************************************************\n",
      "Epoch 1 : Iter [176 / 506]\n",
      "Total Loss: 6.386903285980225\n",
      "**************************************************\n",
      "Epoch 1 : Iter [177 / 506]\n",
      "Total Loss: 6.43156623840332\n",
      "**************************************************\n",
      "Epoch 1 : Iter [178 / 506]\n",
      "Total Loss: 6.453223705291748\n",
      "**************************************************\n",
      "Epoch 1 : Iter [179 / 506]\n",
      "Total Loss: 6.88267183303833\n",
      "**************************************************\n",
      "Epoch 1 : Iter [180 / 506]\n",
      "Total Loss: 6.614814758300781\n",
      "**************************************************\n",
      "Epoch 1 : Iter [181 / 506]\n",
      "Total Loss: 6.666428089141846\n",
      "**************************************************\n",
      "Epoch 1 : Iter [182 / 506]\n",
      "Total Loss: 6.696819305419922\n",
      "**************************************************\n",
      "Epoch 1 : Iter [183 / 506]\n",
      "Total Loss: 6.989357948303223\n",
      "**************************************************\n",
      "Epoch 1 : Iter [184 / 506]\n",
      "Total Loss: 6.873136520385742\n",
      "**************************************************\n",
      "Epoch 1 : Iter [185 / 506]\n",
      "Total Loss: 6.503611087799072\n",
      "**************************************************\n",
      "Epoch 1 : Iter [186 / 506]\n",
      "Total Loss: 6.186511993408203\n",
      "**************************************************\n",
      "Epoch 1 : Iter [187 / 506]\n",
      "Total Loss: 6.53923225402832\n",
      "**************************************************\n",
      "Epoch 1 : Iter [188 / 506]\n",
      "Total Loss: 6.685062408447266\n",
      "**************************************************\n",
      "Epoch 1 : Iter [189 / 506]\n",
      "Total Loss: 6.564493179321289\n",
      "**************************************************\n",
      "Epoch 1 : Iter [190 / 506]\n",
      "Total Loss: 6.554439544677734\n",
      "**************************************************\n",
      "Epoch 1 : Iter [191 / 506]\n",
      "Total Loss: 6.455589771270752\n",
      "**************************************************\n",
      "Epoch 1 : Iter [192 / 506]\n",
      "Total Loss: 6.484609603881836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [193 / 506]\n",
      "Total Loss: 6.933790683746338\n",
      "**************************************************\n",
      "Epoch 1 : Iter [194 / 506]\n",
      "Total Loss: 6.5542521476745605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [195 / 506]\n",
      "Total Loss: 6.425204277038574\n",
      "**************************************************\n",
      "Epoch 1 : Iter [196 / 506]\n",
      "Total Loss: 6.77174186706543\n",
      "**************************************************\n",
      "Epoch 1 : Iter [197 / 506]\n",
      "Total Loss: 6.139797210693359\n",
      "**************************************************\n",
      "Epoch 1 : Iter [198 / 506]\n",
      "Total Loss: 6.375500679016113\n",
      "**************************************************\n",
      "Epoch 1 : Iter [199 / 506]\n",
      "Total Loss: 6.350210189819336\n",
      "**************************************************\n",
      "Epoch 1 : Iter [200 / 506]\n",
      "Total Loss: 6.246234893798828\n",
      "**************************************************\n",
      "Epoch 1 : Iter [201 / 506]\n",
      "Total Loss: 6.356319904327393\n",
      "**************************************************\n",
      "Epoch 1 : Iter [202 / 506]\n",
      "Total Loss: 6.580600738525391\n",
      "**************************************************\n",
      "Epoch 1 : Iter [203 / 506]\n",
      "Total Loss: 6.238805294036865\n",
      "**************************************************\n",
      "Epoch 1 : Iter [204 / 506]\n",
      "Total Loss: 6.542641639709473\n",
      "**************************************************\n",
      "Epoch 1 : Iter [205 / 506]\n",
      "Total Loss: 6.281383037567139\n",
      "**************************************************\n",
      "Epoch 1 : Iter [206 / 506]\n",
      "Total Loss: 6.160243511199951\n",
      "**************************************************\n",
      "Epoch 1 : Iter [207 / 506]\n",
      "Total Loss: 6.320431709289551\n",
      "**************************************************\n",
      "Epoch 1 : Iter [208 / 506]\n",
      "Total Loss: 6.304795265197754\n",
      "**************************************************\n",
      "Epoch 1 : Iter [209 / 506]\n",
      "Total Loss: 6.53594970703125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [210 / 506]\n",
      "Total Loss: 6.263609886169434\n",
      "**************************************************\n",
      "Epoch 1 : Iter [211 / 506]\n",
      "Total Loss: 6.468522548675537\n",
      "**************************************************\n",
      "Epoch 1 : Iter [212 / 506]\n",
      "Total Loss: 6.493293285369873\n",
      "**************************************************\n",
      "Epoch 1 : Iter [213 / 506]\n",
      "Total Loss: 6.017533302307129\n",
      "**************************************************\n",
      "Epoch 1 : Iter [214 / 506]\n",
      "Total Loss: 6.35997200012207\n",
      "**************************************************\n",
      "Epoch 1 : Iter [215 / 506]\n",
      "Total Loss: 6.077674865722656\n",
      "**************************************************\n",
      "Epoch 1 : Iter [216 / 506]\n",
      "Total Loss: 6.263072967529297\n",
      "**************************************************\n",
      "Epoch 1 : Iter [217 / 506]\n",
      "Total Loss: 6.1935553550720215\n",
      "**************************************************\n",
      "Epoch 1 : Iter [218 / 506]\n",
      "Total Loss: 6.323546886444092\n",
      "**************************************************\n",
      "Epoch 1 : Iter [219 / 506]\n",
      "Total Loss: 6.141264915466309\n",
      "**************************************************\n",
      "Epoch 1 : Iter [220 / 506]\n",
      "Total Loss: 6.1913251876831055\n",
      "**************************************************\n",
      "Epoch 1 : Iter [221 / 506]\n",
      "Total Loss: 6.067502021789551\n",
      "**************************************************\n",
      "Epoch 1 : Iter [222 / 506]\n",
      "Total Loss: 6.352078914642334\n",
      "**************************************************\n",
      "Epoch 1 : Iter [223 / 506]\n",
      "Total Loss: 6.386633396148682\n",
      "**************************************************\n",
      "Epoch 1 : Iter [224 / 506]\n",
      "Total Loss: 6.389073848724365\n",
      "**************************************************\n",
      "Epoch 1 : Iter [225 / 506]\n",
      "Total Loss: 6.171487331390381\n",
      "**************************************************\n",
      "Epoch 1 : Iter [226 / 506]\n",
      "Total Loss: 6.548844814300537\n",
      "**************************************************\n",
      "Epoch 1 : Iter [227 / 506]\n",
      "Total Loss: 6.58684778213501\n",
      "**************************************************\n",
      "Epoch 1 : Iter [228 / 506]\n",
      "Total Loss: 6.334689140319824\n",
      "**************************************************\n",
      "Epoch 1 : Iter [229 / 506]\n",
      "Total Loss: 6.679480075836182\n",
      "**************************************************\n",
      "Epoch 1 : Iter [230 / 506]\n",
      "Total Loss: 6.081423759460449\n",
      "**************************************************\n",
      "Epoch 1 : Iter [231 / 506]\n",
      "Total Loss: 6.295772075653076\n",
      "**************************************************\n",
      "Epoch 1 : Iter [232 / 506]\n",
      "Total Loss: 6.204702377319336\n",
      "**************************************************\n",
      "Epoch 1 : Iter [233 / 506]\n",
      "Total Loss: 6.5636186599731445\n",
      "**************************************************\n",
      "Epoch 1 : Iter [234 / 506]\n",
      "Total Loss: 5.9651055335998535\n",
      "**************************************************\n",
      "Epoch 1 : Iter [235 / 506]\n",
      "Total Loss: 6.2377214431762695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [236 / 506]\n",
      "Total Loss: 6.444095611572266\n",
      "**************************************************\n",
      "Epoch 1 : Iter [237 / 506]\n",
      "Total Loss: 6.209129810333252\n",
      "**************************************************\n",
      "Epoch 1 : Iter [238 / 506]\n",
      "Total Loss: 6.0554518699646\n",
      "**************************************************\n",
      "Epoch 1 : Iter [239 / 506]\n",
      "Total Loss: 6.5761494636535645\n",
      "**************************************************\n",
      "Epoch 1 : Iter [240 / 506]\n",
      "Total Loss: 6.4587554931640625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [241 / 506]\n",
      "Total Loss: 6.547833442687988\n",
      "**************************************************\n",
      "Epoch 1 : Iter [242 / 506]\n",
      "Total Loss: 6.333350658416748\n",
      "**************************************************\n",
      "Epoch 1 : Iter [243 / 506]\n",
      "Total Loss: 6.436058521270752\n",
      "**************************************************\n",
      "Epoch 1 : Iter [244 / 506]\n",
      "Total Loss: 6.4028191566467285\n",
      "**************************************************\n",
      "Epoch 1 : Iter [245 / 506]\n",
      "Total Loss: 6.9631242752075195\n",
      "**************************************************\n",
      "Epoch 1 : Iter [246 / 506]\n",
      "Total Loss: 6.1874895095825195\n",
      "**************************************************\n",
      "Epoch 1 : Iter [247 / 506]\n",
      "Total Loss: 6.316196918487549\n",
      "**************************************************\n",
      "Epoch 1 : Iter [248 / 506]\n",
      "Total Loss: 6.287968158721924\n",
      "**************************************************\n",
      "Epoch 1 : Iter [249 / 506]\n",
      "Total Loss: 6.5868144035339355\n",
      "**************************************************\n",
      "Epoch 1 : Iter [250 / 506]\n",
      "Total Loss: 6.410478115081787\n",
      "**************************************************\n",
      "Epoch 1 : Iter [251 / 506]\n",
      "Total Loss: 6.275516033172607\n",
      "**************************************************\n",
      "Epoch 1 : Iter [252 / 506]\n",
      "Total Loss: 6.29713249206543\n",
      "**************************************************\n",
      "Epoch 1 : Iter [253 / 506]\n",
      "Total Loss: 6.422091007232666\n",
      "**************************************************\n",
      "Epoch 1 : Iter [254 / 506]\n",
      "Total Loss: 6.051704406738281\n",
      "**************************************************\n",
      "Epoch 1 : Iter [255 / 506]\n",
      "Total Loss: 6.767396926879883\n",
      "**************************************************\n",
      "Epoch 1 : Iter [256 / 506]\n",
      "Total Loss: 6.154273509979248\n",
      "**************************************************\n",
      "Epoch 1 : Iter [257 / 506]\n",
      "Total Loss: 6.222146987915039\n",
      "**************************************************\n",
      "Epoch 1 : Iter [258 / 506]\n",
      "Total Loss: 6.201030731201172\n",
      "**************************************************\n",
      "Epoch 1 : Iter [259 / 506]\n",
      "Total Loss: 6.048739910125732\n",
      "**************************************************\n",
      "Epoch 1 : Iter [260 / 506]\n",
      "Total Loss: 6.247663497924805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [261 / 506]\n",
      "Total Loss: 7.322278022766113\n",
      "**************************************************\n",
      "Epoch 1 : Iter [262 / 506]\n",
      "Total Loss: 6.454108238220215\n",
      "**************************************************\n",
      "Epoch 1 : Iter [263 / 506]\n",
      "Total Loss: 6.1150431632995605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [264 / 506]\n",
      "Total Loss: 6.050311088562012\n",
      "**************************************************\n",
      "Epoch 1 : Iter [265 / 506]\n",
      "Total Loss: 7.0672478675842285\n",
      "**************************************************\n",
      "Epoch 1 : Iter [266 / 506]\n",
      "Total Loss: 6.211812496185303\n",
      "**************************************************\n",
      "Epoch 1 : Iter [267 / 506]\n",
      "Total Loss: 6.249853610992432\n",
      "**************************************************\n",
      "Epoch 1 : Iter [268 / 506]\n",
      "Total Loss: 6.407768726348877\n",
      "**************************************************\n",
      "Epoch 1 : Iter [269 / 506]\n",
      "Total Loss: 6.677420616149902\n",
      "**************************************************\n",
      "Epoch 1 : Iter [270 / 506]\n",
      "Total Loss: 6.431473255157471\n",
      "**************************************************\n",
      "Epoch 1 : Iter [271 / 506]\n",
      "Total Loss: 6.400393486022949\n",
      "**************************************************\n",
      "Epoch 1 : Iter [272 / 506]\n",
      "Total Loss: 6.454958438873291\n",
      "**************************************************\n",
      "Epoch 1 : Iter [273 / 506]\n",
      "Total Loss: 6.844930171966553\n",
      "**************************************************\n",
      "Epoch 1 : Iter [274 / 506]\n",
      "Total Loss: 6.1611762046813965\n",
      "**************************************************\n",
      "Epoch 1 : Iter [275 / 506]\n",
      "Total Loss: 6.612881660461426\n",
      "**************************************************\n",
      "Epoch 1 : Iter [276 / 506]\n",
      "Total Loss: 6.502218246459961\n",
      "**************************************************\n",
      "Epoch 1 : Iter [277 / 506]\n",
      "Total Loss: 6.465249061584473\n",
      "**************************************************\n",
      "Epoch 1 : Iter [278 / 506]\n",
      "Total Loss: 6.247854709625244\n",
      "**************************************************\n",
      "Epoch 1 : Iter [279 / 506]\n",
      "Total Loss: 6.240297794342041\n",
      "**************************************************\n",
      "Epoch 1 : Iter [280 / 506]\n",
      "Total Loss: 7.059839725494385\n",
      "**************************************************\n",
      "Epoch 1 : Iter [281 / 506]\n",
      "Total Loss: 6.726358413696289\n",
      "**************************************************\n",
      "Epoch 1 : Iter [282 / 506]\n",
      "Total Loss: 6.515217304229736\n",
      "**************************************************\n",
      "Epoch 1 : Iter [283 / 506]\n",
      "Total Loss: 6.746616840362549\n",
      "**************************************************\n",
      "Epoch 1 : Iter [284 / 506]\n",
      "Total Loss: 6.128448963165283\n",
      "**************************************************\n",
      "Epoch 1 : Iter [285 / 506]\n",
      "Total Loss: 6.3152313232421875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [286 / 506]\n",
      "Total Loss: 6.508166313171387\n",
      "**************************************************\n",
      "Epoch 1 : Iter [287 / 506]\n",
      "Total Loss: 6.592214584350586\n",
      "**************************************************\n",
      "Epoch 1 : Iter [288 / 506]\n",
      "Total Loss: 6.14112663269043\n",
      "**************************************************\n",
      "Epoch 1 : Iter [289 / 506]\n",
      "Total Loss: 6.64198112487793\n",
      "**************************************************\n",
      "Epoch 1 : Iter [290 / 506]\n",
      "Total Loss: 6.7685394287109375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [291 / 506]\n",
      "Total Loss: 6.419055938720703\n",
      "**************************************************\n",
      "Epoch 1 : Iter [292 / 506]\n",
      "Total Loss: 6.394818305969238\n",
      "**************************************************\n",
      "Epoch 1 : Iter [293 / 506]\n",
      "Total Loss: 6.339376926422119\n",
      "**************************************************\n",
      "Epoch 1 : Iter [294 / 506]\n",
      "Total Loss: 6.001699447631836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [295 / 506]\n",
      "Total Loss: 6.606382846832275\n",
      "**************************************************\n",
      "Epoch 1 : Iter [296 / 506]\n",
      "Total Loss: 6.359402656555176\n",
      "**************************************************\n",
      "Epoch 1 : Iter [297 / 506]\n",
      "Total Loss: 6.908756732940674\n",
      "**************************************************\n",
      "Epoch 1 : Iter [298 / 506]\n",
      "Total Loss: 6.938108921051025\n",
      "**************************************************\n",
      "Epoch 1 : Iter [299 / 506]\n",
      "Total Loss: 6.524211883544922\n",
      "**************************************************\n",
      "Epoch 1 : Iter [300 / 506]\n",
      "Total Loss: 6.1447224617004395\n",
      "**************************************************\n",
      "Epoch 1 : Iter [301 / 506]\n",
      "Total Loss: 6.336944580078125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [302 / 506]\n",
      "Total Loss: 6.711501598358154\n",
      "**************************************************\n",
      "Epoch 1 : Iter [303 / 506]\n",
      "Total Loss: 6.340713977813721\n",
      "**************************************************\n",
      "Epoch 1 : Iter [304 / 506]\n",
      "Total Loss: 6.152474880218506\n",
      "**************************************************\n",
      "Epoch 1 : Iter [305 / 506]\n",
      "Total Loss: 6.168296813964844\n",
      "**************************************************\n",
      "Epoch 1 : Iter [306 / 506]\n",
      "Total Loss: 6.171846866607666\n",
      "**************************************************\n",
      "Epoch 1 : Iter [307 / 506]\n",
      "Total Loss: 6.4336724281311035\n",
      "**************************************************\n",
      "Epoch 1 : Iter [308 / 506]\n",
      "Total Loss: 6.375003814697266\n",
      "**************************************************\n",
      "Epoch 1 : Iter [309 / 506]\n",
      "Total Loss: 6.131414413452148\n",
      "**************************************************\n",
      "Epoch 1 : Iter [310 / 506]\n",
      "Total Loss: 6.1338958740234375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [311 / 506]\n",
      "Total Loss: 6.4365057945251465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [312 / 506]\n",
      "Total Loss: 6.053142070770264\n",
      "**************************************************\n",
      "Epoch 1 : Iter [313 / 506]\n",
      "Total Loss: 6.30823278427124\n",
      "**************************************************\n",
      "Epoch 1 : Iter [314 / 506]\n",
      "Total Loss: 6.268739223480225\n",
      "**************************************************\n",
      "Epoch 1 : Iter [315 / 506]\n",
      "Total Loss: 6.172733783721924\n",
      "**************************************************\n",
      "Epoch 1 : Iter [316 / 506]\n",
      "Total Loss: 6.289412021636963\n",
      "**************************************************\n",
      "Epoch 1 : Iter [317 / 506]\n",
      "Total Loss: 6.086509704589844\n",
      "**************************************************\n",
      "Epoch 1 : Iter [318 / 506]\n",
      "Total Loss: 6.102060794830322\n",
      "**************************************************\n",
      "Epoch 1 : Iter [319 / 506]\n",
      "Total Loss: 6.894993305206299\n",
      "**************************************************\n",
      "Epoch 1 : Iter [320 / 506]\n",
      "Total Loss: 6.29564094543457\n",
      "**************************************************\n",
      "Epoch 1 : Iter [321 / 506]\n",
      "Total Loss: 6.353780269622803\n",
      "**************************************************\n",
      "Epoch 1 : Iter [322 / 506]\n",
      "Total Loss: 5.760563373565674\n",
      "**************************************************\n",
      "Epoch 1 : Iter [323 / 506]\n",
      "Total Loss: 6.1693572998046875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [324 / 506]\n",
      "Total Loss: 6.13910436630249\n",
      "**************************************************\n",
      "Epoch 1 : Iter [325 / 506]\n",
      "Total Loss: 6.188449859619141\n",
      "**************************************************\n",
      "Epoch 1 : Iter [326 / 506]\n",
      "Total Loss: 6.67724084854126\n",
      "**************************************************\n",
      "Epoch 1 : Iter [327 / 506]\n",
      "Total Loss: 6.245842933654785\n",
      "**************************************************\n",
      "Epoch 1 : Iter [328 / 506]\n",
      "Total Loss: 5.99168586730957\n",
      "**************************************************\n",
      "Epoch 1 : Iter [329 / 506]\n",
      "Total Loss: 6.336906909942627\n",
      "**************************************************\n",
      "Epoch 1 : Iter [330 / 506]\n",
      "Total Loss: 6.062148571014404\n",
      "**************************************************\n",
      "Epoch 1 : Iter [331 / 506]\n",
      "Total Loss: 6.016388893127441\n",
      "**************************************************\n",
      "Epoch 1 : Iter [332 / 506]\n",
      "Total Loss: 6.138190746307373\n",
      "**************************************************\n",
      "Epoch 1 : Iter [333 / 506]\n",
      "Total Loss: 6.299968719482422\n",
      "**************************************************\n",
      "Epoch 1 : Iter [334 / 506]\n",
      "Total Loss: 6.131644248962402\n",
      "**************************************************\n",
      "Epoch 1 : Iter [335 / 506]\n",
      "Total Loss: 5.998325824737549\n",
      "**************************************************\n",
      "Epoch 1 : Iter [336 / 506]\n",
      "Total Loss: 5.891952037811279\n",
      "**************************************************\n",
      "Epoch 1 : Iter [337 / 506]\n",
      "Total Loss: 6.2165985107421875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [338 / 506]\n",
      "Total Loss: 6.144069194793701\n",
      "**************************************************\n",
      "Epoch 1 : Iter [339 / 506]\n",
      "Total Loss: 6.111049175262451\n",
      "**************************************************\n",
      "Epoch 1 : Iter [340 / 506]\n",
      "Total Loss: 5.958716869354248\n",
      "**************************************************\n",
      "Epoch 1 : Iter [341 / 506]\n",
      "Total Loss: 6.228904724121094\n",
      "**************************************************\n",
      "Epoch 1 : Iter [342 / 506]\n",
      "Total Loss: 6.239327907562256\n",
      "**************************************************\n",
      "Epoch 1 : Iter [343 / 506]\n",
      "Total Loss: 5.945483207702637\n",
      "**************************************************\n",
      "Epoch 1 : Iter [344 / 506]\n",
      "Total Loss: 6.073938846588135\n",
      "**************************************************\n",
      "Epoch 1 : Iter [345 / 506]\n",
      "Total Loss: 6.409512996673584\n",
      "**************************************************\n",
      "Epoch 1 : Iter [346 / 506]\n",
      "Total Loss: 6.05375862121582\n",
      "**************************************************\n",
      "Epoch 1 : Iter [347 / 506]\n",
      "Total Loss: 6.243967056274414\n",
      "**************************************************\n",
      "Epoch 1 : Iter [348 / 506]\n",
      "Total Loss: 6.311652660369873\n",
      "**************************************************\n",
      "Epoch 1 : Iter [349 / 506]\n",
      "Total Loss: 5.883232116699219\n",
      "**************************************************\n",
      "Epoch 1 : Iter [350 / 506]\n",
      "Total Loss: 6.200438499450684\n",
      "**************************************************\n",
      "Epoch 1 : Iter [351 / 506]\n",
      "Total Loss: 5.838858604431152\n",
      "**************************************************\n",
      "Epoch 1 : Iter [352 / 506]\n",
      "Total Loss: 6.211902141571045\n",
      "**************************************************\n",
      "Epoch 1 : Iter [353 / 506]\n",
      "Total Loss: 5.973946571350098\n",
      "**************************************************\n",
      "Epoch 1 : Iter [354 / 506]\n",
      "Total Loss: 5.862633228302002\n",
      "**************************************************\n",
      "Epoch 1 : Iter [355 / 506]\n",
      "Total Loss: 6.219775199890137\n",
      "**************************************************\n",
      "Epoch 1 : Iter [356 / 506]\n",
      "Total Loss: 6.128216743469238\n",
      "**************************************************\n",
      "Epoch 1 : Iter [357 / 506]\n",
      "Total Loss: 6.2386016845703125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [358 / 506]\n",
      "Total Loss: 6.076507091522217\n",
      "**************************************************\n",
      "Epoch 1 : Iter [359 / 506]\n",
      "Total Loss: 6.260415077209473\n",
      "**************************************************\n",
      "Epoch 1 : Iter [360 / 506]\n",
      "Total Loss: 6.520803928375244\n",
      "**************************************************\n",
      "Epoch 1 : Iter [361 / 506]\n",
      "Total Loss: 6.358028411865234\n",
      "**************************************************\n",
      "Epoch 1 : Iter [362 / 506]\n",
      "Total Loss: 6.048560619354248\n",
      "**************************************************\n",
      "Epoch 1 : Iter [363 / 506]\n",
      "Total Loss: 6.276490688323975\n",
      "**************************************************\n",
      "Epoch 1 : Iter [364 / 506]\n",
      "Total Loss: 5.99110221862793\n",
      "**************************************************\n",
      "Epoch 1 : Iter [365 / 506]\n",
      "Total Loss: 6.275191783905029\n",
      "**************************************************\n",
      "Epoch 1 : Iter [366 / 506]\n",
      "Total Loss: 6.0010271072387695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [367 / 506]\n",
      "Total Loss: 6.385130405426025\n",
      "**************************************************\n",
      "Epoch 1 : Iter [368 / 506]\n",
      "Total Loss: 6.124016761779785\n",
      "**************************************************\n",
      "Epoch 1 : Iter [369 / 506]\n",
      "Total Loss: 6.004191875457764\n",
      "**************************************************\n",
      "Epoch 1 : Iter [370 / 506]\n",
      "Total Loss: 6.460285186767578\n",
      "**************************************************\n",
      "Epoch 1 : Iter [371 / 506]\n",
      "Total Loss: 6.088727951049805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [372 / 506]\n",
      "Total Loss: 5.759480953216553\n",
      "**************************************************\n",
      "Epoch 1 : Iter [373 / 506]\n",
      "Total Loss: 6.061173439025879\n",
      "**************************************************\n",
      "Epoch 1 : Iter [374 / 506]\n",
      "Total Loss: 6.099117279052734\n",
      "**************************************************\n",
      "Epoch 1 : Iter [375 / 506]\n",
      "Total Loss: 5.8490447998046875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [376 / 506]\n",
      "Total Loss: 6.0995707511901855\n",
      "**************************************************\n",
      "Epoch 1 : Iter [377 / 506]\n",
      "Total Loss: 5.995453357696533\n",
      "**************************************************\n",
      "Epoch 1 : Iter [378 / 506]\n",
      "Total Loss: 6.080806255340576\n",
      "**************************************************\n",
      "Epoch 1 : Iter [379 / 506]\n",
      "Total Loss: 6.027685642242432\n",
      "**************************************************\n",
      "Epoch 1 : Iter [380 / 506]\n",
      "Total Loss: 6.11865234375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [381 / 506]\n",
      "Total Loss: 6.042456150054932\n",
      "**************************************************\n",
      "Epoch 1 : Iter [382 / 506]\n",
      "Total Loss: 6.29469633102417\n",
      "**************************************************\n",
      "Epoch 1 : Iter [383 / 506]\n",
      "Total Loss: 5.866492748260498\n",
      "**************************************************\n",
      "Epoch 1 : Iter [384 / 506]\n",
      "Total Loss: 5.946795463562012\n",
      "**************************************************\n",
      "Epoch 1 : Iter [385 / 506]\n",
      "Total Loss: 6.02272367477417\n",
      "**************************************************\n",
      "Epoch 1 : Iter [386 / 506]\n",
      "Total Loss: 5.803231239318848\n",
      "**************************************************\n",
      "Epoch 1 : Iter [387 / 506]\n",
      "Total Loss: 6.31340217590332\n",
      "**************************************************\n",
      "Epoch 1 : Iter [388 / 506]\n",
      "Total Loss: 5.906922817230225\n",
      "**************************************************\n",
      "Epoch 1 : Iter [389 / 506]\n",
      "Total Loss: 5.846375942230225\n",
      "**************************************************\n",
      "Epoch 1 : Iter [390 / 506]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : Iter [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(stage2_train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     32\u001b[0m iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########################   STAGE 2    #####################################\n",
    "\n",
    "MODEL_PATH = \"q_former.pt\"\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model  = torch.load(MODEL_PATH)\n",
    "    \n",
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "num_epochs = 30\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iteration = 0\n",
    "    print(f\"***************   Epoch {epoch + 1}  ***************\")\n",
    "    for img, input_caption, input_mask in stage2_train_dataloader:\n",
    "        img = img.to(device)\n",
    "        input_caption = input_caption.to(device).squeeze()\n",
    "        input_mask = input_mask.to(device).squeeze()\n",
    "        B, S = input_caption.shape\n",
    "\n",
    "        out = model.stage2(img, input_caption[:,:2], input_caption[:,2:], input_mask[:,:2], (B, S))\n",
    "        total_loss = out.loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} : Iter [{iteration} / {len(stage2_train_dataloader)}]\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\")\n",
    "        print(\"\" + \"*\" * 50)\n",
    "        iteration += 1\n",
    "\n",
    "torch.save(model.state_dict, \"blip2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e442e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f760470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
