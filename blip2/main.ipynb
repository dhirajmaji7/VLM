{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d20a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import Blip2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, dataset, tokenizer\n",
    "importlib.reload(config)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import Blip2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Blip2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_tokenizer = Blip2Tokenizer(config, tokenizer)\n",
    "config.vocab_size = blip2_tokenizer.n_vocab\n",
    "train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=blip2_tokenizer.tokenize_text)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40420224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import timm\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class BertMLPBlock(nn.Module):\n",
    "    def __init__(self, intermediate, output):\n",
    "        super().__init__()\n",
    "        self.intermediate = intermediate\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_output = self.intermediate(x)\n",
    "        return self.output(intermediate_output, x)\n",
    "    \n",
    "\n",
    "class BertEncoderBlock(nn.Module):\n",
    "    def __init__(self, bert_layer, bert_config, is_cross_attn=False):\n",
    "        super().__init__()\n",
    "        self.bert_config = bert_config\n",
    "        self.is_cross_attn = is_cross_attn\n",
    "        self.self_attn = bert_layer.attention\n",
    "        self.mlp_img_transformer = BertMLPBlock(bert_layer.intermediate, bert_layer.output)\n",
    "        self.mlp_text_transformer = BertMLPBlock(\n",
    "                    copy.deepcopy(bert_layer.intermediate), \n",
    "                    copy.deepcopy(bert_layer.output)\n",
    "                    )\n",
    "        if is_cross_attn:\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim=self.bert_config.hidden_size, \n",
    "                                                    num_heads=self.bert_config.num_attention_heads, \n",
    "                                                    batch_first=True)\n",
    "            self.cross_layer_norm = nn.LayerNorm(self.bert_config.hidden_size)\n",
    "        \n",
    "    def forward(self, query_embds, img_embds, text_embds, attn_mask):\n",
    "        _, Qs, _ = query_embds.shape\n",
    "        _, Ts, _ = text_embds.shape\n",
    "\n",
    "        combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
    "\n",
    "        self_attn_output = self.self_attn(combined_embds, attention_mask=attn_mask)[0]\n",
    "        query_embds = combined_embds[:, :Qs]\n",
    "        text_embds= combined_embds[:, Qs:]\n",
    "        \n",
    "        if self.is_cross_attn:\n",
    "            hidden_states = self.cross_attn(query_embds, img_embds, img_embds)[0]\n",
    "            query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
    "\n",
    "        query_embds = self.mlp_img_transformer(query_embds)\n",
    "        text_embds = self.mlp_text_transformer(text_embds)\n",
    "        return query_embds, text_embds\n",
    "\n",
    "\n",
    "class QTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert_cfg  = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\", config = self.bert_cfg)\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, bert_layer in enumerate(self.bert_model.encoder.layer):\n",
    "            self.encoder.append(BertEncoderBlock(bert_layer, self.bert_cfg, i % 2 == 0))\n",
    "        \n",
    "        qs = config.num_queries\n",
    "        ts = config.context_length\n",
    "        combined_seq_len = qs + ts\n",
    "\n",
    "        # ITC Loss Mask\n",
    "        itc_attn_mask = torch.zeros((combined_seq_len, combined_seq_len))\n",
    "        itc_attn_mask[:qs, :qs] = 1\n",
    "        itc_attn_mask[qs:, qs:] = 1\n",
    "\n",
    "        # ITM Loss Mask\n",
    "        itm_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "\n",
    "        # ITG Loss Mask\n",
    "        itg_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "        itg_attn_mask[:qs, qs:] = 0\n",
    "        itg_attn_mask[qs:, qs:] = torch.tril(itg_attn_mask[qs:, qs:], diagonal=0)\n",
    "\n",
    "        self.register_buffer(\"itc_attn_mask\", itc_attn_mask)\n",
    "        self.register_buffer(\"itm_attn_mask\", itm_attn_mask)\n",
    "        self.register_buffer(\"itg_attn_mask\", itg_attn_mask)\n",
    "        \n",
    "    def forward(self, query_embds, img_embds, cls_text_embds, dec_text_embds):\n",
    "\n",
    "        itc_query_embds = query_embds.clone()\n",
    "        itm_query_embds = query_embds.clone()\n",
    "        itg_query_embds = query_embds.clone()\n",
    "\n",
    "        itc_text_embds = cls_text_embds.clone()\n",
    "        itm_text_embds = cls_text_embds.clone()\n",
    "        itg_text_embds = dec_text_embds.clone()\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            itc_query_embds, itc_text_embds = encoder(itc_query_embds, img_embds, itc_text_embds, self.itc_attn_mask)\n",
    "            itm_query_embds, itm_text_embds = encoder(itm_query_embds, img_embds, itm_text_embds, self.itm_attn_mask)\n",
    "            itg_query_embds, itg_text_embds = encoder(itg_query_embds, img_embds, itg_text_embds, self.itg_attn_mask)\n",
    "        \n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds\n",
    "    \n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q_transformer = QTransformer(config)\n",
    "        self.learned_query = nn.Parameter(torch.randn(config.num_queries, config.embedding_dim))\n",
    "        self.output_embedding  = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(config.context_length, config.embedding_dim)\n",
    "\n",
    "        position_ids = torch.arange(self.config.context_length).unsqueeze(0)\n",
    "        self.register_buffer(\"position_ids\", position_ids)\n",
    "\n",
    "    def forward(self, image_embedding: torch.tensor, cls_tokens: torch.tensor, dec_tokens: torch.tensor):\n",
    "        B, S, E = image_embedding.shape\n",
    "        learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        cls_text_embeddings = self.output_embedding(cls_tokens) #(S,768)\n",
    "        cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "        dec_text_embeddings = self.output_embedding(dec_tokens) #(S,768)\n",
    "        dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds = self.q_transformer(\n",
    "            learned_query, image_embedding, cls_text_embeddings, dec_text_embeddings)\n",
    "        itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
    "\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "\n",
    "\n",
    "class Blip2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.image_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "        self.image_encoder.reset_classifier(0)\n",
    "        self.image_proj = nn.Linear(config.img_embd_dim, config.embedding_dim)\n",
    "\n",
    "        self.lm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "        self.q_former = QFormer(config)\n",
    "        self.z_proj = nn.Linear(config.embedding_dim, config.lm_embedding_dim)\n",
    "    \n",
    "    def forward(self, image:torch.tensor, cls_caption:torch.tensor, dec_caption:torch.tensor, stage:int):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "\n",
    "        if(stage == 1):\n",
    "            itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, cls_caption, dec_caption)\n",
    "            return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "        return self.q_former(image_embedding, cls_caption, dec_caption)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b708e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_model = Blip2Model(config)\n",
    "print(blip2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "img, cls_caption, dec_caption = next(iter(train_dataloader))\n",
    "\n",
    "img = img.to(device)\n",
    "cls_caption = cls_caption.to(device)\n",
    "dec_caption = dec_caption.to(device)\n",
    "\n",
    "\n",
    "model = blip2_model.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = model(img, cls_caption, dec_caption, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bffb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4, 0],\n",
      "         [6, 3]],\n",
      "\n",
      "        [[9, 1],\n",
      "         [3, 8]]]) tensor([[[9, 3]],\n",
      "\n",
      "        [[5, 2]]])\n",
      "tensor([[63., 36.],\n",
      "        [84., 47.]])\n"
     ]
    }
   ],
   "source": [
    "class ITCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query_embds, text_embds):\n",
    "        # query_embds: B, 32, d\n",
    "        # text_embds: B, 77, d\n",
    "        text_logit = text_embds[:, :1] # B, 1, d\n",
    "        B, _, _ = text_logit.shape\n",
    "        out = torch.zeros((B, B))\n",
    "        for i in range(B):\n",
    "            for j in range(B):\n",
    "                out[i][j] = torch.max(query_embds[i] @ text_logit[j].T) # 32, 1 -> 1\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73734f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
