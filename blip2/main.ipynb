{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d20a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import Blip2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4b2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, dataset, tokenizer\n",
    "importlib.reload(config)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import Blip2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ccd714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aff7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Blip2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6d6f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: image\n"
     ]
    }
   ],
   "source": [
    "blip2_tokenizer = Blip2Tokenizer(config, tokenizer)\n",
    "config.vocab_size = blip2_tokenizer.n_vocab\n",
    "train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=blip2_tokenizer.tokenize_text)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40420224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import timm\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class BertMLPBlock(nn.Module):\n",
    "    def __init__(self, intermediate, output):\n",
    "        super().__init__()\n",
    "        self.intermediate = intermediate\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_output = self.intermediate(x)\n",
    "        return self.output(intermediate_output, x)\n",
    "    \n",
    "\n",
    "class BertEncoderBlock(nn.Module):\n",
    "    def __init__(self, bert_layer, bert_config, is_cross_attn=False):\n",
    "        super().__init__()\n",
    "        self.bert_config = bert_config\n",
    "        self.is_cross_attn = is_cross_attn\n",
    "        self.self_attn = bert_layer.attention\n",
    "        self.mlp_img_transformer = BertMLPBlock(bert_layer.intermediate, bert_layer.output)\n",
    "        self.mlp_text_transformer = BertMLPBlock(\n",
    "                    copy.deepcopy(bert_layer.intermediate), \n",
    "                    copy.deepcopy(bert_layer.output)\n",
    "                    )\n",
    "        if is_cross_attn:\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim=self.bert_config.hidden_size, \n",
    "                                                    num_heads=self.bert_config.num_attention_heads, \n",
    "                                                    batch_first=True)\n",
    "            self.cross_layer_norm = nn.LayerNorm(self.bert_config.hidden_size)\n",
    "        \n",
    "    def forward(self, query_embds, img_embds, text_embds, attn_mask):\n",
    "        _, Qs, _ = query_embds.shape\n",
    "        _, Ts, _ = text_embds.shape\n",
    "\n",
    "        combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
    "\n",
    "        self_attn_output = self.self_attn(combined_embds, attention_mask=attn_mask)[0]\n",
    "        query_embds = combined_embds[:, :Qs]\n",
    "        text_embds= combined_embds[:, Qs:]\n",
    "        \n",
    "        if self.is_cross_attn:\n",
    "            hidden_states = self.cross_attn(query_embds, img_embds, img_embds)[0]\n",
    "            query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
    "\n",
    "        query_embds = self.mlp_img_transformer(query_embds)\n",
    "        text_embds = self.mlp_text_transformer(text_embds)\n",
    "        return query_embds, text_embds\n",
    "\n",
    "\n",
    "class QTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert_cfg  = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\", config = self.bert_cfg)\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, bert_layer in enumerate(self.bert_model.encoder.layer):\n",
    "            self.encoder.append(BertEncoderBlock(bert_layer, self.bert_cfg, i % 2 == 0))\n",
    "        \n",
    "        qs = config.num_queries\n",
    "        ts = config.context_length\n",
    "        combined_seq_len = qs + ts\n",
    "\n",
    "        ####  STAGE 1: ITC, ITM, ITG Loss Masks ####\n",
    "        # ITC Loss Mask\n",
    "        itc_attn_mask = torch.zeros((combined_seq_len, combined_seq_len))\n",
    "        itc_attn_mask[:qs, :qs] = 1\n",
    "        itc_attn_mask[qs:, qs:] = 1\n",
    "\n",
    "        # ITM Loss Mask\n",
    "        itm_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "\n",
    "        # ITG Loss Mask\n",
    "        itg_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "        itg_attn_mask[:qs, qs:] = 0\n",
    "        itg_attn_mask[qs:, qs:] = torch.tril(itg_attn_mask[qs:, qs:], diagonal=0)\n",
    "\n",
    "        self.register_buffer(\"itc_attn_mask\", itc_attn_mask)\n",
    "        self.register_buffer(\"itm_attn_mask\", itm_attn_mask)\n",
    "        self.register_buffer(\"itg_attn_mask\", itg_attn_mask)\n",
    "\n",
    "        ####  STAGE 2: ####\n",
    "        # ITC Loss Mask will be same as stage 1 and reused for stage 2\n",
    "\n",
    "    def forward(self, query_embds, img_embds, cls_text_embds, dec_text_embds, stage):\n",
    "\n",
    "        itc_query_embds = query_embds.clone()\n",
    "        itm_query_embds = query_embds.clone()\n",
    "        itg_query_embds = query_embds.clone()\n",
    "\n",
    "        itc_text_embds = cls_text_embds.clone()\n",
    "        itm_text_embds = cls_text_embds.clone()\n",
    "        itg_text_embds = dec_text_embds.clone()\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds = None, None, None, None, None, None\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            itc_query_embds, itc_text_embds = encoder(itc_query_embds, img_embds, itc_text_embds, self.itc_attn_mask)\n",
    "            if stage == 1:\n",
    "                itm_query_embds, itm_text_embds = encoder(itm_query_embds, img_embds, itm_text_embds, self.itm_attn_mask)\n",
    "                itg_query_embds, itg_text_embds = encoder(itg_query_embds, img_embds, itg_text_embds, self.itg_attn_mask)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds\n",
    "    \n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q_transformer = QTransformer(config)\n",
    "        self.learned_query = nn.Parameter(torch.randn(config.num_queries, config.embedding_dim))\n",
    "        self.output_embedding  = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(config.context_length, config.embedding_dim)\n",
    "\n",
    "        position_ids = torch.arange(self.config.context_length).unsqueeze(0)\n",
    "        self.register_buffer(\"position_ids\", position_ids)\n",
    "\n",
    "    def forward(self, image_embedding: torch.tensor, cls_tokens: torch.tensor, dec_tokens: torch.tensor, stage:int):\n",
    "        B, S, E = image_embedding.shape\n",
    "        learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        cls_text_embeddings = self.output_embedding(cls_tokens) #(S,768)\n",
    "        cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "        dec_text_embeddings = self.output_embedding(dec_tokens) #(S,768)\n",
    "        dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds = self.q_transformer(\n",
    "            learned_query, image_embedding, cls_text_embeddings, dec_text_embeddings, stage)\n",
    "\n",
    "        if itg_text_embds is not None:\n",
    "            itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
    "        else:\n",
    "            itg_logits = None\n",
    "\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "\n",
    "\n",
    "class Blip2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.image_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "        self.image_encoder.reset_classifier(0)\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.image_proj = nn.Linear(config.img_embd_dim, config.embedding_dim)\n",
    "\n",
    "        self.lm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "        \n",
    "        self.q_former = QFormer(config)\n",
    "        self.z_proj = nn.Linear(config.embedding_dim, config.lm_embedding_dim)\n",
    "    \n",
    "    def forward(self, image:torch.tensor, cls_caption:torch.tensor, dec_caption:torch.tensor, stage:int):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "\n",
    "        if(stage == 1):\n",
    "            itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, cls_caption, dec_caption, stage)\n",
    "            return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "        \n",
    "        elif(stage == 2):\n",
    "            cls_caption_dummy = torch.zeros_like(cls_caption)\n",
    "            dec_caption_dummy = torch.zeros_like(dec_caption)\n",
    "            itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, \n",
    "                                cls_caption_dummy, dec_caption_dummy, stage)\n",
    "            z = self.z_proj(itc_query_embds)  # [B, Qs, D]\n",
    "            \n",
    "            return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "        return self.q_former(image_embedding, cls_caption, dec_caption)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b708e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blip2Model(\n",
      "  (image_encoder): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (image_proj): Linear(in_features=192, out_features=768, bias=True)\n",
      "  (lm_model): T5ForConditionalGeneration(\n",
      "    (shared): Embedding(32128, 512)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 6)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-7): 7 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 6)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-7): 7 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      "  )\n",
      "  (q_former): QFormer(\n",
      "    (q_transformer): QTransformer(\n",
      "      (bert_model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (encoder): ModuleList(\n",
      "        (0): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (cross_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (cross_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): BertEncoderBlock(\n",
      "          (self_attn): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_img_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp_text_transformer): BertMLPBlock(\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_embedding): Embedding(32103, 768)\n",
      "    (position_embedding): Embedding(77, 768)\n",
      "  )\n",
      "  (z_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "blip2_model = Blip2Model(config)\n",
    "print(blip2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d441c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "img, cls_caption, dec_caption = next(iter(train_dataloader))\n",
    "\n",
    "img = img.to(device)\n",
    "cls_caption = cls_caption.to(device)\n",
    "dec_caption = dec_caption.to(device)\n",
    "\n",
    "model = blip2_model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = model(img, cls_caption, dec_caption, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6bffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query_embds, text_embds):\n",
    "        # query_embds: B, 32, d\n",
    "        # text_embds: B, 77, d\n",
    "        text_logit = text_embds[:, :1] # B, 1, d\n",
    "        B, _, _ = text_logit.shape\n",
    "        B, Qs, d = query_embds.shape \n",
    "        query_embds = query_embds.reshape(B * Qs, d)\n",
    "        text_embds = text_logit.squeeze()\n",
    "        logits = query_embds @ text_embds.T   # B*Qs,B\n",
    "        logits = torch.max(logits.reshape(B,Qs,B),dim=1)[0] # B,B\n",
    "        label = torch.arange(B,device=query_embds.device)\n",
    "        return (F.cross_entropy(logits,label)+ F.cross_entropy(logits.T,label)) / 2, logits\n",
    "\n",
    "class ITMLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d = config.embedding_dim\n",
    "        self.classification_layer = nn.Linear(d,2)\n",
    "    \n",
    "    def forward(self, query_embd, label):\n",
    "        # query_embd --> (B,32,768)\n",
    "        #label ->(B,1) B x [0/1]\n",
    "\n",
    "        match_logit = self.classification_layer(query_embd) #(B,32,2)\n",
    "        match_logit = match_logit.mean(dim=1)\n",
    "        return F.cross_entropy(match_logit,label)\n",
    "\n",
    "\n",
    "class ITGLoss(nn.Module):\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def forward(self, itg_logits, label_token):\n",
    "        #itg_logits -> B,S,vocab size\n",
    "        #label_token -> B,S\n",
    "        B, S, V = itg_logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            itg_logits.view(B * S, V),\n",
    "            label_token.view(B * S),\n",
    "            ignore_index=self.pad_token_id\n",
    "        )\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73734f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************   Epoch 1  ***************\n",
      "Epoch 1 : Iter [0 / 506]\n",
      "Total Loss: 87.76075744628906\n",
      "ITC Loss: 26.80521011352539, ITM Loss: 0.7197456359863281, ITG Loss: 60.235801696777344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [1 / 506]\n",
      "Total Loss: 60.467437744140625\n",
      "ITC Loss: 13.219557762145996, ITM Loss: 0.7023423314094543, ITG Loss: 46.54553985595703\n",
      "**************************************************\n",
      "Epoch 1 : Iter [2 / 506]\n",
      "Total Loss: 54.36392593383789\n",
      "ITC Loss: 8.23693561553955, ITM Loss: 0.7711031436920166, ITG Loss: 45.35588836669922\n",
      "**************************************************\n",
      "Epoch 1 : Iter [3 / 506]\n",
      "Total Loss: 48.50190734863281\n",
      "ITC Loss: 5.196666717529297, ITM Loss: 0.7233944535255432, ITG Loss: 42.581844329833984\n",
      "**************************************************\n",
      "Epoch 1 : Iter [4 / 506]\n",
      "Total Loss: 36.243194580078125\n",
      "ITC Loss: 3.8224618434906006, ITM Loss: 0.6984116435050964, ITG Loss: 31.722320556640625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [5 / 506]\n",
      "Total Loss: 32.027671813964844\n",
      "ITC Loss: 4.128055572509766, ITM Loss: 0.6934894919395447, ITG Loss: 27.206127166748047\n",
      "**************************************************\n",
      "Epoch 1 : Iter [6 / 506]\n",
      "Total Loss: 26.51729965209961\n",
      "ITC Loss: 3.3880763053894043, ITM Loss: 0.7009263634681702, ITG Loss: 22.42829704284668\n",
      "**************************************************\n",
      "Epoch 1 : Iter [7 / 506]\n",
      "Total Loss: 29.70342445373535\n",
      "ITC Loss: 3.630037307739258, ITM Loss: 0.6973016262054443, ITG Loss: 25.37608528137207\n",
      "**************************************************\n",
      "Epoch 1 : Iter [8 / 506]\n",
      "Total Loss: 26.814260482788086\n",
      "ITC Loss: 3.7359344959259033, ITM Loss: 0.7014648914337158, ITG Loss: 22.376861572265625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [9 / 506]\n",
      "Total Loss: 26.690914154052734\n",
      "ITC Loss: 3.5573184490203857, ITM Loss: 0.7000972628593445, ITG Loss: 22.43349838256836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [10 / 506]\n",
      "Total Loss: 24.034576416015625\n",
      "ITC Loss: 3.0991930961608887, ITM Loss: 0.6972614526748657, ITG Loss: 20.238121032714844\n",
      "**************************************************\n",
      "Epoch 1 : Iter [11 / 506]\n",
      "Total Loss: 26.05217170715332\n",
      "ITC Loss: 3.3711934089660645, ITM Loss: 0.6957868933677673, ITG Loss: 21.985191345214844\n",
      "**************************************************\n",
      "Epoch 1 : Iter [12 / 506]\n",
      "Total Loss: 24.447973251342773\n",
      "ITC Loss: 3.0490636825561523, ITM Loss: 0.69398033618927, ITG Loss: 20.70492935180664\n",
      "**************************************************\n",
      "Epoch 1 : Iter [13 / 506]\n",
      "Total Loss: 24.794374465942383\n",
      "ITC Loss: 4.066337585449219, ITM Loss: 0.6909084916114807, ITG Loss: 20.037128448486328\n",
      "**************************************************\n",
      "Epoch 1 : Iter [14 / 506]\n",
      "Total Loss: 23.509836196899414\n",
      "ITC Loss: 3.1727185249328613, ITM Loss: 0.6928802728652954, ITG Loss: 19.644237518310547\n",
      "**************************************************\n",
      "Epoch 1 : Iter [15 / 506]\n",
      "Total Loss: 21.112363815307617\n",
      "ITC Loss: 3.0703349113464355, ITM Loss: 0.6932612061500549, ITG Loss: 17.34876823425293\n",
      "**************************************************\n",
      "Epoch 1 : Iter [16 / 506]\n",
      "Total Loss: 19.46944808959961\n",
      "ITC Loss: 3.2176146507263184, ITM Loss: 0.6926301717758179, ITG Loss: 15.559203147888184\n",
      "**************************************************\n",
      "Epoch 1 : Iter [17 / 506]\n",
      "Total Loss: 21.306232452392578\n",
      "ITC Loss: 3.1944966316223145, ITM Loss: 0.6933173537254333, ITG Loss: 17.418418884277344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [18 / 506]\n",
      "Total Loss: 20.37784767150879\n",
      "ITC Loss: 3.3134477138519287, ITM Loss: 0.6920581459999084, ITG Loss: 16.37234115600586\n",
      "**************************************************\n",
      "Epoch 1 : Iter [19 / 506]\n",
      "Total Loss: 20.24619483947754\n",
      "ITC Loss: 3.2574374675750732, ITM Loss: 0.6939728260040283, ITG Loss: 16.294784545898438\n",
      "**************************************************\n",
      "Epoch 1 : Iter [20 / 506]\n",
      "Total Loss: 19.153051376342773\n",
      "ITC Loss: 3.15476655960083, ITM Loss: 0.6927865743637085, ITG Loss: 15.305498123168945\n",
      "**************************************************\n",
      "Epoch 1 : Iter [21 / 506]\n",
      "Total Loss: 19.172138214111328\n",
      "ITC Loss: 3.5262508392333984, ITM Loss: 0.690214216709137, ITG Loss: 14.955674171447754\n",
      "**************************************************\n",
      "Epoch 1 : Iter [22 / 506]\n",
      "Total Loss: 19.253150939941406\n",
      "ITC Loss: 3.047858238220215, ITM Loss: 0.6931676268577576, ITG Loss: 15.512125015258789\n",
      "**************************************************\n",
      "Epoch 1 : Iter [23 / 506]\n",
      "Total Loss: 19.377925872802734\n",
      "ITC Loss: 3.2393150329589844, ITM Loss: 0.6973458528518677, ITG Loss: 15.441265106201172\n",
      "**************************************************\n",
      "Epoch 1 : Iter [24 / 506]\n",
      "Total Loss: 17.981924057006836\n",
      "ITC Loss: 3.123760223388672, ITM Loss: 0.6907029151916504, ITG Loss: 14.167461395263672\n",
      "**************************************************\n",
      "Epoch 1 : Iter [25 / 506]\n",
      "Total Loss: 18.327144622802734\n",
      "ITC Loss: 3.1977291107177734, ITM Loss: 0.6884574294090271, ITG Loss: 14.440958976745605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [26 / 506]\n",
      "Total Loss: 17.53878402709961\n",
      "ITC Loss: 3.2943692207336426, ITM Loss: 0.6941856741905212, ITG Loss: 13.550228118896484\n",
      "**************************************************\n",
      "Epoch 1 : Iter [27 / 506]\n",
      "Total Loss: 17.55575942993164\n",
      "ITC Loss: 3.0545425415039062, ITM Loss: 0.6960511803627014, ITG Loss: 13.80516529083252\n",
      "**************************************************\n",
      "Epoch 1 : Iter [28 / 506]\n",
      "Total Loss: 17.354734420776367\n",
      "ITC Loss: 3.28859806060791, ITM Loss: 0.6935379505157471, ITG Loss: 13.372598648071289\n",
      "**************************************************\n",
      "Epoch 1 : Iter [29 / 506]\n",
      "Total Loss: 15.799627304077148\n",
      "ITC Loss: 2.9975709915161133, ITM Loss: 0.6907440423965454, ITG Loss: 12.111311912536621\n",
      "**************************************************\n",
      "Epoch 1 : Iter [30 / 506]\n",
      "Total Loss: 16.844226837158203\n",
      "ITC Loss: 3.0638303756713867, ITM Loss: 0.6946464776992798, ITG Loss: 13.085749626159668\n",
      "**************************************************\n",
      "Epoch 1 : Iter [31 / 506]\n",
      "Total Loss: 16.9091739654541\n",
      "ITC Loss: 3.0702970027923584, ITM Loss: 0.6914862394332886, ITG Loss: 13.147390365600586\n",
      "**************************************************\n",
      "Epoch 1 : Iter [32 / 506]\n",
      "Total Loss: 17.537214279174805\n",
      "ITC Loss: 3.097102403640747, ITM Loss: 0.6923159956932068, ITG Loss: 13.747795104980469\n",
      "**************************************************\n",
      "Epoch 1 : Iter [33 / 506]\n",
      "Total Loss: 15.150550842285156\n",
      "ITC Loss: 3.106471061706543, ITM Loss: 0.6915507912635803, ITG Loss: 11.352529525756836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [34 / 506]\n",
      "Total Loss: 16.422536849975586\n",
      "ITC Loss: 2.9364616870880127, ITM Loss: 0.6966679692268372, ITG Loss: 12.789406776428223\n",
      "**************************************************\n",
      "Epoch 1 : Iter [35 / 506]\n",
      "Total Loss: 16.203899383544922\n",
      "ITC Loss: 2.942685604095459, ITM Loss: 0.697288990020752, ITG Loss: 12.563925743103027\n",
      "**************************************************\n",
      "Epoch 1 : Iter [36 / 506]\n",
      "Total Loss: 15.326984405517578\n",
      "ITC Loss: 3.0231809616088867, ITM Loss: 0.6941159963607788, ITG Loss: 11.609686851501465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [37 / 506]\n",
      "Total Loss: 16.188907623291016\n",
      "ITC Loss: 3.225864887237549, ITM Loss: 0.6927281618118286, ITG Loss: 12.27031421661377\n",
      "**************************************************\n",
      "Epoch 1 : Iter [38 / 506]\n",
      "Total Loss: 15.345976829528809\n",
      "ITC Loss: 2.9670755863189697, ITM Loss: 0.6933198571205139, ITG Loss: 11.68558120727539\n",
      "**************************************************\n",
      "Epoch 1 : Iter [39 / 506]\n",
      "Total Loss: 16.008663177490234\n",
      "ITC Loss: 3.0159053802490234, ITM Loss: 0.6912356019020081, ITG Loss: 12.301523208618164\n",
      "**************************************************\n",
      "Epoch 1 : Iter [40 / 506]\n",
      "Total Loss: 15.978281021118164\n",
      "ITC Loss: 3.0752923488616943, ITM Loss: 0.6983757615089417, ITG Loss: 12.204612731933594\n",
      "**************************************************\n",
      "Epoch 1 : Iter [41 / 506]\n",
      "Total Loss: 16.01877212524414\n",
      "ITC Loss: 2.9498581886291504, ITM Loss: 0.6943655014038086, ITG Loss: 12.37454891204834\n",
      "**************************************************\n",
      "Epoch 1 : Iter [42 / 506]\n",
      "Total Loss: 15.399168014526367\n",
      "ITC Loss: 3.186842918395996, ITM Loss: 0.6904127597808838, ITG Loss: 11.521912574768066\n",
      "**************************************************\n",
      "Epoch 1 : Iter [43 / 506]\n",
      "Total Loss: 15.827144622802734\n",
      "ITC Loss: 3.34952974319458, ITM Loss: 0.6971137523651123, ITG Loss: 11.780501365661621\n",
      "**************************************************\n",
      "Epoch 1 : Iter [44 / 506]\n",
      "Total Loss: 15.197776794433594\n",
      "ITC Loss: 3.066483497619629, ITM Loss: 0.6922813653945923, ITG Loss: 11.439011573791504\n",
      "**************************************************\n",
      "Epoch 1 : Iter [45 / 506]\n",
      "Total Loss: 15.258460998535156\n",
      "ITC Loss: 3.0436739921569824, ITM Loss: 0.6941081881523132, ITG Loss: 11.520678520202637\n",
      "**************************************************\n",
      "Epoch 1 : Iter [46 / 506]\n",
      "Total Loss: 16.380115509033203\n",
      "ITC Loss: 3.047801971435547, ITM Loss: 0.6949375867843628, ITG Loss: 12.637374877929688\n",
      "**************************************************\n",
      "Epoch 1 : Iter [47 / 506]\n",
      "Total Loss: 15.025572776794434\n",
      "ITC Loss: 3.006089210510254, ITM Loss: 0.6892015337944031, ITG Loss: 11.330282211303711\n",
      "**************************************************\n",
      "Epoch 1 : Iter [48 / 506]\n",
      "Total Loss: 14.1014404296875\n",
      "ITC Loss: 2.932340621948242, ITM Loss: 0.6928200721740723, ITG Loss: 10.476280212402344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [49 / 506]\n",
      "Total Loss: 14.118854522705078\n",
      "ITC Loss: 3.319042682647705, ITM Loss: 0.6902546882629395, ITG Loss: 10.109557151794434\n",
      "**************************************************\n",
      "Epoch 1 : Iter [50 / 506]\n",
      "Total Loss: 14.624401092529297\n",
      "ITC Loss: 2.84468936920166, ITM Loss: 0.6943215727806091, ITG Loss: 11.085390090942383\n",
      "**************************************************\n",
      "Epoch 1 : Iter [51 / 506]\n",
      "Total Loss: 14.272836685180664\n",
      "ITC Loss: 2.9485559463500977, ITM Loss: 0.6919884085655212, ITG Loss: 10.632291793823242\n",
      "**************************************************\n",
      "Epoch 1 : Iter [52 / 506]\n",
      "Total Loss: 14.312044143676758\n",
      "ITC Loss: 3.020864486694336, ITM Loss: 0.6953258514404297, ITG Loss: 10.595853805541992\n",
      "**************************************************\n",
      "Epoch 1 : Iter [53 / 506]\n",
      "Total Loss: 14.818378448486328\n",
      "ITC Loss: 3.1632280349731445, ITM Loss: 0.6902564167976379, ITG Loss: 10.96489429473877\n",
      "**************************************************\n",
      "Epoch 1 : Iter [54 / 506]\n",
      "Total Loss: 14.914219856262207\n",
      "ITC Loss: 3.1298747062683105, ITM Loss: 0.6938768029212952, ITG Loss: 11.090468406677246\n",
      "**************************************************\n",
      "Epoch 1 : Iter [55 / 506]\n",
      "Total Loss: 14.306894302368164\n",
      "ITC Loss: 3.245147705078125, ITM Loss: 0.6937277317047119, ITG Loss: 10.368019104003906\n",
      "**************************************************\n",
      "Epoch 1 : Iter [56 / 506]\n",
      "Total Loss: 14.483753204345703\n",
      "ITC Loss: 3.0333361625671387, ITM Loss: 0.6953034400939941, ITG Loss: 10.75511360168457\n",
      "**************************************************\n",
      "Epoch 1 : Iter [57 / 506]\n",
      "Total Loss: 15.145580291748047\n",
      "ITC Loss: 3.028087854385376, ITM Loss: 0.6899339556694031, ITG Loss: 11.427558898925781\n",
      "**************************************************\n",
      "Epoch 1 : Iter [58 / 506]\n",
      "Total Loss: 13.586601257324219\n",
      "ITC Loss: 2.9532294273376465, ITM Loss: 0.6959251165390015, ITG Loss: 9.937446594238281\n",
      "**************************************************\n",
      "Epoch 1 : Iter [59 / 506]\n",
      "Total Loss: 15.10099983215332\n",
      "ITC Loss: 2.9900522232055664, ITM Loss: 0.6936516165733337, ITG Loss: 11.417296409606934\n",
      "**************************************************\n",
      "Epoch 1 : Iter [60 / 506]\n",
      "Total Loss: 14.429062843322754\n",
      "ITC Loss: 2.966433525085449, ITM Loss: 0.6946666836738586, ITG Loss: 10.767962455749512\n",
      "**************************************************\n",
      "Epoch 1 : Iter [61 / 506]\n",
      "Total Loss: 13.66516399383545\n",
      "ITC Loss: 2.895108699798584, ITM Loss: 0.6936548352241516, ITG Loss: 10.076400756835938\n",
      "**************************************************\n",
      "Epoch 1 : Iter [62 / 506]\n",
      "Total Loss: 14.110175132751465\n",
      "ITC Loss: 2.9147353172302246, ITM Loss: 0.694722592830658, ITG Loss: 10.500717163085938\n",
      "**************************************************\n",
      "Epoch 1 : Iter [63 / 506]\n",
      "Total Loss: 14.219170570373535\n",
      "ITC Loss: 3.078542947769165, ITM Loss: 0.6908004283905029, ITG Loss: 10.449827194213867\n",
      "**************************************************\n",
      "Epoch 1 : Iter [64 / 506]\n",
      "Total Loss: 14.135672569274902\n",
      "ITC Loss: 3.054198741912842, ITM Loss: 0.6940549612045288, ITG Loss: 10.387418746948242\n",
      "**************************************************\n",
      "Epoch 1 : Iter [65 / 506]\n",
      "Total Loss: 13.562688827514648\n",
      "ITC Loss: 2.898303508758545, ITM Loss: 0.6943269968032837, ITG Loss: 9.97005844116211\n",
      "**************************************************\n",
      "Epoch 1 : Iter [66 / 506]\n",
      "Total Loss: 13.462238311767578\n",
      "ITC Loss: 2.993821859359741, ITM Loss: 0.6921544075012207, ITG Loss: 9.776262283325195\n",
      "**************************************************\n",
      "Epoch 1 : Iter [67 / 506]\n",
      "Total Loss: 13.26370620727539\n",
      "ITC Loss: 2.9882473945617676, ITM Loss: 0.6936358213424683, ITG Loss: 9.581822395324707\n",
      "**************************************************\n",
      "Epoch 1 : Iter [68 / 506]\n",
      "Total Loss: 13.442819595336914\n",
      "ITC Loss: 2.961759090423584, ITM Loss: 0.6914842128753662, ITG Loss: 9.789576530456543\n",
      "**************************************************\n",
      "Epoch 1 : Iter [69 / 506]\n",
      "Total Loss: 13.701736450195312\n",
      "ITC Loss: 3.0054423809051514, ITM Loss: 0.6957180500030518, ITG Loss: 10.00057601928711\n",
      "**************************************************\n",
      "Epoch 1 : Iter [70 / 506]\n",
      "Total Loss: 13.645322799682617\n",
      "ITC Loss: 2.9015440940856934, ITM Loss: 0.6954692006111145, ITG Loss: 10.048309326171875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [71 / 506]\n",
      "Total Loss: 13.565153121948242\n",
      "ITC Loss: 3.0325801372528076, ITM Loss: 0.6952198147773743, ITG Loss: 9.837353706359863\n",
      "**************************************************\n",
      "Epoch 1 : Iter [72 / 506]\n",
      "Total Loss: 13.396036148071289\n",
      "ITC Loss: 3.027453899383545, ITM Loss: 0.6947364211082458, ITG Loss: 9.673845291137695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [73 / 506]\n",
      "Total Loss: 13.653722763061523\n",
      "ITC Loss: 2.9688196182250977, ITM Loss: 0.695545494556427, ITG Loss: 9.989357948303223\n",
      "**************************************************\n",
      "Epoch 1 : Iter [74 / 506]\n",
      "Total Loss: 13.383733749389648\n",
      "ITC Loss: 2.98067569732666, ITM Loss: 0.6924567818641663, ITG Loss: 9.710600852966309\n",
      "**************************************************\n",
      "Epoch 1 : Iter [75 / 506]\n",
      "Total Loss: 13.245265007019043\n",
      "ITC Loss: 2.887026071548462, ITM Loss: 0.6971978545188904, ITG Loss: 9.661041259765625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [76 / 506]\n",
      "Total Loss: 13.335073471069336\n",
      "ITC Loss: 2.9552407264709473, ITM Loss: 0.6935469508171082, ITG Loss: 9.686285972595215\n",
      "**************************************************\n",
      "Epoch 1 : Iter [77 / 506]\n",
      "Total Loss: 13.154914855957031\n",
      "ITC Loss: 2.9108076095581055, ITM Loss: 0.6935533285140991, ITG Loss: 9.550554275512695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [78 / 506]\n",
      "Total Loss: 12.774388313293457\n",
      "ITC Loss: 2.940716505050659, ITM Loss: 0.6915774941444397, ITG Loss: 9.142094612121582\n",
      "**************************************************\n",
      "Epoch 1 : Iter [79 / 506]\n",
      "Total Loss: 12.966054916381836\n",
      "ITC Loss: 2.9652228355407715, ITM Loss: 0.6938049793243408, ITG Loss: 9.307026863098145\n",
      "**************************************************\n",
      "Epoch 1 : Iter [80 / 506]\n",
      "Total Loss: 13.07853889465332\n",
      "ITC Loss: 2.84004545211792, ITM Loss: 0.6939382553100586, ITG Loss: 9.544554710388184\n",
      "**************************************************\n",
      "Epoch 1 : Iter [81 / 506]\n",
      "Total Loss: 12.896890640258789\n",
      "ITC Loss: 2.8681488037109375, ITM Loss: 0.690316379070282, ITG Loss: 9.338425636291504\n",
      "**************************************************\n",
      "Epoch 1 : Iter [82 / 506]\n",
      "Total Loss: 13.603954315185547\n",
      "ITC Loss: 2.9437854290008545, ITM Loss: 0.6945520043373108, ITG Loss: 9.965617179870605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [83 / 506]\n",
      "Total Loss: 13.328413009643555\n",
      "ITC Loss: 2.909348964691162, ITM Loss: 0.6923320293426514, ITG Loss: 9.72673225402832\n",
      "**************************************************\n",
      "Epoch 1 : Iter [84 / 506]\n",
      "Total Loss: 14.016764640808105\n",
      "ITC Loss: 3.006047010421753, ITM Loss: 0.6913333535194397, ITG Loss: 10.319384574890137\n",
      "**************************************************\n",
      "Epoch 1 : Iter [85 / 506]\n",
      "Total Loss: 12.888565063476562\n",
      "ITC Loss: 2.877005100250244, ITM Loss: 0.6963182687759399, ITG Loss: 9.315241813659668\n",
      "**************************************************\n",
      "Epoch 1 : Iter [86 / 506]\n",
      "Total Loss: 13.260501861572266\n",
      "ITC Loss: 2.881409168243408, ITM Loss: 0.694002628326416, ITG Loss: 9.685090065002441\n",
      "**************************************************\n",
      "Epoch 1 : Iter [87 / 506]\n",
      "Total Loss: 12.719528198242188\n",
      "ITC Loss: 2.891510248184204, ITM Loss: 0.6919928789138794, ITG Loss: 9.136025428771973\n",
      "**************************************************\n",
      "Epoch 1 : Iter [88 / 506]\n",
      "Total Loss: 14.08406925201416\n",
      "ITC Loss: 3.0126025676727295, ITM Loss: 0.6968375444412231, ITG Loss: 10.374629020690918\n",
      "**************************************************\n",
      "Epoch 1 : Iter [89 / 506]\n",
      "Total Loss: 13.23116683959961\n",
      "ITC Loss: 2.947056293487549, ITM Loss: 0.6937937140464783, ITG Loss: 9.590316772460938\n",
      "**************************************************\n",
      "Epoch 1 : Iter [90 / 506]\n",
      "Total Loss: 13.084310531616211\n",
      "ITC Loss: 2.883479118347168, ITM Loss: 0.687944769859314, ITG Loss: 9.512887001037598\n",
      "**************************************************\n",
      "Epoch 1 : Iter [91 / 506]\n",
      "Total Loss: 12.910189628601074\n",
      "ITC Loss: 3.0254621505737305, ITM Loss: 0.6895419359207153, ITG Loss: 9.195185661315918\n",
      "**************************************************\n",
      "Epoch 1 : Iter [92 / 506]\n",
      "Total Loss: 13.041502952575684\n",
      "ITC Loss: 2.960935354232788, ITM Loss: 0.6924089193344116, ITG Loss: 9.388158798217773\n",
      "**************************************************\n",
      "Epoch 1 : Iter [93 / 506]\n",
      "Total Loss: 13.084030151367188\n",
      "ITC Loss: 2.909109115600586, ITM Loss: 0.6944475173950195, ITG Loss: 9.480473518371582\n",
      "**************************************************\n",
      "Epoch 1 : Iter [94 / 506]\n",
      "Total Loss: 12.36198902130127\n",
      "ITC Loss: 3.000576972961426, ITM Loss: 0.6898204684257507, ITG Loss: 8.671591758728027\n",
      "**************************************************\n",
      "Epoch 1 : Iter [95 / 506]\n",
      "Total Loss: 12.121376037597656\n",
      "ITC Loss: 2.9591307640075684, ITM Loss: 0.6937331557273865, ITG Loss: 8.468511581420898\n",
      "**************************************************\n",
      "Epoch 1 : Iter [96 / 506]\n",
      "Total Loss: 12.304938316345215\n",
      "ITC Loss: 2.885364055633545, ITM Loss: 0.6929324865341187, ITG Loss: 8.726641654968262\n",
      "**************************************************\n",
      "Epoch 1 : Iter [97 / 506]\n",
      "Total Loss: 13.038131713867188\n",
      "ITC Loss: 2.8193204402923584, ITM Loss: 0.6974809765815735, ITG Loss: 9.521330833435059\n",
      "**************************************************\n",
      "Epoch 1 : Iter [98 / 506]\n",
      "Total Loss: 12.732322692871094\n",
      "ITC Loss: 2.8430705070495605, ITM Loss: 0.6972271800041199, ITG Loss: 9.192025184631348\n",
      "**************************************************\n",
      "Epoch 1 : Iter [99 / 506]\n",
      "Total Loss: 12.387917518615723\n",
      "ITC Loss: 2.9429712295532227, ITM Loss: 0.6900519132614136, ITG Loss: 8.754894256591797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [100 / 506]\n",
      "Total Loss: 13.024394989013672\n",
      "ITC Loss: 2.936290740966797, ITM Loss: 0.6919376850128174, ITG Loss: 9.396166801452637\n",
      "**************************************************\n",
      "Epoch 1 : Iter [101 / 506]\n",
      "Total Loss: 11.947380065917969\n",
      "ITC Loss: 2.933786392211914, ITM Loss: 0.6918927431106567, ITG Loss: 8.321701049804688\n",
      "**************************************************\n",
      "Epoch 1 : Iter [102 / 506]\n",
      "Total Loss: 12.758829116821289\n",
      "ITC Loss: 2.9046363830566406, ITM Loss: 0.6947038769721985, ITG Loss: 9.159488677978516\n",
      "**************************************************\n",
      "Epoch 1 : Iter [103 / 506]\n",
      "Total Loss: 12.24370288848877\n",
      "ITC Loss: 2.8474278450012207, ITM Loss: 0.6978175044059753, ITG Loss: 8.698457717895508\n",
      "**************************************************\n",
      "Epoch 1 : Iter [104 / 506]\n",
      "Total Loss: 11.915179252624512\n",
      "ITC Loss: 2.905848741531372, ITM Loss: 0.6946940422058105, ITG Loss: 8.31463623046875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [105 / 506]\n",
      "Total Loss: 12.43519115447998\n",
      "ITC Loss: 2.946096420288086, ITM Loss: 0.6943950057029724, ITG Loss: 8.794699668884277\n",
      "**************************************************\n",
      "Epoch 1 : Iter [106 / 506]\n",
      "Total Loss: 12.447186470031738\n",
      "ITC Loss: 2.890784740447998, ITM Loss: 0.6936323046684265, ITG Loss: 8.86276912689209\n",
      "**************************************************\n",
      "Epoch 1 : Iter [107 / 506]\n",
      "Total Loss: 13.606660842895508\n",
      "ITC Loss: 3.523733377456665, ITM Loss: 0.6934050917625427, ITG Loss: 9.389522552490234\n",
      "**************************************************\n",
      "Epoch 1 : Iter [108 / 506]\n",
      "Total Loss: 12.36461067199707\n",
      "ITC Loss: 2.8358359336853027, ITM Loss: 0.697605550289154, ITG Loss: 8.831169128417969\n",
      "**************************************************\n",
      "Epoch 1 : Iter [109 / 506]\n",
      "Total Loss: 12.708464622497559\n",
      "ITC Loss: 3.0184435844421387, ITM Loss: 0.6960101127624512, ITG Loss: 8.994010925292969\n",
      "**************************************************\n",
      "Epoch 1 : Iter [110 / 506]\n",
      "Total Loss: 12.893364906311035\n",
      "ITC Loss: 2.985961437225342, ITM Loss: 0.6947882771492004, ITG Loss: 9.212615013122559\n",
      "**************************************************\n",
      "Epoch 1 : Iter [111 / 506]\n",
      "Total Loss: 12.489107131958008\n",
      "ITC Loss: 2.9063754081726074, ITM Loss: 0.6949241757392883, ITG Loss: 8.887807846069336\n",
      "**************************************************\n",
      "Epoch 1 : Iter [112 / 506]\n",
      "Total Loss: 11.616185188293457\n",
      "ITC Loss: 2.913566827774048, ITM Loss: 0.6942949891090393, ITG Loss: 8.008323669433594\n",
      "**************************************************\n",
      "Epoch 1 : Iter [113 / 506]\n",
      "Total Loss: 11.925044059753418\n",
      "ITC Loss: 2.9583261013031006, ITM Loss: 0.6929486989974976, ITG Loss: 8.27376937866211\n",
      "**************************************************\n",
      "Epoch 1 : Iter [114 / 506]\n",
      "Total Loss: 12.592567443847656\n",
      "ITC Loss: 3.0612709522247314, ITM Loss: 0.6924002170562744, ITG Loss: 8.838896751403809\n",
      "**************************************************\n",
      "Epoch 1 : Iter [115 / 506]\n",
      "Total Loss: 11.728813171386719\n",
      "ITC Loss: 2.7942333221435547, ITM Loss: 0.6937543153762817, ITG Loss: 8.240825653076172\n",
      "**************************************************\n",
      "Epoch 1 : Iter [116 / 506]\n",
      "Total Loss: 12.551197052001953\n",
      "ITC Loss: 3.13619327545166, ITM Loss: 0.6943278312683105, ITG Loss: 8.720675468444824\n",
      "**************************************************\n",
      "Epoch 1 : Iter [117 / 506]\n",
      "Total Loss: 11.930032730102539\n",
      "ITC Loss: 2.996708869934082, ITM Loss: 0.6975691914558411, ITG Loss: 8.23575496673584\n",
      "**************************************************\n",
      "Epoch 1 : Iter [118 / 506]\n",
      "Total Loss: 11.775761604309082\n",
      "ITC Loss: 2.952401638031006, ITM Loss: 0.6947703957557678, ITG Loss: 8.128589630126953\n",
      "**************************************************\n",
      "Epoch 1 : Iter [119 / 506]\n",
      "Total Loss: 12.584644317626953\n",
      "ITC Loss: 3.006565570831299, ITM Loss: 0.695331871509552, ITG Loss: 8.882746696472168\n",
      "**************************************************\n",
      "Epoch 1 : Iter [120 / 506]\n",
      "Total Loss: 11.930720329284668\n",
      "ITC Loss: 2.905459403991699, ITM Loss: 0.6945513486862183, ITG Loss: 8.330709457397461\n",
      "**************************************************\n",
      "Epoch 1 : Iter [121 / 506]\n",
      "Total Loss: 11.520818710327148\n",
      "ITC Loss: 2.9864187240600586, ITM Loss: 0.6938191056251526, ITG Loss: 7.840580463409424\n",
      "**************************************************\n",
      "Epoch 1 : Iter [122 / 506]\n",
      "Total Loss: 11.621261596679688\n",
      "ITC Loss: 2.9525413513183594, ITM Loss: 0.6938133239746094, ITG Loss: 7.9749064445495605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [123 / 506]\n",
      "Total Loss: 12.189266204833984\n",
      "ITC Loss: 2.943889856338501, ITM Loss: 0.6910526752471924, ITG Loss: 8.554323196411133\n",
      "**************************************************\n",
      "Epoch 1 : Iter [124 / 506]\n",
      "Total Loss: 12.053377151489258\n",
      "ITC Loss: 3.065347671508789, ITM Loss: 0.6945747137069702, ITG Loss: 8.29345417022705\n",
      "**************************************************\n",
      "Epoch 1 : Iter [125 / 506]\n",
      "Total Loss: 12.310941696166992\n",
      "ITC Loss: 2.9258639812469482, ITM Loss: 0.6974521279335022, ITG Loss: 8.687625885009766\n",
      "**************************************************\n",
      "Epoch 1 : Iter [126 / 506]\n",
      "Total Loss: 10.918806076049805\n",
      "ITC Loss: 3.0116777420043945, ITM Loss: 0.6898524761199951, ITG Loss: 7.217276096343994\n",
      "**************************************************\n",
      "Epoch 1 : Iter [127 / 506]\n",
      "Total Loss: 11.174844741821289\n",
      "ITC Loss: 2.9091804027557373, ITM Loss: 0.6908525228500366, ITG Loss: 7.5748114585876465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [128 / 506]\n",
      "Total Loss: 11.908578872680664\n",
      "ITC Loss: 2.84615421295166, ITM Loss: 0.6947157979011536, ITG Loss: 8.367709159851074\n",
      "**************************************************\n",
      "Epoch 1 : Iter [129 / 506]\n",
      "Total Loss: 12.67237377166748\n",
      "ITC Loss: 2.9027304649353027, ITM Loss: 0.6923811435699463, ITG Loss: 9.077261924743652\n",
      "**************************************************\n",
      "Epoch 1 : Iter [130 / 506]\n",
      "Total Loss: 10.804974555969238\n",
      "ITC Loss: 2.9140782356262207, ITM Loss: 0.6972297430038452, ITG Loss: 7.193666458129883\n",
      "**************************************************\n",
      "Epoch 1 : Iter [131 / 506]\n",
      "Total Loss: 11.547070503234863\n",
      "ITC Loss: 2.957414388656616, ITM Loss: 0.6924371123313904, ITG Loss: 7.897218704223633\n",
      "**************************************************\n",
      "Epoch 1 : Iter [132 / 506]\n",
      "Total Loss: 11.099003791809082\n",
      "ITC Loss: 2.849350929260254, ITM Loss: 0.697038471698761, ITG Loss: 7.552614688873291\n",
      "**************************************************\n",
      "Epoch 1 : Iter [133 / 506]\n",
      "Total Loss: 11.776382446289062\n",
      "ITC Loss: 2.966153144836426, ITM Loss: 0.6909980177879333, ITG Loss: 8.119231224060059\n",
      "**************************************************\n",
      "Epoch 1 : Iter [134 / 506]\n",
      "Total Loss: 11.321669578552246\n",
      "ITC Loss: 2.891465663909912, ITM Loss: 0.695464551448822, ITG Loss: 7.734739303588867\n",
      "**************************************************\n",
      "Epoch 1 : Iter [135 / 506]\n",
      "Total Loss: 12.00291633605957\n",
      "ITC Loss: 2.9332473278045654, ITM Loss: 0.6947611570358276, ITG Loss: 8.374907493591309\n",
      "**************************************************\n",
      "Epoch 1 : Iter [136 / 506]\n",
      "Total Loss: 11.17648696899414\n",
      "ITC Loss: 2.885890483856201, ITM Loss: 0.6929141879081726, ITG Loss: 7.597682476043701\n",
      "**************************************************\n",
      "Epoch 1 : Iter [137 / 506]\n",
      "Total Loss: 12.373685836791992\n",
      "ITC Loss: 2.852479934692383, ITM Loss: 0.6904751062393188, ITG Loss: 8.830731391906738\n",
      "**************************************************\n",
      "Epoch 1 : Iter [138 / 506]\n",
      "Total Loss: 11.230369567871094\n",
      "ITC Loss: 2.8737382888793945, ITM Loss: 0.6939257979393005, ITG Loss: 7.662705898284912\n",
      "**************************************************\n",
      "Epoch 1 : Iter [139 / 506]\n",
      "Total Loss: 12.022104263305664\n",
      "ITC Loss: 2.8997621536254883, ITM Loss: 0.6968428492546082, ITG Loss: 8.425498962402344\n",
      "**************************************************\n",
      "Epoch 1 : Iter [140 / 506]\n",
      "Total Loss: 10.559078216552734\n",
      "ITC Loss: 2.9359614849090576, ITM Loss: 0.6924657821655273, ITG Loss: 6.9306511878967285\n",
      "**************************************************\n",
      "Epoch 1 : Iter [141 / 506]\n",
      "Total Loss: 11.028495788574219\n",
      "ITC Loss: 2.8511476516723633, ITM Loss: 0.6904397010803223, ITG Loss: 7.486907958984375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [142 / 506]\n",
      "Total Loss: 11.335637092590332\n",
      "ITC Loss: 2.8915488719940186, ITM Loss: 0.6923694014549255, ITG Loss: 7.751718521118164\n",
      "**************************************************\n",
      "Epoch 1 : Iter [143 / 506]\n",
      "Total Loss: 10.460189819335938\n",
      "ITC Loss: 2.8953285217285156, ITM Loss: 0.6938673853874207, ITG Loss: 6.8709940910339355\n",
      "**************************************************\n",
      "Epoch 1 : Iter [144 / 506]\n",
      "Total Loss: 11.266670227050781\n",
      "ITC Loss: 2.82865047454834, ITM Loss: 0.6921082735061646, ITG Loss: 7.745912075042725\n",
      "**************************************************\n",
      "Epoch 1 : Iter [145 / 506]\n",
      "Total Loss: 10.635736465454102\n",
      "ITC Loss: 2.926642656326294, ITM Loss: 0.6947559714317322, ITG Loss: 7.01433801651001\n",
      "**************************************************\n",
      "Epoch 1 : Iter [146 / 506]\n",
      "Total Loss: 10.080799102783203\n",
      "ITC Loss: 2.9428672790527344, ITM Loss: 0.6936981081962585, ITG Loss: 6.4442338943481445\n",
      "**************************************************\n",
      "Epoch 1 : Iter [147 / 506]\n",
      "Total Loss: 11.600312232971191\n",
      "ITC Loss: 2.8691234588623047, ITM Loss: 0.6984604001045227, ITG Loss: 8.03272819519043\n",
      "**************************************************\n",
      "Epoch 1 : Iter [148 / 506]\n",
      "Total Loss: 11.381149291992188\n",
      "ITC Loss: 2.9523916244506836, ITM Loss: 0.6935901045799255, ITG Loss: 7.735167503356934\n",
      "**************************************************\n",
      "Epoch 1 : Iter [149 / 506]\n",
      "Total Loss: 10.962667465209961\n",
      "ITC Loss: 2.909989356994629, ITM Loss: 0.6903954148292542, ITG Loss: 7.3622822761535645\n",
      "**************************************************\n",
      "Epoch 1 : Iter [150 / 506]\n",
      "Total Loss: 9.802068710327148\n",
      "ITC Loss: 2.913393974304199, ITM Loss: 0.6920021772384644, ITG Loss: 6.1966729164123535\n",
      "**************************************************\n",
      "Epoch 1 : Iter [151 / 506]\n",
      "Total Loss: 11.13568115234375\n",
      "ITC Loss: 2.9564075469970703, ITM Loss: 0.696377158164978, ITG Loss: 7.482896327972412\n",
      "**************************************************\n",
      "Epoch 1 : Iter [152 / 506]\n",
      "Total Loss: 11.265159606933594\n",
      "ITC Loss: 2.8782970905303955, ITM Loss: 0.6917829513549805, ITG Loss: 7.695079326629639\n",
      "**************************************************\n",
      "Epoch 1 : Iter [153 / 506]\n",
      "Total Loss: 10.624691009521484\n",
      "ITC Loss: 2.8538715839385986, ITM Loss: 0.6946684122085571, ITG Loss: 7.076151371002197\n",
      "**************************************************\n",
      "Epoch 1 : Iter [154 / 506]\n",
      "Total Loss: 11.262832641601562\n",
      "ITC Loss: 2.9235620498657227, ITM Loss: 0.6935415863990784, ITG Loss: 7.645729064941406\n",
      "**************************************************\n",
      "Epoch 1 : Iter [155 / 506]\n",
      "Total Loss: 10.856059074401855\n",
      "ITC Loss: 2.846651554107666, ITM Loss: 0.6934282779693604, ITG Loss: 7.31597900390625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [156 / 506]\n",
      "Total Loss: 10.505298614501953\n",
      "ITC Loss: 2.8551807403564453, ITM Loss: 0.6925996541976929, ITG Loss: 6.957518100738525\n",
      "**************************************************\n",
      "Epoch 1 : Iter [157 / 506]\n",
      "Total Loss: 11.091055870056152\n",
      "ITC Loss: 2.940420627593994, ITM Loss: 0.6927654147148132, ITG Loss: 7.457869529724121\n",
      "**************************************************\n",
      "Epoch 1 : Iter [158 / 506]\n",
      "Total Loss: 11.72667121887207\n",
      "ITC Loss: 2.9218239784240723, ITM Loss: 0.6914883852005005, ITG Loss: 8.113358497619629\n",
      "**************************************************\n",
      "Epoch 1 : Iter [159 / 506]\n",
      "Total Loss: 10.633861541748047\n",
      "ITC Loss: 2.883312225341797, ITM Loss: 0.6895447373390198, ITG Loss: 7.061004161834717\n",
      "**************************************************\n",
      "Epoch 1 : Iter [160 / 506]\n",
      "Total Loss: 10.875625610351562\n",
      "ITC Loss: 2.9304776191711426, ITM Loss: 0.6931192278862, ITG Loss: 7.252028465270996\n",
      "**************************************************\n",
      "Epoch 1 : Iter [161 / 506]\n",
      "Total Loss: 9.490235328674316\n",
      "ITC Loss: 2.8211703300476074, ITM Loss: 0.6944454908370972, ITG Loss: 5.974619388580322\n",
      "**************************************************\n",
      "Epoch 1 : Iter [162 / 506]\n",
      "Total Loss: 10.53554916381836\n",
      "ITC Loss: 2.904926300048828, ITM Loss: 0.6907188892364502, ITG Loss: 6.939903736114502\n",
      "**************************************************\n",
      "Epoch 1 : Iter [163 / 506]\n",
      "Total Loss: 10.67774772644043\n",
      "ITC Loss: 2.820730686187744, ITM Loss: 0.6933982968330383, ITG Loss: 7.163619041442871\n",
      "**************************************************\n",
      "Epoch 1 : Iter [164 / 506]\n",
      "Total Loss: 10.76685905456543\n",
      "ITC Loss: 2.961266040802002, ITM Loss: 0.6934852600097656, ITG Loss: 7.11210823059082\n",
      "**************************************************\n",
      "Epoch 1 : Iter [165 / 506]\n",
      "Total Loss: 11.516571044921875\n",
      "ITC Loss: 2.857606887817383, ITM Loss: 0.6912808418273926, ITG Loss: 7.967683792114258\n",
      "**************************************************\n",
      "Epoch 1 : Iter [166 / 506]\n",
      "Total Loss: 11.788652420043945\n",
      "ITC Loss: 2.902101516723633, ITM Loss: 0.6948083639144897, ITG Loss: 8.191741943359375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [167 / 506]\n",
      "Total Loss: 11.258946418762207\n",
      "ITC Loss: 2.87284255027771, ITM Loss: 0.6934632658958435, ITG Loss: 7.692640781402588\n",
      "**************************************************\n",
      "Epoch 1 : Iter [168 / 506]\n",
      "Total Loss: 11.219889640808105\n",
      "ITC Loss: 2.836735248565674, ITM Loss: 0.6945726275444031, ITG Loss: 7.688581466674805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [169 / 506]\n",
      "Total Loss: 9.814594268798828\n",
      "ITC Loss: 2.8271336555480957, ITM Loss: 0.6923186182975769, ITG Loss: 6.295141696929932\n",
      "**************************************************\n",
      "Epoch 1 : Iter [170 / 506]\n",
      "Total Loss: 10.32526683807373\n",
      "ITC Loss: 2.8636698722839355, ITM Loss: 0.692147433757782, ITG Loss: 6.769449234008789\n",
      "**************************************************\n",
      "Epoch 1 : Iter [171 / 506]\n",
      "Total Loss: 11.133808135986328\n",
      "ITC Loss: 2.8216638565063477, ITM Loss: 0.6957405209541321, ITG Loss: 7.616403579711914\n",
      "**************************************************\n",
      "Epoch 1 : Iter [172 / 506]\n",
      "Total Loss: 9.803504943847656\n",
      "ITC Loss: 2.8085737228393555, ITM Loss: 0.6923474073410034, ITG Loss: 6.302583694458008\n",
      "**************************************************\n",
      "Epoch 1 : Iter [173 / 506]\n",
      "Total Loss: 10.358430862426758\n",
      "ITC Loss: 2.881387710571289, ITM Loss: 0.6941874027252197, ITG Loss: 6.78285551071167\n",
      "**************************************************\n",
      "Epoch 1 : Iter [174 / 506]\n",
      "Total Loss: 10.838922500610352\n",
      "ITC Loss: 2.912207841873169, ITM Loss: 0.6965326070785522, ITG Loss: 7.23018217086792\n",
      "**************************************************\n",
      "Epoch 1 : Iter [175 / 506]\n",
      "Total Loss: 10.627412796020508\n",
      "ITC Loss: 2.927460193634033, ITM Loss: 0.6935387253761292, ITG Loss: 7.006413459777832\n",
      "**************************************************\n",
      "Epoch 1 : Iter [176 / 506]\n",
      "Total Loss: 9.585168838500977\n",
      "ITC Loss: 2.8398611545562744, ITM Loss: 0.6923668384552002, ITG Loss: 6.052940845489502\n",
      "**************************************************\n",
      "Epoch 1 : Iter [177 / 506]\n",
      "Total Loss: 10.541219711303711\n",
      "ITC Loss: 2.8904478549957275, ITM Loss: 0.6941433548927307, ITG Loss: 6.956628799438477\n",
      "**************************************************\n",
      "Epoch 1 : Iter [178 / 506]\n",
      "Total Loss: 10.450813293457031\n",
      "ITC Loss: 2.8441221714019775, ITM Loss: 0.6927575469017029, ITG Loss: 6.913933277130127\n",
      "**************************************************\n",
      "Epoch 1 : Iter [179 / 506]\n",
      "Total Loss: 10.656694412231445\n",
      "ITC Loss: 2.76682710647583, ITM Loss: 0.6926045417785645, ITG Loss: 7.197262763977051\n",
      "**************************************************\n",
      "Epoch 1 : Iter [180 / 506]\n",
      "Total Loss: 10.402103424072266\n",
      "ITC Loss: 2.881911277770996, ITM Loss: 0.6934065222740173, ITG Loss: 6.826785087585449\n",
      "**************************************************\n",
      "Epoch 1 : Iter [181 / 506]\n",
      "Total Loss: 10.312744140625\n",
      "ITC Loss: 2.8801560401916504, ITM Loss: 0.6934456825256348, ITG Loss: 6.739142417907715\n",
      "**************************************************\n",
      "Epoch 1 : Iter [182 / 506]\n",
      "Total Loss: 10.71875286102295\n",
      "ITC Loss: 2.944817066192627, ITM Loss: 0.6936285495758057, ITG Loss: 7.0803070068359375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [183 / 506]\n",
      "Total Loss: 9.601953506469727\n",
      "ITC Loss: 2.84877347946167, ITM Loss: 0.6945323944091797, ITG Loss: 6.058647155761719\n",
      "**************************************************\n",
      "Epoch 1 : Iter [184 / 506]\n",
      "Total Loss: 10.140602111816406\n",
      "ITC Loss: 2.889073610305786, ITM Loss: 0.6925221085548401, ITG Loss: 6.559006690979004\n",
      "**************************************************\n",
      "Epoch 1 : Iter [185 / 506]\n",
      "Total Loss: 10.690186500549316\n",
      "ITC Loss: 2.9106967449188232, ITM Loss: 0.6960502862930298, ITG Loss: 7.083439350128174\n",
      "**************************************************\n",
      "Epoch 1 : Iter [186 / 506]\n",
      "Total Loss: 10.599401473999023\n",
      "ITC Loss: 2.9853744506835938, ITM Loss: 0.6916782259941101, ITG Loss: 6.922348976135254\n",
      "**************************************************\n",
      "Epoch 1 : Iter [187 / 506]\n",
      "Total Loss: 11.36861801147461\n",
      "ITC Loss: 2.910914897918701, ITM Loss: 0.692371129989624, ITG Loss: 7.765332221984863\n",
      "**************************************************\n",
      "Epoch 1 : Iter [188 / 506]\n",
      "Total Loss: 10.438796997070312\n",
      "ITC Loss: 2.9022018909454346, ITM Loss: 0.6882384419441223, ITG Loss: 6.848356246948242\n",
      "**************************************************\n",
      "Epoch 1 : Iter [189 / 506]\n",
      "Total Loss: 10.904627799987793\n",
      "ITC Loss: 2.904181480407715, ITM Loss: 0.6924895644187927, ITG Loss: 7.307956695556641\n",
      "**************************************************\n",
      "Epoch 1 : Iter [190 / 506]\n",
      "Total Loss: 11.026327133178711\n",
      "ITC Loss: 2.8113534450531006, ITM Loss: 0.6938021779060364, ITG Loss: 7.5211710929870605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [191 / 506]\n",
      "Total Loss: 10.181614875793457\n",
      "ITC Loss: 2.9202141761779785, ITM Loss: 0.6911126375198364, ITG Loss: 6.570288181304932\n",
      "**************************************************\n",
      "Epoch 1 : Iter [192 / 506]\n",
      "Total Loss: 10.840025901794434\n",
      "ITC Loss: 2.860408306121826, ITM Loss: 0.6968071460723877, ITG Loss: 7.282810211181641\n",
      "**************************************************\n",
      "Epoch 1 : Iter [193 / 506]\n",
      "Total Loss: 10.718719482421875\n",
      "ITC Loss: 2.861154556274414, ITM Loss: 0.6956368088722229, ITG Loss: 7.161928176879883\n",
      "**************************************************\n",
      "Epoch 1 : Iter [194 / 506]\n",
      "Total Loss: 10.013887405395508\n",
      "ITC Loss: 2.8059167861938477, ITM Loss: 0.6967066526412964, ITG Loss: 6.511263847351074\n",
      "**************************************************\n",
      "Epoch 1 : Iter [195 / 506]\n",
      "Total Loss: 10.850542068481445\n",
      "ITC Loss: 2.8809542655944824, ITM Loss: 0.6939077377319336, ITG Loss: 7.2756805419921875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [196 / 506]\n",
      "Total Loss: 10.139181137084961\n",
      "ITC Loss: 2.900087833404541, ITM Loss: 0.6944960951805115, ITG Loss: 6.544597148895264\n",
      "**************************************************\n",
      "Epoch 1 : Iter [197 / 506]\n",
      "Total Loss: 9.89405632019043\n",
      "ITC Loss: 2.9341139793395996, ITM Loss: 0.690603494644165, ITG Loss: 6.269339084625244\n",
      "**************************************************\n",
      "Epoch 1 : Iter [198 / 506]\n",
      "Total Loss: 9.559676170349121\n",
      "ITC Loss: 2.90260648727417, ITM Loss: 0.69410640001297, ITG Loss: 5.962963104248047\n",
      "**************************************************\n",
      "Epoch 1 : Iter [199 / 506]\n",
      "Total Loss: 9.924642562866211\n",
      "ITC Loss: 2.885000228881836, ITM Loss: 0.694357693195343, ITG Loss: 6.345284461975098\n",
      "**************************************************\n",
      "Epoch 1 : Iter [200 / 506]\n",
      "Total Loss: 10.968074798583984\n",
      "ITC Loss: 2.811285972595215, ITM Loss: 0.6883366703987122, ITG Loss: 7.468451976776123\n",
      "**************************************************\n",
      "Epoch 1 : Iter [201 / 506]\n",
      "Total Loss: 10.342771530151367\n",
      "ITC Loss: 2.8686981201171875, ITM Loss: 0.6942581534385681, ITG Loss: 6.779814720153809\n",
      "**************************************************\n",
      "Epoch 1 : Iter [202 / 506]\n",
      "Total Loss: 10.405259132385254\n",
      "ITC Loss: 2.849083423614502, ITM Loss: 0.6953789591789246, ITG Loss: 6.8607964515686035\n",
      "**************************************************\n",
      "Epoch 1 : Iter [203 / 506]\n",
      "Total Loss: 9.672867774963379\n",
      "ITC Loss: 2.8551900386810303, ITM Loss: 0.695479154586792, ITG Loss: 6.122198581695557\n",
      "**************************************************\n",
      "Epoch 1 : Iter [204 / 506]\n",
      "Total Loss: 10.400945663452148\n",
      "ITC Loss: 2.860513210296631, ITM Loss: 0.6977035403251648, ITG Loss: 6.842729091644287\n",
      "**************************************************\n",
      "Epoch 1 : Iter [205 / 506]\n",
      "Total Loss: 10.312711715698242\n",
      "ITC Loss: 2.858151912689209, ITM Loss: 0.6909646987915039, ITG Loss: 6.763595104217529\n",
      "**************************************************\n",
      "Epoch 1 : Iter [206 / 506]\n",
      "Total Loss: 10.959756851196289\n",
      "ITC Loss: 2.9274237155914307, ITM Loss: 0.6949670910835266, ITG Loss: 7.337365627288818\n",
      "**************************************************\n",
      "Epoch 1 : Iter [207 / 506]\n",
      "Total Loss: 9.937536239624023\n",
      "ITC Loss: 2.8979849815368652, ITM Loss: 0.6917903423309326, ITG Loss: 6.347761154174805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [208 / 506]\n",
      "Total Loss: 10.031200408935547\n",
      "ITC Loss: 2.846789836883545, ITM Loss: 0.6922369003295898, ITG Loss: 6.49217414855957\n",
      "**************************************************\n",
      "Epoch 1 : Iter [209 / 506]\n",
      "Total Loss: 10.304001808166504\n",
      "ITC Loss: 2.9089605808258057, ITM Loss: 0.693145751953125, ITG Loss: 6.701895713806152\n",
      "**************************************************\n",
      "Epoch 1 : Iter [210 / 506]\n",
      "Total Loss: 10.715801239013672\n",
      "ITC Loss: 2.8793296813964844, ITM Loss: 0.6921032667160034, ITG Loss: 7.144367694854736\n",
      "**************************************************\n",
      "Epoch 1 : Iter [211 / 506]\n",
      "Total Loss: 9.490764617919922\n",
      "ITC Loss: 2.849900245666504, ITM Loss: 0.6940904855728149, ITG Loss: 5.946773529052734\n",
      "**************************************************\n",
      "Epoch 1 : Iter [212 / 506]\n",
      "Total Loss: 9.853796005249023\n",
      "ITC Loss: 2.9208364486694336, ITM Loss: 0.6910429000854492, ITG Loss: 6.241917133331299\n",
      "**************************************************\n",
      "Epoch 1 : Iter [213 / 506]\n",
      "Total Loss: 10.196714401245117\n",
      "ITC Loss: 2.8995249271392822, ITM Loss: 0.6915665864944458, ITG Loss: 6.605623245239258\n",
      "**************************************************\n",
      "Epoch 1 : Iter [214 / 506]\n",
      "Total Loss: 10.9957857131958\n",
      "ITC Loss: 2.899425506591797, ITM Loss: 0.6977364420890808, ITG Loss: 7.398623943328857\n",
      "**************************************************\n",
      "Epoch 1 : Iter [215 / 506]\n",
      "Total Loss: 10.572745323181152\n",
      "ITC Loss: 2.9298789501190186, ITM Loss: 0.6937198042869568, ITG Loss: 6.949146747589111\n",
      "**************************************************\n",
      "Epoch 1 : Iter [216 / 506]\n",
      "Total Loss: 9.642046928405762\n",
      "ITC Loss: 2.8520641326904297, ITM Loss: 0.6908155679702759, ITG Loss: 6.099167346954346\n",
      "**************************************************\n",
      "Epoch 1 : Iter [217 / 506]\n",
      "Total Loss: 10.536291122436523\n",
      "ITC Loss: 2.885918617248535, ITM Loss: 0.6955887675285339, ITG Loss: 6.954783916473389\n",
      "**************************************************\n",
      "Epoch 1 : Iter [218 / 506]\n",
      "Total Loss: 10.157327651977539\n",
      "ITC Loss: 2.865879774093628, ITM Loss: 0.6937474012374878, ITG Loss: 6.597700595855713\n",
      "**************************************************\n",
      "Epoch 1 : Iter [219 / 506]\n",
      "Total Loss: 9.686284065246582\n",
      "ITC Loss: 2.8140769004821777, ITM Loss: 0.6904283165931702, ITG Loss: 6.181778907775879\n",
      "**************************************************\n",
      "Epoch 1 : Iter [220 / 506]\n",
      "Total Loss: 10.023128509521484\n",
      "ITC Loss: 2.8641576766967773, ITM Loss: 0.6941138505935669, ITG Loss: 6.464857578277588\n",
      "**************************************************\n",
      "Epoch 1 : Iter [221 / 506]\n",
      "Total Loss: 10.125612258911133\n",
      "ITC Loss: 2.8672919273376465, ITM Loss: 0.695286750793457, ITG Loss: 6.563033580780029\n",
      "**************************************************\n",
      "Epoch 1 : Iter [222 / 506]\n",
      "Total Loss: 9.833578109741211\n",
      "ITC Loss: 2.858210325241089, ITM Loss: 0.6941848993301392, ITG Loss: 6.281182765960693\n",
      "**************************************************\n",
      "Epoch 1 : Iter [223 / 506]\n",
      "Total Loss: 10.333150863647461\n",
      "ITC Loss: 2.8505136966705322, ITM Loss: 0.6952184438705444, ITG Loss: 6.787418842315674\n",
      "**************************************************\n",
      "Epoch 1 : Iter [224 / 506]\n",
      "Total Loss: 9.721936225891113\n",
      "ITC Loss: 2.830474853515625, ITM Loss: 0.6952349543571472, ITG Loss: 6.196226119995117\n",
      "**************************************************\n",
      "Epoch 1 : Iter [225 / 506]\n",
      "Total Loss: 10.244450569152832\n",
      "ITC Loss: 2.8611369132995605, ITM Loss: 0.6936790943145752, ITG Loss: 6.689634799957275\n",
      "**************************************************\n",
      "Epoch 1 : Iter [226 / 506]\n",
      "Total Loss: 10.581439018249512\n",
      "ITC Loss: 2.8608148097991943, ITM Loss: 0.6950807571411133, ITG Loss: 7.025543212890625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [227 / 506]\n",
      "Total Loss: 9.805543899536133\n",
      "ITC Loss: 2.8620171546936035, ITM Loss: 0.693050742149353, ITG Loss: 6.250475883483887\n",
      "**************************************************\n",
      "Epoch 1 : Iter [228 / 506]\n",
      "Total Loss: 10.532394409179688\n",
      "ITC Loss: 2.8073782920837402, ITM Loss: 0.6903828978538513, ITG Loss: 7.034633636474609\n",
      "**************************************************\n",
      "Epoch 1 : Iter [229 / 506]\n",
      "Total Loss: 10.74238395690918\n",
      "ITC Loss: 2.8651480674743652, ITM Loss: 0.6960470080375671, ITG Loss: 7.181188583374023\n",
      "**************************************************\n",
      "Epoch 1 : Iter [230 / 506]\n",
      "Total Loss: 9.382965087890625\n",
      "ITC Loss: 2.8290910720825195, ITM Loss: 0.6917699575424194, ITG Loss: 5.862104415893555\n",
      "**************************************************\n",
      "Epoch 1 : Iter [231 / 506]\n",
      "Total Loss: 9.784242630004883\n",
      "ITC Loss: 2.872161865234375, ITM Loss: 0.690195620059967, ITG Loss: 6.2218852043151855\n",
      "**************************************************\n",
      "Epoch 1 : Iter [232 / 506]\n",
      "Total Loss: 10.57449722290039\n",
      "ITC Loss: 2.861473560333252, ITM Loss: 0.6951206922531128, ITG Loss: 7.017902374267578\n",
      "**************************************************\n",
      "Epoch 1 : Iter [233 / 506]\n",
      "Total Loss: 9.105785369873047\n",
      "ITC Loss: 2.9358036518096924, ITM Loss: 0.6964886784553528, ITG Loss: 5.473492622375488\n",
      "**************************************************\n",
      "Epoch 1 : Iter [234 / 506]\n",
      "Total Loss: 10.452533721923828\n",
      "ITC Loss: 2.885852813720703, ITM Loss: 0.6921701431274414, ITG Loss: 6.874511241912842\n",
      "**************************************************\n",
      "Epoch 1 : Iter [235 / 506]\n",
      "Total Loss: 9.367650985717773\n",
      "ITC Loss: 2.8866236209869385, ITM Loss: 0.694930911064148, ITG Loss: 5.786096096038818\n",
      "**************************************************\n",
      "Epoch 1 : Iter [236 / 506]\n",
      "Total Loss: 9.61026668548584\n",
      "ITC Loss: 2.817412853240967, ITM Loss: 0.6948922276496887, ITG Loss: 6.09796142578125\n",
      "**************************************************\n",
      "Epoch 1 : Iter [237 / 506]\n",
      "Total Loss: 10.188833236694336\n",
      "ITC Loss: 2.899336814880371, ITM Loss: 0.6944507360458374, ITG Loss: 6.59504508972168\n",
      "**************************************************\n",
      "Epoch 1 : Iter [238 / 506]\n",
      "Total Loss: 9.970340728759766\n",
      "ITC Loss: 2.775331974029541, ITM Loss: 0.6909793019294739, ITG Loss: 6.504029750823975\n",
      "**************************************************\n",
      "Epoch 1 : Iter [239 / 506]\n",
      "Total Loss: 10.2392578125\n",
      "ITC Loss: 2.8444480895996094, ITM Loss: 0.6936716437339783, ITG Loss: 6.701138019561768\n",
      "**************************************************\n",
      "Epoch 1 : Iter [240 / 506]\n",
      "Total Loss: 10.170149803161621\n",
      "ITC Loss: 2.8421006202697754, ITM Loss: 0.6939191818237305, ITG Loss: 6.634130001068115\n",
      "**************************************************\n",
      "Epoch 1 : Iter [241 / 506]\n",
      "Total Loss: 9.480928421020508\n",
      "ITC Loss: 2.924625873565674, ITM Loss: 0.6946865320205688, ITG Loss: 5.8616156578063965\n",
      "**************************************************\n",
      "Epoch 1 : Iter [242 / 506]\n",
      "Total Loss: 10.130729675292969\n",
      "ITC Loss: 2.802173614501953, ITM Loss: 0.6945293545722961, ITG Loss: 6.634027004241943\n",
      "**************************************************\n",
      "Epoch 1 : Iter [243 / 506]\n",
      "Total Loss: 9.931015014648438\n",
      "ITC Loss: 2.801990509033203, ITM Loss: 0.6943526268005371, ITG Loss: 6.434671878814697\n",
      "**************************************************\n",
      "Epoch 1 : Iter [244 / 506]\n",
      "Total Loss: 10.3742094039917\n",
      "ITC Loss: 2.829463481903076, ITM Loss: 0.6905655264854431, ITG Loss: 6.854180335998535\n",
      "**************************************************\n",
      "Epoch 1 : Iter [245 / 506]\n",
      "Total Loss: 9.200615882873535\n",
      "ITC Loss: 2.8843846321105957, ITM Loss: 0.6939411163330078, ITG Loss: 5.622290134429932\n",
      "**************************************************\n",
      "Epoch 1 : Iter [246 / 506]\n",
      "Total Loss: 10.545846939086914\n",
      "ITC Loss: 2.897158145904541, ITM Loss: 0.6941383481025696, ITG Loss: 6.954550266265869\n",
      "**************************************************\n",
      "Epoch 1 : Iter [247 / 506]\n",
      "Total Loss: 10.207767486572266\n",
      "ITC Loss: 2.8658270835876465, ITM Loss: 0.6917403340339661, ITG Loss: 6.650199890136719\n",
      "**************************************************\n",
      "Epoch 1 : Iter [248 / 506]\n",
      "Total Loss: 10.102155685424805\n",
      "ITC Loss: 2.8487071990966797, ITM Loss: 0.6918520927429199, ITG Loss: 6.561596870422363\n",
      "**************************************************\n",
      "Epoch 1 : Iter [249 / 506]\n",
      "Total Loss: 9.960113525390625\n",
      "ITC Loss: 2.829958438873291, ITM Loss: 0.6925413608551025, ITG Loss: 6.4376139640808105\n",
      "**************************************************\n",
      "Epoch 1 : Iter [250 / 506]\n",
      "Total Loss: 10.282278060913086\n",
      "ITC Loss: 2.931680202484131, ITM Loss: 0.6921393275260925, ITG Loss: 6.658458709716797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [251 / 506]\n",
      "Total Loss: 9.890718460083008\n",
      "ITC Loss: 2.819899320602417, ITM Loss: 0.6929621696472168, ITG Loss: 6.377857208251953\n",
      "**************************************************\n",
      "Epoch 1 : Iter [252 / 506]\n",
      "Total Loss: 9.590075492858887\n",
      "ITC Loss: 2.890023946762085, ITM Loss: 0.6942569613456726, ITG Loss: 6.005794525146484\n",
      "**************************************************\n",
      "Epoch 1 : Iter [253 / 506]\n",
      "Total Loss: 9.68639850616455\n",
      "ITC Loss: 2.8801960945129395, ITM Loss: 0.694016695022583, ITG Loss: 6.112185478210449\n",
      "**************************************************\n",
      "Epoch 1 : Iter [254 / 506]\n",
      "Total Loss: 10.122060775756836\n",
      "ITC Loss: 2.8178601264953613, ITM Loss: 0.6900321841239929, ITG Loss: 6.614168167114258\n",
      "**************************************************\n",
      "Epoch 1 : Iter [255 / 506]\n",
      "Total Loss: 10.414318084716797\n",
      "ITC Loss: 2.86592435836792, ITM Loss: 0.6943659782409668, ITG Loss: 6.85402774810791\n",
      "**************************************************\n",
      "Epoch 1 : Iter [256 / 506]\n",
      "Total Loss: 9.551074981689453\n",
      "ITC Loss: 2.84787654876709, ITM Loss: 0.6941254138946533, ITG Loss: 6.009073257446289\n",
      "**************************************************\n",
      "Epoch 1 : Iter [257 / 506]\n",
      "Total Loss: 10.224893569946289\n",
      "ITC Loss: 2.855567455291748, ITM Loss: 0.6945993304252625, ITG Loss: 6.674726963043213\n",
      "**************************************************\n",
      "Epoch 1 : Iter [258 / 506]\n",
      "Total Loss: 9.86746597290039\n",
      "ITC Loss: 2.8629579544067383, ITM Loss: 0.6961755752563477, ITG Loss: 6.3083319664001465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [259 / 506]\n",
      "Total Loss: 9.352489471435547\n",
      "ITC Loss: 2.803196430206299, ITM Loss: 0.6927986145019531, ITG Loss: 5.856494426727295\n",
      "**************************************************\n",
      "Epoch 1 : Iter [260 / 506]\n",
      "Total Loss: 10.200700759887695\n",
      "ITC Loss: 2.862931966781616, ITM Loss: 0.6938073635101318, ITG Loss: 6.643961429595947\n",
      "**************************************************\n",
      "Epoch 1 : Iter [261 / 506]\n",
      "Total Loss: 9.764486312866211\n",
      "ITC Loss: 2.845569610595703, ITM Loss: 0.6950727701187134, ITG Loss: 6.223843574523926\n",
      "**************************************************\n",
      "Epoch 1 : Iter [262 / 506]\n",
      "Total Loss: 10.376625061035156\n",
      "ITC Loss: 2.8714938163757324, ITM Loss: 0.6920910477638245, ITG Loss: 6.813039779663086\n",
      "**************************************************\n",
      "Epoch 1 : Iter [263 / 506]\n",
      "Total Loss: 10.350138664245605\n",
      "ITC Loss: 2.8240418434143066, ITM Loss: 0.6931465864181519, ITG Loss: 6.832950115203857\n",
      "**************************************************\n",
      "Epoch 1 : Iter [264 / 506]\n",
      "Total Loss: 9.726543426513672\n",
      "ITC Loss: 2.843050003051758, ITM Loss: 0.6935203075408936, ITG Loss: 6.1899733543396\n",
      "**************************************************\n",
      "Epoch 1 : Iter [265 / 506]\n",
      "Total Loss: 9.894937515258789\n",
      "ITC Loss: 2.826010227203369, ITM Loss: 0.6939224004745483, ITG Loss: 6.375004768371582\n",
      "**************************************************\n",
      "Epoch 1 : Iter [266 / 506]\n",
      "Total Loss: 9.978604316711426\n",
      "ITC Loss: 2.8408591747283936, ITM Loss: 0.6918641924858093, ITG Loss: 6.445880889892578\n",
      "**************************************************\n",
      "Epoch 1 : Iter [267 / 506]\n",
      "Total Loss: 10.806158065795898\n",
      "ITC Loss: 2.8390607833862305, ITM Loss: 0.6927201151847839, ITG Loss: 7.27437686920166\n",
      "**************************************************\n",
      "Epoch 1 : Iter [268 / 506]\n",
      "Total Loss: 9.870627403259277\n",
      "ITC Loss: 2.839231252670288, ITM Loss: 0.6934411525726318, ITG Loss: 6.337954998016357\n",
      "**************************************************\n",
      "Epoch 1 : Iter [269 / 506]\n",
      "Total Loss: 9.322919845581055\n",
      "ITC Loss: 2.842151641845703, ITM Loss: 0.6945949792861938, ITG Loss: 5.786173343658447\n",
      "**************************************************\n",
      "Epoch 1 : Iter [270 / 506]\n",
      "Total Loss: 9.807097434997559\n",
      "ITC Loss: 2.884873628616333, ITM Loss: 0.6927548050880432, ITG Loss: 6.229468822479248\n",
      "**************************************************\n",
      "Epoch 1 : Iter [271 / 506]\n",
      "Total Loss: 10.106783866882324\n",
      "ITC Loss: 2.830549955368042, ITM Loss: 0.6955996751785278, ITG Loss: 6.580634117126465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [272 / 506]\n",
      "Total Loss: 9.864900588989258\n",
      "ITC Loss: 2.8281517028808594, ITM Loss: 0.6957518458366394, ITG Loss: 6.340997219085693\n",
      "**************************************************\n",
      "Epoch 1 : Iter [273 / 506]\n",
      "Total Loss: 10.180290222167969\n",
      "ITC Loss: 2.838132381439209, ITM Loss: 0.6954278945922852, ITG Loss: 6.646729469299316\n",
      "**************************************************\n",
      "Epoch 1 : Iter [274 / 506]\n",
      "Total Loss: 9.357170104980469\n",
      "ITC Loss: 2.816925525665283, ITM Loss: 0.6963090896606445, ITG Loss: 5.843935489654541\n",
      "**************************************************\n",
      "Epoch 1 : Iter [275 / 506]\n",
      "Total Loss: 9.59853744506836\n",
      "ITC Loss: 2.835505723953247, ITM Loss: 0.6892030239105225, ITG Loss: 6.073829174041748\n",
      "**************************************************\n",
      "Epoch 1 : Iter [276 / 506]\n",
      "Total Loss: 10.551636695861816\n",
      "ITC Loss: 2.8329718112945557, ITM Loss: 0.695102870464325, ITG Loss: 7.023561954498291\n",
      "**************************************************\n",
      "Epoch 1 : Iter [277 / 506]\n",
      "Total Loss: 9.846725463867188\n",
      "ITC Loss: 2.829857587814331, ITM Loss: 0.6890201568603516, ITG Loss: 6.327847480773926\n",
      "**************************************************\n",
      "Epoch 1 : Iter [278 / 506]\n",
      "Total Loss: 9.746585845947266\n",
      "ITC Loss: 2.898198127746582, ITM Loss: 0.6985053420066833, ITG Loss: 6.149881839752197\n",
      "**************************************************\n",
      "Epoch 1 : Iter [279 / 506]\n",
      "Total Loss: 9.980720520019531\n",
      "ITC Loss: 2.853604793548584, ITM Loss: 0.692155659198761, ITG Loss: 6.434959888458252\n",
      "**************************************************\n",
      "Epoch 1 : Iter [280 / 506]\n",
      "Total Loss: 9.860679626464844\n",
      "ITC Loss: 2.8130011558532715, ITM Loss: 0.6977752447128296, ITG Loss: 6.349902629852295\n",
      "**************************************************\n",
      "Epoch 1 : Iter [281 / 506]\n",
      "Total Loss: 8.869976043701172\n",
      "ITC Loss: 2.879634141921997, ITM Loss: 0.6929674744606018, ITG Loss: 5.297374248504639\n",
      "**************************************************\n",
      "Epoch 1 : Iter [282 / 506]\n",
      "Total Loss: 9.503183364868164\n",
      "ITC Loss: 2.8238437175750732, ITM Loss: 0.6921741962432861, ITG Loss: 5.987165451049805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [283 / 506]\n",
      "Total Loss: 9.973296165466309\n",
      "ITC Loss: 2.854336738586426, ITM Loss: 0.6907008290290833, ITG Loss: 6.428258895874023\n",
      "**************************************************\n",
      "Epoch 1 : Iter [284 / 506]\n",
      "Total Loss: 10.227577209472656\n",
      "ITC Loss: 2.8595852851867676, ITM Loss: 0.6938213109970093, ITG Loss: 6.67417049407959\n",
      "**************************************************\n",
      "Epoch 1 : Iter [285 / 506]\n",
      "Total Loss: 9.550539016723633\n",
      "ITC Loss: 2.862819194793701, ITM Loss: 0.6959906816482544, ITG Loss: 5.991728782653809\n",
      "**************************************************\n",
      "Epoch 1 : Iter [286 / 506]\n",
      "Total Loss: 8.746963500976562\n",
      "ITC Loss: 2.8238041400909424, ITM Loss: 0.6945022940635681, ITG Loss: 5.228657245635986\n",
      "**************************************************\n",
      "Epoch 1 : Iter [287 / 506]\n",
      "Total Loss: 9.505229949951172\n",
      "ITC Loss: 2.843782424926758, ITM Loss: 0.695881187915802, ITG Loss: 5.965566158294678\n",
      "**************************************************\n",
      "Epoch 1 : Iter [288 / 506]\n",
      "Total Loss: 9.1007661819458\n",
      "ITC Loss: 2.8665554523468018, ITM Loss: 0.6930698752403259, ITG Loss: 5.541140556335449\n",
      "**************************************************\n",
      "Epoch 1 : Iter [289 / 506]\n",
      "Total Loss: 9.780169486999512\n",
      "ITC Loss: 2.820298194885254, ITM Loss: 0.6930919289588928, ITG Loss: 6.26677942276001\n",
      "**************************************************\n",
      "Epoch 1 : Iter [290 / 506]\n",
      "Total Loss: 9.8226318359375\n",
      "ITC Loss: 2.822392463684082, ITM Loss: 0.6935318112373352, ITG Loss: 6.306707382202148\n",
      "**************************************************\n",
      "Epoch 1 : Iter [291 / 506]\n",
      "Total Loss: 8.656137466430664\n",
      "ITC Loss: 2.8170523643493652, ITM Loss: 0.6958121657371521, ITG Loss: 5.143272876739502\n",
      "**************************************************\n",
      "Epoch 1 : Iter [292 / 506]\n",
      "Total Loss: 9.39471435546875\n",
      "ITC Loss: 2.8315813541412354, ITM Loss: 0.6896055340766907, ITG Loss: 5.8735270500183105\n",
      "**************************************************\n",
      "Epoch 1 : Iter [293 / 506]\n",
      "Total Loss: 9.145706176757812\n",
      "ITC Loss: 2.8053462505340576, ITM Loss: 0.6927618384361267, ITG Loss: 5.647597789764404\n",
      "**************************************************\n",
      "Epoch 1 : Iter [294 / 506]\n",
      "Total Loss: 9.667890548706055\n",
      "ITC Loss: 2.794862747192383, ITM Loss: 0.6930406093597412, ITG Loss: 6.179986953735352\n",
      "**************************************************\n",
      "Epoch 1 : Iter [295 / 506]\n",
      "Total Loss: 9.607320785522461\n",
      "ITC Loss: 2.822299003601074, ITM Loss: 0.6923848986625671, ITG Loss: 6.092637062072754\n",
      "**************************************************\n",
      "Epoch 1 : Iter [296 / 506]\n",
      "Total Loss: 8.639286994934082\n",
      "ITC Loss: 2.84682035446167, ITM Loss: 0.6981229782104492, ITG Loss: 5.094343662261963\n",
      "**************************************************\n",
      "Epoch 1 : Iter [297 / 506]\n",
      "Total Loss: 8.809157371520996\n",
      "ITC Loss: 2.860421657562256, ITM Loss: 0.696069061756134, ITG Loss: 5.252666473388672\n",
      "**************************************************\n",
      "Epoch 1 : Iter [298 / 506]\n",
      "Total Loss: 8.998750686645508\n",
      "ITC Loss: 2.813856601715088, ITM Loss: 0.6947413086891174, ITG Loss: 5.490152359008789\n",
      "**************************************************\n",
      "Epoch 1 : Iter [299 / 506]\n",
      "Total Loss: 10.046063423156738\n",
      "ITC Loss: 2.8189587593078613, ITM Loss: 0.6931453347206116, ITG Loss: 6.53395938873291\n",
      "**************************************************\n",
      "Epoch 1 : Iter [300 / 506]\n",
      "Total Loss: 9.47461986541748\n",
      "ITC Loss: 2.8361144065856934, ITM Loss: 0.6930858492851257, ITG Loss: 5.9454193115234375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [301 / 506]\n",
      "Total Loss: 9.293214797973633\n",
      "ITC Loss: 2.8177099227905273, ITM Loss: 0.6943858861923218, ITG Loss: 5.781119346618652\n",
      "**************************************************\n",
      "Epoch 1 : Iter [302 / 506]\n",
      "Total Loss: 9.429387092590332\n",
      "ITC Loss: 2.8732962608337402, ITM Loss: 0.6967514157295227, ITG Loss: 5.859339714050293\n",
      "**************************************************\n",
      "Epoch 1 : Iter [303 / 506]\n",
      "Total Loss: 8.984529495239258\n",
      "ITC Loss: 2.7993874549865723, ITM Loss: 0.6968002319335938, ITG Loss: 5.488341331481934\n",
      "**************************************************\n",
      "Epoch 1 : Iter [304 / 506]\n",
      "Total Loss: 9.676025390625\n",
      "ITC Loss: 2.8189401626586914, ITM Loss: 0.6912019848823547, ITG Loss: 6.165883541107178\n",
      "**************************************************\n",
      "Epoch 1 : Iter [305 / 506]\n",
      "Total Loss: 9.159246444702148\n",
      "ITC Loss: 2.8223094940185547, ITM Loss: 0.6945054531097412, ITG Loss: 5.642431259155273\n",
      "**************************************************\n",
      "Epoch 1 : Iter [306 / 506]\n",
      "Total Loss: 9.402669906616211\n",
      "ITC Loss: 2.8555169105529785, ITM Loss: 0.6963730454444885, ITG Loss: 5.850780487060547\n",
      "**************************************************\n",
      "Epoch 1 : Iter [307 / 506]\n",
      "Total Loss: 9.64686393737793\n",
      "ITC Loss: 2.84407377243042, ITM Loss: 0.6913623809814453, ITG Loss: 6.111428260803223\n",
      "**************************************************\n",
      "Epoch 1 : Iter [308 / 506]\n",
      "Total Loss: 8.974052429199219\n",
      "ITC Loss: 2.8372349739074707, ITM Loss: 0.6954796314239502, ITG Loss: 5.441338062286377\n",
      "**************************************************\n",
      "Epoch 1 : Iter [309 / 506]\n",
      "Total Loss: 10.016127586364746\n",
      "ITC Loss: 2.806520700454712, ITM Loss: 0.695134162902832, ITG Loss: 6.514472961425781\n",
      "**************************************************\n",
      "Epoch 1 : Iter [310 / 506]\n",
      "Total Loss: 9.398271560668945\n",
      "ITC Loss: 2.861982583999634, ITM Loss: 0.6956876516342163, ITG Loss: 5.840600967407227\n",
      "**************************************************\n",
      "Epoch 1 : Iter [311 / 506]\n",
      "Total Loss: 9.958175659179688\n",
      "ITC Loss: 2.9114153385162354, ITM Loss: 0.6907416582107544, ITG Loss: 6.356019020080566\n",
      "**************************************************\n",
      "Epoch 1 : Iter [312 / 506]\n",
      "Total Loss: 10.145424842834473\n",
      "ITC Loss: 2.803112268447876, ITM Loss: 0.6925415396690369, ITG Loss: 6.649770736694336\n",
      "**************************************************\n",
      "Epoch 1 : Iter [313 / 506]\n",
      "Total Loss: 8.806400299072266\n",
      "ITC Loss: 2.838522434234619, ITM Loss: 0.6931160092353821, ITG Loss: 5.27476167678833\n",
      "**************************************************\n",
      "Epoch 1 : Iter [314 / 506]\n",
      "Total Loss: 8.560699462890625\n",
      "ITC Loss: 2.863481044769287, ITM Loss: 0.6933812499046326, ITG Loss: 5.0038371086120605\n",
      "**************************************************\n",
      "Epoch 1 : Iter [315 / 506]\n",
      "Total Loss: 9.471985816955566\n",
      "ITC Loss: 2.8302993774414062, ITM Loss: 0.6916999816894531, ITG Loss: 5.949986457824707\n",
      "**************************************************\n",
      "Epoch 1 : Iter [316 / 506]\n",
      "Total Loss: 9.39773178100586\n",
      "ITC Loss: 2.8083252906799316, ITM Loss: 0.6928163170814514, ITG Loss: 5.896589756011963\n",
      "**************************************************\n",
      "Epoch 1 : Iter [317 / 506]\n",
      "Total Loss: 8.708612442016602\n",
      "ITC Loss: 2.7709107398986816, ITM Loss: 0.6959361433982849, ITG Loss: 5.241765975952148\n",
      "**************************************************\n",
      "Epoch 1 : Iter [318 / 506]\n",
      "Total Loss: 9.174760818481445\n",
      "ITC Loss: 2.869999408721924, ITM Loss: 0.6937350630760193, ITG Loss: 5.611025810241699\n",
      "**************************************************\n",
      "Epoch 1 : Iter [319 / 506]\n",
      "Total Loss: 8.600445747375488\n",
      "ITC Loss: 2.7907938957214355, ITM Loss: 0.6915838122367859, ITG Loss: 5.118067741394043\n",
      "**************************************************\n",
      "Epoch 1 : Iter [320 / 506]\n",
      "Total Loss: 9.09521770477295\n",
      "ITC Loss: 2.8266615867614746, ITM Loss: 0.6941028237342834, ITG Loss: 5.574453353881836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [321 / 506]\n",
      "Total Loss: 8.97409439086914\n",
      "ITC Loss: 2.850266456604004, ITM Loss: 0.6975254416465759, ITG Loss: 5.426301956176758\n",
      "**************************************************\n",
      "Epoch 1 : Iter [322 / 506]\n",
      "Total Loss: 9.461280822753906\n",
      "ITC Loss: 2.892015218734741, ITM Loss: 0.6923138499259949, ITG Loss: 5.876951217651367\n",
      "**************************************************\n",
      "Epoch 1 : Iter [323 / 506]\n",
      "Total Loss: 9.069334030151367\n",
      "ITC Loss: 2.8727197647094727, ITM Loss: 0.6933680772781372, ITG Loss: 5.503246307373047\n",
      "**************************************************\n",
      "Epoch 1 : Iter [324 / 506]\n",
      "Total Loss: 8.79393196105957\n",
      "ITC Loss: 2.8068323135375977, ITM Loss: 0.6947135329246521, ITG Loss: 5.292385578155518\n",
      "**************************************************\n",
      "Epoch 1 : Iter [325 / 506]\n",
      "Total Loss: 9.303688049316406\n",
      "ITC Loss: 2.8805434703826904, ITM Loss: 0.6899544596672058, ITG Loss: 5.733190536499023\n",
      "**************************************************\n",
      "Epoch 1 : Iter [326 / 506]\n",
      "Total Loss: 8.684640884399414\n",
      "ITC Loss: 2.812692642211914, ITM Loss: 0.6902654767036438, ITG Loss: 5.181682586669922\n",
      "**************************************************\n",
      "Epoch 1 : Iter [327 / 506]\n",
      "Total Loss: 8.91386890411377\n",
      "ITC Loss: 2.826512336730957, ITM Loss: 0.6945492625236511, ITG Loss: 5.392807483673096\n",
      "**************************************************\n",
      "Epoch 1 : Iter [328 / 506]\n",
      "Total Loss: 9.60176944732666\n",
      "ITC Loss: 2.8114089965820312, ITM Loss: 0.6940301060676575, ITG Loss: 6.096330642700195\n",
      "**************************************************\n",
      "Epoch 1 : Iter [329 / 506]\n",
      "Total Loss: 9.241525650024414\n",
      "ITC Loss: 2.849440813064575, ITM Loss: 0.6905220746994019, ITG Loss: 5.701563358306885\n",
      "**************************************************\n",
      "Epoch 1 : Iter [330 / 506]\n",
      "Total Loss: 9.06353759765625\n",
      "ITC Loss: 2.821666717529297, ITM Loss: 0.6912779211997986, ITG Loss: 5.550593376159668\n",
      "**************************************************\n",
      "Epoch 1 : Iter [331 / 506]\n",
      "Total Loss: 10.012650489807129\n",
      "ITC Loss: 2.8414883613586426, ITM Loss: 0.6921632289886475, ITG Loss: 6.478999137878418\n",
      "**************************************************\n",
      "Epoch 1 : Iter [332 / 506]\n",
      "Total Loss: 9.440568923950195\n",
      "ITC Loss: 2.845127582550049, ITM Loss: 0.6908883452415466, ITG Loss: 5.904552459716797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [333 / 506]\n",
      "Total Loss: 8.92471694946289\n",
      "ITC Loss: 2.8261632919311523, ITM Loss: 0.6934719085693359, ITG Loss: 5.405081748962402\n",
      "**************************************************\n",
      "Epoch 1 : Iter [334 / 506]\n",
      "Total Loss: 8.96097183227539\n",
      "ITC Loss: 2.8656253814697266, ITM Loss: 0.6918430328369141, ITG Loss: 5.40350341796875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [335 / 506]\n",
      "Total Loss: 8.532925605773926\n",
      "ITC Loss: 2.841547966003418, ITM Loss: 0.6945130228996277, ITG Loss: 4.996864318847656\n",
      "**************************************************\n",
      "Epoch 1 : Iter [336 / 506]\n",
      "Total Loss: 9.18864631652832\n",
      "ITC Loss: 2.8714990615844727, ITM Loss: 0.6954358816146851, ITG Loss: 5.621711254119873\n",
      "**************************************************\n",
      "Epoch 1 : Iter [337 / 506]\n",
      "Total Loss: 9.318763732910156\n",
      "ITC Loss: 2.757699489593506, ITM Loss: 0.6902519464492798, ITG Loss: 5.870812892913818\n",
      "**************************************************\n",
      "Epoch 1 : Iter [338 / 506]\n",
      "Total Loss: 9.664166450500488\n",
      "ITC Loss: 2.852146863937378, ITM Loss: 0.6940394639968872, ITG Loss: 6.117980003356934\n",
      "**************************************************\n",
      "Epoch 1 : Iter [339 / 506]\n",
      "Total Loss: 9.638479232788086\n",
      "ITC Loss: 2.800581932067871, ITM Loss: 0.6974084377288818, ITG Loss: 6.140489101409912\n",
      "**************************************************\n",
      "Epoch 1 : Iter [340 / 506]\n",
      "Total Loss: 8.807967185974121\n",
      "ITC Loss: 2.8402326107025146, ITM Loss: 0.6912386417388916, ITG Loss: 5.276495933532715\n",
      "**************************************************\n",
      "Epoch 1 : Iter [341 / 506]\n",
      "Total Loss: 9.031050682067871\n",
      "ITC Loss: 2.828160285949707, ITM Loss: 0.692075252532959, ITG Loss: 5.510815143585205\n",
      "**************************************************\n",
      "Epoch 1 : Iter [342 / 506]\n",
      "Total Loss: 8.939327239990234\n",
      "ITC Loss: 2.823976993560791, ITM Loss: 0.6940591335296631, ITG Loss: 5.421290874481201\n",
      "**************************************************\n",
      "Epoch 1 : Iter [343 / 506]\n",
      "Total Loss: 8.611822128295898\n",
      "ITC Loss: 2.827164649963379, ITM Loss: 0.6910303235054016, ITG Loss: 5.093626976013184\n",
      "**************************************************\n",
      "Epoch 1 : Iter [344 / 506]\n",
      "Total Loss: 8.587966918945312\n",
      "ITC Loss: 2.828461170196533, ITM Loss: 0.6952875852584839, ITG Loss: 5.064218044281006\n",
      "**************************************************\n",
      "Epoch 1 : Iter [345 / 506]\n",
      "Total Loss: 8.461631774902344\n",
      "ITC Loss: 2.859649658203125, ITM Loss: 0.6925053000450134, ITG Loss: 4.909477233886719\n",
      "**************************************************\n",
      "Epoch 1 : Iter [346 / 506]\n",
      "Total Loss: 8.442865371704102\n",
      "ITC Loss: 2.8499553203582764, ITM Loss: 0.6906816363334656, ITG Loss: 4.902227878570557\n",
      "**************************************************\n",
      "Epoch 1 : Iter [347 / 506]\n",
      "Total Loss: 8.97840404510498\n",
      "ITC Loss: 2.8518497943878174, ITM Loss: 0.6914600133895874, ITG Loss: 5.435094356536865\n",
      "**************************************************\n",
      "Epoch 1 : Iter [348 / 506]\n",
      "Total Loss: 8.677507400512695\n",
      "ITC Loss: 2.820484161376953, ITM Loss: 0.6905103921890259, ITG Loss: 5.166512966156006\n",
      "**************************************************\n",
      "Epoch 1 : Iter [349 / 506]\n",
      "Total Loss: 9.017799377441406\n",
      "ITC Loss: 2.8269758224487305, ITM Loss: 0.6973986029624939, ITG Loss: 5.493424415588379\n",
      "**************************************************\n",
      "Epoch 1 : Iter [350 / 506]\n",
      "Total Loss: 9.137592315673828\n",
      "ITC Loss: 2.834929943084717, ITM Loss: 0.6906596422195435, ITG Loss: 5.612003326416016\n",
      "**************************************************\n",
      "Epoch 1 : Iter [351 / 506]\n",
      "Total Loss: 8.682184219360352\n",
      "ITC Loss: 2.7969584465026855, ITM Loss: 0.6939102411270142, ITG Loss: 5.191315174102783\n",
      "**************************************************\n",
      "Epoch 1 : Iter [352 / 506]\n",
      "Total Loss: 9.15496826171875\n",
      "ITC Loss: 2.8714613914489746, ITM Loss: 0.6934556365013123, ITG Loss: 5.590051174163818\n",
      "**************************************************\n",
      "Epoch 1 : Iter [353 / 506]\n",
      "Total Loss: 9.064753532409668\n",
      "ITC Loss: 2.797661066055298, ITM Loss: 0.6935098171234131, ITG Loss: 5.573582649230957\n",
      "**************************************************\n",
      "Epoch 1 : Iter [354 / 506]\n",
      "Total Loss: 8.845863342285156\n",
      "ITC Loss: 2.8171324729919434, ITM Loss: 0.6921812295913696, ITG Loss: 5.336550235748291\n",
      "**************************************************\n",
      "Epoch 1 : Iter [355 / 506]\n",
      "Total Loss: 9.245654106140137\n",
      "ITC Loss: 2.8608827590942383, ITM Loss: 0.6933554410934448, ITG Loss: 5.691415786743164\n",
      "**************************************************\n",
      "Epoch 1 : Iter [356 / 506]\n",
      "Total Loss: 8.995617866516113\n",
      "ITC Loss: 2.8513383865356445, ITM Loss: 0.6915280222892761, ITG Loss: 5.452751159667969\n",
      "**************************************************\n",
      "Epoch 1 : Iter [357 / 506]\n",
      "Total Loss: 9.099586486816406\n",
      "ITC Loss: 2.8098411560058594, ITM Loss: 0.6904415488243103, ITG Loss: 5.599303245544434\n",
      "**************************************************\n",
      "Epoch 1 : Iter [358 / 506]\n",
      "Total Loss: 8.841887474060059\n",
      "ITC Loss: 2.815998077392578, ITM Loss: 0.695866584777832, ITG Loss: 5.330022811889648\n",
      "**************************************************\n",
      "Epoch 1 : Iter [359 / 506]\n",
      "Total Loss: 8.692167282104492\n",
      "ITC Loss: 2.790842056274414, ITM Loss: 0.6953855752944946, ITG Loss: 5.205940246582031\n",
      "**************************************************\n",
      "Epoch 1 : Iter [360 / 506]\n",
      "Total Loss: 9.088321685791016\n",
      "ITC Loss: 2.783754348754883, ITM Loss: 0.6935889720916748, ITG Loss: 5.610978126525879\n",
      "**************************************************\n",
      "Epoch 1 : Iter [361 / 506]\n",
      "Total Loss: 9.130260467529297\n",
      "ITC Loss: 2.8268566131591797, ITM Loss: 0.6931507587432861, ITG Loss: 5.61025333404541\n",
      "**************************************************\n",
      "Epoch 1 : Iter [362 / 506]\n",
      "Total Loss: 9.02690315246582\n",
      "ITC Loss: 2.811724901199341, ITM Loss: 0.6921190023422241, ITG Loss: 5.523059368133545\n",
      "**************************************************\n",
      "Epoch 1 : Iter [363 / 506]\n",
      "Total Loss: 9.309151649475098\n",
      "ITC Loss: 2.8208413124084473, ITM Loss: 0.6911746859550476, ITG Loss: 5.797135353088379\n",
      "**************************************************\n",
      "Epoch 1 : Iter [364 / 506]\n",
      "Total Loss: 8.492633819580078\n",
      "ITC Loss: 2.8587613105773926, ITM Loss: 0.6928220391273499, ITG Loss: 4.9410505294799805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [365 / 506]\n",
      "Total Loss: 9.232176780700684\n",
      "ITC Loss: 2.8893232345581055, ITM Loss: 0.6944282650947571, ITG Loss: 5.648425102233887\n",
      "**************************************************\n",
      "Epoch 1 : Iter [366 / 506]\n",
      "Total Loss: 9.08210277557373\n",
      "ITC Loss: 2.809432029724121, ITM Loss: 0.6881628632545471, ITG Loss: 5.584507942199707\n",
      "**************************************************\n",
      "Epoch 1 : Iter [367 / 506]\n",
      "Total Loss: 8.18339729309082\n",
      "ITC Loss: 2.81536865234375, ITM Loss: 0.693739116191864, ITG Loss: 4.674289703369141\n",
      "**************************************************\n",
      "Epoch 1 : Iter [368 / 506]\n",
      "Total Loss: 9.513853073120117\n",
      "ITC Loss: 2.8418571949005127, ITM Loss: 0.6930974721908569, ITG Loss: 5.978898048400879\n",
      "**************************************************\n",
      "Epoch 1 : Iter [369 / 506]\n",
      "Total Loss: 8.334623336791992\n",
      "ITC Loss: 2.8148670196533203, ITM Loss: 0.6956381797790527, ITG Loss: 4.824118137359619\n",
      "**************************************************\n",
      "Epoch 1 : Iter [370 / 506]\n",
      "Total Loss: 9.223156929016113\n",
      "ITC Loss: 2.8646273612976074, ITM Loss: 0.6995761394500732, ITG Loss: 5.658953666687012\n",
      "**************************************************\n",
      "Epoch 1 : Iter [371 / 506]\n",
      "Total Loss: 8.754705429077148\n",
      "ITC Loss: 2.8060784339904785, ITM Loss: 0.6972560882568359, ITG Loss: 5.251371383666992\n",
      "**************************************************\n",
      "Epoch 1 : Iter [372 / 506]\n",
      "Total Loss: 8.057698249816895\n",
      "ITC Loss: 2.7998688220977783, ITM Loss: 0.6946165561676025, ITG Loss: 4.563212871551514\n",
      "**************************************************\n",
      "Epoch 1 : Iter [373 / 506]\n",
      "Total Loss: 8.41657829284668\n",
      "ITC Loss: 2.8698651790618896, ITM Loss: 0.6901639699935913, ITG Loss: 4.8565497398376465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [374 / 506]\n",
      "Total Loss: 8.975926399230957\n",
      "ITC Loss: 2.862517833709717, ITM Loss: 0.6922088265419006, ITG Loss: 5.421199798583984\n",
      "**************************************************\n",
      "Epoch 1 : Iter [375 / 506]\n",
      "Total Loss: 8.833205223083496\n",
      "ITC Loss: 2.8558127880096436, ITM Loss: 0.6949548125267029, ITG Loss: 5.282437801361084\n",
      "**************************************************\n",
      "Epoch 1 : Iter [376 / 506]\n",
      "Total Loss: 8.340251922607422\n",
      "ITC Loss: 2.819988965988159, ITM Loss: 0.6947404146194458, ITG Loss: 4.8255228996276855\n",
      "**************************************************\n",
      "Epoch 1 : Iter [377 / 506]\n",
      "Total Loss: 8.50119686126709\n",
      "ITC Loss: 2.8081085681915283, ITM Loss: 0.6949079632759094, ITG Loss: 4.998180389404297\n",
      "**************************************************\n",
      "Epoch 1 : Iter [378 / 506]\n",
      "Total Loss: 9.059908866882324\n",
      "ITC Loss: 2.8185172080993652, ITM Loss: 0.6945053339004517, ITG Loss: 5.546886444091797\n",
      "**************************************************\n",
      "Epoch 1 : Iter [379 / 506]\n",
      "Total Loss: 9.112276077270508\n",
      "ITC Loss: 2.811567544937134, ITM Loss: 0.6961598992347717, ITG Loss: 5.604548931121826\n",
      "**************************************************\n",
      "Epoch 1 : Iter [380 / 506]\n",
      "Total Loss: 9.039592742919922\n",
      "ITC Loss: 2.794752597808838, ITM Loss: 0.6932190656661987, ITG Loss: 5.5516204833984375\n",
      "**************************************************\n",
      "Epoch 1 : Iter [381 / 506]\n",
      "Total Loss: 8.496051788330078\n",
      "ITC Loss: 2.7858715057373047, ITM Loss: 0.6989307999610901, ITG Loss: 5.011249542236328\n",
      "**************************************************\n",
      "Epoch 1 : Iter [382 / 506]\n",
      "Total Loss: 8.412178993225098\n",
      "ITC Loss: 2.840209484100342, ITM Loss: 0.6921679973602295, ITG Loss: 4.8798017501831055\n",
      "**************************************************\n",
      "Epoch 1 : Iter [383 / 506]\n",
      "Total Loss: 8.961712837219238\n",
      "ITC Loss: 2.8511486053466797, ITM Loss: 0.694347083568573, ITG Loss: 5.416216850280762\n",
      "**************************************************\n",
      "Epoch 1 : Iter [384 / 506]\n",
      "Total Loss: 8.754202842712402\n",
      "ITC Loss: 2.8114590644836426, ITM Loss: 0.6947634220123291, ITG Loss: 5.247980117797852\n",
      "**************************************************\n",
      "Epoch 1 : Iter [385 / 506]\n",
      "Total Loss: 8.425352096557617\n",
      "ITC Loss: 2.7974979877471924, ITM Loss: 0.6929358839988708, ITG Loss: 4.934918403625488\n",
      "**************************************************\n",
      "Epoch 1 : Iter [386 / 506]\n",
      "Total Loss: 9.157930374145508\n",
      "ITC Loss: 2.8543474674224854, ITM Loss: 0.690237283706665, ITG Loss: 5.613346099853516\n",
      "**************************************************\n",
      "Epoch 1 : Iter [387 / 506]\n",
      "Total Loss: 8.781198501586914\n",
      "ITC Loss: 2.818659782409668, ITM Loss: 0.694681704044342, ITG Loss: 5.267857551574707\n",
      "**************************************************\n",
      "Epoch 1 : Iter [388 / 506]\n",
      "Total Loss: 9.146600723266602\n",
      "ITC Loss: 2.8170552253723145, ITM Loss: 0.6928249597549438, ITG Loss: 5.636720180511475\n",
      "**************************************************\n",
      "Epoch 1 : Iter [389 / 506]\n",
      "Total Loss: 8.726306915283203\n",
      "ITC Loss: 2.826085090637207, ITM Loss: 0.6948046088218689, ITG Loss: 5.205417156219482\n",
      "**************************************************\n",
      "Epoch 1 : Iter [390 / 506]\n",
      "Total Loss: 9.059842109680176\n",
      "ITC Loss: 2.7837533950805664, ITM Loss: 0.6951947212219238, ITG Loss: 5.5808939933776855\n",
      "**************************************************\n",
      "Epoch 1 : Iter [391 / 506]\n",
      "Total Loss: 8.478033065795898\n",
      "ITC Loss: 2.8399062156677246, ITM Loss: 0.6919724345207214, ITG Loss: 4.946154594421387\n",
      "**************************************************\n",
      "Epoch 1 : Iter [392 / 506]\n",
      "Total Loss: 8.950990676879883\n",
      "ITC Loss: 2.7979278564453125, ITM Loss: 0.6990798711776733, ITG Loss: 5.453982830047607\n",
      "**************************************************\n",
      "Epoch 1 : Iter [393 / 506]\n",
      "Total Loss: 9.293062210083008\n",
      "ITC Loss: 2.8247060775756836, ITM Loss: 0.6937240362167358, ITG Loss: 5.774631500244141\n",
      "**************************************************\n",
      "Epoch 1 : Iter [394 / 506]\n",
      "Total Loss: 8.962202072143555\n",
      "ITC Loss: 2.8293094635009766, ITM Loss: 0.6917020082473755, ITG Loss: 5.441190242767334\n",
      "**************************************************\n",
      "Epoch 1 : Iter [395 / 506]\n",
      "Total Loss: 9.58585262298584\n",
      "ITC Loss: 2.8841283321380615, ITM Loss: 0.6961356997489929, ITG Loss: 6.005588531494141\n",
      "**************************************************\n",
      "Epoch 1 : Iter [396 / 506]\n",
      "Total Loss: 8.509171485900879\n",
      "ITC Loss: 2.7731313705444336, ITM Loss: 0.6886436343193054, ITG Loss: 5.047396659851074\n",
      "**************************************************\n",
      "Epoch 1 : Iter [397 / 506]\n",
      "Total Loss: 8.885727882385254\n",
      "ITC Loss: 2.81449556350708, ITM Loss: 0.6976449489593506, ITG Loss: 5.373587131500244\n",
      "**************************************************\n",
      "Epoch 1 : Iter [398 / 506]\n",
      "Total Loss: 8.78991413116455\n",
      "ITC Loss: 2.8038294315338135, ITM Loss: 0.6923075914382935, ITG Loss: 5.293776988983154\n",
      "**************************************************\n",
      "Epoch 1 : Iter [399 / 506]\n",
      "Total Loss: 8.65713882446289\n",
      "ITC Loss: 2.815218925476074, ITM Loss: 0.6916441917419434, ITG Loss: 5.150275707244873\n",
      "**************************************************\n",
      "Epoch 1 : Iter [400 / 506]\n",
      "Total Loss: 8.857667922973633\n",
      "ITC Loss: 2.8806073665618896, ITM Loss: 0.692710280418396, ITG Loss: 5.2843499183654785\n",
      "**************************************************\n",
      "Epoch 1 : Iter [401 / 506]\n",
      "Total Loss: 8.317686080932617\n",
      "ITC Loss: 2.8242945671081543, ITM Loss: 0.6979077458381653, ITG Loss: 4.795483589172363\n",
      "**************************************************\n",
      "Epoch 1 : Iter [402 / 506]\n",
      "Total Loss: 9.193668365478516\n",
      "ITC Loss: 2.8123626708984375, ITM Loss: 0.6946706175804138, ITG Loss: 5.686635494232178\n",
      "**************************************************\n",
      "Epoch 1 : Iter [403 / 506]\n",
      "Total Loss: 8.897464752197266\n",
      "ITC Loss: 2.8007779121398926, ITM Loss: 0.6947198510169983, ITG Loss: 5.401967525482178\n",
      "**************************************************\n",
      "Epoch 1 : Iter [404 / 506]\n",
      "Total Loss: 9.030559539794922\n",
      "ITC Loss: 2.838216781616211, ITM Loss: 0.6896748542785645, ITG Loss: 5.5026679039001465\n",
      "**************************************************\n",
      "Epoch 1 : Iter [405 / 506]\n",
      "Total Loss: 8.25894546508789\n",
      "ITC Loss: 2.8137288093566895, ITM Loss: 0.6892656683921814, ITG Loss: 4.755950450897217\n",
      "**************************************************\n",
      "Epoch 1 : Iter [406 / 506]\n",
      "Total Loss: 9.547836303710938\n",
      "ITC Loss: 2.845249891281128, ITM Loss: 0.6889803409576416, ITG Loss: 6.013606548309326\n",
      "**************************************************\n",
      "Epoch 1 : Iter [407 / 506]\n",
      "Total Loss: 8.639382362365723\n",
      "ITC Loss: 2.844956398010254, ITM Loss: 0.6949726343154907, ITG Loss: 5.099453449249268\n",
      "**************************************************\n",
      "Epoch 1 : Iter [408 / 506]\n",
      "Total Loss: 8.608972549438477\n",
      "ITC Loss: 2.8205485343933105, ITM Loss: 0.695694625377655, ITG Loss: 5.092729568481445\n",
      "**************************************************\n",
      "Epoch 1 : Iter [409 / 506]\n",
      "Total Loss: 8.988521575927734\n",
      "ITC Loss: 2.85721755027771, ITM Loss: 0.6921942234039307, ITG Loss: 5.4391093254089355\n",
      "**************************************************\n",
      "Epoch 1 : Iter [410 / 506]\n",
      "Total Loss: 8.849538803100586\n",
      "ITC Loss: 2.8411402702331543, ITM Loss: 0.6914592385292053, ITG Loss: 5.316938877105713\n",
      "**************************************************\n",
      "Epoch 1 : Iter [411 / 506]\n",
      "Total Loss: 8.515103340148926\n",
      "ITC Loss: 2.863217353820801, ITM Loss: 0.6925533413887024, ITG Loss: 4.959332466125488\n",
      "**************************************************\n",
      "Epoch 1 : Iter [412 / 506]\n",
      "Total Loss: 9.209563255310059\n",
      "ITC Loss: 2.8528342247009277, ITM Loss: 0.6938542723655701, ITG Loss: 5.662874698638916\n",
      "**************************************************\n",
      "Epoch 1 : Iter [413 / 506]\n",
      "Total Loss: 8.646368980407715\n",
      "ITC Loss: 2.84311580657959, ITM Loss: 0.6993291974067688, ITG Loss: 5.10392427444458\n",
      "**************************************************\n",
      "Epoch 1 : Iter [414 / 506]\n",
      "Total Loss: 8.918304443359375\n",
      "ITC Loss: 2.790903091430664, ITM Loss: 0.6931979656219482, ITG Loss: 5.434203147888184\n",
      "**************************************************\n",
      "Epoch 1 : Iter [415 / 506]\n",
      "Total Loss: 8.720222473144531\n",
      "ITC Loss: 2.7713429927825928, ITM Loss: 0.6956457495689392, ITG Loss: 5.253233432769775\n",
      "**************************************************\n",
      "Epoch 1 : Iter [416 / 506]\n",
      "Total Loss: 8.78726577758789\n",
      "ITC Loss: 2.825716733932495, ITM Loss: 0.6963899731636047, ITG Loss: 5.2651591300964355\n",
      "**************************************************\n",
      "Epoch 1 : Iter [417 / 506]\n",
      "Total Loss: 8.4395751953125\n",
      "ITC Loss: 2.8207173347473145, ITM Loss: 0.6918346285820007, ITG Loss: 4.927022933959961\n",
      "**************************************************\n",
      "Epoch 1 : Iter [418 / 506]\n",
      "Total Loss: 9.199167251586914\n",
      "ITC Loss: 2.8438949584960938, ITM Loss: 0.6941287517547607, ITG Loss: 5.661143779754639\n",
      "**************************************************\n",
      "Epoch 1 : Iter [419 / 506]\n",
      "Total Loss: 8.940900802612305\n",
      "ITC Loss: 2.823753833770752, ITM Loss: 0.6935157775878906, ITG Loss: 5.423630714416504\n",
      "**************************************************\n",
      "Epoch 1 : Iter [420 / 506]\n",
      "Total Loss: 8.527182579040527\n",
      "ITC Loss: 2.825244665145874, ITM Loss: 0.6931759715080261, ITG Loss: 5.008761882781982\n",
      "**************************************************\n",
      "Epoch 1 : Iter [421 / 506]\n",
      "Total Loss: 8.382999420166016\n",
      "ITC Loss: 2.8180971145629883, ITM Loss: 0.6952719688415527, ITG Loss: 4.869630813598633\n",
      "**************************************************\n",
      "Epoch 1 : Iter [422 / 506]\n",
      "Total Loss: 8.684396743774414\n",
      "ITC Loss: 2.8130927085876465, ITM Loss: 0.6906185746192932, ITG Loss: 5.180685520172119\n",
      "**************************************************\n",
      "Epoch 1 : Iter [423 / 506]\n",
      "Total Loss: 8.846660614013672\n",
      "ITC Loss: 2.852818489074707, ITM Loss: 0.6944118738174438, ITG Loss: 5.2994303703308105\n",
      "**************************************************\n",
      "Epoch 1 : Iter [424 / 506]\n",
      "Total Loss: 8.03230094909668\n",
      "ITC Loss: 2.7846624851226807, ITM Loss: 0.6915265321731567, ITG Loss: 4.556111812591553\n",
      "**************************************************\n",
      "Epoch 1 : Iter [425 / 506]\n",
      "Total Loss: 8.071122169494629\n",
      "ITC Loss: 2.851522207260132, ITM Loss: 0.6950243711471558, ITG Loss: 4.524575710296631\n",
      "**************************************************\n",
      "Epoch 1 : Iter [426 / 506]\n",
      "Total Loss: 8.361992835998535\n",
      "ITC Loss: 2.7592220306396484, ITM Loss: 0.695022463798523, ITG Loss: 4.907748222351074\n",
      "**************************************************\n",
      "Epoch 1 : Iter [427 / 506]\n",
      "Total Loss: 8.90955924987793\n",
      "ITC Loss: 2.816819667816162, ITM Loss: 0.6916636228561401, ITG Loss: 5.40107536315918\n",
      "**************************************************\n",
      "Epoch 1 : Iter [428 / 506]\n",
      "Total Loss: 8.621787071228027\n",
      "ITC Loss: 2.840731620788574, ITM Loss: 0.6914478540420532, ITG Loss: 5.0896077156066895\n",
      "**************************************************\n",
      "Epoch 1 : Iter [429 / 506]\n",
      "Total Loss: 8.918362617492676\n",
      "ITC Loss: 2.8448193073272705, ITM Loss: 0.6942453980445862, ITG Loss: 5.379298210144043\n",
      "**************************************************\n",
      "Epoch 1 : Iter [430 / 506]\n",
      "Total Loss: 7.846226692199707\n",
      "ITC Loss: 2.865011692047119, ITM Loss: 0.691895604133606, ITG Loss: 4.2893195152282715\n",
      "**************************************************\n",
      "Epoch 1 : Iter [431 / 506]\n",
      "Total Loss: 8.285097122192383\n",
      "ITC Loss: 2.8710033893585205, ITM Loss: 0.695812463760376, ITG Loss: 4.718280792236328\n",
      "**************************************************\n",
      "Epoch 1 : Iter [432 / 506]\n",
      "Total Loss: 8.083002090454102\n",
      "ITC Loss: 2.8178114891052246, ITM Loss: 0.695753812789917, ITG Loss: 4.569436550140381\n",
      "**************************************************\n",
      "Epoch 1 : Iter [433 / 506]\n",
      "Total Loss: 8.585416793823242\n",
      "ITC Loss: 2.8506228923797607, ITM Loss: 0.6952329874038696, ITG Loss: 5.0395612716674805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [434 / 506]\n",
      "Total Loss: 8.992510795593262\n",
      "ITC Loss: 2.85012149810791, ITM Loss: 0.6953256726264954, ITG Loss: 5.44706392288208\n",
      "**************************************************\n",
      "Epoch 1 : Iter [435 / 506]\n",
      "Total Loss: 8.724117279052734\n",
      "ITC Loss: 2.841108560562134, ITM Loss: 0.6880860328674316, ITG Loss: 5.19492244720459\n",
      "**************************************************\n",
      "Epoch 1 : Iter [436 / 506]\n",
      "Total Loss: 8.687638282775879\n",
      "ITC Loss: 2.8462605476379395, ITM Loss: 0.6948166489601135, ITG Loss: 5.146561145782471\n",
      "**************************************************\n",
      "Epoch 1 : Iter [437 / 506]\n",
      "Total Loss: 8.404378890991211\n",
      "ITC Loss: 2.7634778022766113, ITM Loss: 0.696119487285614, ITG Loss: 4.944781303405762\n",
      "**************************************************\n",
      "Epoch 1 : Iter [438 / 506]\n",
      "Total Loss: 8.326014518737793\n",
      "ITC Loss: 2.761152505874634, ITM Loss: 0.6948754191398621, ITG Loss: 4.869986534118652\n",
      "**************************************************\n",
      "Epoch 1 : Iter [439 / 506]\n",
      "Total Loss: 8.605485916137695\n",
      "ITC Loss: 2.8081471920013428, ITM Loss: 0.6946200728416443, ITG Loss: 5.102718353271484\n",
      "**************************************************\n",
      "Epoch 1 : Iter [440 / 506]\n",
      "Total Loss: 8.131669998168945\n",
      "ITC Loss: 2.8109138011932373, ITM Loss: 0.6937588453292847, ITG Loss: 4.626997470855713\n",
      "**************************************************\n",
      "Epoch 1 : Iter [441 / 506]\n",
      "Total Loss: 9.055072784423828\n",
      "ITC Loss: 2.849336624145508, ITM Loss: 0.6932776570320129, ITG Loss: 5.512458801269531\n",
      "**************************************************\n",
      "Epoch 1 : Iter [442 / 506]\n",
      "Total Loss: 8.443472862243652\n",
      "ITC Loss: 2.773787498474121, ITM Loss: 0.6930350661277771, ITG Loss: 4.976650238037109\n",
      "**************************************************\n",
      "Epoch 1 : Iter [443 / 506]\n",
      "Total Loss: 8.774405479431152\n",
      "ITC Loss: 2.782564640045166, ITM Loss: 0.6941614747047424, ITG Loss: 5.297679424285889\n",
      "**************************************************\n",
      "Epoch 1 : Iter [444 / 506]\n",
      "Total Loss: 8.793725967407227\n",
      "ITC Loss: 2.8123233318328857, ITM Loss: 0.6938823461532593, ITG Loss: 5.287519931793213\n",
      "**************************************************\n",
      "Epoch 1 : Iter [445 / 506]\n",
      "Total Loss: 8.551143646240234\n",
      "ITC Loss: 2.8009305000305176, ITM Loss: 0.6921557784080505, ITG Loss: 5.058056831359863\n",
      "**************************************************\n",
      "Epoch 1 : Iter [446 / 506]\n",
      "Total Loss: 8.275341033935547\n",
      "ITC Loss: 2.817326545715332, ITM Loss: 0.691422700881958, ITG Loss: 4.766592025756836\n",
      "**************************************************\n",
      "Epoch 1 : Iter [447 / 506]\n",
      "Total Loss: 8.547179222106934\n",
      "ITC Loss: 2.790900230407715, ITM Loss: 0.6892572641372681, ITG Loss: 5.06702184677124\n",
      "**************************************************\n",
      "Epoch 1 : Iter [448 / 506]\n",
      "Total Loss: 8.55369758605957\n",
      "ITC Loss: 2.8154962062835693, ITM Loss: 0.6917674541473389, ITG Loss: 5.046433925628662\n",
      "**************************************************\n",
      "Epoch 1 : Iter [449 / 506]\n",
      "Total Loss: 8.523161888122559\n",
      "ITC Loss: 2.820120334625244, ITM Loss: 0.6938812136650085, ITG Loss: 5.009160041809082\n",
      "**************************************************\n",
      "Epoch 1 : Iter [450 / 506]\n",
      "Total Loss: 8.431753158569336\n",
      "ITC Loss: 2.8191659450531006, ITM Loss: 0.6929208040237427, ITG Loss: 4.919666290283203\n",
      "**************************************************\n",
      "Epoch 1 : Iter [451 / 506]\n",
      "Total Loss: 8.451889038085938\n",
      "ITC Loss: 2.8452486991882324, ITM Loss: 0.6919988989830017, ITG Loss: 4.9146409034729\n",
      "**************************************************\n",
      "Epoch 1 : Iter [452 / 506]\n",
      "Total Loss: 8.096643447875977\n",
      "ITC Loss: 2.805098295211792, ITM Loss: 0.6907506585121155, ITG Loss: 4.600794792175293\n",
      "**************************************************\n",
      "Epoch 1 : Iter [453 / 506]\n",
      "Total Loss: 8.500314712524414\n",
      "ITC Loss: 2.823871612548828, ITM Loss: 0.6950739026069641, ITG Loss: 4.9813690185546875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [454 / 506]\n",
      "Total Loss: 8.411794662475586\n",
      "ITC Loss: 2.786346435546875, ITM Loss: 0.6922479867935181, ITG Loss: 4.933200836181641\n",
      "**************************************************\n",
      "Epoch 1 : Iter [455 / 506]\n",
      "Total Loss: 8.51360034942627\n",
      "ITC Loss: 2.847930669784546, ITM Loss: 0.6921327114105225, ITG Loss: 4.973536968231201\n",
      "**************************************************\n",
      "Epoch 1 : Iter [456 / 506]\n",
      "Total Loss: 8.503902435302734\n",
      "ITC Loss: 2.8091201782226562, ITM Loss: 0.6941832900047302, ITG Loss: 5.000598430633545\n",
      "**************************************************\n",
      "Epoch 1 : Iter [457 / 506]\n",
      "Total Loss: 8.603546142578125\n",
      "ITC Loss: 2.788081169128418, ITM Loss: 0.6942391991615295, ITG Loss: 5.1212263107299805\n",
      "**************************************************\n",
      "Epoch 1 : Iter [458 / 506]\n",
      "Total Loss: 8.706757545471191\n",
      "ITC Loss: 2.821190118789673, ITM Loss: 0.6932874321937561, ITG Loss: 5.192280292510986\n",
      "**************************************************\n",
      "Epoch 1 : Iter [459 / 506]\n",
      "Total Loss: 8.145909309387207\n",
      "ITC Loss: 2.7971653938293457, ITM Loss: 0.6911942362785339, ITG Loss: 4.657549858093262\n",
      "**************************************************\n",
      "Epoch 1 : Iter [460 / 506]\n",
      "Total Loss: 8.452543258666992\n",
      "ITC Loss: 2.8114705085754395, ITM Loss: 0.6961553692817688, ITG Loss: 4.944917678833008\n",
      "**************************************************\n",
      "Epoch 1 : Iter [461 / 506]\n",
      "Total Loss: 8.701904296875\n",
      "ITC Loss: 2.8395657539367676, ITM Loss: 0.6938019394874573, ITG Loss: 5.168536186218262\n",
      "**************************************************\n",
      "Epoch 1 : Iter [462 / 506]\n",
      "Total Loss: 8.57427978515625\n",
      "ITC Loss: 2.7996082305908203, ITM Loss: 0.6890559196472168, ITG Loss: 5.085615634918213\n",
      "**************************************************\n",
      "Epoch 1 : Iter [463 / 506]\n",
      "Total Loss: 8.284920692443848\n",
      "ITC Loss: 2.849454879760742, ITM Loss: 0.6931678056716919, ITG Loss: 4.742298126220703\n",
      "**************************************************\n",
      "Epoch 1 : Iter [464 / 506]\n",
      "Total Loss: 8.03406047821045\n",
      "ITC Loss: 2.8009533882141113, ITM Loss: 0.6962829232215881, ITG Loss: 4.5368242263793945\n",
      "**************************************************\n",
      "Epoch 1 : Iter [465 / 506]\n",
      "Total Loss: 8.09697437286377\n",
      "ITC Loss: 2.8102917671203613, ITM Loss: 0.6955180764198303, ITG Loss: 4.591164588928223\n",
      "**************************************************\n",
      "Epoch 1 : Iter [466 / 506]\n",
      "Total Loss: 8.50683307647705\n",
      "ITC Loss: 2.854288101196289, ITM Loss: 0.6948649287223816, ITG Loss: 4.957679748535156\n",
      "**************************************************\n",
      "Epoch 1 : Iter [467 / 506]\n",
      "Total Loss: 8.22021198272705\n",
      "ITC Loss: 2.8024866580963135, ITM Loss: 0.6922836303710938, ITG Loss: 4.725441932678223\n",
      "**************************************************\n",
      "Epoch 1 : Iter [468 / 506]\n",
      "Total Loss: 8.15042495727539\n",
      "ITC Loss: 2.781092882156372, ITM Loss: 0.6935942769050598, ITG Loss: 4.675737380981445\n",
      "**************************************************\n",
      "Epoch 1 : Iter [469 / 506]\n",
      "Total Loss: 8.181473731994629\n",
      "ITC Loss: 2.8133654594421387, ITM Loss: 0.6977456212043762, ITG Loss: 4.670362949371338\n",
      "**************************************************\n",
      "Epoch 1 : Iter [470 / 506]\n",
      "Total Loss: 8.678284645080566\n",
      "ITC Loss: 2.820830821990967, ITM Loss: 0.6923922896385193, ITG Loss: 5.1650614738464355\n",
      "**************************************************\n",
      "Epoch 1 : Iter [471 / 506]\n",
      "Total Loss: 8.41363525390625\n",
      "ITC Loss: 2.8061161041259766, ITM Loss: 0.6923893094062805, ITG Loss: 4.915130138397217\n",
      "**************************************************\n",
      "Epoch 1 : Iter [472 / 506]\n",
      "Total Loss: 9.158199310302734\n",
      "ITC Loss: 2.8170714378356934, ITM Loss: 0.6954957842826843, ITG Loss: 5.645631790161133\n",
      "**************************************************\n",
      "Epoch 1 : Iter [473 / 506]\n",
      "Total Loss: 8.130518913269043\n",
      "ITC Loss: 2.8189940452575684, ITM Loss: 0.6984837651252747, ITG Loss: 4.613041400909424\n",
      "**************************************************\n",
      "Epoch 1 : Iter [474 / 506]\n",
      "Total Loss: 8.443761825561523\n",
      "ITC Loss: 2.8415002822875977, ITM Loss: 0.6909673810005188, ITG Loss: 4.911293983459473\n",
      "**************************************************\n",
      "Epoch 1 : Iter [475 / 506]\n",
      "Total Loss: 8.007851600646973\n",
      "ITC Loss: 2.794734477996826, ITM Loss: 0.6905204653739929, ITG Loss: 4.522596836090088\n",
      "**************************************************\n",
      "Epoch 1 : Iter [476 / 506]\n",
      "Total Loss: 8.470417976379395\n",
      "ITC Loss: 2.7864232063293457, ITM Loss: 0.6948983669281006, ITG Loss: 4.989096164703369\n",
      "**************************************************\n",
      "Epoch 1 : Iter [477 / 506]\n",
      "Total Loss: 8.063131332397461\n",
      "ITC Loss: 2.7883501052856445, ITM Loss: 0.6917232275009155, ITG Loss: 4.5830583572387695\n",
      "**************************************************\n",
      "Epoch 1 : Iter [478 / 506]\n",
      "Total Loss: 8.719928741455078\n",
      "ITC Loss: 2.821845531463623, ITM Loss: 0.6914418935775757, ITG Loss: 5.20664119720459\n",
      "**************************************************\n",
      "Epoch 1 : Iter [479 / 506]\n",
      "Total Loss: 8.686529159545898\n",
      "ITC Loss: 2.8510165214538574, ITM Loss: 0.692969799041748, ITG Loss: 5.142542839050293\n",
      "**************************************************\n",
      "Epoch 1 : Iter [480 / 506]\n",
      "Total Loss: 8.097246170043945\n",
      "ITC Loss: 2.7850146293640137, ITM Loss: 0.6929243206977844, ITG Loss: 4.619307041168213\n",
      "**************************************************\n",
      "Epoch 1 : Iter [481 / 506]\n",
      "Total Loss: 8.362735748291016\n",
      "ITC Loss: 2.763857841491699, ITM Loss: 0.6935725808143616, ITG Loss: 4.905304908752441\n",
      "**************************************************\n",
      "Epoch 1 : Iter [482 / 506]\n",
      "Total Loss: 8.55015754699707\n",
      "ITC Loss: 2.808962821960449, ITM Loss: 0.6936696767807007, ITG Loss: 5.047525405883789\n",
      "**************************************************\n",
      "Epoch 1 : Iter [483 / 506]\n",
      "Total Loss: 8.43231201171875\n",
      "ITC Loss: 2.7950685024261475, ITM Loss: 0.6922359466552734, ITG Loss: 4.94500732421875\n",
      "**************************************************\n",
      "Epoch 1 : Iter [484 / 506]\n",
      "Total Loss: 8.532382011413574\n",
      "ITC Loss: 2.8571290969848633, ITM Loss: 0.694631814956665, ITG Loss: 4.980621337890625\n",
      "**************************************************\n",
      "Epoch 1 : Iter [485 / 506]\n",
      "Total Loss: 8.134519577026367\n",
      "ITC Loss: 2.829789161682129, ITM Loss: 0.6945539712905884, ITG Loss: 4.610177040100098\n",
      "**************************************************\n",
      "Epoch 1 : Iter [486 / 506]\n",
      "Total Loss: 8.375320434570312\n",
      "ITC Loss: 2.8188061714172363, ITM Loss: 0.6936064958572388, ITG Loss: 4.862908363342285\n",
      "**************************************************\n",
      "Epoch 1 : Iter [487 / 506]\n",
      "Total Loss: 8.555635452270508\n",
      "ITC Loss: 2.8387451171875, ITM Loss: 0.6926632523536682, ITG Loss: 5.024227619171143\n",
      "**************************************************\n",
      "Epoch 1 : Iter [488 / 506]\n",
      "Total Loss: 8.975225448608398\n",
      "ITC Loss: 2.78810977935791, ITM Loss: 0.6915916204452515, ITG Loss: 5.4955244064331055\n",
      "**************************************************\n",
      "Epoch 1 : Iter [489 / 506]\n",
      "Total Loss: 8.222878456115723\n",
      "ITC Loss: 2.796123504638672, ITM Loss: 0.6929376721382141, ITG Loss: 4.733817100524902\n",
      "**************************************************\n",
      "Epoch 1 : Iter [490 / 506]\n",
      "Total Loss: 7.957452774047852\n",
      "ITC Loss: 2.8296003341674805, ITM Loss: 0.6909465193748474, ITG Loss: 4.436905860900879\n",
      "**************************************************\n",
      "Epoch 1 : Iter [491 / 506]\n",
      "Total Loss: 7.836928367614746\n",
      "ITC Loss: 2.7914834022521973, ITM Loss: 0.6975833773612976, ITG Loss: 4.3478617668151855\n",
      "**************************************************\n",
      "Epoch 1 : Iter [492 / 506]\n",
      "Total Loss: 8.300525665283203\n",
      "ITC Loss: 2.791159152984619, ITM Loss: 0.6924710869789124, ITG Loss: 4.816895008087158\n",
      "**************************************************\n",
      "Epoch 1 : Iter [493 / 506]\n",
      "Total Loss: 8.436162948608398\n",
      "ITC Loss: 2.795888900756836, ITM Loss: 0.6942881345748901, ITG Loss: 4.945986270904541\n",
      "**************************************************\n",
      "Epoch 1 : Iter [494 / 506]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : Iter [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mITC Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitc_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ITM Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitm_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ITG Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitg_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)\n",
    "num_epochs = 3\n",
    "\n",
    "itc_loss_func = ITCLoss()\n",
    "itm_loss_func = ITMLoss(config)\n",
    "itg_loss_func = ITGLoss(blip2_tokenizer.pad_token_id)\n",
    "\n",
    "itc_loss_func = itc_loss_func.to(device)\n",
    "itm_loss_func = itm_loss_func.to(device)\n",
    "itg_loss_func = itg_loss_func.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iter = 0\n",
    "    print(f\"***************   Epoch {epoch + 1}  ***************\")\n",
    "    for img, cls_caption, dec_caption in train_dataloader:\n",
    "        img = img.to(device)\n",
    "        cls_caption = cls_caption.to(device)\n",
    "        dec_caption = dec_caption.to(device)\n",
    "        B, _, _, _ = img.shape\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = model(img, cls_caption, dec_caption, 1)\n",
    "\n",
    "        # ITC Loss\n",
    "        itc_loss, itc_logits = itc_loss_func(itc_query_embds, itc_text_embds)\n",
    "\n",
    "        # ITM Loss\n",
    "        idx = torch.arange(B,device = device)\n",
    "        itc_logits[idx,idx] = -1e9\n",
    "        next_best_text_value , next_best_text_idx = torch.max(itc_logits,dim=1)\n",
    "        mismatched_cls_caption = cls_caption[next_best_text_idx]\n",
    "        mismatched_dec_caption = dec_caption[next_best_text_idx]\n",
    "\n",
    "        _,_,mismatched_itm_query_embeds,_,_ = model(img, mismatched_cls_caption, mismatched_dec_caption,1)\n",
    "\n",
    "        itm_query_embed_concatenated = torch.concat((itm_query_embds, mismatched_itm_query_embeds) ,dim=0 )\n",
    "        itm_labels = torch.zeros(2 * B, dtype=torch.long).to(device)\n",
    "        itm_labels[B:] = 1\n",
    "        itm_loss = itm_loss_func(itm_query_embed_concatenated, itm_labels)\n",
    "\n",
    "        # ITG Loss\n",
    "        itg_labels = torch.concat((dec_caption[:, 1:], dec_caption[:, -1].unsqueeze(1)), dim=1)\n",
    "        itg_loss = itg_loss_func(itg_logits, itg_labels)\n",
    "\n",
    "\n",
    "        total_loss = itc_loss + itm_loss + itg_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} : Iter [{iter} / {len(train_dataloader)}]\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\")\n",
    "        print(f\"ITC Loss: {itc_loss}, ITM Loss: {itm_loss}, ITG Loss: {itg_loss}\")\n",
    "        print(\"\" + \"*\" * 50)\n",
    "        iter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0fcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e442e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f760470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
