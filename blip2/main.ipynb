{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import FlanT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, dataset, tokenizer\n",
    "importlib.reload(config)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import FlanT5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ccd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "t5_model_name = \"google/flan-t5-small\"\n",
    "bert_tokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Blip2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5_tokenizer = FlanT5Tokenizer(config, bert_tokenizer)\n",
    "config.vocab_size = flan_t5_tokenizer.n_vocab\n",
    "train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=flan_t5_tokenizer.tokenize_text)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40420224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import timm\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class BertMLPBlock(nn.Module):\n",
    "    def __init__(self, intermediate, output):\n",
    "        super().__init__()\n",
    "        self.intermediate = intermediate\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_output = self.intermediate(x)\n",
    "        return self.output(intermediate_output, x)\n",
    "    \n",
    "\n",
    "class BertEncoderBlock(nn.Module):\n",
    "    def __init__(self, bert_layer, bert_config, is_cross_attn=False):\n",
    "        super().__init__()\n",
    "        self.bert_config = bert_config\n",
    "        self.is_cross_attn = is_cross_attn\n",
    "        self.self_attn = bert_layer.attention\n",
    "        self.mlp_img_transformer = BertMLPBlock(bert_layer.intermediate, bert_layer.output)\n",
    "        self.mlp_text_transformer = BertMLPBlock(\n",
    "                    copy.deepcopy(bert_layer.intermediate), \n",
    "                    copy.deepcopy(bert_layer.output)\n",
    "                    )\n",
    "        if is_cross_attn:\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim=self.bert_config.hidden_size, \n",
    "                                                    num_heads=self.bert_config.num_attention_heads, \n",
    "                                                    batch_first=True)\n",
    "            self.cross_layer_norm = nn.LayerNorm(self.bert_config.hidden_size)\n",
    "        \n",
    "    def forward(self, query_embds, img_embds, text_embds, attn_mask):\n",
    "        _, Qs, _ = query_embds.shape\n",
    "        _, Ts, _ = text_embds.shape\n",
    "\n",
    "        combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
    "\n",
    "        self_attn_output = self.self_attn(combined_embds, attention_mask=attn_mask)[0]\n",
    "        query_embds = combined_embds[:, :Qs]\n",
    "        text_embds= combined_embds[:, Qs:]\n",
    "        \n",
    "        if self.is_cross_attn:\n",
    "            hidden_states = self.cross_attn(query_embds, img_embds, img_embds)[0]\n",
    "            query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
    "\n",
    "        query_embds = self.mlp_img_transformer(query_embds)\n",
    "        text_embds = self.mlp_text_transformer(text_embds)\n",
    "        return query_embds, text_embds\n",
    "\n",
    "\n",
    "class QTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert_cfg  = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\", config = self.bert_cfg)\n",
    "        \n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, bert_layer in enumerate(self.bert_model.encoder.layer):\n",
    "            self.encoder.append(BertEncoderBlock(bert_layer, self.bert_cfg, i % 2 == 0))\n",
    "        \n",
    "        qs = config.num_queries\n",
    "        ts = config.context_length\n",
    "        combined_seq_len = qs + ts\n",
    "\n",
    "        ####  STAGE 1: ITC, ITM, ITG Loss Masks ####\n",
    "        # ITC Loss Mask\n",
    "        itc_attn_mask = torch.zeros((combined_seq_len, combined_seq_len))\n",
    "        itc_attn_mask[:qs, :qs] = 1\n",
    "        itc_attn_mask[qs:, qs:] = 1\n",
    "\n",
    "        # ITM Loss Mask\n",
    "        itm_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "\n",
    "        # ITG Loss Mask\n",
    "        itg_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "        itg_attn_mask[:qs, qs:] = 0\n",
    "        itg_attn_mask[qs:, qs:] = torch.tril(itg_attn_mask[qs:, qs:], diagonal=0)\n",
    "\n",
    "        self.register_buffer(\"itc_attn_mask\", itc_attn_mask)\n",
    "        self.register_buffer(\"itm_attn_mask\", itm_attn_mask)\n",
    "        self.register_buffer(\"itg_attn_mask\", itg_attn_mask)\n",
    "\n",
    "        ####  STAGE 2: ####\n",
    "        # ITC Loss Mask will be same as stage 1 and reused for stage 2\n",
    "\n",
    "    def forward(self, query_embds, img_embds, cls_text_embds, dec_text_embds, stage):\n",
    "\n",
    "        itc_query_embds = query_embds.clone()\n",
    "        itm_query_embds = query_embds.clone()\n",
    "        itg_query_embds = query_embds.clone()\n",
    "\n",
    "        itc_text_embds = cls_text_embds.clone()\n",
    "        itm_text_embds = cls_text_embds.clone()\n",
    "        itg_text_embds = dec_text_embds.clone()\n",
    "\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            itc_query_embds, itc_text_embds = encoder(itc_query_embds, img_embds, itc_text_embds, self.itc_attn_mask)\n",
    "            if stage == 1:\n",
    "                itm_query_embds, itm_text_embds = encoder(itm_query_embds, img_embds, itm_text_embds, self.itm_attn_mask)\n",
    "                itg_query_embds, itg_text_embds = encoder(itg_query_embds, img_embds, itg_text_embds, self.itg_attn_mask)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds\n",
    "    \n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q_transformer = QTransformer(config)\n",
    "        self.learned_query = nn.Parameter(torch.randn(config.num_queries, config.embedding_dim))\n",
    "        self.output_embedding  = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(config.context_length, config.embedding_dim)\n",
    "\n",
    "        position_ids = torch.arange(self.config.context_length).unsqueeze(0)\n",
    "        self.register_buffer(\"position_ids\", position_ids)\n",
    "\n",
    "    def forward(self, image_embedding: torch.tensor, cls_tokens: torch.tensor, dec_tokens: torch.tensor, stage:int):\n",
    "        B, S, E = image_embedding.shape\n",
    "        learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        cls_text_embeddings = self.output_embedding(cls_tokens) #(S,768)\n",
    "        cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "        dec_text_embeddings = self.output_embedding(dec_tokens) #(S,768)\n",
    "        dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds = self.q_transformer(\n",
    "            learned_query, image_embedding, cls_text_embeddings, dec_text_embeddings, stage)\n",
    "\n",
    "        if itg_text_embds is not None:\n",
    "            itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
    "        else:\n",
    "            itg_logits = None\n",
    "\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "\n",
    "\n",
    "class FlanT5Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlanT5Model, self).__init__()\n",
    "        self.lm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "        for param in self.lm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, query_embedding, input_token, label, enc_mask):\n",
    "        #query_embd : (B,32,512)\n",
    "        # input_token : (B,L)\n",
    "        B, Q, d = query_embedding.shape\n",
    "        device = query_embedding.device\n",
    "        with torch.no_grad():\n",
    "            input_embd = self.lm_model.encoder.embed_tokens(input_token)  #(B,L,512)\n",
    "\n",
    "        encoder_input = torch.concat((query_embedding, input_embd) , dim = 1)\n",
    "\n",
    "        prefix_mask = torch.ones((B, Q ), dtype= enc_mask.dtype, device=device)\n",
    "        attention_mask = torch.concat((prefix_mask, enc_mask) , dim=1)  # [B, 32+L]\n",
    "\n",
    "        out = self.lm_model(inputs_embeds=encoder_input,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=label,\n",
    "                                return_dict=True)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def predict(self, query_embedding, input_token, enc_mask):\n",
    "        B, Q, d = query_embedding.shape\n",
    "        device = query_embedding.device\n",
    "        with torch.no_grad():\n",
    "            input_embd = self.lm_model.encoder.embed_tokens(input_token)  #(B,L,512)\n",
    "\n",
    "        encoder_input = torch.concat((query_embedding, input_embd) , dim = 1)\n",
    "\n",
    "        prefix_mask = torch.ones((B, Q ), dtype= enc_mask.dtype, device=device)\n",
    "        attention_mask = torch.concat((prefix_mask, enc_mask) , dim=1)  # [B, 32+L]\n",
    "        \n",
    "        enc_out = self.lm_model.encoder(\n",
    "            inputs_embeds=encoder_input,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "            )\n",
    "\n",
    "        gen_ids = self.lm_model.generate(\n",
    "            encoder_outputs=enc_out,\n",
    "            max_new_tokens=30,\n",
    "            decoder_start_token_id=self.lm_model.config.decoder_start_token_id,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        return gen_ids\n",
    "\n",
    "\n",
    "class Blip2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.image_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "        self.image_encoder.reset_classifier(0)\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.image_proj = nn.Linear(config.img_embd_dim, config.embedding_dim)\n",
    "\n",
    "        self.q_former = QFormer(config)\n",
    "        self.z_proj = nn.Linear(config.embedding_dim, config.lm_embedding_dim)\n",
    "\n",
    "        self.lm_model = FlanT5Model()\n",
    "    \n",
    "    \n",
    "    def stage1(self, image:torch.tensor, cls_caption:torch.tensor, dec_caption:torch.tensor):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, cls_caption, dec_caption, 1)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "    \n",
    "    \n",
    "    def stage2(self, image, input_token, label, enc_mask, dummy_input_size):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "        \n",
    "        cls_caption_dummy = torch.zeros(dummy_input_size)\n",
    "        dec_caption_dummy = torch.zeros(dummy_input_size)\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, \n",
    "                                                            cls_caption_dummy, dec_caption_dummy, 2)\n",
    "        \n",
    "        z = self.z_proj(itc_query_embds)  # [B, Qs, D]\n",
    "\n",
    "        out = self.lm_model(z, input_token, label, enc_mask)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b708e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Blip2Model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6bffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query_embds, text_embds):\n",
    "        # query_embds: B, 32, d\n",
    "        # text_embds: B, 77, d\n",
    "        text_logit = text_embds[:, :1] # B, 1, d\n",
    "        B, _, _ = text_logit.shape\n",
    "        B, Qs, d = query_embds.shape \n",
    "        query_embds = query_embds.reshape(B * Qs, d)\n",
    "        text_embds = text_logit.squeeze()\n",
    "        logits = query_embds @ text_embds.T   # B*Qs,B\n",
    "        logits = torch.max(logits.reshape(B,Qs,B),dim=1)[0] # B,B\n",
    "        label = torch.arange(B,device=query_embds.device)\n",
    "        return (F.cross_entropy(logits,label)+ F.cross_entropy(logits.T,label)) / 2, logits\n",
    "\n",
    "\n",
    "class ITMLoss(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        d = config.embedding_dim\n",
    "        self.classification_layer = nn.Linear(d,2)\n",
    "    \n",
    "    def forward(self, query_embd, label):\n",
    "        # query_embd --> (B,32,768)\n",
    "        #label ->(B,1) B x [0/1]\n",
    "\n",
    "        match_logit = self.classification_layer(query_embd) #(B,32,2)\n",
    "        match_logit = match_logit.mean(dim=1)\n",
    "        return F.cross_entropy(match_logit,label)\n",
    "\n",
    "\n",
    "class ITGLoss(nn.Module):\n",
    "    def __init__(self, pad_token_id):\n",
    "        super().__init__()\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def forward(self, itg_logits, label_token):\n",
    "        #itg_logits -> B,S,vocab size\n",
    "        #label_token -> B,S\n",
    "        B, S, V = itg_logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            itg_logits.view(B * S, V),\n",
    "            label_token.view(B * S),\n",
    "            ignore_index=self.pad_token_id\n",
    "        )\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73734f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "num_epochs = 3\n",
    "\n",
    "itc_loss_func = ITCLoss()\n",
    "itm_loss_func = ITMLoss(config)\n",
    "itg_loss_func = ITGLoss(blip2_tokenizer.pad_token_id)\n",
    "\n",
    "itc_loss_func = itc_loss_func.to(device)\n",
    "itm_loss_func = itm_loss_func.to(device)\n",
    "itg_loss_func = itg_loss_func.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iter = 0\n",
    "    print(f\"***************   Epoch {epoch + 1}  ***************\")\n",
    "    for img, cls_caption, dec_caption in train_dataloader:\n",
    "        img = img.to(device)\n",
    "        cls_caption = cls_caption.to(device)\n",
    "        dec_caption = dec_caption.to(device)\n",
    "        B, _, _, _ = img.shape\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = model(img, cls_caption, dec_caption, 1)\n",
    "\n",
    "        # ITC Loss\n",
    "        itc_loss, itc_logits = itc_loss_func(itc_query_embds, itc_text_embds)\n",
    "\n",
    "        # ITM Loss\n",
    "        idx = torch.arange(B,device = device)\n",
    "        itc_logits[idx,idx] = -1e9\n",
    "        next_best_text_value , next_best_text_idx = torch.max(itc_logits,dim=1)\n",
    "        mismatched_cls_caption = cls_caption[next_best_text_idx]\n",
    "        mismatched_dec_caption = dec_caption[next_best_text_idx]\n",
    "\n",
    "        _,_,mismatched_itm_query_embeds,_,_ = model(img, mismatched_cls_caption, mismatched_dec_caption,1)\n",
    "\n",
    "        itm_query_embed_concatenated = torch.concat((itm_query_embds, mismatched_itm_query_embeds) ,dim=0 )\n",
    "        itm_labels = torch.zeros(2 * B, dtype=torch.long).to(device)\n",
    "        itm_labels[B:] = 1\n",
    "        itm_loss = itm_loss_func(itm_query_embed_concatenated, itm_labels)\n",
    "\n",
    "        # ITG Loss\n",
    "        itg_labels = torch.concat((dec_caption[:, 1:], dec_caption[:, -1].unsqueeze(1)), dim=1)\n",
    "        itg_loss = itg_loss_func(itg_logits, itg_labels)\n",
    "\n",
    "\n",
    "        total_loss = itc_loss + itm_loss + itg_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} : Iter [{iter} / {len(train_dataloader)}]\")\n",
    "        print(f\"Total Loss: {total_loss.item()}\")\n",
    "        print(f\"ITC Loss: {itc_loss}, ITM Loss: {itm_loss}, ITG Loss: {itg_loss}\")\n",
    "        print(\"\" + \"*\" * 50)\n",
    "        iter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6353080",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "num_epochs = 3\n",
    "\n",
    "itc_loss_func = ITCLoss()\n",
    "itm_loss_func = ITMLoss(config)\n",
    "itg_loss_func = ITGLoss(blip2_tokenizer.pad_token_id)\n",
    "\n",
    "itc_loss_func = itc_loss_func.to(device)\n",
    "itm_loss_func = itm_loss_func.to(device)\n",
    "itg_loss_func = itg_loss_func.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t5_tokenizer.batch_decode(gen_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e442e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f760470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
