{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af41cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "from config import VitConfig\n",
    "from model import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4ac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import model, config\n",
    "importlib.reload(config)\n",
    "importlib.reload(model)\n",
    "\n",
    "from config import VitConfig\n",
    "from model import VisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27dbe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VitConfig()\n",
    "model = VisionTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc45094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((config.img_size,config.img_size))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15632017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIFAR labels to human readable labels\n",
    "CIFAR10_CLASSES = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "data = next(iter(train_loader))\n",
    "images, labels = data\n",
    "\n",
    "# Check the shape of the images and labels\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "#decode the first image\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(2, 2))  # keep this small to avoid blur\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')  # no interpolation\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "print(CIFAR10_CLASSES[labels[0].item()])\n",
    "print(images[0])\n",
    "imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61887ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 50\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_loader), eta_min=1e-6)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbf988ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    iter = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"[Epoch {epoch + 1}] Training Batch [{iter + 1}/{len(train_loader)}]: Loss {loss.item()}\")\n",
    "        iter += 1\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2196cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********  Epoch 1/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Training Batch [1/391]: Loss 2.6821706295013428\n",
      "[Epoch 1] Training Batch [2/391]: Loss 4.082300186157227\n",
      "[Epoch 1] Training Batch [3/391]: Loss 5.352705001831055\n",
      "[Epoch 1] Training Batch [4/391]: Loss 4.727759838104248\n",
      "[Epoch 1] Training Batch [5/391]: Loss 4.957481861114502\n",
      "[Epoch 1] Training Batch [6/391]: Loss 4.103341102600098\n",
      "[Epoch 1] Training Batch [7/391]: Loss 3.6007978916168213\n",
      "[Epoch 1] Training Batch [8/391]: Loss 3.0070228576660156\n",
      "[Epoch 1] Training Batch [9/391]: Loss 2.9414572715759277\n",
      "[Epoch 1] Training Batch [10/391]: Loss 3.197847366333008\n",
      "[Epoch 1] Training Batch [11/391]: Loss 2.878739833831787\n",
      "[Epoch 1] Training Batch [12/391]: Loss 2.665365219116211\n",
      "[Epoch 1] Training Batch [13/391]: Loss 2.511077880859375\n",
      "[Epoch 1] Training Batch [14/391]: Loss 2.4874930381774902\n",
      "[Epoch 1] Training Batch [15/391]: Loss 2.4537875652313232\n",
      "[Epoch 1] Training Batch [16/391]: Loss 2.3713698387145996\n",
      "[Epoch 1] Training Batch [17/391]: Loss 2.681516647338867\n",
      "[Epoch 1] Training Batch [18/391]: Loss 2.4770889282226562\n",
      "[Epoch 1] Training Batch [19/391]: Loss 2.5030784606933594\n",
      "[Epoch 1] Training Batch [20/391]: Loss 2.3911781311035156\n",
      "[Epoch 1] Training Batch [21/391]: Loss 2.4188625812530518\n",
      "[Epoch 1] Training Batch [22/391]: Loss 2.3664727210998535\n",
      "[Epoch 1] Training Batch [23/391]: Loss 2.3335540294647217\n",
      "[Epoch 1] Training Batch [24/391]: Loss 2.2500791549682617\n",
      "[Epoch 1] Training Batch [25/391]: Loss 2.469118356704712\n",
      "[Epoch 1] Training Batch [26/391]: Loss 2.421156167984009\n",
      "[Epoch 1] Training Batch [27/391]: Loss 2.338146209716797\n",
      "[Epoch 1] Training Batch [28/391]: Loss 2.3387563228607178\n",
      "[Epoch 1] Training Batch [29/391]: Loss 2.309351682662964\n",
      "[Epoch 1] Training Batch [30/391]: Loss 2.2941737174987793\n",
      "[Epoch 1] Training Batch [31/391]: Loss 2.264291524887085\n",
      "[Epoch 1] Training Batch [32/391]: Loss 2.281074285507202\n",
      "[Epoch 1] Training Batch [33/391]: Loss 2.3520233631134033\n",
      "[Epoch 1] Training Batch [34/391]: Loss 2.1580841541290283\n",
      "[Epoch 1] Training Batch [35/391]: Loss 2.161532402038574\n",
      "[Epoch 1] Training Batch [36/391]: Loss 2.4048352241516113\n",
      "[Epoch 1] Training Batch [37/391]: Loss 2.2688848972320557\n",
      "[Epoch 1] Training Batch [38/391]: Loss 2.081301689147949\n",
      "[Epoch 1] Training Batch [39/391]: Loss 2.2585384845733643\n",
      "[Epoch 1] Training Batch [40/391]: Loss 2.205878734588623\n",
      "[Epoch 1] Training Batch [41/391]: Loss 2.3189401626586914\n",
      "[Epoch 1] Training Batch [42/391]: Loss 2.232351064682007\n",
      "[Epoch 1] Training Batch [43/391]: Loss 2.2468132972717285\n",
      "[Epoch 1] Training Batch [44/391]: Loss 2.223147392272949\n",
      "[Epoch 1] Training Batch [45/391]: Loss 2.129239082336426\n",
      "[Epoch 1] Training Batch [46/391]: Loss 2.2164666652679443\n",
      "[Epoch 1] Training Batch [47/391]: Loss 2.1569273471832275\n",
      "[Epoch 1] Training Batch [48/391]: Loss 2.1731884479522705\n",
      "[Epoch 1] Training Batch [49/391]: Loss 2.287883996963501\n",
      "[Epoch 1] Training Batch [50/391]: Loss 2.2960710525512695\n",
      "[Epoch 1] Training Batch [51/391]: Loss 2.1595773696899414\n",
      "[Epoch 1] Training Batch [52/391]: Loss 2.2426819801330566\n",
      "[Epoch 1] Training Batch [53/391]: Loss 2.295966625213623\n",
      "[Epoch 1] Training Batch [54/391]: Loss 2.0967845916748047\n",
      "[Epoch 1] Training Batch [55/391]: Loss 2.112238645553589\n",
      "[Epoch 1] Training Batch [56/391]: Loss 2.1825361251831055\n",
      "[Epoch 1] Training Batch [57/391]: Loss 2.3493518829345703\n",
      "[Epoch 1] Training Batch [58/391]: Loss 2.1877262592315674\n",
      "[Epoch 1] Training Batch [59/391]: Loss 2.3110885620117188\n",
      "[Epoch 1] Training Batch [60/391]: Loss 2.1967315673828125\n",
      "[Epoch 1] Training Batch [61/391]: Loss 2.0483670234680176\n",
      "[Epoch 1] Training Batch [62/391]: Loss 2.2068285942077637\n",
      "[Epoch 1] Training Batch [63/391]: Loss 2.1313869953155518\n",
      "[Epoch 1] Training Batch [64/391]: Loss 2.1354002952575684\n",
      "[Epoch 1] Training Batch [65/391]: Loss 2.1969101428985596\n",
      "[Epoch 1] Training Batch [66/391]: Loss 2.0903983116149902\n",
      "[Epoch 1] Training Batch [67/391]: Loss 2.0673916339874268\n",
      "[Epoch 1] Training Batch [68/391]: Loss 2.2592062950134277\n",
      "[Epoch 1] Training Batch [69/391]: Loss 2.1374614238739014\n",
      "[Epoch 1] Training Batch [70/391]: Loss 2.156148910522461\n",
      "[Epoch 1] Training Batch [71/391]: Loss 2.1700289249420166\n",
      "[Epoch 1] Training Batch [72/391]: Loss 2.089280605316162\n",
      "[Epoch 1] Training Batch [73/391]: Loss 2.167296886444092\n",
      "[Epoch 1] Training Batch [74/391]: Loss 2.0653939247131348\n",
      "[Epoch 1] Training Batch [75/391]: Loss 2.1643824577331543\n",
      "[Epoch 1] Training Batch [76/391]: Loss 2.1933674812316895\n",
      "[Epoch 1] Training Batch [77/391]: Loss 2.1208341121673584\n",
      "[Epoch 1] Training Batch [78/391]: Loss 2.3042516708374023\n",
      "[Epoch 1] Training Batch [79/391]: Loss 2.090763568878174\n",
      "[Epoch 1] Training Batch [80/391]: Loss 2.2591359615325928\n",
      "[Epoch 1] Training Batch [81/391]: Loss 2.108696222305298\n",
      "[Epoch 1] Training Batch [82/391]: Loss 2.138648748397827\n",
      "[Epoch 1] Training Batch [83/391]: Loss 2.162750244140625\n",
      "[Epoch 1] Training Batch [84/391]: Loss 2.0790047645568848\n",
      "[Epoch 1] Training Batch [85/391]: Loss 2.1311888694763184\n",
      "[Epoch 1] Training Batch [86/391]: Loss 2.193708896636963\n",
      "[Epoch 1] Training Batch [87/391]: Loss 2.053623914718628\n",
      "[Epoch 1] Training Batch [88/391]: Loss 2.1827449798583984\n",
      "[Epoch 1] Training Batch [89/391]: Loss 2.2483503818511963\n",
      "[Epoch 1] Training Batch [90/391]: Loss 2.1102166175842285\n",
      "[Epoch 1] Training Batch [91/391]: Loss 2.0654850006103516\n",
      "[Epoch 1] Training Batch [92/391]: Loss 2.1288864612579346\n",
      "[Epoch 1] Training Batch [93/391]: Loss 2.3000450134277344\n",
      "[Epoch 1] Training Batch [94/391]: Loss 2.1353673934936523\n",
      "[Epoch 1] Training Batch [95/391]: Loss 2.1675450801849365\n",
      "[Epoch 1] Training Batch [96/391]: Loss 2.1505768299102783\n",
      "[Epoch 1] Training Batch [97/391]: Loss 2.3483169078826904\n",
      "[Epoch 1] Training Batch [98/391]: Loss 2.1236631870269775\n",
      "[Epoch 1] Training Batch [99/391]: Loss 2.1348555088043213\n",
      "[Epoch 1] Training Batch [100/391]: Loss 2.2190427780151367\n",
      "[Epoch 1] Training Batch [101/391]: Loss 2.0893375873565674\n",
      "[Epoch 1] Training Batch [102/391]: Loss 2.1666829586029053\n",
      "[Epoch 1] Training Batch [103/391]: Loss 2.0997745990753174\n",
      "[Epoch 1] Training Batch [104/391]: Loss 2.03861141204834\n",
      "[Epoch 1] Training Batch [105/391]: Loss 2.0912985801696777\n",
      "[Epoch 1] Training Batch [106/391]: Loss 2.1780309677124023\n",
      "[Epoch 1] Training Batch [107/391]: Loss 2.1028411388397217\n",
      "[Epoch 1] Training Batch [108/391]: Loss 2.0386555194854736\n",
      "[Epoch 1] Training Batch [109/391]: Loss 2.2257742881774902\n",
      "[Epoch 1] Training Batch [110/391]: Loss 2.2084646224975586\n",
      "[Epoch 1] Training Batch [111/391]: Loss 2.0083703994750977\n",
      "[Epoch 1] Training Batch [112/391]: Loss 2.106010675430298\n",
      "[Epoch 1] Training Batch [113/391]: Loss 2.1216423511505127\n",
      "[Epoch 1] Training Batch [114/391]: Loss 2.1373424530029297\n",
      "[Epoch 1] Training Batch [115/391]: Loss 2.189885377883911\n",
      "[Epoch 1] Training Batch [116/391]: Loss 2.126051664352417\n",
      "[Epoch 1] Training Batch [117/391]: Loss 2.26623797416687\n",
      "[Epoch 1] Training Batch [118/391]: Loss 1.9977704286575317\n",
      "[Epoch 1] Training Batch [119/391]: Loss 2.0986573696136475\n",
      "[Epoch 1] Training Batch [120/391]: Loss 2.1253130435943604\n",
      "[Epoch 1] Training Batch [121/391]: Loss 2.232226848602295\n",
      "[Epoch 1] Training Batch [122/391]: Loss 2.027756452560425\n",
      "[Epoch 1] Training Batch [123/391]: Loss 2.264469623565674\n",
      "[Epoch 1] Training Batch [124/391]: Loss 2.310107946395874\n",
      "[Epoch 1] Training Batch [125/391]: Loss 2.039778232574463\n",
      "[Epoch 1] Training Batch [126/391]: Loss 2.2079288959503174\n",
      "[Epoch 1] Training Batch [127/391]: Loss 2.034501552581787\n",
      "[Epoch 1] Training Batch [128/391]: Loss 2.067096710205078\n",
      "[Epoch 1] Training Batch [129/391]: Loss 2.128187894821167\n",
      "[Epoch 1] Training Batch [130/391]: Loss 2.101823091506958\n",
      "[Epoch 1] Training Batch [131/391]: Loss 2.208507776260376\n",
      "[Epoch 1] Training Batch [132/391]: Loss 2.1286633014678955\n",
      "[Epoch 1] Training Batch [133/391]: Loss 2.0369956493377686\n",
      "[Epoch 1] Training Batch [134/391]: Loss 2.1649158000946045\n",
      "[Epoch 1] Training Batch [135/391]: Loss 2.0407183170318604\n",
      "[Epoch 1] Training Batch [136/391]: Loss 2.064951181411743\n",
      "[Epoch 1] Training Batch [137/391]: Loss 2.08125901222229\n",
      "[Epoch 1] Training Batch [138/391]: Loss 1.9953315258026123\n",
      "[Epoch 1] Training Batch [139/391]: Loss 2.1878585815429688\n",
      "[Epoch 1] Training Batch [140/391]: Loss 2.184730052947998\n",
      "[Epoch 1] Training Batch [141/391]: Loss 2.047020435333252\n",
      "[Epoch 1] Training Batch [142/391]: Loss 2.0981943607330322\n",
      "[Epoch 1] Training Batch [143/391]: Loss 2.0283968448638916\n",
      "[Epoch 1] Training Batch [144/391]: Loss 2.1101722717285156\n",
      "[Epoch 1] Training Batch [145/391]: Loss 1.9249975681304932\n",
      "[Epoch 1] Training Batch [146/391]: Loss 2.078407049179077\n",
      "[Epoch 1] Training Batch [147/391]: Loss 2.159327507019043\n",
      "[Epoch 1] Training Batch [148/391]: Loss 2.219038963317871\n",
      "[Epoch 1] Training Batch [149/391]: Loss 1.9508286714553833\n",
      "[Epoch 1] Training Batch [150/391]: Loss 2.0878682136535645\n",
      "[Epoch 1] Training Batch [151/391]: Loss 2.0603485107421875\n",
      "[Epoch 1] Training Batch [152/391]: Loss 2.135590076446533\n",
      "[Epoch 1] Training Batch [153/391]: Loss 1.8650492429733276\n",
      "[Epoch 1] Training Batch [154/391]: Loss 1.9508517980575562\n",
      "[Epoch 1] Training Batch [155/391]: Loss 1.9817529916763306\n",
      "[Epoch 1] Training Batch [156/391]: Loss 2.03735613822937\n",
      "[Epoch 1] Training Batch [157/391]: Loss 2.0125515460968018\n",
      "[Epoch 1] Training Batch [158/391]: Loss 1.9572269916534424\n",
      "[Epoch 1] Training Batch [159/391]: Loss 1.981083869934082\n",
      "[Epoch 1] Training Batch [160/391]: Loss 1.9888616800308228\n",
      "[Epoch 1] Training Batch [161/391]: Loss 2.0681302547454834\n",
      "[Epoch 1] Training Batch [162/391]: Loss 1.9687942266464233\n",
      "[Epoch 1] Training Batch [163/391]: Loss 1.9887561798095703\n",
      "[Epoch 1] Training Batch [164/391]: Loss 2.0153450965881348\n",
      "[Epoch 1] Training Batch [165/391]: Loss 2.0671465396881104\n",
      "[Epoch 1] Training Batch [166/391]: Loss 1.9340204000473022\n",
      "[Epoch 1] Training Batch [167/391]: Loss 1.9326159954071045\n",
      "[Epoch 1] Training Batch [168/391]: Loss 2.321279287338257\n",
      "[Epoch 1] Training Batch [169/391]: Loss 1.986515998840332\n",
      "[Epoch 1] Training Batch [170/391]: Loss 2.1437413692474365\n",
      "[Epoch 1] Training Batch [171/391]: Loss 2.1008827686309814\n",
      "[Epoch 1] Training Batch [172/391]: Loss 1.974661111831665\n",
      "[Epoch 1] Training Batch [173/391]: Loss 1.984359622001648\n",
      "[Epoch 1] Training Batch [174/391]: Loss 2.0678181648254395\n",
      "[Epoch 1] Training Batch [175/391]: Loss 2.1003530025482178\n",
      "[Epoch 1] Training Batch [176/391]: Loss 1.8873491287231445\n",
      "[Epoch 1] Training Batch [177/391]: Loss 2.0061466693878174\n",
      "[Epoch 1] Training Batch [178/391]: Loss 2.029832363128662\n",
      "[Epoch 1] Training Batch [179/391]: Loss 1.8968173265457153\n",
      "[Epoch 1] Training Batch [180/391]: Loss 2.030978202819824\n",
      "[Epoch 1] Training Batch [181/391]: Loss 1.9507893323898315\n",
      "[Epoch 1] Training Batch [182/391]: Loss 1.927254557609558\n",
      "[Epoch 1] Training Batch [183/391]: Loss 2.034675359725952\n",
      "[Epoch 1] Training Batch [184/391]: Loss 2.0079216957092285\n",
      "[Epoch 1] Training Batch [185/391]: Loss 2.0358879566192627\n",
      "[Epoch 1] Training Batch [186/391]: Loss 2.1345973014831543\n",
      "[Epoch 1] Training Batch [187/391]: Loss 1.9221667051315308\n",
      "[Epoch 1] Training Batch [188/391]: Loss 1.9160757064819336\n",
      "[Epoch 1] Training Batch [189/391]: Loss 2.046522855758667\n",
      "[Epoch 1] Training Batch [190/391]: Loss 2.1522700786590576\n",
      "[Epoch 1] Training Batch [191/391]: Loss 1.915409803390503\n",
      "[Epoch 1] Training Batch [192/391]: Loss 2.117971420288086\n",
      "[Epoch 1] Training Batch [193/391]: Loss 2.0706944465637207\n",
      "[Epoch 1] Training Batch [194/391]: Loss 1.9005005359649658\n",
      "[Epoch 1] Training Batch [195/391]: Loss 1.8812283277511597\n",
      "[Epoch 1] Training Batch [196/391]: Loss 2.0351688861846924\n",
      "[Epoch 1] Training Batch [197/391]: Loss 1.9890557527542114\n",
      "[Epoch 1] Training Batch [198/391]: Loss 1.9417699575424194\n",
      "[Epoch 1] Training Batch [199/391]: Loss 1.962958574295044\n",
      "[Epoch 1] Training Batch [200/391]: Loss 1.872370958328247\n",
      "[Epoch 1] Training Batch [201/391]: Loss 1.9095145463943481\n",
      "[Epoch 1] Training Batch [202/391]: Loss 2.1007280349731445\n",
      "[Epoch 1] Training Batch [203/391]: Loss 1.9741371870040894\n",
      "[Epoch 1] Training Batch [204/391]: Loss 1.9891316890716553\n",
      "[Epoch 1] Training Batch [205/391]: Loss 1.946836233139038\n",
      "[Epoch 1] Training Batch [206/391]: Loss 1.886548638343811\n",
      "[Epoch 1] Training Batch [207/391]: Loss 1.8752328157424927\n",
      "[Epoch 1] Training Batch [208/391]: Loss 1.8582760095596313\n",
      "[Epoch 1] Training Batch [209/391]: Loss 2.0827064514160156\n",
      "[Epoch 1] Training Batch [210/391]: Loss 1.7599902153015137\n",
      "[Epoch 1] Training Batch [211/391]: Loss 1.8683445453643799\n",
      "[Epoch 1] Training Batch [212/391]: Loss 1.9453845024108887\n",
      "[Epoch 1] Training Batch [213/391]: Loss 1.9256616830825806\n",
      "[Epoch 1] Training Batch [214/391]: Loss 1.878920555114746\n",
      "[Epoch 1] Training Batch [215/391]: Loss 1.9890543222427368\n",
      "[Epoch 1] Training Batch [216/391]: Loss 1.8680320978164673\n",
      "[Epoch 1] Training Batch [217/391]: Loss 1.9910435676574707\n",
      "[Epoch 1] Training Batch [218/391]: Loss 1.927862524986267\n",
      "[Epoch 1] Training Batch [219/391]: Loss 1.9036595821380615\n",
      "[Epoch 1] Training Batch [220/391]: Loss 1.961983323097229\n",
      "[Epoch 1] Training Batch [221/391]: Loss 1.9313242435455322\n",
      "[Epoch 1] Training Batch [222/391]: Loss 2.128139019012451\n",
      "[Epoch 1] Training Batch [223/391]: Loss 1.7971463203430176\n",
      "[Epoch 1] Training Batch [224/391]: Loss 1.8601404428482056\n",
      "[Epoch 1] Training Batch [225/391]: Loss 1.894315242767334\n",
      "[Epoch 1] Training Batch [226/391]: Loss 1.946225643157959\n",
      "[Epoch 1] Training Batch [227/391]: Loss 1.8627816438674927\n",
      "[Epoch 1] Training Batch [228/391]: Loss 1.8900015354156494\n",
      "[Epoch 1] Training Batch [229/391]: Loss 1.9124798774719238\n",
      "[Epoch 1] Training Batch [230/391]: Loss 1.910822868347168\n",
      "[Epoch 1] Training Batch [231/391]: Loss 1.903526782989502\n",
      "[Epoch 1] Training Batch [232/391]: Loss 1.8252588510513306\n",
      "[Epoch 1] Training Batch [233/391]: Loss 1.79477858543396\n",
      "[Epoch 1] Training Batch [234/391]: Loss 1.9424079656600952\n",
      "[Epoch 1] Training Batch [235/391]: Loss 1.9416693449020386\n",
      "[Epoch 1] Training Batch [236/391]: Loss 1.8476905822753906\n",
      "[Epoch 1] Training Batch [237/391]: Loss 1.7759149074554443\n",
      "[Epoch 1] Training Batch [238/391]: Loss 1.8236048221588135\n",
      "[Epoch 1] Training Batch [239/391]: Loss 1.9108998775482178\n",
      "[Epoch 1] Training Batch [240/391]: Loss 1.9230903387069702\n",
      "[Epoch 1] Training Batch [241/391]: Loss 1.804019808769226\n",
      "[Epoch 1] Training Batch [242/391]: Loss 1.8819198608398438\n",
      "[Epoch 1] Training Batch [243/391]: Loss 1.8982661962509155\n",
      "[Epoch 1] Training Batch [244/391]: Loss 1.7650669813156128\n",
      "[Epoch 1] Training Batch [245/391]: Loss 1.849671483039856\n",
      "[Epoch 1] Training Batch [246/391]: Loss 1.8110206127166748\n",
      "[Epoch 1] Training Batch [247/391]: Loss 1.8027280569076538\n",
      "[Epoch 1] Training Batch [248/391]: Loss 1.7335983514785767\n",
      "[Epoch 1] Training Batch [249/391]: Loss 1.6789125204086304\n",
      "[Epoch 1] Training Batch [250/391]: Loss 1.835098147392273\n",
      "[Epoch 1] Training Batch [251/391]: Loss 1.9408844709396362\n",
      "[Epoch 1] Training Batch [252/391]: Loss 1.8633779287338257\n",
      "[Epoch 1] Training Batch [253/391]: Loss 1.7980437278747559\n",
      "[Epoch 1] Training Batch [254/391]: Loss 1.641379952430725\n",
      "[Epoch 1] Training Batch [255/391]: Loss 1.5479620695114136\n",
      "[Epoch 1] Training Batch [256/391]: Loss 1.8696012496948242\n",
      "[Epoch 1] Training Batch [257/391]: Loss 2.0631537437438965\n",
      "[Epoch 1] Training Batch [258/391]: Loss 1.9008086919784546\n",
      "[Epoch 1] Training Batch [259/391]: Loss 1.7598259449005127\n",
      "[Epoch 1] Training Batch [260/391]: Loss 1.7272218465805054\n",
      "[Epoch 1] Training Batch [261/391]: Loss 1.7890185117721558\n",
      "[Epoch 1] Training Batch [262/391]: Loss 1.6041957139968872\n",
      "[Epoch 1] Training Batch [263/391]: Loss 1.9928902387619019\n",
      "[Epoch 1] Training Batch [264/391]: Loss 1.9150272607803345\n",
      "[Epoch 1] Training Batch [265/391]: Loss 1.8173534870147705\n",
      "[Epoch 1] Training Batch [266/391]: Loss 1.8510679006576538\n",
      "[Epoch 1] Training Batch [267/391]: Loss 1.7239757776260376\n",
      "[Epoch 1] Training Batch [268/391]: Loss 1.8231147527694702\n",
      "[Epoch 1] Training Batch [269/391]: Loss 1.866952896118164\n",
      "[Epoch 1] Training Batch [270/391]: Loss 1.8237988948822021\n",
      "[Epoch 1] Training Batch [271/391]: Loss 2.0176215171813965\n",
      "[Epoch 1] Training Batch [272/391]: Loss 1.8707362413406372\n",
      "[Epoch 1] Training Batch [273/391]: Loss 1.6837358474731445\n",
      "[Epoch 1] Training Batch [274/391]: Loss 1.8162544965744019\n",
      "[Epoch 1] Training Batch [275/391]: Loss 1.8614134788513184\n",
      "[Epoch 1] Training Batch [276/391]: Loss 2.052602529525757\n",
      "[Epoch 1] Training Batch [277/391]: Loss 1.861891508102417\n",
      "[Epoch 1] Training Batch [278/391]: Loss 1.794177770614624\n",
      "[Epoch 1] Training Batch [279/391]: Loss 1.7505216598510742\n",
      "[Epoch 1] Training Batch [280/391]: Loss 1.7449039220809937\n",
      "[Epoch 1] Training Batch [281/391]: Loss 1.827466368675232\n",
      "[Epoch 1] Training Batch [282/391]: Loss 1.8270231485366821\n",
      "[Epoch 1] Training Batch [283/391]: Loss 1.8198806047439575\n",
      "[Epoch 1] Training Batch [284/391]: Loss 1.8368867635726929\n",
      "[Epoch 1] Training Batch [285/391]: Loss 1.7398918867111206\n",
      "[Epoch 1] Training Batch [286/391]: Loss 1.7715046405792236\n",
      "[Epoch 1] Training Batch [287/391]: Loss 1.9908301830291748\n",
      "[Epoch 1] Training Batch [288/391]: Loss 1.6832900047302246\n",
      "[Epoch 1] Training Batch [289/391]: Loss 1.8060964345932007\n",
      "[Epoch 1] Training Batch [290/391]: Loss 1.832571029663086\n",
      "[Epoch 1] Training Batch [291/391]: Loss 1.7566417455673218\n",
      "[Epoch 1] Training Batch [292/391]: Loss 1.9032983779907227\n",
      "[Epoch 1] Training Batch [293/391]: Loss 1.7940635681152344\n",
      "[Epoch 1] Training Batch [294/391]: Loss 1.8039954900741577\n",
      "[Epoch 1] Training Batch [295/391]: Loss 1.7562178373336792\n",
      "[Epoch 1] Training Batch [296/391]: Loss 1.6475800275802612\n",
      "[Epoch 1] Training Batch [297/391]: Loss 1.7627476453781128\n",
      "[Epoch 1] Training Batch [298/391]: Loss 1.73923659324646\n",
      "[Epoch 1] Training Batch [299/391]: Loss 1.867875576019287\n",
      "[Epoch 1] Training Batch [300/391]: Loss 1.9041060209274292\n",
      "[Epoch 1] Training Batch [301/391]: Loss 1.6732831001281738\n",
      "[Epoch 1] Training Batch [302/391]: Loss 1.7598952054977417\n",
      "[Epoch 1] Training Batch [303/391]: Loss 1.7576550245285034\n",
      "[Epoch 1] Training Batch [304/391]: Loss 1.8451416492462158\n",
      "[Epoch 1] Training Batch [305/391]: Loss 1.8388054370880127\n",
      "[Epoch 1] Training Batch [306/391]: Loss 1.7990225553512573\n",
      "[Epoch 1] Training Batch [307/391]: Loss 1.8724017143249512\n",
      "[Epoch 1] Training Batch [308/391]: Loss 1.7744081020355225\n",
      "[Epoch 1] Training Batch [309/391]: Loss 1.8080573081970215\n",
      "[Epoch 1] Training Batch [310/391]: Loss 1.8962324857711792\n",
      "[Epoch 1] Training Batch [311/391]: Loss 1.7776750326156616\n",
      "[Epoch 1] Training Batch [312/391]: Loss 1.7903133630752563\n",
      "[Epoch 1] Training Batch [313/391]: Loss 1.9201921224594116\n",
      "[Epoch 1] Training Batch [314/391]: Loss 1.7713629007339478\n",
      "[Epoch 1] Training Batch [315/391]: Loss 1.7414129972457886\n",
      "[Epoch 1] Training Batch [316/391]: Loss 1.7626440525054932\n",
      "[Epoch 1] Training Batch [317/391]: Loss 1.7715697288513184\n",
      "[Epoch 1] Training Batch [318/391]: Loss 1.7867718935012817\n",
      "[Epoch 1] Training Batch [319/391]: Loss 1.9218002557754517\n",
      "[Epoch 1] Training Batch [320/391]: Loss 1.7136967182159424\n",
      "[Epoch 1] Training Batch [321/391]: Loss 1.779827356338501\n",
      "[Epoch 1] Training Batch [322/391]: Loss 1.6592634916305542\n",
      "[Epoch 1] Training Batch [323/391]: Loss 1.710572600364685\n",
      "[Epoch 1] Training Batch [324/391]: Loss 1.8463764190673828\n",
      "[Epoch 1] Training Batch [325/391]: Loss 1.7652652263641357\n",
      "[Epoch 1] Training Batch [326/391]: Loss 1.7379841804504395\n",
      "[Epoch 1] Training Batch [327/391]: Loss 1.7206202745437622\n",
      "[Epoch 1] Training Batch [328/391]: Loss 1.8507171869277954\n",
      "[Epoch 1] Training Batch [329/391]: Loss 1.7647324800491333\n",
      "[Epoch 1] Training Batch [330/391]: Loss 1.7580513954162598\n",
      "[Epoch 1] Training Batch [331/391]: Loss 1.736111044883728\n",
      "[Epoch 1] Training Batch [332/391]: Loss 1.760595440864563\n",
      "[Epoch 1] Training Batch [333/391]: Loss 1.8726463317871094\n",
      "[Epoch 1] Training Batch [334/391]: Loss 1.6833120584487915\n",
      "[Epoch 1] Training Batch [335/391]: Loss 1.8368152379989624\n",
      "[Epoch 1] Training Batch [336/391]: Loss 1.7143003940582275\n",
      "[Epoch 1] Training Batch [337/391]: Loss 1.8713537454605103\n",
      "[Epoch 1] Training Batch [338/391]: Loss 1.8994609117507935\n",
      "[Epoch 1] Training Batch [339/391]: Loss 1.721434473991394\n",
      "[Epoch 1] Training Batch [340/391]: Loss 1.7011603116989136\n",
      "[Epoch 1] Training Batch [341/391]: Loss 1.746482491493225\n",
      "[Epoch 1] Training Batch [342/391]: Loss 1.7125880718231201\n",
      "[Epoch 1] Training Batch [343/391]: Loss 2.0542521476745605\n",
      "[Epoch 1] Training Batch [344/391]: Loss 1.7364647388458252\n",
      "[Epoch 1] Training Batch [345/391]: Loss 1.6665233373641968\n",
      "[Epoch 1] Training Batch [346/391]: Loss 1.843942403793335\n",
      "[Epoch 1] Training Batch [347/391]: Loss 1.722377061843872\n",
      "[Epoch 1] Training Batch [348/391]: Loss 1.6896048784255981\n",
      "[Epoch 1] Training Batch [349/391]: Loss 1.753963589668274\n",
      "[Epoch 1] Training Batch [350/391]: Loss 1.7382827997207642\n",
      "[Epoch 1] Training Batch [351/391]: Loss 1.7360460758209229\n",
      "[Epoch 1] Training Batch [352/391]: Loss 1.7633575201034546\n",
      "[Epoch 1] Training Batch [353/391]: Loss 1.5941649675369263\n",
      "[Epoch 1] Training Batch [354/391]: Loss 1.8198872804641724\n",
      "[Epoch 1] Training Batch [355/391]: Loss 1.7939319610595703\n",
      "[Epoch 1] Training Batch [356/391]: Loss 1.759994626045227\n",
      "[Epoch 1] Training Batch [357/391]: Loss 1.6074219942092896\n",
      "[Epoch 1] Training Batch [358/391]: Loss 1.8875021934509277\n",
      "[Epoch 1] Training Batch [359/391]: Loss 1.8006263971328735\n",
      "[Epoch 1] Training Batch [360/391]: Loss 1.7513192892074585\n",
      "[Epoch 1] Training Batch [361/391]: Loss 1.7785784006118774\n",
      "[Epoch 1] Training Batch [362/391]: Loss 1.674980640411377\n",
      "[Epoch 1] Training Batch [363/391]: Loss 1.82447350025177\n",
      "[Epoch 1] Training Batch [364/391]: Loss 1.9139606952667236\n",
      "[Epoch 1] Training Batch [365/391]: Loss 1.761597752571106\n",
      "[Epoch 1] Training Batch [366/391]: Loss 1.7693216800689697\n",
      "[Epoch 1] Training Batch [367/391]: Loss 1.712528944015503\n",
      "[Epoch 1] Training Batch [368/391]: Loss 1.7960416078567505\n",
      "[Epoch 1] Training Batch [369/391]: Loss 1.6544042825698853\n",
      "[Epoch 1] Training Batch [370/391]: Loss 1.667197585105896\n",
      "[Epoch 1] Training Batch [371/391]: Loss 1.7540501356124878\n",
      "[Epoch 1] Training Batch [372/391]: Loss 1.703760027885437\n",
      "[Epoch 1] Training Batch [373/391]: Loss 1.86133873462677\n",
      "[Epoch 1] Training Batch [374/391]: Loss 1.7585906982421875\n",
      "[Epoch 1] Training Batch [375/391]: Loss 1.6117357015609741\n",
      "[Epoch 1] Training Batch [376/391]: Loss 1.7284808158874512\n",
      "[Epoch 1] Training Batch [377/391]: Loss 1.8047348260879517\n",
      "[Epoch 1] Training Batch [378/391]: Loss 1.8906214237213135\n",
      "[Epoch 1] Training Batch [379/391]: Loss 1.7442599534988403\n",
      "[Epoch 1] Training Batch [380/391]: Loss 1.6833064556121826\n",
      "[Epoch 1] Training Batch [381/391]: Loss 1.777235746383667\n",
      "[Epoch 1] Training Batch [382/391]: Loss 1.6570360660552979\n",
      "[Epoch 1] Training Batch [383/391]: Loss 1.6350927352905273\n",
      "[Epoch 1] Training Batch [384/391]: Loss 1.8232537508010864\n",
      "[Epoch 1] Training Batch [385/391]: Loss 1.6981358528137207\n",
      "[Epoch 1] Training Batch [386/391]: Loss 1.7413930892944336\n",
      "[Epoch 1] Training Batch [387/391]: Loss 1.669234275817871\n",
      "[Epoch 1] Training Batch [388/391]: Loss 1.6764397621154785\n",
      "[Epoch 1] Training Batch [389/391]: Loss 1.6029640436172485\n",
      "[Epoch 1] Training Batch [390/391]: Loss 1.8348162174224854\n",
      "[Epoch 1] Training Batch [391/391]: Loss 1.586846113204956\n",
      "Epoch 1 - Train Loss: 2.0264\n",
      "*********  Epoch 2/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Training Batch [1/391]: Loss 1.670844316482544\n",
      "[Epoch 2] Training Batch [2/391]: Loss 1.7542189359664917\n",
      "[Epoch 2] Training Batch [3/391]: Loss 1.8448328971862793\n",
      "[Epoch 2] Training Batch [4/391]: Loss 1.6357768774032593\n",
      "[Epoch 2] Training Batch [5/391]: Loss 1.7511875629425049\n",
      "[Epoch 2] Training Batch [6/391]: Loss 1.7241209745407104\n",
      "[Epoch 2] Training Batch [7/391]: Loss 1.5796761512756348\n",
      "[Epoch 2] Training Batch [8/391]: Loss 1.5920685529708862\n",
      "[Epoch 2] Training Batch [9/391]: Loss 1.7541483640670776\n",
      "[Epoch 2] Training Batch [10/391]: Loss 1.7185919284820557\n",
      "[Epoch 2] Training Batch [11/391]: Loss 1.7704540491104126\n",
      "[Epoch 2] Training Batch [12/391]: Loss 1.6501933336257935\n",
      "[Epoch 2] Training Batch [13/391]: Loss 1.7082444429397583\n",
      "[Epoch 2] Training Batch [14/391]: Loss 1.775173544883728\n",
      "[Epoch 2] Training Batch [15/391]: Loss 1.850807547569275\n",
      "[Epoch 2] Training Batch [16/391]: Loss 1.700176477432251\n",
      "[Epoch 2] Training Batch [17/391]: Loss 1.6954054832458496\n",
      "[Epoch 2] Training Batch [18/391]: Loss 1.752166986465454\n",
      "[Epoch 2] Training Batch [19/391]: Loss 1.7125192880630493\n",
      "[Epoch 2] Training Batch [20/391]: Loss 1.6384137868881226\n",
      "[Epoch 2] Training Batch [21/391]: Loss 1.7654519081115723\n",
      "[Epoch 2] Training Batch [22/391]: Loss 1.7405146360397339\n",
      "[Epoch 2] Training Batch [23/391]: Loss 1.5412153005599976\n",
      "[Epoch 2] Training Batch [24/391]: Loss 1.6738210916519165\n",
      "[Epoch 2] Training Batch [25/391]: Loss 1.8600174188613892\n",
      "[Epoch 2] Training Batch [26/391]: Loss 1.5438491106033325\n",
      "[Epoch 2] Training Batch [27/391]: Loss 1.5926705598831177\n",
      "[Epoch 2] Training Batch [28/391]: Loss 1.6426832675933838\n",
      "[Epoch 2] Training Batch [29/391]: Loss 1.7769408226013184\n",
      "[Epoch 2] Training Batch [30/391]: Loss 1.8649301528930664\n",
      "[Epoch 2] Training Batch [31/391]: Loss 1.8467135429382324\n",
      "[Epoch 2] Training Batch [32/391]: Loss 1.5703208446502686\n",
      "[Epoch 2] Training Batch [33/391]: Loss 1.664947748184204\n",
      "[Epoch 2] Training Batch [34/391]: Loss 1.9398908615112305\n",
      "[Epoch 2] Training Batch [35/391]: Loss 1.8603421449661255\n",
      "[Epoch 2] Training Batch [36/391]: Loss 1.583631992340088\n",
      "[Epoch 2] Training Batch [37/391]: Loss 1.693684458732605\n",
      "[Epoch 2] Training Batch [38/391]: Loss 1.7354573011398315\n",
      "[Epoch 2] Training Batch [39/391]: Loss 1.6802294254302979\n",
      "[Epoch 2] Training Batch [40/391]: Loss 1.519943118095398\n",
      "[Epoch 2] Training Batch [41/391]: Loss 1.7090222835540771\n",
      "[Epoch 2] Training Batch [42/391]: Loss 1.6912143230438232\n",
      "[Epoch 2] Training Batch [43/391]: Loss 1.7746386528015137\n",
      "[Epoch 2] Training Batch [44/391]: Loss 1.619576334953308\n",
      "[Epoch 2] Training Batch [45/391]: Loss 1.682714819908142\n",
      "[Epoch 2] Training Batch [46/391]: Loss 1.6643368005752563\n",
      "[Epoch 2] Training Batch [47/391]: Loss 1.602867841720581\n",
      "[Epoch 2] Training Batch [48/391]: Loss 1.7903014421463013\n",
      "[Epoch 2] Training Batch [49/391]: Loss 1.8247617483139038\n",
      "[Epoch 2] Training Batch [50/391]: Loss 1.4830173254013062\n",
      "[Epoch 2] Training Batch [51/391]: Loss 1.7654134035110474\n",
      "[Epoch 2] Training Batch [52/391]: Loss 1.8551701307296753\n",
      "[Epoch 2] Training Batch [53/391]: Loss 1.623021125793457\n",
      "[Epoch 2] Training Batch [54/391]: Loss 1.7222223281860352\n",
      "[Epoch 2] Training Batch [55/391]: Loss 1.8127131462097168\n",
      "[Epoch 2] Training Batch [56/391]: Loss 1.6488282680511475\n",
      "[Epoch 2] Training Batch [57/391]: Loss 1.6586027145385742\n",
      "[Epoch 2] Training Batch [58/391]: Loss 1.5228935480117798\n",
      "[Epoch 2] Training Batch [59/391]: Loss 1.7493940591812134\n",
      "[Epoch 2] Training Batch [60/391]: Loss 1.814632773399353\n",
      "[Epoch 2] Training Batch [61/391]: Loss 1.5930204391479492\n",
      "[Epoch 2] Training Batch [62/391]: Loss 1.7143324613571167\n",
      "[Epoch 2] Training Batch [63/391]: Loss 1.6509617567062378\n",
      "[Epoch 2] Training Batch [64/391]: Loss 1.6277707815170288\n",
      "[Epoch 2] Training Batch [65/391]: Loss 1.6395511627197266\n",
      "[Epoch 2] Training Batch [66/391]: Loss 1.7546284198760986\n",
      "[Epoch 2] Training Batch [67/391]: Loss 1.517156958580017\n",
      "[Epoch 2] Training Batch [68/391]: Loss 1.5692065954208374\n",
      "[Epoch 2] Training Batch [69/391]: Loss 1.9064350128173828\n",
      "[Epoch 2] Training Batch [70/391]: Loss 1.5312944650650024\n",
      "[Epoch 2] Training Batch [71/391]: Loss 1.383885383605957\n",
      "[Epoch 2] Training Batch [72/391]: Loss 1.7237226963043213\n",
      "[Epoch 2] Training Batch [73/391]: Loss 1.7108930349349976\n",
      "[Epoch 2] Training Batch [74/391]: Loss 1.6673870086669922\n",
      "[Epoch 2] Training Batch [75/391]: Loss 1.684117317199707\n",
      "[Epoch 2] Training Batch [76/391]: Loss 1.8804116249084473\n",
      "[Epoch 2] Training Batch [77/391]: Loss 1.7174420356750488\n",
      "[Epoch 2] Training Batch [78/391]: Loss 1.8457143306732178\n",
      "[Epoch 2] Training Batch [79/391]: Loss 1.687952995300293\n",
      "[Epoch 2] Training Batch [80/391]: Loss 1.777339220046997\n",
      "[Epoch 2] Training Batch [81/391]: Loss 1.7852025032043457\n",
      "[Epoch 2] Training Batch [82/391]: Loss 1.6100528240203857\n",
      "[Epoch 2] Training Batch [83/391]: Loss 1.642935872077942\n",
      "[Epoch 2] Training Batch [84/391]: Loss 1.7363682985305786\n",
      "[Epoch 2] Training Batch [85/391]: Loss 1.6251906156539917\n",
      "[Epoch 2] Training Batch [86/391]: Loss 1.6033886671066284\n",
      "[Epoch 2] Training Batch [87/391]: Loss 1.5782322883605957\n",
      "[Epoch 2] Training Batch [88/391]: Loss 1.544744849205017\n",
      "[Epoch 2] Training Batch [89/391]: Loss 1.6404935121536255\n",
      "[Epoch 2] Training Batch [90/391]: Loss 1.5579676628112793\n",
      "[Epoch 2] Training Batch [91/391]: Loss 1.5657707452774048\n",
      "[Epoch 2] Training Batch [92/391]: Loss 1.5886361598968506\n",
      "[Epoch 2] Training Batch [93/391]: Loss 1.5770381689071655\n",
      "[Epoch 2] Training Batch [94/391]: Loss 1.6894285678863525\n",
      "[Epoch 2] Training Batch [95/391]: Loss 1.4433943033218384\n",
      "[Epoch 2] Training Batch [96/391]: Loss 1.6740152835845947\n",
      "[Epoch 2] Training Batch [97/391]: Loss 1.8028247356414795\n",
      "[Epoch 2] Training Batch [98/391]: Loss 1.6780617237091064\n",
      "[Epoch 2] Training Batch [99/391]: Loss 1.6611018180847168\n",
      "[Epoch 2] Training Batch [100/391]: Loss 1.711099624633789\n",
      "[Epoch 2] Training Batch [101/391]: Loss 1.6759617328643799\n",
      "[Epoch 2] Training Batch [102/391]: Loss 1.6417522430419922\n",
      "[Epoch 2] Training Batch [103/391]: Loss 1.7798190116882324\n",
      "[Epoch 2] Training Batch [104/391]: Loss 1.6095027923583984\n",
      "[Epoch 2] Training Batch [105/391]: Loss 1.7278879880905151\n",
      "[Epoch 2] Training Batch [106/391]: Loss 1.7164666652679443\n",
      "[Epoch 2] Training Batch [107/391]: Loss 1.4087049961090088\n",
      "[Epoch 2] Training Batch [108/391]: Loss 1.6143989562988281\n",
      "[Epoch 2] Training Batch [109/391]: Loss 1.4576650857925415\n",
      "[Epoch 2] Training Batch [110/391]: Loss 1.701859951019287\n",
      "[Epoch 2] Training Batch [111/391]: Loss 1.7315374612808228\n",
      "[Epoch 2] Training Batch [112/391]: Loss 1.561100959777832\n",
      "[Epoch 2] Training Batch [113/391]: Loss 1.6274141073226929\n",
      "[Epoch 2] Training Batch [114/391]: Loss 1.5199754238128662\n",
      "[Epoch 2] Training Batch [115/391]: Loss 1.5386438369750977\n",
      "[Epoch 2] Training Batch [116/391]: Loss 1.68156099319458\n",
      "[Epoch 2] Training Batch [117/391]: Loss 1.7497652769088745\n",
      "[Epoch 2] Training Batch [118/391]: Loss 1.716086506843567\n",
      "[Epoch 2] Training Batch [119/391]: Loss 1.6436808109283447\n",
      "[Epoch 2] Training Batch [120/391]: Loss 1.714422583580017\n",
      "[Epoch 2] Training Batch [121/391]: Loss 1.7995994091033936\n",
      "[Epoch 2] Training Batch [122/391]: Loss 1.7059420347213745\n",
      "[Epoch 2] Training Batch [123/391]: Loss 1.6829588413238525\n",
      "[Epoch 2] Training Batch [124/391]: Loss 1.6211040019989014\n",
      "[Epoch 2] Training Batch [125/391]: Loss 1.7131695747375488\n",
      "[Epoch 2] Training Batch [126/391]: Loss 1.7563785314559937\n",
      "[Epoch 2] Training Batch [127/391]: Loss 1.5941921472549438\n",
      "[Epoch 2] Training Batch [128/391]: Loss 1.7082324028015137\n",
      "[Epoch 2] Training Batch [129/391]: Loss 1.5481789112091064\n",
      "[Epoch 2] Training Batch [130/391]: Loss 1.4930431842803955\n",
      "[Epoch 2] Training Batch [131/391]: Loss 1.6245226860046387\n",
      "[Epoch 2] Training Batch [132/391]: Loss 1.6336445808410645\n",
      "[Epoch 2] Training Batch [133/391]: Loss 1.5113835334777832\n",
      "[Epoch 2] Training Batch [134/391]: Loss 1.7666784524917603\n",
      "[Epoch 2] Training Batch [135/391]: Loss 1.5856170654296875\n",
      "[Epoch 2] Training Batch [136/391]: Loss 1.6650469303131104\n",
      "[Epoch 2] Training Batch [137/391]: Loss 1.4968950748443604\n",
      "[Epoch 2] Training Batch [138/391]: Loss 1.690051555633545\n",
      "[Epoch 2] Training Batch [139/391]: Loss 1.778341293334961\n",
      "[Epoch 2] Training Batch [140/391]: Loss 1.6873937845230103\n",
      "[Epoch 2] Training Batch [141/391]: Loss 1.6720130443572998\n",
      "[Epoch 2] Training Batch [142/391]: Loss 1.5821384191513062\n",
      "[Epoch 2] Training Batch [143/391]: Loss 1.5642849206924438\n",
      "[Epoch 2] Training Batch [144/391]: Loss 1.5448473691940308\n",
      "[Epoch 2] Training Batch [145/391]: Loss 1.8170132637023926\n",
      "[Epoch 2] Training Batch [146/391]: Loss 1.6549874544143677\n",
      "[Epoch 2] Training Batch [147/391]: Loss 1.609894037246704\n",
      "[Epoch 2] Training Batch [148/391]: Loss 1.608039140701294\n",
      "[Epoch 2] Training Batch [149/391]: Loss 1.6259045600891113\n",
      "[Epoch 2] Training Batch [150/391]: Loss 1.605067253112793\n",
      "[Epoch 2] Training Batch [151/391]: Loss 1.6153979301452637\n",
      "[Epoch 2] Training Batch [152/391]: Loss 1.7015901803970337\n",
      "[Epoch 2] Training Batch [153/391]: Loss 1.6403632164001465\n",
      "[Epoch 2] Training Batch [154/391]: Loss 1.6092183589935303\n",
      "[Epoch 2] Training Batch [155/391]: Loss 1.5772080421447754\n",
      "[Epoch 2] Training Batch [156/391]: Loss 1.6051846742630005\n",
      "[Epoch 2] Training Batch [157/391]: Loss 1.6199175119400024\n",
      "[Epoch 2] Training Batch [158/391]: Loss 1.6719951629638672\n",
      "[Epoch 2] Training Batch [159/391]: Loss 1.6256427764892578\n",
      "[Epoch 2] Training Batch [160/391]: Loss 1.4963818788528442\n",
      "[Epoch 2] Training Batch [161/391]: Loss 1.3663923740386963\n",
      "[Epoch 2] Training Batch [162/391]: Loss 1.5822409391403198\n",
      "[Epoch 2] Training Batch [163/391]: Loss 1.754479169845581\n",
      "[Epoch 2] Training Batch [164/391]: Loss 1.7297332286834717\n",
      "[Epoch 2] Training Batch [165/391]: Loss 1.5830167531967163\n",
      "[Epoch 2] Training Batch [166/391]: Loss 1.4595088958740234\n",
      "[Epoch 2] Training Batch [167/391]: Loss 1.7414802312850952\n",
      "[Epoch 2] Training Batch [168/391]: Loss 1.7247698307037354\n",
      "[Epoch 2] Training Batch [169/391]: Loss 1.5481318235397339\n",
      "[Epoch 2] Training Batch [170/391]: Loss 1.6027672290802002\n",
      "[Epoch 2] Training Batch [171/391]: Loss 1.7458585500717163\n",
      "[Epoch 2] Training Batch [172/391]: Loss 1.6226829290390015\n",
      "[Epoch 2] Training Batch [173/391]: Loss 1.754968523979187\n",
      "[Epoch 2] Training Batch [174/391]: Loss 1.654502034187317\n",
      "[Epoch 2] Training Batch [175/391]: Loss 1.6255720853805542\n",
      "[Epoch 2] Training Batch [176/391]: Loss 1.6204640865325928\n",
      "[Epoch 2] Training Batch [177/391]: Loss 1.511610507965088\n",
      "[Epoch 2] Training Batch [178/391]: Loss 1.5655906200408936\n",
      "[Epoch 2] Training Batch [179/391]: Loss 1.5756208896636963\n",
      "[Epoch 2] Training Batch [180/391]: Loss 1.492918848991394\n",
      "[Epoch 2] Training Batch [181/391]: Loss 1.501169204711914\n",
      "[Epoch 2] Training Batch [182/391]: Loss 1.546491026878357\n",
      "[Epoch 2] Training Batch [183/391]: Loss 1.6019185781478882\n",
      "[Epoch 2] Training Batch [184/391]: Loss 1.5246280431747437\n",
      "[Epoch 2] Training Batch [185/391]: Loss 1.4994274377822876\n",
      "[Epoch 2] Training Batch [186/391]: Loss 1.5604995489120483\n",
      "[Epoch 2] Training Batch [187/391]: Loss 1.5044758319854736\n",
      "[Epoch 2] Training Batch [188/391]: Loss 1.4315721988677979\n",
      "[Epoch 2] Training Batch [189/391]: Loss 1.605089783668518\n",
      "[Epoch 2] Training Batch [190/391]: Loss 1.5382226705551147\n",
      "[Epoch 2] Training Batch [191/391]: Loss 1.430376648902893\n",
      "[Epoch 2] Training Batch [192/391]: Loss 1.668439269065857\n",
      "[Epoch 2] Training Batch [193/391]: Loss 1.5103768110275269\n",
      "[Epoch 2] Training Batch [194/391]: Loss 1.7126951217651367\n",
      "[Epoch 2] Training Batch [195/391]: Loss 1.6219961643218994\n",
      "[Epoch 2] Training Batch [196/391]: Loss 1.693548321723938\n",
      "[Epoch 2] Training Batch [197/391]: Loss 1.7023650407791138\n",
      "[Epoch 2] Training Batch [198/391]: Loss 1.413194179534912\n",
      "[Epoch 2] Training Batch [199/391]: Loss 1.4689462184906006\n",
      "[Epoch 2] Training Batch [200/391]: Loss 1.6122280359268188\n",
      "[Epoch 2] Training Batch [201/391]: Loss 1.7256882190704346\n",
      "[Epoch 2] Training Batch [202/391]: Loss 1.573715090751648\n",
      "[Epoch 2] Training Batch [203/391]: Loss 1.587295413017273\n",
      "[Epoch 2] Training Batch [204/391]: Loss 1.5044878721237183\n",
      "[Epoch 2] Training Batch [205/391]: Loss 1.5046011209487915\n",
      "[Epoch 2] Training Batch [206/391]: Loss 1.6098748445510864\n",
      "[Epoch 2] Training Batch [207/391]: Loss 1.4991519451141357\n",
      "[Epoch 2] Training Batch [208/391]: Loss 1.6836191415786743\n",
      "[Epoch 2] Training Batch [209/391]: Loss 1.4016419649124146\n",
      "[Epoch 2] Training Batch [210/391]: Loss 1.6596803665161133\n",
      "[Epoch 2] Training Batch [211/391]: Loss 1.3215577602386475\n",
      "[Epoch 2] Training Batch [212/391]: Loss 1.483295202255249\n",
      "[Epoch 2] Training Batch [213/391]: Loss 1.550957202911377\n",
      "[Epoch 2] Training Batch [214/391]: Loss 1.562156319618225\n",
      "[Epoch 2] Training Batch [215/391]: Loss 1.4570813179016113\n",
      "[Epoch 2] Training Batch [216/391]: Loss 1.4234287738800049\n",
      "[Epoch 2] Training Batch [217/391]: Loss 1.447681188583374\n",
      "[Epoch 2] Training Batch [218/391]: Loss 1.4654122591018677\n",
      "[Epoch 2] Training Batch [219/391]: Loss 1.4329111576080322\n",
      "[Epoch 2] Training Batch [220/391]: Loss 1.5913423299789429\n",
      "[Epoch 2] Training Batch [221/391]: Loss 1.5729488134384155\n",
      "[Epoch 2] Training Batch [222/391]: Loss 1.501541256904602\n",
      "[Epoch 2] Training Batch [223/391]: Loss 1.4478416442871094\n",
      "[Epoch 2] Training Batch [224/391]: Loss 1.6413822174072266\n",
      "[Epoch 2] Training Batch [225/391]: Loss 1.6022372245788574\n",
      "[Epoch 2] Training Batch [226/391]: Loss 1.6256752014160156\n",
      "[Epoch 2] Training Batch [227/391]: Loss 1.5844752788543701\n",
      "[Epoch 2] Training Batch [228/391]: Loss 1.5606907606124878\n",
      "[Epoch 2] Training Batch [229/391]: Loss 1.6414179801940918\n",
      "[Epoch 2] Training Batch [230/391]: Loss 1.5893579721450806\n",
      "[Epoch 2] Training Batch [231/391]: Loss 1.574052095413208\n",
      "[Epoch 2] Training Batch [232/391]: Loss 1.6243319511413574\n",
      "[Epoch 2] Training Batch [233/391]: Loss 1.5909380912780762\n",
      "[Epoch 2] Training Batch [234/391]: Loss 1.6531219482421875\n",
      "[Epoch 2] Training Batch [235/391]: Loss 1.8146910667419434\n",
      "[Epoch 2] Training Batch [236/391]: Loss 1.3705533742904663\n",
      "[Epoch 2] Training Batch [237/391]: Loss 1.4416122436523438\n",
      "[Epoch 2] Training Batch [238/391]: Loss 1.8415716886520386\n",
      "[Epoch 2] Training Batch [239/391]: Loss 1.5613157749176025\n",
      "[Epoch 2] Training Batch [240/391]: Loss 1.564186692237854\n",
      "[Epoch 2] Training Batch [241/391]: Loss 1.6666864156723022\n",
      "[Epoch 2] Training Batch [242/391]: Loss 1.5486981868743896\n",
      "[Epoch 2] Training Batch [243/391]: Loss 1.4943492412567139\n",
      "[Epoch 2] Training Batch [244/391]: Loss 1.6788440942764282\n",
      "[Epoch 2] Training Batch [245/391]: Loss 1.5097311735153198\n",
      "[Epoch 2] Training Batch [246/391]: Loss 1.550148844718933\n",
      "[Epoch 2] Training Batch [247/391]: Loss 1.451575517654419\n",
      "[Epoch 2] Training Batch [248/391]: Loss 1.419751763343811\n",
      "[Epoch 2] Training Batch [249/391]: Loss 1.4642013311386108\n",
      "[Epoch 2] Training Batch [250/391]: Loss 1.365317702293396\n",
      "[Epoch 2] Training Batch [251/391]: Loss 1.4719520807266235\n",
      "[Epoch 2] Training Batch [252/391]: Loss 1.6592786312103271\n",
      "[Epoch 2] Training Batch [253/391]: Loss 1.4011938571929932\n",
      "[Epoch 2] Training Batch [254/391]: Loss 1.2728219032287598\n",
      "[Epoch 2] Training Batch [255/391]: Loss 1.6110684871673584\n",
      "[Epoch 2] Training Batch [256/391]: Loss 1.598446011543274\n",
      "[Epoch 2] Training Batch [257/391]: Loss 1.5244585275650024\n",
      "[Epoch 2] Training Batch [258/391]: Loss 1.5741006135940552\n",
      "[Epoch 2] Training Batch [259/391]: Loss 1.514449119567871\n",
      "[Epoch 2] Training Batch [260/391]: Loss 1.3504303693771362\n",
      "[Epoch 2] Training Batch [261/391]: Loss 1.5486016273498535\n",
      "[Epoch 2] Training Batch [262/391]: Loss 1.7931815385818481\n",
      "[Epoch 2] Training Batch [263/391]: Loss 1.5586220026016235\n",
      "[Epoch 2] Training Batch [264/391]: Loss 1.403350830078125\n",
      "[Epoch 2] Training Batch [265/391]: Loss 1.4580464363098145\n",
      "[Epoch 2] Training Batch [266/391]: Loss 1.4266620874404907\n",
      "[Epoch 2] Training Batch [267/391]: Loss 1.5154740810394287\n",
      "[Epoch 2] Training Batch [268/391]: Loss 1.558110237121582\n",
      "[Epoch 2] Training Batch [269/391]: Loss 1.595426082611084\n",
      "[Epoch 2] Training Batch [270/391]: Loss 1.41464364528656\n",
      "[Epoch 2] Training Batch [271/391]: Loss 1.5984742641448975\n",
      "[Epoch 2] Training Batch [272/391]: Loss 1.4756568670272827\n",
      "[Epoch 2] Training Batch [273/391]: Loss 1.5510419607162476\n",
      "[Epoch 2] Training Batch [274/391]: Loss 1.4178506135940552\n",
      "[Epoch 2] Training Batch [275/391]: Loss 1.7061153650283813\n",
      "[Epoch 2] Training Batch [276/391]: Loss 1.4807424545288086\n",
      "[Epoch 2] Training Batch [277/391]: Loss 1.5661673545837402\n",
      "[Epoch 2] Training Batch [278/391]: Loss 1.5664058923721313\n",
      "[Epoch 2] Training Batch [279/391]: Loss 1.5489771366119385\n",
      "[Epoch 2] Training Batch [280/391]: Loss 1.516133189201355\n",
      "[Epoch 2] Training Batch [281/391]: Loss 1.4055291414260864\n",
      "[Epoch 2] Training Batch [282/391]: Loss 1.4769762754440308\n",
      "[Epoch 2] Training Batch [283/391]: Loss 1.4994549751281738\n",
      "[Epoch 2] Training Batch [284/391]: Loss 1.4140894412994385\n",
      "[Epoch 2] Training Batch [285/391]: Loss 1.425632357597351\n",
      "[Epoch 2] Training Batch [286/391]: Loss 1.5393469333648682\n",
      "[Epoch 2] Training Batch [287/391]: Loss 1.4778491258621216\n",
      "[Epoch 2] Training Batch [288/391]: Loss 1.5128008127212524\n",
      "[Epoch 2] Training Batch [289/391]: Loss 1.563336730003357\n",
      "[Epoch 2] Training Batch [290/391]: Loss 1.4910842180252075\n",
      "[Epoch 2] Training Batch [291/391]: Loss 1.5640687942504883\n",
      "[Epoch 2] Training Batch [292/391]: Loss 1.437324047088623\n",
      "[Epoch 2] Training Batch [293/391]: Loss 1.5812180042266846\n",
      "[Epoch 2] Training Batch [294/391]: Loss 1.550601601600647\n",
      "[Epoch 2] Training Batch [295/391]: Loss 1.4834251403808594\n",
      "[Epoch 2] Training Batch [296/391]: Loss 1.3196388483047485\n",
      "[Epoch 2] Training Batch [297/391]: Loss 1.595808982849121\n",
      "[Epoch 2] Training Batch [298/391]: Loss 1.5125553607940674\n",
      "[Epoch 2] Training Batch [299/391]: Loss 1.4646029472351074\n",
      "[Epoch 2] Training Batch [300/391]: Loss 1.57916259765625\n",
      "[Epoch 2] Training Batch [301/391]: Loss 1.6638213396072388\n",
      "[Epoch 2] Training Batch [302/391]: Loss 1.4529327154159546\n",
      "[Epoch 2] Training Batch [303/391]: Loss 1.7122812271118164\n",
      "[Epoch 2] Training Batch [304/391]: Loss 1.6151567697525024\n",
      "[Epoch 2] Training Batch [305/391]: Loss 1.5993577241897583\n",
      "[Epoch 2] Training Batch [306/391]: Loss 1.55464768409729\n",
      "[Epoch 2] Training Batch [307/391]: Loss 1.3496534824371338\n",
      "[Epoch 2] Training Batch [308/391]: Loss 1.601855754852295\n",
      "[Epoch 2] Training Batch [309/391]: Loss 1.435174584388733\n",
      "[Epoch 2] Training Batch [310/391]: Loss 1.579177737236023\n",
      "[Epoch 2] Training Batch [311/391]: Loss 1.457696557044983\n",
      "[Epoch 2] Training Batch [312/391]: Loss 1.5519521236419678\n",
      "[Epoch 2] Training Batch [313/391]: Loss 1.8109322786331177\n",
      "[Epoch 2] Training Batch [314/391]: Loss 1.5574345588684082\n",
      "[Epoch 2] Training Batch [315/391]: Loss 1.4979884624481201\n",
      "[Epoch 2] Training Batch [316/391]: Loss 1.5555462837219238\n",
      "[Epoch 2] Training Batch [317/391]: Loss 1.5299696922302246\n",
      "[Epoch 2] Training Batch [318/391]: Loss 1.5458056926727295\n",
      "[Epoch 2] Training Batch [319/391]: Loss 1.502699613571167\n",
      "[Epoch 2] Training Batch [320/391]: Loss 1.6986603736877441\n",
      "[Epoch 2] Training Batch [321/391]: Loss 1.4460312128067017\n",
      "[Epoch 2] Training Batch [322/391]: Loss 1.6726700067520142\n",
      "[Epoch 2] Training Batch [323/391]: Loss 1.6509757041931152\n",
      "[Epoch 2] Training Batch [324/391]: Loss 1.46529221534729\n",
      "[Epoch 2] Training Batch [325/391]: Loss 1.6107709407806396\n",
      "[Epoch 2] Training Batch [326/391]: Loss 1.5709558725357056\n",
      "[Epoch 2] Training Batch [327/391]: Loss 1.3440897464752197\n",
      "[Epoch 2] Training Batch [328/391]: Loss 1.5573515892028809\n",
      "[Epoch 2] Training Batch [329/391]: Loss 1.3786689043045044\n",
      "[Epoch 2] Training Batch [330/391]: Loss 1.538672924041748\n",
      "[Epoch 2] Training Batch [331/391]: Loss 1.4051955938339233\n",
      "[Epoch 2] Training Batch [332/391]: Loss 1.4104381799697876\n",
      "[Epoch 2] Training Batch [333/391]: Loss 1.5659162998199463\n",
      "[Epoch 2] Training Batch [334/391]: Loss 1.4619896411895752\n",
      "[Epoch 2] Training Batch [335/391]: Loss 1.381147861480713\n",
      "[Epoch 2] Training Batch [336/391]: Loss 1.4636861085891724\n",
      "[Epoch 2] Training Batch [337/391]: Loss 1.4843659400939941\n",
      "[Epoch 2] Training Batch [338/391]: Loss 1.4838907718658447\n",
      "[Epoch 2] Training Batch [339/391]: Loss 1.5076452493667603\n",
      "[Epoch 2] Training Batch [340/391]: Loss 1.420624017715454\n",
      "[Epoch 2] Training Batch [341/391]: Loss 1.4946483373641968\n",
      "[Epoch 2] Training Batch [342/391]: Loss 1.3879706859588623\n",
      "[Epoch 2] Training Batch [343/391]: Loss 1.544541835784912\n",
      "[Epoch 2] Training Batch [344/391]: Loss 1.5517454147338867\n",
      "[Epoch 2] Training Batch [345/391]: Loss 1.2285362482070923\n",
      "[Epoch 2] Training Batch [346/391]: Loss 1.4025599956512451\n",
      "[Epoch 2] Training Batch [347/391]: Loss 1.2716130018234253\n",
      "[Epoch 2] Training Batch [348/391]: Loss 1.3447189331054688\n",
      "[Epoch 2] Training Batch [349/391]: Loss 1.5075215101242065\n",
      "[Epoch 2] Training Batch [350/391]: Loss 1.4905246496200562\n",
      "[Epoch 2] Training Batch [351/391]: Loss 1.2927550077438354\n",
      "[Epoch 2] Training Batch [352/391]: Loss 1.5874358415603638\n",
      "[Epoch 2] Training Batch [353/391]: Loss 1.4146842956542969\n",
      "[Epoch 2] Training Batch [354/391]: Loss 1.7051855325698853\n",
      "[Epoch 2] Training Batch [355/391]: Loss 1.5213204622268677\n",
      "[Epoch 2] Training Batch [356/391]: Loss 1.287889003753662\n",
      "[Epoch 2] Training Batch [357/391]: Loss 1.503371238708496\n",
      "[Epoch 2] Training Batch [358/391]: Loss 1.5571078062057495\n",
      "[Epoch 2] Training Batch [359/391]: Loss 1.4520986080169678\n",
      "[Epoch 2] Training Batch [360/391]: Loss 1.4172134399414062\n",
      "[Epoch 2] Training Batch [361/391]: Loss 1.462295413017273\n",
      "[Epoch 2] Training Batch [362/391]: Loss 1.4987963438034058\n",
      "[Epoch 2] Training Batch [363/391]: Loss 1.2618319988250732\n",
      "[Epoch 2] Training Batch [364/391]: Loss 1.2677061557769775\n",
      "[Epoch 2] Training Batch [365/391]: Loss 1.4963618516921997\n",
      "[Epoch 2] Training Batch [366/391]: Loss 1.470994234085083\n",
      "[Epoch 2] Training Batch [367/391]: Loss 1.6218018531799316\n",
      "[Epoch 2] Training Batch [368/391]: Loss 1.409065842628479\n",
      "[Epoch 2] Training Batch [369/391]: Loss 1.49260675907135\n",
      "[Epoch 2] Training Batch [370/391]: Loss 1.3921180963516235\n",
      "[Epoch 2] Training Batch [371/391]: Loss 1.4942909479141235\n",
      "[Epoch 2] Training Batch [372/391]: Loss 1.6662269830703735\n",
      "[Epoch 2] Training Batch [373/391]: Loss 1.5041552782058716\n",
      "[Epoch 2] Training Batch [374/391]: Loss 1.5104714632034302\n",
      "[Epoch 2] Training Batch [375/391]: Loss 1.488660454750061\n",
      "[Epoch 2] Training Batch [376/391]: Loss 1.5589138269424438\n",
      "[Epoch 2] Training Batch [377/391]: Loss 1.4603478908538818\n",
      "[Epoch 2] Training Batch [378/391]: Loss 1.4422742128372192\n",
      "[Epoch 2] Training Batch [379/391]: Loss 1.4335604906082153\n",
      "[Epoch 2] Training Batch [380/391]: Loss 1.502137303352356\n",
      "[Epoch 2] Training Batch [381/391]: Loss 1.4784389734268188\n",
      "[Epoch 2] Training Batch [382/391]: Loss 1.4506709575653076\n",
      "[Epoch 2] Training Batch [383/391]: Loss 1.4684191942214966\n",
      "[Epoch 2] Training Batch [384/391]: Loss 1.5113985538482666\n",
      "[Epoch 2] Training Batch [385/391]: Loss 1.4367116689682007\n",
      "[Epoch 2] Training Batch [386/391]: Loss 1.4735854864120483\n",
      "[Epoch 2] Training Batch [387/391]: Loss 1.316659688949585\n",
      "[Epoch 2] Training Batch [388/391]: Loss 1.5208007097244263\n",
      "[Epoch 2] Training Batch [389/391]: Loss 1.366281509399414\n",
      "[Epoch 2] Training Batch [390/391]: Loss 1.3161897659301758\n",
      "[Epoch 2] Training Batch [391/391]: Loss 1.4086005687713623\n",
      "Epoch 2 - Train Loss: 1.5823\n",
      "*********  Epoch 3/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Training Batch [1/391]: Loss 1.3515568971633911\n",
      "[Epoch 3] Training Batch [2/391]: Loss 1.554766297340393\n",
      "[Epoch 3] Training Batch [3/391]: Loss 1.345585584640503\n",
      "[Epoch 3] Training Batch [4/391]: Loss 1.525038719177246\n",
      "[Epoch 3] Training Batch [5/391]: Loss 1.4567469358444214\n",
      "[Epoch 3] Training Batch [6/391]: Loss 1.261030673980713\n",
      "[Epoch 3] Training Batch [7/391]: Loss 1.4140222072601318\n",
      "[Epoch 3] Training Batch [8/391]: Loss 1.3919998407363892\n",
      "[Epoch 3] Training Batch [9/391]: Loss 1.4566571712493896\n",
      "[Epoch 3] Training Batch [10/391]: Loss 1.475289225578308\n",
      "[Epoch 3] Training Batch [11/391]: Loss 1.4317057132720947\n",
      "[Epoch 3] Training Batch [12/391]: Loss 1.3349323272705078\n",
      "[Epoch 3] Training Batch [13/391]: Loss 1.3496602773666382\n",
      "[Epoch 3] Training Batch [14/391]: Loss 1.3356904983520508\n",
      "[Epoch 3] Training Batch [15/391]: Loss 1.3073848485946655\n",
      "[Epoch 3] Training Batch [16/391]: Loss 1.4730943441390991\n",
      "[Epoch 3] Training Batch [17/391]: Loss 1.3368743658065796\n",
      "[Epoch 3] Training Batch [18/391]: Loss 1.4610188007354736\n",
      "[Epoch 3] Training Batch [19/391]: Loss 1.30939519405365\n",
      "[Epoch 3] Training Batch [20/391]: Loss 1.3891186714172363\n",
      "[Epoch 3] Training Batch [21/391]: Loss 1.4568921327590942\n",
      "[Epoch 3] Training Batch [22/391]: Loss 1.1833330392837524\n",
      "[Epoch 3] Training Batch [23/391]: Loss 1.3882235288619995\n",
      "[Epoch 3] Training Batch [24/391]: Loss 1.4297457933425903\n",
      "[Epoch 3] Training Batch [25/391]: Loss 1.4448790550231934\n",
      "[Epoch 3] Training Batch [26/391]: Loss 1.4209951162338257\n",
      "[Epoch 3] Training Batch [27/391]: Loss 1.3850499391555786\n",
      "[Epoch 3] Training Batch [28/391]: Loss 1.5202791690826416\n",
      "[Epoch 3] Training Batch [29/391]: Loss 1.3883885145187378\n",
      "[Epoch 3] Training Batch [30/391]: Loss 1.2989075183868408\n",
      "[Epoch 3] Training Batch [31/391]: Loss 1.6353567838668823\n",
      "[Epoch 3] Training Batch [32/391]: Loss 1.3368220329284668\n",
      "[Epoch 3] Training Batch [33/391]: Loss 1.336174726486206\n",
      "[Epoch 3] Training Batch [34/391]: Loss 1.3422621488571167\n",
      "[Epoch 3] Training Batch [35/391]: Loss 1.5165680646896362\n",
      "[Epoch 3] Training Batch [36/391]: Loss 1.4779698848724365\n",
      "[Epoch 3] Training Batch [37/391]: Loss 1.4151451587677002\n",
      "[Epoch 3] Training Batch [38/391]: Loss 1.4768025875091553\n",
      "[Epoch 3] Training Batch [39/391]: Loss 1.28961181640625\n",
      "[Epoch 3] Training Batch [40/391]: Loss 1.4057950973510742\n",
      "[Epoch 3] Training Batch [41/391]: Loss 1.4476653337478638\n",
      "[Epoch 3] Training Batch [42/391]: Loss 1.4039793014526367\n",
      "[Epoch 3] Training Batch [43/391]: Loss 1.3201197385787964\n",
      "[Epoch 3] Training Batch [44/391]: Loss 1.2889883518218994\n",
      "[Epoch 3] Training Batch [45/391]: Loss 1.4249380826950073\n",
      "[Epoch 3] Training Batch [46/391]: Loss 1.3610306978225708\n",
      "[Epoch 3] Training Batch [47/391]: Loss 1.4438852071762085\n",
      "[Epoch 3] Training Batch [48/391]: Loss 1.3027772903442383\n",
      "[Epoch 3] Training Batch [49/391]: Loss 1.4367222785949707\n",
      "[Epoch 3] Training Batch [50/391]: Loss 1.3119367361068726\n",
      "[Epoch 3] Training Batch [51/391]: Loss 1.5285738706588745\n",
      "[Epoch 3] Training Batch [52/391]: Loss 1.336032509803772\n",
      "[Epoch 3] Training Batch [53/391]: Loss 1.4821335077285767\n",
      "[Epoch 3] Training Batch [54/391]: Loss 1.4169120788574219\n",
      "[Epoch 3] Training Batch [55/391]: Loss 1.2809780836105347\n",
      "[Epoch 3] Training Batch [56/391]: Loss 1.3688936233520508\n",
      "[Epoch 3] Training Batch [57/391]: Loss 1.2943800687789917\n",
      "[Epoch 3] Training Batch [58/391]: Loss 1.349920392036438\n",
      "[Epoch 3] Training Batch [59/391]: Loss 1.4818015098571777\n",
      "[Epoch 3] Training Batch [60/391]: Loss 1.300638198852539\n",
      "[Epoch 3] Training Batch [61/391]: Loss 1.4331134557724\n",
      "[Epoch 3] Training Batch [62/391]: Loss 1.3800009489059448\n",
      "[Epoch 3] Training Batch [63/391]: Loss 1.3709509372711182\n",
      "[Epoch 3] Training Batch [64/391]: Loss 1.4264109134674072\n",
      "[Epoch 3] Training Batch [65/391]: Loss 1.4014335870742798\n",
      "[Epoch 3] Training Batch [66/391]: Loss 1.4330041408538818\n",
      "[Epoch 3] Training Batch [67/391]: Loss 1.3575162887573242\n",
      "[Epoch 3] Training Batch [68/391]: Loss 1.4735318422317505\n",
      "[Epoch 3] Training Batch [69/391]: Loss 1.383424997329712\n",
      "[Epoch 3] Training Batch [70/391]: Loss 1.4584683179855347\n",
      "[Epoch 3] Training Batch [71/391]: Loss 1.2956054210662842\n",
      "[Epoch 3] Training Batch [72/391]: Loss 1.2495027780532837\n",
      "[Epoch 3] Training Batch [73/391]: Loss 1.3864682912826538\n",
      "[Epoch 3] Training Batch [74/391]: Loss 1.4635792970657349\n",
      "[Epoch 3] Training Batch [75/391]: Loss 1.3965164422988892\n",
      "[Epoch 3] Training Batch [76/391]: Loss 1.4791436195373535\n",
      "[Epoch 3] Training Batch [77/391]: Loss 1.3137484788894653\n",
      "[Epoch 3] Training Batch [78/391]: Loss 1.262062907218933\n",
      "[Epoch 3] Training Batch [79/391]: Loss 1.4523818492889404\n",
      "[Epoch 3] Training Batch [80/391]: Loss 1.3754117488861084\n",
      "[Epoch 3] Training Batch [81/391]: Loss 1.4034392833709717\n",
      "[Epoch 3] Training Batch [82/391]: Loss 1.3198133707046509\n",
      "[Epoch 3] Training Batch [83/391]: Loss 1.400746464729309\n",
      "[Epoch 3] Training Batch [84/391]: Loss 1.4212685823440552\n",
      "[Epoch 3] Training Batch [85/391]: Loss 1.2280832529067993\n",
      "[Epoch 3] Training Batch [86/391]: Loss 1.3234379291534424\n",
      "[Epoch 3] Training Batch [87/391]: Loss 1.2700670957565308\n",
      "[Epoch 3] Training Batch [88/391]: Loss 1.5019352436065674\n",
      "[Epoch 3] Training Batch [89/391]: Loss 1.2898783683776855\n",
      "[Epoch 3] Training Batch [90/391]: Loss 1.401190161705017\n",
      "[Epoch 3] Training Batch [91/391]: Loss 1.4489283561706543\n",
      "[Epoch 3] Training Batch [92/391]: Loss 1.4055461883544922\n",
      "[Epoch 3] Training Batch [93/391]: Loss 1.481620192527771\n",
      "[Epoch 3] Training Batch [94/391]: Loss 1.396075963973999\n",
      "[Epoch 3] Training Batch [95/391]: Loss 1.6254557371139526\n",
      "[Epoch 3] Training Batch [96/391]: Loss 1.382306456565857\n",
      "[Epoch 3] Training Batch [97/391]: Loss 1.2778488397598267\n",
      "[Epoch 3] Training Batch [98/391]: Loss 1.3666256666183472\n",
      "[Epoch 3] Training Batch [99/391]: Loss 1.3302265405654907\n",
      "[Epoch 3] Training Batch [100/391]: Loss 1.2807464599609375\n",
      "[Epoch 3] Training Batch [101/391]: Loss 1.5772169828414917\n",
      "[Epoch 3] Training Batch [102/391]: Loss 1.3917601108551025\n",
      "[Epoch 3] Training Batch [103/391]: Loss 1.532416820526123\n",
      "[Epoch 3] Training Batch [104/391]: Loss 1.413358449935913\n",
      "[Epoch 3] Training Batch [105/391]: Loss 1.3364688158035278\n",
      "[Epoch 3] Training Batch [106/391]: Loss 1.3022997379302979\n",
      "[Epoch 3] Training Batch [107/391]: Loss 1.333634614944458\n",
      "[Epoch 3] Training Batch [108/391]: Loss 1.6242427825927734\n",
      "[Epoch 3] Training Batch [109/391]: Loss 1.6684412956237793\n",
      "[Epoch 3] Training Batch [110/391]: Loss 1.3292006254196167\n",
      "[Epoch 3] Training Batch [111/391]: Loss 1.4509893655776978\n",
      "[Epoch 3] Training Batch [112/391]: Loss 1.4728758335113525\n",
      "[Epoch 3] Training Batch [113/391]: Loss 1.3326079845428467\n",
      "[Epoch 3] Training Batch [114/391]: Loss 1.4359500408172607\n",
      "[Epoch 3] Training Batch [115/391]: Loss 1.515153408050537\n",
      "[Epoch 3] Training Batch [116/391]: Loss 1.3606045246124268\n",
      "[Epoch 3] Training Batch [117/391]: Loss 1.4254941940307617\n",
      "[Epoch 3] Training Batch [118/391]: Loss 1.4341261386871338\n",
      "[Epoch 3] Training Batch [119/391]: Loss 1.3279093503952026\n",
      "[Epoch 3] Training Batch [120/391]: Loss 1.5020699501037598\n",
      "[Epoch 3] Training Batch [121/391]: Loss 1.5320465564727783\n",
      "[Epoch 3] Training Batch [122/391]: Loss 1.3545256853103638\n",
      "[Epoch 3] Training Batch [123/391]: Loss 1.3113696575164795\n",
      "[Epoch 3] Training Batch [124/391]: Loss 1.4458376169204712\n",
      "[Epoch 3] Training Batch [125/391]: Loss 1.4463523626327515\n",
      "[Epoch 3] Training Batch [126/391]: Loss 1.717965841293335\n",
      "[Epoch 3] Training Batch [127/391]: Loss 1.5071672201156616\n",
      "[Epoch 3] Training Batch [128/391]: Loss 1.3884079456329346\n",
      "[Epoch 3] Training Batch [129/391]: Loss 1.3217930793762207\n",
      "[Epoch 3] Training Batch [130/391]: Loss 1.3064022064208984\n",
      "[Epoch 3] Training Batch [131/391]: Loss 1.3625142574310303\n",
      "[Epoch 3] Training Batch [132/391]: Loss 1.225146770477295\n",
      "[Epoch 3] Training Batch [133/391]: Loss 1.3833532333374023\n",
      "[Epoch 3] Training Batch [134/391]: Loss 1.4597960710525513\n",
      "[Epoch 3] Training Batch [135/391]: Loss 1.4964169263839722\n",
      "[Epoch 3] Training Batch [136/391]: Loss 1.5433112382888794\n",
      "[Epoch 3] Training Batch [137/391]: Loss 1.4146233797073364\n",
      "[Epoch 3] Training Batch [138/391]: Loss 1.4816763401031494\n",
      "[Epoch 3] Training Batch [139/391]: Loss 1.3333892822265625\n",
      "[Epoch 3] Training Batch [140/391]: Loss 1.3637328147888184\n",
      "[Epoch 3] Training Batch [141/391]: Loss 1.5060935020446777\n",
      "[Epoch 3] Training Batch [142/391]: Loss 1.3903812170028687\n",
      "[Epoch 3] Training Batch [143/391]: Loss 1.5764317512512207\n",
      "[Epoch 3] Training Batch [144/391]: Loss 1.4833145141601562\n",
      "[Epoch 3] Training Batch [145/391]: Loss 1.3982317447662354\n",
      "[Epoch 3] Training Batch [146/391]: Loss 1.3585119247436523\n",
      "[Epoch 3] Training Batch [147/391]: Loss 1.5803302526474\n",
      "[Epoch 3] Training Batch [148/391]: Loss 1.4192415475845337\n",
      "[Epoch 3] Training Batch [149/391]: Loss 1.3361619710922241\n",
      "[Epoch 3] Training Batch [150/391]: Loss 1.2847503423690796\n",
      "[Epoch 3] Training Batch [151/391]: Loss 1.417687177658081\n",
      "[Epoch 3] Training Batch [152/391]: Loss 1.5976665019989014\n",
      "[Epoch 3] Training Batch [153/391]: Loss 1.393489122390747\n",
      "[Epoch 3] Training Batch [154/391]: Loss 1.5131558179855347\n",
      "[Epoch 3] Training Batch [155/391]: Loss 1.4888004064559937\n",
      "[Epoch 3] Training Batch [156/391]: Loss 1.5137639045715332\n",
      "[Epoch 3] Training Batch [157/391]: Loss 1.3387583494186401\n",
      "[Epoch 3] Training Batch [158/391]: Loss 1.4450026750564575\n",
      "[Epoch 3] Training Batch [159/391]: Loss 1.3612008094787598\n",
      "[Epoch 3] Training Batch [160/391]: Loss 1.5381749868392944\n",
      "[Epoch 3] Training Batch [161/391]: Loss 1.2931759357452393\n",
      "[Epoch 3] Training Batch [162/391]: Loss 1.4264863729476929\n",
      "[Epoch 3] Training Batch [163/391]: Loss 1.3809870481491089\n",
      "[Epoch 3] Training Batch [164/391]: Loss 1.3151243925094604\n",
      "[Epoch 3] Training Batch [165/391]: Loss 1.5057852268218994\n",
      "[Epoch 3] Training Batch [166/391]: Loss 1.4522408246994019\n",
      "[Epoch 3] Training Batch [167/391]: Loss 1.3896267414093018\n",
      "[Epoch 3] Training Batch [168/391]: Loss 1.3520777225494385\n",
      "[Epoch 3] Training Batch [169/391]: Loss 1.3196619749069214\n",
      "[Epoch 3] Training Batch [170/391]: Loss 1.4184410572052002\n",
      "[Epoch 3] Training Batch [171/391]: Loss 1.40864098072052\n",
      "[Epoch 3] Training Batch [172/391]: Loss 1.4879099130630493\n",
      "[Epoch 3] Training Batch [173/391]: Loss 1.273120403289795\n",
      "[Epoch 3] Training Batch [174/391]: Loss 1.2532838582992554\n",
      "[Epoch 3] Training Batch [175/391]: Loss 1.4731749296188354\n",
      "[Epoch 3] Training Batch [176/391]: Loss 1.5419210195541382\n",
      "[Epoch 3] Training Batch [177/391]: Loss 1.472281813621521\n",
      "[Epoch 3] Training Batch [178/391]: Loss 1.5771474838256836\n",
      "[Epoch 3] Training Batch [179/391]: Loss 1.4272733926773071\n",
      "[Epoch 3] Training Batch [180/391]: Loss 1.4194222688674927\n",
      "[Epoch 3] Training Batch [181/391]: Loss 1.4174455404281616\n",
      "[Epoch 3] Training Batch [182/391]: Loss 1.497235655784607\n",
      "[Epoch 3] Training Batch [183/391]: Loss 1.3272380828857422\n",
      "[Epoch 3] Training Batch [184/391]: Loss 1.5514726638793945\n",
      "[Epoch 3] Training Batch [185/391]: Loss 1.450486660003662\n",
      "[Epoch 3] Training Batch [186/391]: Loss 1.416866660118103\n",
      "[Epoch 3] Training Batch [187/391]: Loss 1.425091028213501\n",
      "[Epoch 3] Training Batch [188/391]: Loss 1.4571975469589233\n",
      "[Epoch 3] Training Batch [189/391]: Loss 1.5541189908981323\n",
      "[Epoch 3] Training Batch [190/391]: Loss 1.4798157215118408\n",
      "[Epoch 3] Training Batch [191/391]: Loss 1.4660741090774536\n",
      "[Epoch 3] Training Batch [192/391]: Loss 1.4002776145935059\n",
      "[Epoch 3] Training Batch [193/391]: Loss 1.3710808753967285\n",
      "[Epoch 3] Training Batch [194/391]: Loss 1.3760584592819214\n",
      "[Epoch 3] Training Batch [195/391]: Loss 1.4100075960159302\n",
      "[Epoch 3] Training Batch [196/391]: Loss 1.6304974555969238\n",
      "[Epoch 3] Training Batch [197/391]: Loss 1.3651736974716187\n",
      "[Epoch 3] Training Batch [198/391]: Loss 1.2755236625671387\n",
      "[Epoch 3] Training Batch [199/391]: Loss 1.2085436582565308\n",
      "[Epoch 3] Training Batch [200/391]: Loss 1.2257095575332642\n",
      "[Epoch 3] Training Batch [201/391]: Loss 1.443200707435608\n",
      "[Epoch 3] Training Batch [202/391]: Loss 1.3443700075149536\n",
      "[Epoch 3] Training Batch [203/391]: Loss 1.4989418983459473\n",
      "[Epoch 3] Training Batch [204/391]: Loss 1.4552186727523804\n",
      "[Epoch 3] Training Batch [205/391]: Loss 1.4068467617034912\n",
      "[Epoch 3] Training Batch [206/391]: Loss 1.2195007801055908\n",
      "[Epoch 3] Training Batch [207/391]: Loss 1.401660442352295\n",
      "[Epoch 3] Training Batch [208/391]: Loss 1.3124456405639648\n",
      "[Epoch 3] Training Batch [209/391]: Loss 1.3922441005706787\n",
      "[Epoch 3] Training Batch [210/391]: Loss 1.5349762439727783\n",
      "[Epoch 3] Training Batch [211/391]: Loss 1.354000449180603\n",
      "[Epoch 3] Training Batch [212/391]: Loss 1.318656325340271\n",
      "[Epoch 3] Training Batch [213/391]: Loss 1.375787377357483\n",
      "[Epoch 3] Training Batch [214/391]: Loss 1.3440742492675781\n",
      "[Epoch 3] Training Batch [215/391]: Loss 1.2643001079559326\n",
      "[Epoch 3] Training Batch [216/391]: Loss 1.29557466506958\n",
      "[Epoch 3] Training Batch [217/391]: Loss 1.3359129428863525\n",
      "[Epoch 3] Training Batch [218/391]: Loss 1.4338574409484863\n",
      "[Epoch 3] Training Batch [219/391]: Loss 1.3620611429214478\n",
      "[Epoch 3] Training Batch [220/391]: Loss 1.4197007417678833\n",
      "[Epoch 3] Training Batch [221/391]: Loss 1.3727120161056519\n",
      "[Epoch 3] Training Batch [222/391]: Loss 1.3757829666137695\n",
      "[Epoch 3] Training Batch [223/391]: Loss 1.6719369888305664\n",
      "[Epoch 3] Training Batch [224/391]: Loss 1.335071325302124\n",
      "[Epoch 3] Training Batch [225/391]: Loss 1.6088054180145264\n",
      "[Epoch 3] Training Batch [226/391]: Loss 1.340282917022705\n",
      "[Epoch 3] Training Batch [227/391]: Loss 1.4007773399353027\n",
      "[Epoch 3] Training Batch [228/391]: Loss 1.4129637479782104\n",
      "[Epoch 3] Training Batch [229/391]: Loss 1.3774858713150024\n",
      "[Epoch 3] Training Batch [230/391]: Loss 1.3209328651428223\n",
      "[Epoch 3] Training Batch [231/391]: Loss 1.4376260042190552\n",
      "[Epoch 3] Training Batch [232/391]: Loss 1.2020078897476196\n",
      "[Epoch 3] Training Batch [233/391]: Loss 1.4216684103012085\n",
      "[Epoch 3] Training Batch [234/391]: Loss 1.437045931816101\n",
      "[Epoch 3] Training Batch [235/391]: Loss 1.459403395652771\n",
      "[Epoch 3] Training Batch [236/391]: Loss 1.3548988103866577\n",
      "[Epoch 3] Training Batch [237/391]: Loss 1.519162893295288\n",
      "[Epoch 3] Training Batch [238/391]: Loss 1.3664981126785278\n",
      "[Epoch 3] Training Batch [239/391]: Loss 1.3474587202072144\n",
      "[Epoch 3] Training Batch [240/391]: Loss 1.3044999837875366\n",
      "[Epoch 3] Training Batch [241/391]: Loss 1.4674663543701172\n",
      "[Epoch 3] Training Batch [242/391]: Loss 1.5105762481689453\n",
      "[Epoch 3] Training Batch [243/391]: Loss 1.5299186706542969\n",
      "[Epoch 3] Training Batch [244/391]: Loss 1.3761452436447144\n",
      "[Epoch 3] Training Batch [245/391]: Loss 1.3929897546768188\n",
      "[Epoch 3] Training Batch [246/391]: Loss 1.4446805715560913\n",
      "[Epoch 3] Training Batch [247/391]: Loss 1.292314887046814\n",
      "[Epoch 3] Training Batch [248/391]: Loss 1.1014938354492188\n",
      "[Epoch 3] Training Batch [249/391]: Loss 1.290223240852356\n",
      "[Epoch 3] Training Batch [250/391]: Loss 1.4216750860214233\n",
      "[Epoch 3] Training Batch [251/391]: Loss 1.5565935373306274\n",
      "[Epoch 3] Training Batch [252/391]: Loss 1.2546228170394897\n",
      "[Epoch 3] Training Batch [253/391]: Loss 1.3273639678955078\n",
      "[Epoch 3] Training Batch [254/391]: Loss 1.375475287437439\n",
      "[Epoch 3] Training Batch [255/391]: Loss 1.2779449224472046\n",
      "[Epoch 3] Training Batch [256/391]: Loss 1.3978643417358398\n",
      "[Epoch 3] Training Batch [257/391]: Loss 1.2864283323287964\n",
      "[Epoch 3] Training Batch [258/391]: Loss 1.2819348573684692\n",
      "[Epoch 3] Training Batch [259/391]: Loss 1.3968724012374878\n",
      "[Epoch 3] Training Batch [260/391]: Loss 1.3801121711730957\n",
      "[Epoch 3] Training Batch [261/391]: Loss 1.3561556339263916\n",
      "[Epoch 3] Training Batch [262/391]: Loss 1.332951307296753\n",
      "[Epoch 3] Training Batch [263/391]: Loss 1.3920342922210693\n",
      "[Epoch 3] Training Batch [264/391]: Loss 1.6008459329605103\n",
      "[Epoch 3] Training Batch [265/391]: Loss 1.503790020942688\n",
      "[Epoch 3] Training Batch [266/391]: Loss 1.3604110479354858\n",
      "[Epoch 3] Training Batch [267/391]: Loss 1.462353229522705\n",
      "[Epoch 3] Training Batch [268/391]: Loss 1.4573615789413452\n",
      "[Epoch 3] Training Batch [269/391]: Loss 1.3743503093719482\n",
      "[Epoch 3] Training Batch [270/391]: Loss 1.296961784362793\n",
      "[Epoch 3] Training Batch [271/391]: Loss 1.4150134325027466\n",
      "[Epoch 3] Training Batch [272/391]: Loss 1.4561192989349365\n",
      "[Epoch 3] Training Batch [273/391]: Loss 1.591702938079834\n",
      "[Epoch 3] Training Batch [274/391]: Loss 1.4184215068817139\n",
      "[Epoch 3] Training Batch [275/391]: Loss 1.357297658920288\n",
      "[Epoch 3] Training Batch [276/391]: Loss 1.3936012983322144\n",
      "[Epoch 3] Training Batch [277/391]: Loss 1.4410353899002075\n",
      "[Epoch 3] Training Batch [278/391]: Loss 1.3363155126571655\n",
      "[Epoch 3] Training Batch [279/391]: Loss 1.3809162378311157\n",
      "[Epoch 3] Training Batch [280/391]: Loss 1.3745206594467163\n",
      "[Epoch 3] Training Batch [281/391]: Loss 1.3479965925216675\n",
      "[Epoch 3] Training Batch [282/391]: Loss 1.2041852474212646\n",
      "[Epoch 3] Training Batch [283/391]: Loss 1.3915623426437378\n",
      "[Epoch 3] Training Batch [284/391]: Loss 1.389654517173767\n",
      "[Epoch 3] Training Batch [285/391]: Loss 1.3832594156265259\n",
      "[Epoch 3] Training Batch [286/391]: Loss 1.3791265487670898\n",
      "[Epoch 3] Training Batch [287/391]: Loss 1.3588848114013672\n",
      "[Epoch 3] Training Batch [288/391]: Loss 1.3383601903915405\n",
      "[Epoch 3] Training Batch [289/391]: Loss 1.207712173461914\n",
      "[Epoch 3] Training Batch [290/391]: Loss 1.3427222967147827\n",
      "[Epoch 3] Training Batch [291/391]: Loss 1.4520353078842163\n",
      "[Epoch 3] Training Batch [292/391]: Loss 1.3800606727600098\n",
      "[Epoch 3] Training Batch [293/391]: Loss 1.3549960851669312\n",
      "[Epoch 3] Training Batch [294/391]: Loss 1.3363864421844482\n",
      "[Epoch 3] Training Batch [295/391]: Loss 1.3143662214279175\n",
      "[Epoch 3] Training Batch [296/391]: Loss 1.255597472190857\n",
      "[Epoch 3] Training Batch [297/391]: Loss 1.288329005241394\n",
      "[Epoch 3] Training Batch [298/391]: Loss 1.0833685398101807\n",
      "[Epoch 3] Training Batch [299/391]: Loss 1.3422797918319702\n",
      "[Epoch 3] Training Batch [300/391]: Loss 1.6131823062896729\n",
      "[Epoch 3] Training Batch [301/391]: Loss 1.522983193397522\n",
      "[Epoch 3] Training Batch [302/391]: Loss 1.5346901416778564\n",
      "[Epoch 3] Training Batch [303/391]: Loss 1.2455958127975464\n",
      "[Epoch 3] Training Batch [304/391]: Loss 1.2406967878341675\n",
      "[Epoch 3] Training Batch [305/391]: Loss 1.312709093093872\n",
      "[Epoch 3] Training Batch [306/391]: Loss 1.4268465042114258\n",
      "[Epoch 3] Training Batch [307/391]: Loss 1.4149821996688843\n",
      "[Epoch 3] Training Batch [308/391]: Loss 1.2435890436172485\n",
      "[Epoch 3] Training Batch [309/391]: Loss 1.3017991781234741\n",
      "[Epoch 3] Training Batch [310/391]: Loss 1.267248511314392\n",
      "[Epoch 3] Training Batch [311/391]: Loss 1.3801302909851074\n",
      "[Epoch 3] Training Batch [312/391]: Loss 1.3191627264022827\n",
      "[Epoch 3] Training Batch [313/391]: Loss 1.3536269664764404\n",
      "[Epoch 3] Training Batch [314/391]: Loss 1.2187657356262207\n",
      "[Epoch 3] Training Batch [315/391]: Loss 1.3110305070877075\n",
      "[Epoch 3] Training Batch [316/391]: Loss 1.4207968711853027\n",
      "[Epoch 3] Training Batch [317/391]: Loss 1.1712599992752075\n",
      "[Epoch 3] Training Batch [318/391]: Loss 1.50205659866333\n",
      "[Epoch 3] Training Batch [319/391]: Loss 1.5338796377182007\n",
      "[Epoch 3] Training Batch [320/391]: Loss 1.358130693435669\n",
      "[Epoch 3] Training Batch [321/391]: Loss 1.5410484075546265\n",
      "[Epoch 3] Training Batch [322/391]: Loss 1.0656318664550781\n",
      "[Epoch 3] Training Batch [323/391]: Loss 1.3216135501861572\n",
      "[Epoch 3] Training Batch [324/391]: Loss 1.3181266784667969\n",
      "[Epoch 3] Training Batch [325/391]: Loss 1.3707882165908813\n",
      "[Epoch 3] Training Batch [326/391]: Loss 1.503759503364563\n",
      "[Epoch 3] Training Batch [327/391]: Loss 1.1821202039718628\n",
      "[Epoch 3] Training Batch [328/391]: Loss 1.298890233039856\n",
      "[Epoch 3] Training Batch [329/391]: Loss 1.5822569131851196\n",
      "[Epoch 3] Training Batch [330/391]: Loss 1.3118315935134888\n",
      "[Epoch 3] Training Batch [331/391]: Loss 1.4827313423156738\n",
      "[Epoch 3] Training Batch [332/391]: Loss 1.3210300207138062\n",
      "[Epoch 3] Training Batch [333/391]: Loss 1.3612216711044312\n",
      "[Epoch 3] Training Batch [334/391]: Loss 1.2634364366531372\n",
      "[Epoch 3] Training Batch [335/391]: Loss 1.3241710662841797\n",
      "[Epoch 3] Training Batch [336/391]: Loss 1.3834078311920166\n",
      "[Epoch 3] Training Batch [337/391]: Loss 1.4222158193588257\n",
      "[Epoch 3] Training Batch [338/391]: Loss 1.2512571811676025\n",
      "[Epoch 3] Training Batch [339/391]: Loss 1.2245571613311768\n",
      "[Epoch 3] Training Batch [340/391]: Loss 1.329852819442749\n",
      "[Epoch 3] Training Batch [341/391]: Loss 1.3822985887527466\n",
      "[Epoch 3] Training Batch [342/391]: Loss 1.4085170030593872\n",
      "[Epoch 3] Training Batch [343/391]: Loss 1.3160122632980347\n",
      "[Epoch 3] Training Batch [344/391]: Loss 1.4628562927246094\n",
      "[Epoch 3] Training Batch [345/391]: Loss 1.3296408653259277\n",
      "[Epoch 3] Training Batch [346/391]: Loss 1.4602293968200684\n",
      "[Epoch 3] Training Batch [347/391]: Loss 1.3677812814712524\n",
      "[Epoch 3] Training Batch [348/391]: Loss 1.2077339887619019\n",
      "[Epoch 3] Training Batch [349/391]: Loss 1.3155128955841064\n",
      "[Epoch 3] Training Batch [350/391]: Loss 1.2561216354370117\n",
      "[Epoch 3] Training Batch [351/391]: Loss 1.3139917850494385\n",
      "[Epoch 3] Training Batch [352/391]: Loss 1.3741942644119263\n",
      "[Epoch 3] Training Batch [353/391]: Loss 1.2482655048370361\n",
      "[Epoch 3] Training Batch [354/391]: Loss 1.2006421089172363\n",
      "[Epoch 3] Training Batch [355/391]: Loss 1.3936753273010254\n",
      "[Epoch 3] Training Batch [356/391]: Loss 1.5113420486450195\n",
      "[Epoch 3] Training Batch [357/391]: Loss 1.3602356910705566\n",
      "[Epoch 3] Training Batch [358/391]: Loss 1.286207675933838\n",
      "[Epoch 3] Training Batch [359/391]: Loss 1.4281578063964844\n",
      "[Epoch 3] Training Batch [360/391]: Loss 1.380164384841919\n",
      "[Epoch 3] Training Batch [361/391]: Loss 1.481454610824585\n",
      "[Epoch 3] Training Batch [362/391]: Loss 1.3337332010269165\n",
      "[Epoch 3] Training Batch [363/391]: Loss 1.2504383325576782\n",
      "[Epoch 3] Training Batch [364/391]: Loss 1.4751083850860596\n",
      "[Epoch 3] Training Batch [365/391]: Loss 1.3045905828475952\n",
      "[Epoch 3] Training Batch [366/391]: Loss 1.4049465656280518\n",
      "[Epoch 3] Training Batch [367/391]: Loss 1.2926051616668701\n",
      "[Epoch 3] Training Batch [368/391]: Loss 1.4248874187469482\n",
      "[Epoch 3] Training Batch [369/391]: Loss 1.3095113039016724\n",
      "[Epoch 3] Training Batch [370/391]: Loss 1.2749056816101074\n",
      "[Epoch 3] Training Batch [371/391]: Loss 1.2319414615631104\n",
      "[Epoch 3] Training Batch [372/391]: Loss 1.2052325010299683\n",
      "[Epoch 3] Training Batch [373/391]: Loss 1.540970802307129\n",
      "[Epoch 3] Training Batch [374/391]: Loss 1.401269793510437\n",
      "[Epoch 3] Training Batch [375/391]: Loss 1.2796287536621094\n",
      "[Epoch 3] Training Batch [376/391]: Loss 1.1104929447174072\n",
      "[Epoch 3] Training Batch [377/391]: Loss 1.4288486242294312\n",
      "[Epoch 3] Training Batch [378/391]: Loss 1.253993034362793\n",
      "[Epoch 3] Training Batch [379/391]: Loss 1.4070422649383545\n",
      "[Epoch 3] Training Batch [380/391]: Loss 1.3723727464675903\n",
      "[Epoch 3] Training Batch [381/391]: Loss 1.3489775657653809\n",
      "[Epoch 3] Training Batch [382/391]: Loss 1.137277603149414\n",
      "[Epoch 3] Training Batch [383/391]: Loss 1.3110404014587402\n",
      "[Epoch 3] Training Batch [384/391]: Loss 1.3826123476028442\n",
      "[Epoch 3] Training Batch [385/391]: Loss 1.342014193534851\n",
      "[Epoch 3] Training Batch [386/391]: Loss 1.423525094985962\n",
      "[Epoch 3] Training Batch [387/391]: Loss 1.1572790145874023\n",
      "[Epoch 3] Training Batch [388/391]: Loss 1.2329241037368774\n",
      "[Epoch 3] Training Batch [389/391]: Loss 1.354960322380066\n",
      "[Epoch 3] Training Batch [390/391]: Loss 1.1989730596542358\n",
      "[Epoch 3] Training Batch [391/391]: Loss 1.3063149452209473\n",
      "Epoch 3 - Train Loss: 1.3851\n",
      "*********  Epoch 4/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Training Batch [1/391]: Loss 1.243931770324707\n",
      "[Epoch 4] Training Batch [2/391]: Loss 1.4076707363128662\n",
      "[Epoch 4] Training Batch [3/391]: Loss 1.2688733339309692\n",
      "[Epoch 4] Training Batch [4/391]: Loss 1.2243653535842896\n",
      "[Epoch 4] Training Batch [5/391]: Loss 1.1586912870407104\n",
      "[Epoch 4] Training Batch [6/391]: Loss 1.2769136428833008\n",
      "[Epoch 4] Training Batch [7/391]: Loss 1.337611198425293\n",
      "[Epoch 4] Training Batch [8/391]: Loss 1.1460009813308716\n",
      "[Epoch 4] Training Batch [9/391]: Loss 1.4144126176834106\n",
      "[Epoch 4] Training Batch [10/391]: Loss 1.1626019477844238\n",
      "[Epoch 4] Training Batch [11/391]: Loss 1.4085664749145508\n",
      "[Epoch 4] Training Batch [12/391]: Loss 1.3495464324951172\n",
      "[Epoch 4] Training Batch [13/391]: Loss 1.350993275642395\n",
      "[Epoch 4] Training Batch [14/391]: Loss 1.292736530303955\n",
      "[Epoch 4] Training Batch [15/391]: Loss 1.3950531482696533\n",
      "[Epoch 4] Training Batch [16/391]: Loss 1.1984786987304688\n",
      "[Epoch 4] Training Batch [17/391]: Loss 1.1746869087219238\n",
      "[Epoch 4] Training Batch [18/391]: Loss 1.0683414936065674\n",
      "[Epoch 4] Training Batch [19/391]: Loss 1.230795979499817\n",
      "[Epoch 4] Training Batch [20/391]: Loss 1.297910213470459\n",
      "[Epoch 4] Training Batch [21/391]: Loss 1.1709338426589966\n",
      "[Epoch 4] Training Batch [22/391]: Loss 1.284347653388977\n",
      "[Epoch 4] Training Batch [23/391]: Loss 1.2337812185287476\n",
      "[Epoch 4] Training Batch [24/391]: Loss 1.1863967180252075\n",
      "[Epoch 4] Training Batch [25/391]: Loss 1.2671666145324707\n",
      "[Epoch 4] Training Batch [26/391]: Loss 1.1941111087799072\n",
      "[Epoch 4] Training Batch [27/391]: Loss 1.0990992784500122\n",
      "[Epoch 4] Training Batch [28/391]: Loss 1.2396124601364136\n",
      "[Epoch 4] Training Batch [29/391]: Loss 1.1356624364852905\n",
      "[Epoch 4] Training Batch [30/391]: Loss 1.3197147846221924\n",
      "[Epoch 4] Training Batch [31/391]: Loss 1.389025330543518\n",
      "[Epoch 4] Training Batch [32/391]: Loss 1.4133317470550537\n",
      "[Epoch 4] Training Batch [33/391]: Loss 1.1682178974151611\n",
      "[Epoch 4] Training Batch [34/391]: Loss 1.441543698310852\n",
      "[Epoch 4] Training Batch [35/391]: Loss 1.3662892580032349\n",
      "[Epoch 4] Training Batch [36/391]: Loss 1.1435397863388062\n",
      "[Epoch 4] Training Batch [37/391]: Loss 1.2179945707321167\n",
      "[Epoch 4] Training Batch [38/391]: Loss 1.2314430475234985\n",
      "[Epoch 4] Training Batch [39/391]: Loss 1.258036732673645\n",
      "[Epoch 4] Training Batch [40/391]: Loss 1.3298310041427612\n",
      "[Epoch 4] Training Batch [41/391]: Loss 1.187877893447876\n",
      "[Epoch 4] Training Batch [42/391]: Loss 1.1968218088150024\n",
      "[Epoch 4] Training Batch [43/391]: Loss 1.3984968662261963\n",
      "[Epoch 4] Training Batch [44/391]: Loss 1.4243812561035156\n",
      "[Epoch 4] Training Batch [45/391]: Loss 1.3037450313568115\n",
      "[Epoch 4] Training Batch [46/391]: Loss 1.0349600315093994\n",
      "[Epoch 4] Training Batch [47/391]: Loss 1.386933445930481\n",
      "[Epoch 4] Training Batch [48/391]: Loss 1.1994940042495728\n",
      "[Epoch 4] Training Batch [49/391]: Loss 1.4262113571166992\n",
      "[Epoch 4] Training Batch [50/391]: Loss 1.2904924154281616\n",
      "[Epoch 4] Training Batch [51/391]: Loss 1.3626959323883057\n",
      "[Epoch 4] Training Batch [52/391]: Loss 1.3052960634231567\n",
      "[Epoch 4] Training Batch [53/391]: Loss 1.3265211582183838\n",
      "[Epoch 4] Training Batch [54/391]: Loss 1.1935360431671143\n",
      "[Epoch 4] Training Batch [55/391]: Loss 1.2402023077011108\n",
      "[Epoch 4] Training Batch [56/391]: Loss 1.1153959035873413\n",
      "[Epoch 4] Training Batch [57/391]: Loss 1.196569800376892\n",
      "[Epoch 4] Training Batch [58/391]: Loss 1.3244006633758545\n",
      "[Epoch 4] Training Batch [59/391]: Loss 1.3528128862380981\n",
      "[Epoch 4] Training Batch [60/391]: Loss 1.3862786293029785\n",
      "[Epoch 4] Training Batch [61/391]: Loss 1.1371177434921265\n",
      "[Epoch 4] Training Batch [62/391]: Loss 1.244066834449768\n",
      "[Epoch 4] Training Batch [63/391]: Loss 1.1797739267349243\n",
      "[Epoch 4] Training Batch [64/391]: Loss 1.4758713245391846\n",
      "[Epoch 4] Training Batch [65/391]: Loss 1.2570077180862427\n",
      "[Epoch 4] Training Batch [66/391]: Loss 1.1882127523422241\n",
      "[Epoch 4] Training Batch [67/391]: Loss 1.3185434341430664\n",
      "[Epoch 4] Training Batch [68/391]: Loss 1.4924581050872803\n",
      "[Epoch 4] Training Batch [69/391]: Loss 1.2527234554290771\n",
      "[Epoch 4] Training Batch [70/391]: Loss 1.1262608766555786\n",
      "[Epoch 4] Training Batch [71/391]: Loss 1.3345153331756592\n",
      "[Epoch 4] Training Batch [72/391]: Loss 1.228311538696289\n",
      "[Epoch 4] Training Batch [73/391]: Loss 1.2998946905136108\n",
      "[Epoch 4] Training Batch [74/391]: Loss 1.276807188987732\n",
      "[Epoch 4] Training Batch [75/391]: Loss 1.3914493322372437\n",
      "[Epoch 4] Training Batch [76/391]: Loss 1.350671648979187\n",
      "[Epoch 4] Training Batch [77/391]: Loss 1.2840251922607422\n",
      "[Epoch 4] Training Batch [78/391]: Loss 1.1525280475616455\n",
      "[Epoch 4] Training Batch [79/391]: Loss 1.3630321025848389\n",
      "[Epoch 4] Training Batch [80/391]: Loss 1.5560095310211182\n",
      "[Epoch 4] Training Batch [81/391]: Loss 1.2269439697265625\n",
      "[Epoch 4] Training Batch [82/391]: Loss 1.2116055488586426\n",
      "[Epoch 4] Training Batch [83/391]: Loss 1.1674509048461914\n",
      "[Epoch 4] Training Batch [84/391]: Loss 1.2347276210784912\n",
      "[Epoch 4] Training Batch [85/391]: Loss 1.2844493389129639\n",
      "[Epoch 4] Training Batch [86/391]: Loss 1.2899197340011597\n",
      "[Epoch 4] Training Batch [87/391]: Loss 1.1230673789978027\n",
      "[Epoch 4] Training Batch [88/391]: Loss 1.2201613187789917\n",
      "[Epoch 4] Training Batch [89/391]: Loss 1.4323570728302002\n",
      "[Epoch 4] Training Batch [90/391]: Loss 1.3682280778884888\n",
      "[Epoch 4] Training Batch [91/391]: Loss 1.221815824508667\n",
      "[Epoch 4] Training Batch [92/391]: Loss 1.1761890649795532\n",
      "[Epoch 4] Training Batch [93/391]: Loss 1.2649085521697998\n",
      "[Epoch 4] Training Batch [94/391]: Loss 1.2712974548339844\n",
      "[Epoch 4] Training Batch [95/391]: Loss 1.1362667083740234\n",
      "[Epoch 4] Training Batch [96/391]: Loss 1.3727121353149414\n",
      "[Epoch 4] Training Batch [97/391]: Loss 1.5552655458450317\n",
      "[Epoch 4] Training Batch [98/391]: Loss 1.2088353633880615\n",
      "[Epoch 4] Training Batch [99/391]: Loss 1.3312158584594727\n",
      "[Epoch 4] Training Batch [100/391]: Loss 1.2321749925613403\n",
      "[Epoch 4] Training Batch [101/391]: Loss 1.2619242668151855\n",
      "[Epoch 4] Training Batch [102/391]: Loss 1.2991266250610352\n",
      "[Epoch 4] Training Batch [103/391]: Loss 1.3906382322311401\n",
      "[Epoch 4] Training Batch [104/391]: Loss 1.153120994567871\n",
      "[Epoch 4] Training Batch [105/391]: Loss 1.2457187175750732\n",
      "[Epoch 4] Training Batch [106/391]: Loss 1.0332529544830322\n",
      "[Epoch 4] Training Batch [107/391]: Loss 1.2655400037765503\n",
      "[Epoch 4] Training Batch [108/391]: Loss 1.4140311479568481\n",
      "[Epoch 4] Training Batch [109/391]: Loss 1.1524301767349243\n",
      "[Epoch 4] Training Batch [110/391]: Loss 1.211527943611145\n",
      "[Epoch 4] Training Batch [111/391]: Loss 1.266484022140503\n",
      "[Epoch 4] Training Batch [112/391]: Loss 1.3077889680862427\n",
      "[Epoch 4] Training Batch [113/391]: Loss 1.1874536275863647\n",
      "[Epoch 4] Training Batch [114/391]: Loss 1.3623082637786865\n",
      "[Epoch 4] Training Batch [115/391]: Loss 1.0966644287109375\n",
      "[Epoch 4] Training Batch [116/391]: Loss 1.1545547246932983\n",
      "[Epoch 4] Training Batch [117/391]: Loss 1.4500222206115723\n",
      "[Epoch 4] Training Batch [118/391]: Loss 1.3600157499313354\n",
      "[Epoch 4] Training Batch [119/391]: Loss 1.2019848823547363\n",
      "[Epoch 4] Training Batch [120/391]: Loss 1.1774017810821533\n",
      "[Epoch 4] Training Batch [121/391]: Loss 1.1087172031402588\n",
      "[Epoch 4] Training Batch [122/391]: Loss 1.197379231452942\n",
      "[Epoch 4] Training Batch [123/391]: Loss 1.3194949626922607\n",
      "[Epoch 4] Training Batch [124/391]: Loss 1.0190720558166504\n",
      "[Epoch 4] Training Batch [125/391]: Loss 1.2747507095336914\n",
      "[Epoch 4] Training Batch [126/391]: Loss 1.2952382564544678\n",
      "[Epoch 4] Training Batch [127/391]: Loss 1.3149235248565674\n",
      "[Epoch 4] Training Batch [128/391]: Loss 1.210898756980896\n",
      "[Epoch 4] Training Batch [129/391]: Loss 1.3638700246810913\n",
      "[Epoch 4] Training Batch [130/391]: Loss 1.2599375247955322\n",
      "[Epoch 4] Training Batch [131/391]: Loss 1.1816694736480713\n",
      "[Epoch 4] Training Batch [132/391]: Loss 1.2308622598648071\n",
      "[Epoch 4] Training Batch [133/391]: Loss 1.0587223768234253\n",
      "[Epoch 4] Training Batch [134/391]: Loss 1.1379777193069458\n",
      "[Epoch 4] Training Batch [135/391]: Loss 1.4235976934432983\n",
      "[Epoch 4] Training Batch [136/391]: Loss 1.4315567016601562\n",
      "[Epoch 4] Training Batch [137/391]: Loss 1.1250834465026855\n",
      "[Epoch 4] Training Batch [138/391]: Loss 1.1843260526657104\n",
      "[Epoch 4] Training Batch [139/391]: Loss 1.3319768905639648\n",
      "[Epoch 4] Training Batch [140/391]: Loss 1.1656569242477417\n",
      "[Epoch 4] Training Batch [141/391]: Loss 1.2972848415374756\n",
      "[Epoch 4] Training Batch [142/391]: Loss 1.2468827962875366\n",
      "[Epoch 4] Training Batch [143/391]: Loss 1.2036548852920532\n",
      "[Epoch 4] Training Batch [144/391]: Loss 1.1381245851516724\n",
      "[Epoch 4] Training Batch [145/391]: Loss 1.256881594657898\n",
      "[Epoch 4] Training Batch [146/391]: Loss 1.358161211013794\n",
      "[Epoch 4] Training Batch [147/391]: Loss 1.1839148998260498\n",
      "[Epoch 4] Training Batch [148/391]: Loss 1.2731961011886597\n",
      "[Epoch 4] Training Batch [149/391]: Loss 1.523736596107483\n",
      "[Epoch 4] Training Batch [150/391]: Loss 1.1674549579620361\n",
      "[Epoch 4] Training Batch [151/391]: Loss 1.214166283607483\n",
      "[Epoch 4] Training Batch [152/391]: Loss 1.2021849155426025\n",
      "[Epoch 4] Training Batch [153/391]: Loss 1.2187397480010986\n",
      "[Epoch 4] Training Batch [154/391]: Loss 1.153061032295227\n",
      "[Epoch 4] Training Batch [155/391]: Loss 1.0929087400436401\n",
      "[Epoch 4] Training Batch [156/391]: Loss 1.2721447944641113\n",
      "[Epoch 4] Training Batch [157/391]: Loss 1.1929516792297363\n",
      "[Epoch 4] Training Batch [158/391]: Loss 1.2632198333740234\n",
      "[Epoch 4] Training Batch [159/391]: Loss 1.4549803733825684\n",
      "[Epoch 4] Training Batch [160/391]: Loss 1.3633580207824707\n",
      "[Epoch 4] Training Batch [161/391]: Loss 1.2976584434509277\n",
      "[Epoch 4] Training Batch [162/391]: Loss 1.2466317415237427\n",
      "[Epoch 4] Training Batch [163/391]: Loss 1.1870503425598145\n",
      "[Epoch 4] Training Batch [164/391]: Loss 1.285746693611145\n",
      "[Epoch 4] Training Batch [165/391]: Loss 1.3261487483978271\n",
      "[Epoch 4] Training Batch [166/391]: Loss 1.2315372228622437\n",
      "[Epoch 4] Training Batch [167/391]: Loss 1.4338542222976685\n",
      "[Epoch 4] Training Batch [168/391]: Loss 1.2703629732131958\n",
      "[Epoch 4] Training Batch [169/391]: Loss 1.168955683708191\n",
      "[Epoch 4] Training Batch [170/391]: Loss 1.1558526754379272\n",
      "[Epoch 4] Training Batch [171/391]: Loss 1.3274264335632324\n",
      "[Epoch 4] Training Batch [172/391]: Loss 1.1587224006652832\n",
      "[Epoch 4] Training Batch [173/391]: Loss 1.4116263389587402\n",
      "[Epoch 4] Training Batch [174/391]: Loss 1.1694517135620117\n",
      "[Epoch 4] Training Batch [175/391]: Loss 1.2127368450164795\n",
      "[Epoch 4] Training Batch [176/391]: Loss 1.3149480819702148\n",
      "[Epoch 4] Training Batch [177/391]: Loss 1.2151992321014404\n",
      "[Epoch 4] Training Batch [178/391]: Loss 1.2587376832962036\n",
      "[Epoch 4] Training Batch [179/391]: Loss 1.2188050746917725\n",
      "[Epoch 4] Training Batch [180/391]: Loss 1.2570217847824097\n",
      "[Epoch 4] Training Batch [181/391]: Loss 1.242884874343872\n",
      "[Epoch 4] Training Batch [182/391]: Loss 1.2972490787506104\n",
      "[Epoch 4] Training Batch [183/391]: Loss 1.3637568950653076\n",
      "[Epoch 4] Training Batch [184/391]: Loss 1.3465410470962524\n",
      "[Epoch 4] Training Batch [185/391]: Loss 1.2415093183517456\n",
      "[Epoch 4] Training Batch [186/391]: Loss 1.4252779483795166\n",
      "[Epoch 4] Training Batch [187/391]: Loss 1.1137410402297974\n",
      "[Epoch 4] Training Batch [188/391]: Loss 1.4667731523513794\n",
      "[Epoch 4] Training Batch [189/391]: Loss 1.1594822406768799\n",
      "[Epoch 4] Training Batch [190/391]: Loss 1.2436213493347168\n",
      "[Epoch 4] Training Batch [191/391]: Loss 1.3464452028274536\n",
      "[Epoch 4] Training Batch [192/391]: Loss 1.4164091348648071\n",
      "[Epoch 4] Training Batch [193/391]: Loss 1.2022045850753784\n",
      "[Epoch 4] Training Batch [194/391]: Loss 1.0989612340927124\n",
      "[Epoch 4] Training Batch [195/391]: Loss 1.2140039205551147\n",
      "[Epoch 4] Training Batch [196/391]: Loss 1.1817948818206787\n",
      "[Epoch 4] Training Batch [197/391]: Loss 1.3248108625411987\n",
      "[Epoch 4] Training Batch [198/391]: Loss 1.1613028049468994\n",
      "[Epoch 4] Training Batch [199/391]: Loss 1.2255324125289917\n",
      "[Epoch 4] Training Batch [200/391]: Loss 1.1489267349243164\n",
      "[Epoch 4] Training Batch [201/391]: Loss 1.4748001098632812\n",
      "[Epoch 4] Training Batch [202/391]: Loss 1.3329776525497437\n",
      "[Epoch 4] Training Batch [203/391]: Loss 1.2632910013198853\n",
      "[Epoch 4] Training Batch [204/391]: Loss 1.2319765090942383\n",
      "[Epoch 4] Training Batch [205/391]: Loss 1.2407537698745728\n",
      "[Epoch 4] Training Batch [206/391]: Loss 1.3168232440948486\n",
      "[Epoch 4] Training Batch [207/391]: Loss 1.2871524095535278\n",
      "[Epoch 4] Training Batch [208/391]: Loss 1.2386707067489624\n",
      "[Epoch 4] Training Batch [209/391]: Loss 1.1601154804229736\n",
      "[Epoch 4] Training Batch [210/391]: Loss 1.1278176307678223\n",
      "[Epoch 4] Training Batch [211/391]: Loss 1.2184715270996094\n",
      "[Epoch 4] Training Batch [212/391]: Loss 1.0950545072555542\n",
      "[Epoch 4] Training Batch [213/391]: Loss 1.1247036457061768\n",
      "[Epoch 4] Training Batch [214/391]: Loss 1.3368852138519287\n",
      "[Epoch 4] Training Batch [215/391]: Loss 1.1497888565063477\n",
      "[Epoch 4] Training Batch [216/391]: Loss 1.2346456050872803\n",
      "[Epoch 4] Training Batch [217/391]: Loss 1.2134594917297363\n",
      "[Epoch 4] Training Batch [218/391]: Loss 1.2584426403045654\n",
      "[Epoch 4] Training Batch [219/391]: Loss 1.181282877922058\n",
      "[Epoch 4] Training Batch [220/391]: Loss 1.2253859043121338\n",
      "[Epoch 4] Training Batch [221/391]: Loss 1.2136595249176025\n",
      "[Epoch 4] Training Batch [222/391]: Loss 1.257752537727356\n",
      "[Epoch 4] Training Batch [223/391]: Loss 1.3258174657821655\n",
      "[Epoch 4] Training Batch [224/391]: Loss 1.1413123607635498\n",
      "[Epoch 4] Training Batch [225/391]: Loss 1.1823787689208984\n",
      "[Epoch 4] Training Batch [226/391]: Loss 1.2355090379714966\n",
      "[Epoch 4] Training Batch [227/391]: Loss 1.1942598819732666\n",
      "[Epoch 4] Training Batch [228/391]: Loss 1.1037931442260742\n",
      "[Epoch 4] Training Batch [229/391]: Loss 1.1462513208389282\n",
      "[Epoch 4] Training Batch [230/391]: Loss 1.2164866924285889\n",
      "[Epoch 4] Training Batch [231/391]: Loss 1.172310471534729\n",
      "[Epoch 4] Training Batch [232/391]: Loss 1.1261347532272339\n",
      "[Epoch 4] Training Batch [233/391]: Loss 1.4139214754104614\n",
      "[Epoch 4] Training Batch [234/391]: Loss 1.3562766313552856\n",
      "[Epoch 4] Training Batch [235/391]: Loss 1.3674707412719727\n",
      "[Epoch 4] Training Batch [236/391]: Loss 1.2743183374404907\n",
      "[Epoch 4] Training Batch [237/391]: Loss 1.1983839273452759\n",
      "[Epoch 4] Training Batch [238/391]: Loss 1.1709015369415283\n",
      "[Epoch 4] Training Batch [239/391]: Loss 1.275689959526062\n",
      "[Epoch 4] Training Batch [240/391]: Loss 0.9599565863609314\n",
      "[Epoch 4] Training Batch [241/391]: Loss 1.1928297281265259\n",
      "[Epoch 4] Training Batch [242/391]: Loss 1.208931565284729\n",
      "[Epoch 4] Training Batch [243/391]: Loss 1.299392580986023\n",
      "[Epoch 4] Training Batch [244/391]: Loss 1.3452335596084595\n",
      "[Epoch 4] Training Batch [245/391]: Loss 1.2661359310150146\n",
      "[Epoch 4] Training Batch [246/391]: Loss 1.2853394746780396\n",
      "[Epoch 4] Training Batch [247/391]: Loss 1.4247556924819946\n",
      "[Epoch 4] Training Batch [248/391]: Loss 1.1440485715866089\n",
      "[Epoch 4] Training Batch [249/391]: Loss 1.48478364944458\n",
      "[Epoch 4] Training Batch [250/391]: Loss 1.3163104057312012\n",
      "[Epoch 4] Training Batch [251/391]: Loss 1.2833385467529297\n",
      "[Epoch 4] Training Batch [252/391]: Loss 1.0986603498458862\n",
      "[Epoch 4] Training Batch [253/391]: Loss 1.389542818069458\n",
      "[Epoch 4] Training Batch [254/391]: Loss 1.1874949932098389\n",
      "[Epoch 4] Training Batch [255/391]: Loss 1.2292330265045166\n",
      "[Epoch 4] Training Batch [256/391]: Loss 1.1677097082138062\n",
      "[Epoch 4] Training Batch [257/391]: Loss 1.342844009399414\n",
      "[Epoch 4] Training Batch [258/391]: Loss 1.2975664138793945\n",
      "[Epoch 4] Training Batch [259/391]: Loss 1.2352056503295898\n",
      "[Epoch 4] Training Batch [260/391]: Loss 1.3268210887908936\n",
      "[Epoch 4] Training Batch [261/391]: Loss 1.327862024307251\n",
      "[Epoch 4] Training Batch [262/391]: Loss 1.2594536542892456\n",
      "[Epoch 4] Training Batch [263/391]: Loss 1.364359736442566\n",
      "[Epoch 4] Training Batch [264/391]: Loss 1.297799825668335\n",
      "[Epoch 4] Training Batch [265/391]: Loss 1.1088814735412598\n",
      "[Epoch 4] Training Batch [266/391]: Loss 1.2440638542175293\n",
      "[Epoch 4] Training Batch [267/391]: Loss 1.1543394327163696\n",
      "[Epoch 4] Training Batch [268/391]: Loss 1.2142099142074585\n",
      "[Epoch 4] Training Batch [269/391]: Loss 1.4226480722427368\n",
      "[Epoch 4] Training Batch [270/391]: Loss 1.1766029596328735\n",
      "[Epoch 4] Training Batch [271/391]: Loss 1.1157832145690918\n",
      "[Epoch 4] Training Batch [272/391]: Loss 1.323880910873413\n",
      "[Epoch 4] Training Batch [273/391]: Loss 1.291615605354309\n",
      "[Epoch 4] Training Batch [274/391]: Loss 1.2035027742385864\n",
      "[Epoch 4] Training Batch [275/391]: Loss 1.1362899541854858\n",
      "[Epoch 4] Training Batch [276/391]: Loss 1.3261624574661255\n",
      "[Epoch 4] Training Batch [277/391]: Loss 1.2198325395584106\n",
      "[Epoch 4] Training Batch [278/391]: Loss 1.3304791450500488\n",
      "[Epoch 4] Training Batch [279/391]: Loss 1.1569584608078003\n",
      "[Epoch 4] Training Batch [280/391]: Loss 1.075995922088623\n",
      "[Epoch 4] Training Batch [281/391]: Loss 1.1563482284545898\n",
      "[Epoch 4] Training Batch [282/391]: Loss 1.1498357057571411\n",
      "[Epoch 4] Training Batch [283/391]: Loss 1.4043010473251343\n",
      "[Epoch 4] Training Batch [284/391]: Loss 1.3527591228485107\n",
      "[Epoch 4] Training Batch [285/391]: Loss 1.3919869661331177\n",
      "[Epoch 4] Training Batch [286/391]: Loss 1.2621475458145142\n",
      "[Epoch 4] Training Batch [287/391]: Loss 1.1897279024124146\n",
      "[Epoch 4] Training Batch [288/391]: Loss 1.1127574443817139\n",
      "[Epoch 4] Training Batch [289/391]: Loss 1.1837235689163208\n",
      "[Epoch 4] Training Batch [290/391]: Loss 1.286817193031311\n",
      "[Epoch 4] Training Batch [291/391]: Loss 1.2147632837295532\n",
      "[Epoch 4] Training Batch [292/391]: Loss 1.3239386081695557\n",
      "[Epoch 4] Training Batch [293/391]: Loss 1.1970497369766235\n",
      "[Epoch 4] Training Batch [294/391]: Loss 1.1291711330413818\n",
      "[Epoch 4] Training Batch [295/391]: Loss 1.3680899143218994\n",
      "[Epoch 4] Training Batch [296/391]: Loss 1.1123799085617065\n",
      "[Epoch 4] Training Batch [297/391]: Loss 1.2246185541152954\n",
      "[Epoch 4] Training Batch [298/391]: Loss 1.2312899827957153\n",
      "[Epoch 4] Training Batch [299/391]: Loss 1.3506704568862915\n",
      "[Epoch 4] Training Batch [300/391]: Loss 1.2609236240386963\n",
      "[Epoch 4] Training Batch [301/391]: Loss 1.1568857431411743\n",
      "[Epoch 4] Training Batch [302/391]: Loss 1.2285162210464478\n",
      "[Epoch 4] Training Batch [303/391]: Loss 1.3564192056655884\n",
      "[Epoch 4] Training Batch [304/391]: Loss 1.1889891624450684\n",
      "[Epoch 4] Training Batch [305/391]: Loss 1.1108986139297485\n",
      "[Epoch 4] Training Batch [306/391]: Loss 1.243128776550293\n",
      "[Epoch 4] Training Batch [307/391]: Loss 1.2794387340545654\n",
      "[Epoch 4] Training Batch [308/391]: Loss 1.1594499349594116\n",
      "[Epoch 4] Training Batch [309/391]: Loss 1.2162253856658936\n",
      "[Epoch 4] Training Batch [310/391]: Loss 1.2256956100463867\n",
      "[Epoch 4] Training Batch [311/391]: Loss 1.3429745435714722\n",
      "[Epoch 4] Training Batch [312/391]: Loss 1.207036018371582\n",
      "[Epoch 4] Training Batch [313/391]: Loss 1.1840986013412476\n",
      "[Epoch 4] Training Batch [314/391]: Loss 1.2145326137542725\n",
      "[Epoch 4] Training Batch [315/391]: Loss 1.2250021696090698\n",
      "[Epoch 4] Training Batch [316/391]: Loss 1.141326665878296\n",
      "[Epoch 4] Training Batch [317/391]: Loss 1.2190691232681274\n",
      "[Epoch 4] Training Batch [318/391]: Loss 1.4862751960754395\n",
      "[Epoch 4] Training Batch [319/391]: Loss 1.2801631689071655\n",
      "[Epoch 4] Training Batch [320/391]: Loss 1.2056843042373657\n",
      "[Epoch 4] Training Batch [321/391]: Loss 1.1954331398010254\n",
      "[Epoch 4] Training Batch [322/391]: Loss 1.3054287433624268\n",
      "[Epoch 4] Training Batch [323/391]: Loss 1.2335419654846191\n",
      "[Epoch 4] Training Batch [324/391]: Loss 1.283583402633667\n",
      "[Epoch 4] Training Batch [325/391]: Loss 1.1144647598266602\n",
      "[Epoch 4] Training Batch [326/391]: Loss 1.267149567604065\n",
      "[Epoch 4] Training Batch [327/391]: Loss 1.3925994634628296\n",
      "[Epoch 4] Training Batch [328/391]: Loss 1.2292368412017822\n",
      "[Epoch 4] Training Batch [329/391]: Loss 1.080487608909607\n",
      "[Epoch 4] Training Batch [330/391]: Loss 1.0508564710617065\n",
      "[Epoch 4] Training Batch [331/391]: Loss 1.1912885904312134\n",
      "[Epoch 4] Training Batch [332/391]: Loss 1.1581361293792725\n",
      "[Epoch 4] Training Batch [333/391]: Loss 1.292557716369629\n",
      "[Epoch 4] Training Batch [334/391]: Loss 1.0605683326721191\n",
      "[Epoch 4] Training Batch [335/391]: Loss 1.2110435962677002\n",
      "[Epoch 4] Training Batch [336/391]: Loss 1.2248587608337402\n",
      "[Epoch 4] Training Batch [337/391]: Loss 1.3112893104553223\n",
      "[Epoch 4] Training Batch [338/391]: Loss 1.235738754272461\n",
      "[Epoch 4] Training Batch [339/391]: Loss 1.2869640588760376\n",
      "[Epoch 4] Training Batch [340/391]: Loss 1.3996961116790771\n",
      "[Epoch 4] Training Batch [341/391]: Loss 1.1723085641860962\n",
      "[Epoch 4] Training Batch [342/391]: Loss 1.0642554759979248\n",
      "[Epoch 4] Training Batch [343/391]: Loss 1.1534072160720825\n",
      "[Epoch 4] Training Batch [344/391]: Loss 1.130226731300354\n",
      "[Epoch 4] Training Batch [345/391]: Loss 1.09613037109375\n",
      "[Epoch 4] Training Batch [346/391]: Loss 1.0728237628936768\n",
      "[Epoch 4] Training Batch [347/391]: Loss 1.225603461265564\n",
      "[Epoch 4] Training Batch [348/391]: Loss 1.1420633792877197\n",
      "[Epoch 4] Training Batch [349/391]: Loss 1.315685510635376\n",
      "[Epoch 4] Training Batch [350/391]: Loss 1.2202166318893433\n",
      "[Epoch 4] Training Batch [351/391]: Loss 1.1774075031280518\n",
      "[Epoch 4] Training Batch [352/391]: Loss 1.0537245273590088\n",
      "[Epoch 4] Training Batch [353/391]: Loss 1.2760719060897827\n",
      "[Epoch 4] Training Batch [354/391]: Loss 1.3873875141143799\n",
      "[Epoch 4] Training Batch [355/391]: Loss 1.4916510581970215\n",
      "[Epoch 4] Training Batch [356/391]: Loss 1.2175946235656738\n",
      "[Epoch 4] Training Batch [357/391]: Loss 1.0794637203216553\n",
      "[Epoch 4] Training Batch [358/391]: Loss 1.3966270685195923\n",
      "[Epoch 4] Training Batch [359/391]: Loss 1.2663830518722534\n",
      "[Epoch 4] Training Batch [360/391]: Loss 1.2305757999420166\n",
      "[Epoch 4] Training Batch [361/391]: Loss 1.3303015232086182\n",
      "[Epoch 4] Training Batch [362/391]: Loss 1.258231282234192\n",
      "[Epoch 4] Training Batch [363/391]: Loss 1.1005076169967651\n",
      "[Epoch 4] Training Batch [364/391]: Loss 1.1163113117218018\n",
      "[Epoch 4] Training Batch [365/391]: Loss 1.2967815399169922\n",
      "[Epoch 4] Training Batch [366/391]: Loss 0.9852282404899597\n",
      "[Epoch 4] Training Batch [367/391]: Loss 1.3044463396072388\n",
      "[Epoch 4] Training Batch [368/391]: Loss 1.3423713445663452\n",
      "[Epoch 4] Training Batch [369/391]: Loss 1.302577257156372\n",
      "[Epoch 4] Training Batch [370/391]: Loss 1.2283271551132202\n",
      "[Epoch 4] Training Batch [371/391]: Loss 1.1606301069259644\n",
      "[Epoch 4] Training Batch [372/391]: Loss 1.2193599939346313\n",
      "[Epoch 4] Training Batch [373/391]: Loss 1.216722011566162\n",
      "[Epoch 4] Training Batch [374/391]: Loss 1.258618712425232\n",
      "[Epoch 4] Training Batch [375/391]: Loss 1.1646647453308105\n",
      "[Epoch 4] Training Batch [376/391]: Loss 1.0781058073043823\n",
      "[Epoch 4] Training Batch [377/391]: Loss 1.1107767820358276\n",
      "[Epoch 4] Training Batch [378/391]: Loss 1.1217999458312988\n",
      "[Epoch 4] Training Batch [379/391]: Loss 1.2258570194244385\n",
      "[Epoch 4] Training Batch [380/391]: Loss 1.3354188203811646\n",
      "[Epoch 4] Training Batch [381/391]: Loss 1.166776180267334\n",
      "[Epoch 4] Training Batch [382/391]: Loss 1.2551429271697998\n",
      "[Epoch 4] Training Batch [383/391]: Loss 1.356199026107788\n",
      "[Epoch 4] Training Batch [384/391]: Loss 1.2018909454345703\n",
      "[Epoch 4] Training Batch [385/391]: Loss 1.2638435363769531\n",
      "[Epoch 4] Training Batch [386/391]: Loss 1.1824870109558105\n",
      "[Epoch 4] Training Batch [387/391]: Loss 1.421113133430481\n",
      "[Epoch 4] Training Batch [388/391]: Loss 1.205296277999878\n",
      "[Epoch 4] Training Batch [389/391]: Loss 1.3576916456222534\n",
      "[Epoch 4] Training Batch [390/391]: Loss 1.2680869102478027\n",
      "[Epoch 4] Training Batch [391/391]: Loss 1.0889902114868164\n",
      "Epoch 4 - Train Loss: 1.2480\n",
      "*********  Epoch 5/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Training Batch [1/391]: Loss 1.197948694229126\n",
      "[Epoch 5] Training Batch [2/391]: Loss 1.0287623405456543\n",
      "[Epoch 5] Training Batch [3/391]: Loss 1.3504419326782227\n",
      "[Epoch 5] Training Batch [4/391]: Loss 1.2958003282546997\n",
      "[Epoch 5] Training Batch [5/391]: Loss 1.169813632965088\n",
      "[Epoch 5] Training Batch [6/391]: Loss 1.1898884773254395\n",
      "[Epoch 5] Training Batch [7/391]: Loss 1.1018309593200684\n",
      "[Epoch 5] Training Batch [8/391]: Loss 1.2262991666793823\n",
      "[Epoch 5] Training Batch [9/391]: Loss 1.0983585119247437\n",
      "[Epoch 5] Training Batch [10/391]: Loss 1.1445106267929077\n",
      "[Epoch 5] Training Batch [11/391]: Loss 1.1845027208328247\n",
      "[Epoch 5] Training Batch [12/391]: Loss 1.0376908779144287\n",
      "[Epoch 5] Training Batch [13/391]: Loss 1.1797701120376587\n",
      "[Epoch 5] Training Batch [14/391]: Loss 1.0520223379135132\n",
      "[Epoch 5] Training Batch [15/391]: Loss 1.1754807233810425\n",
      "[Epoch 5] Training Batch [16/391]: Loss 1.2090295553207397\n",
      "[Epoch 5] Training Batch [17/391]: Loss 1.1009875535964966\n",
      "[Epoch 5] Training Batch [18/391]: Loss 1.1525923013687134\n",
      "[Epoch 5] Training Batch [19/391]: Loss 0.9620601534843445\n",
      "[Epoch 5] Training Batch [20/391]: Loss 1.167473316192627\n",
      "[Epoch 5] Training Batch [21/391]: Loss 0.9976911544799805\n",
      "[Epoch 5] Training Batch [22/391]: Loss 1.1702948808670044\n",
      "[Epoch 5] Training Batch [23/391]: Loss 1.2491010427474976\n",
      "[Epoch 5] Training Batch [24/391]: Loss 1.0481687784194946\n",
      "[Epoch 5] Training Batch [25/391]: Loss 1.1423780918121338\n",
      "[Epoch 5] Training Batch [26/391]: Loss 0.9921208024024963\n",
      "[Epoch 5] Training Batch [27/391]: Loss 0.9837217926979065\n",
      "[Epoch 5] Training Batch [28/391]: Loss 1.0794296264648438\n",
      "[Epoch 5] Training Batch [29/391]: Loss 1.0303648710250854\n",
      "[Epoch 5] Training Batch [30/391]: Loss 0.9454886317253113\n",
      "[Epoch 5] Training Batch [31/391]: Loss 1.029639720916748\n",
      "[Epoch 5] Training Batch [32/391]: Loss 1.120192289352417\n",
      "[Epoch 5] Training Batch [33/391]: Loss 1.2973599433898926\n",
      "[Epoch 5] Training Batch [34/391]: Loss 1.25554358959198\n",
      "[Epoch 5] Training Batch [35/391]: Loss 1.2021725177764893\n",
      "[Epoch 5] Training Batch [36/391]: Loss 1.1718993186950684\n",
      "[Epoch 5] Training Batch [37/391]: Loss 1.187554121017456\n",
      "[Epoch 5] Training Batch [38/391]: Loss 1.0654757022857666\n",
      "[Epoch 5] Training Batch [39/391]: Loss 1.0229828357696533\n",
      "[Epoch 5] Training Batch [40/391]: Loss 1.0733072757720947\n",
      "[Epoch 5] Training Batch [41/391]: Loss 0.9101680517196655\n",
      "[Epoch 5] Training Batch [42/391]: Loss 1.1297322511672974\n",
      "[Epoch 5] Training Batch [43/391]: Loss 1.2653849124908447\n",
      "[Epoch 5] Training Batch [44/391]: Loss 1.1957582235336304\n",
      "[Epoch 5] Training Batch [45/391]: Loss 1.2098625898361206\n",
      "[Epoch 5] Training Batch [46/391]: Loss 1.061343789100647\n",
      "[Epoch 5] Training Batch [47/391]: Loss 1.0346357822418213\n",
      "[Epoch 5] Training Batch [48/391]: Loss 1.0055049657821655\n",
      "[Epoch 5] Training Batch [49/391]: Loss 1.1233686208724976\n",
      "[Epoch 5] Training Batch [50/391]: Loss 1.1720218658447266\n",
      "[Epoch 5] Training Batch [51/391]: Loss 1.1493635177612305\n",
      "[Epoch 5] Training Batch [52/391]: Loss 1.263140082359314\n",
      "[Epoch 5] Training Batch [53/391]: Loss 1.1545318365097046\n",
      "[Epoch 5] Training Batch [54/391]: Loss 1.0388565063476562\n",
      "[Epoch 5] Training Batch [55/391]: Loss 1.1130943298339844\n",
      "[Epoch 5] Training Batch [56/391]: Loss 1.0869691371917725\n",
      "[Epoch 5] Training Batch [57/391]: Loss 1.1959701776504517\n",
      "[Epoch 5] Training Batch [58/391]: Loss 1.112162470817566\n",
      "[Epoch 5] Training Batch [59/391]: Loss 1.0786089897155762\n",
      "[Epoch 5] Training Batch [60/391]: Loss 0.981123685836792\n",
      "[Epoch 5] Training Batch [61/391]: Loss 1.0642313957214355\n",
      "[Epoch 5] Training Batch [62/391]: Loss 1.0706114768981934\n",
      "[Epoch 5] Training Batch [63/391]: Loss 1.2192840576171875\n",
      "[Epoch 5] Training Batch [64/391]: Loss 0.966471791267395\n",
      "[Epoch 5] Training Batch [65/391]: Loss 1.0380809307098389\n",
      "[Epoch 5] Training Batch [66/391]: Loss 1.110508680343628\n",
      "[Epoch 5] Training Batch [67/391]: Loss 1.0395290851593018\n",
      "[Epoch 5] Training Batch [68/391]: Loss 1.1798804998397827\n",
      "[Epoch 5] Training Batch [69/391]: Loss 1.0844281911849976\n",
      "[Epoch 5] Training Batch [70/391]: Loss 1.2869256734848022\n",
      "[Epoch 5] Training Batch [71/391]: Loss 1.1560802459716797\n",
      "[Epoch 5] Training Batch [72/391]: Loss 1.0365440845489502\n",
      "[Epoch 5] Training Batch [73/391]: Loss 1.2375338077545166\n",
      "[Epoch 5] Training Batch [74/391]: Loss 1.1652393341064453\n",
      "[Epoch 5] Training Batch [75/391]: Loss 1.189117193222046\n",
      "[Epoch 5] Training Batch [76/391]: Loss 1.2335015535354614\n",
      "[Epoch 5] Training Batch [77/391]: Loss 1.0276501178741455\n",
      "[Epoch 5] Training Batch [78/391]: Loss 1.2143076658248901\n",
      "[Epoch 5] Training Batch [79/391]: Loss 1.1068944931030273\n",
      "[Epoch 5] Training Batch [80/391]: Loss 1.0932941436767578\n",
      "[Epoch 5] Training Batch [81/391]: Loss 1.1757057905197144\n",
      "[Epoch 5] Training Batch [82/391]: Loss 0.9886389374732971\n",
      "[Epoch 5] Training Batch [83/391]: Loss 1.1598507165908813\n",
      "[Epoch 5] Training Batch [84/391]: Loss 1.0433062314987183\n",
      "[Epoch 5] Training Batch [85/391]: Loss 1.1149516105651855\n",
      "[Epoch 5] Training Batch [86/391]: Loss 1.1406960487365723\n",
      "[Epoch 5] Training Batch [87/391]: Loss 1.2145296335220337\n",
      "[Epoch 5] Training Batch [88/391]: Loss 1.0262038707733154\n",
      "[Epoch 5] Training Batch [89/391]: Loss 1.0981720685958862\n",
      "[Epoch 5] Training Batch [90/391]: Loss 1.1963999271392822\n",
      "[Epoch 5] Training Batch [91/391]: Loss 1.1353614330291748\n",
      "[Epoch 5] Training Batch [92/391]: Loss 1.046186923980713\n",
      "[Epoch 5] Training Batch [93/391]: Loss 1.0795766115188599\n",
      "[Epoch 5] Training Batch [94/391]: Loss 1.0583781003952026\n",
      "[Epoch 5] Training Batch [95/391]: Loss 1.2371677160263062\n",
      "[Epoch 5] Training Batch [96/391]: Loss 1.1696149110794067\n",
      "[Epoch 5] Training Batch [97/391]: Loss 1.1957194805145264\n",
      "[Epoch 5] Training Batch [98/391]: Loss 1.046875\n",
      "[Epoch 5] Training Batch [99/391]: Loss 1.1297138929367065\n",
      "[Epoch 5] Training Batch [100/391]: Loss 1.1810604333877563\n",
      "[Epoch 5] Training Batch [101/391]: Loss 1.1658540964126587\n",
      "[Epoch 5] Training Batch [102/391]: Loss 0.9649865031242371\n",
      "[Epoch 5] Training Batch [103/391]: Loss 1.0311940908432007\n",
      "[Epoch 5] Training Batch [104/391]: Loss 1.1209166049957275\n",
      "[Epoch 5] Training Batch [105/391]: Loss 0.9848446846008301\n",
      "[Epoch 5] Training Batch [106/391]: Loss 1.328141212463379\n",
      "[Epoch 5] Training Batch [107/391]: Loss 1.1266275644302368\n",
      "[Epoch 5] Training Batch [108/391]: Loss 1.1488300561904907\n",
      "[Epoch 5] Training Batch [109/391]: Loss 1.093813180923462\n",
      "[Epoch 5] Training Batch [110/391]: Loss 1.29378342628479\n",
      "[Epoch 5] Training Batch [111/391]: Loss 1.1994670629501343\n",
      "[Epoch 5] Training Batch [112/391]: Loss 1.1231790781021118\n",
      "[Epoch 5] Training Batch [113/391]: Loss 1.2130948305130005\n",
      "[Epoch 5] Training Batch [114/391]: Loss 1.1780383586883545\n",
      "[Epoch 5] Training Batch [115/391]: Loss 1.0789846181869507\n",
      "[Epoch 5] Training Batch [116/391]: Loss 1.026767611503601\n",
      "[Epoch 5] Training Batch [117/391]: Loss 1.1619782447814941\n",
      "[Epoch 5] Training Batch [118/391]: Loss 1.1956861019134521\n",
      "[Epoch 5] Training Batch [119/391]: Loss 1.368263602256775\n",
      "[Epoch 5] Training Batch [120/391]: Loss 1.2202033996582031\n",
      "[Epoch 5] Training Batch [121/391]: Loss 1.1966651678085327\n",
      "[Epoch 5] Training Batch [122/391]: Loss 1.1585452556610107\n",
      "[Epoch 5] Training Batch [123/391]: Loss 1.1180657148361206\n",
      "[Epoch 5] Training Batch [124/391]: Loss 1.1444571018218994\n",
      "[Epoch 5] Training Batch [125/391]: Loss 1.0949124097824097\n",
      "[Epoch 5] Training Batch [126/391]: Loss 1.1812865734100342\n",
      "[Epoch 5] Training Batch [127/391]: Loss 1.0113216638565063\n",
      "[Epoch 5] Training Batch [128/391]: Loss 1.218814730644226\n",
      "[Epoch 5] Training Batch [129/391]: Loss 1.0384730100631714\n",
      "[Epoch 5] Training Batch [130/391]: Loss 1.1752080917358398\n",
      "[Epoch 5] Training Batch [131/391]: Loss 1.16215181350708\n",
      "[Epoch 5] Training Batch [132/391]: Loss 1.25106680393219\n",
      "[Epoch 5] Training Batch [133/391]: Loss 1.1483734846115112\n",
      "[Epoch 5] Training Batch [134/391]: Loss 1.2041904926300049\n",
      "[Epoch 5] Training Batch [135/391]: Loss 1.1005104780197144\n",
      "[Epoch 5] Training Batch [136/391]: Loss 1.0398417711257935\n",
      "[Epoch 5] Training Batch [137/391]: Loss 1.1192878484725952\n",
      "[Epoch 5] Training Batch [138/391]: Loss 1.064808964729309\n",
      "[Epoch 5] Training Batch [139/391]: Loss 1.1456329822540283\n",
      "[Epoch 5] Training Batch [140/391]: Loss 1.2577649354934692\n",
      "[Epoch 5] Training Batch [141/391]: Loss 1.1271772384643555\n",
      "[Epoch 5] Training Batch [142/391]: Loss 1.0240696668624878\n",
      "[Epoch 5] Training Batch [143/391]: Loss 1.2198742628097534\n",
      "[Epoch 5] Training Batch [144/391]: Loss 1.294196605682373\n",
      "[Epoch 5] Training Batch [145/391]: Loss 1.2083654403686523\n",
      "[Epoch 5] Training Batch [146/391]: Loss 1.1008001565933228\n",
      "[Epoch 5] Training Batch [147/391]: Loss 1.1218444108963013\n",
      "[Epoch 5] Training Batch [148/391]: Loss 1.1816022396087646\n",
      "[Epoch 5] Training Batch [149/391]: Loss 1.1348341703414917\n",
      "[Epoch 5] Training Batch [150/391]: Loss 1.0759413242340088\n",
      "[Epoch 5] Training Batch [151/391]: Loss 1.1081398725509644\n",
      "[Epoch 5] Training Batch [152/391]: Loss 1.2674862146377563\n",
      "[Epoch 5] Training Batch [153/391]: Loss 1.2273800373077393\n",
      "[Epoch 5] Training Batch [154/391]: Loss 1.0477290153503418\n",
      "[Epoch 5] Training Batch [155/391]: Loss 1.0389002561569214\n",
      "[Epoch 5] Training Batch [156/391]: Loss 1.3189152479171753\n",
      "[Epoch 5] Training Batch [157/391]: Loss 1.235184907913208\n",
      "[Epoch 5] Training Batch [158/391]: Loss 1.158124566078186\n",
      "[Epoch 5] Training Batch [159/391]: Loss 0.9886078238487244\n",
      "[Epoch 5] Training Batch [160/391]: Loss 1.053051233291626\n",
      "[Epoch 5] Training Batch [161/391]: Loss 1.2446554899215698\n",
      "[Epoch 5] Training Batch [162/391]: Loss 1.0122473239898682\n",
      "[Epoch 5] Training Batch [163/391]: Loss 1.166847825050354\n",
      "[Epoch 5] Training Batch [164/391]: Loss 1.114983081817627\n",
      "[Epoch 5] Training Batch [165/391]: Loss 1.1907134056091309\n",
      "[Epoch 5] Training Batch [166/391]: Loss 1.0919175148010254\n",
      "[Epoch 5] Training Batch [167/391]: Loss 1.285377860069275\n",
      "[Epoch 5] Training Batch [168/391]: Loss 1.0076321363449097\n",
      "[Epoch 5] Training Batch [169/391]: Loss 1.1455798149108887\n",
      "[Epoch 5] Training Batch [170/391]: Loss 0.9569129943847656\n",
      "[Epoch 5] Training Batch [171/391]: Loss 0.9774169325828552\n",
      "[Epoch 5] Training Batch [172/391]: Loss 1.2162219285964966\n",
      "[Epoch 5] Training Batch [173/391]: Loss 1.1333072185516357\n",
      "[Epoch 5] Training Batch [174/391]: Loss 1.0209581851959229\n",
      "[Epoch 5] Training Batch [175/391]: Loss 1.0375334024429321\n",
      "[Epoch 5] Training Batch [176/391]: Loss 1.2198679447174072\n",
      "[Epoch 5] Training Batch [177/391]: Loss 1.238826870918274\n",
      "[Epoch 5] Training Batch [178/391]: Loss 1.068471074104309\n",
      "[Epoch 5] Training Batch [179/391]: Loss 1.1842998266220093\n",
      "[Epoch 5] Training Batch [180/391]: Loss 1.0416420698165894\n",
      "[Epoch 5] Training Batch [181/391]: Loss 1.0759742259979248\n",
      "[Epoch 5] Training Batch [182/391]: Loss 1.147175669670105\n",
      "[Epoch 5] Training Batch [183/391]: Loss 1.0801297426223755\n",
      "[Epoch 5] Training Batch [184/391]: Loss 1.1173069477081299\n",
      "[Epoch 5] Training Batch [185/391]: Loss 1.1638033390045166\n",
      "[Epoch 5] Training Batch [186/391]: Loss 1.1117970943450928\n",
      "[Epoch 5] Training Batch [187/391]: Loss 0.9434489607810974\n",
      "[Epoch 5] Training Batch [188/391]: Loss 1.0178353786468506\n",
      "[Epoch 5] Training Batch [189/391]: Loss 1.1619619131088257\n",
      "[Epoch 5] Training Batch [190/391]: Loss 1.1179323196411133\n",
      "[Epoch 5] Training Batch [191/391]: Loss 1.1075979471206665\n",
      "[Epoch 5] Training Batch [192/391]: Loss 1.2272521257400513\n",
      "[Epoch 5] Training Batch [193/391]: Loss 1.1099530458450317\n",
      "[Epoch 5] Training Batch [194/391]: Loss 1.1630373001098633\n",
      "[Epoch 5] Training Batch [195/391]: Loss 1.088483452796936\n",
      "[Epoch 5] Training Batch [196/391]: Loss 1.2152172327041626\n",
      "[Epoch 5] Training Batch [197/391]: Loss 1.1130820512771606\n",
      "[Epoch 5] Training Batch [198/391]: Loss 1.1288009881973267\n",
      "[Epoch 5] Training Batch [199/391]: Loss 1.102842092514038\n",
      "[Epoch 5] Training Batch [200/391]: Loss 1.3224209547042847\n",
      "[Epoch 5] Training Batch [201/391]: Loss 1.038392186164856\n",
      "[Epoch 5] Training Batch [202/391]: Loss 1.1222883462905884\n",
      "[Epoch 5] Training Batch [203/391]: Loss 1.0678763389587402\n",
      "[Epoch 5] Training Batch [204/391]: Loss 1.155106544494629\n",
      "[Epoch 5] Training Batch [205/391]: Loss 1.090836524963379\n",
      "[Epoch 5] Training Batch [206/391]: Loss 0.9945198893547058\n",
      "[Epoch 5] Training Batch [207/391]: Loss 0.975901186466217\n",
      "[Epoch 5] Training Batch [208/391]: Loss 1.1136481761932373\n",
      "[Epoch 5] Training Batch [209/391]: Loss 1.0852181911468506\n",
      "[Epoch 5] Training Batch [210/391]: Loss 1.069029450416565\n",
      "[Epoch 5] Training Batch [211/391]: Loss 1.077012062072754\n",
      "[Epoch 5] Training Batch [212/391]: Loss 1.2779595851898193\n",
      "[Epoch 5] Training Batch [213/391]: Loss 1.121941328048706\n",
      "[Epoch 5] Training Batch [214/391]: Loss 1.0167351961135864\n",
      "[Epoch 5] Training Batch [215/391]: Loss 1.1771113872528076\n",
      "[Epoch 5] Training Batch [216/391]: Loss 1.1312503814697266\n",
      "[Epoch 5] Training Batch [217/391]: Loss 1.0272905826568604\n",
      "[Epoch 5] Training Batch [218/391]: Loss 1.102652907371521\n",
      "[Epoch 5] Training Batch [219/391]: Loss 1.0597031116485596\n",
      "[Epoch 5] Training Batch [220/391]: Loss 1.041833758354187\n",
      "[Epoch 5] Training Batch [221/391]: Loss 1.2199400663375854\n",
      "[Epoch 5] Training Batch [222/391]: Loss 1.0167080163955688\n",
      "[Epoch 5] Training Batch [223/391]: Loss 0.9740914106369019\n",
      "[Epoch 5] Training Batch [224/391]: Loss 1.1002925634384155\n",
      "[Epoch 5] Training Batch [225/391]: Loss 1.102632999420166\n",
      "[Epoch 5] Training Batch [226/391]: Loss 1.2314149141311646\n",
      "[Epoch 5] Training Batch [227/391]: Loss 1.1213288307189941\n",
      "[Epoch 5] Training Batch [228/391]: Loss 1.00898277759552\n",
      "[Epoch 5] Training Batch [229/391]: Loss 1.0741266012191772\n",
      "[Epoch 5] Training Batch [230/391]: Loss 1.1769843101501465\n",
      "[Epoch 5] Training Batch [231/391]: Loss 1.2028050422668457\n",
      "[Epoch 5] Training Batch [232/391]: Loss 1.0317978858947754\n",
      "[Epoch 5] Training Batch [233/391]: Loss 1.1416243314743042\n",
      "[Epoch 5] Training Batch [234/391]: Loss 1.133528709411621\n",
      "[Epoch 5] Training Batch [235/391]: Loss 0.9964050054550171\n",
      "[Epoch 5] Training Batch [236/391]: Loss 1.2006542682647705\n",
      "[Epoch 5] Training Batch [237/391]: Loss 1.1977776288986206\n",
      "[Epoch 5] Training Batch [238/391]: Loss 0.8876509070396423\n",
      "[Epoch 5] Training Batch [239/391]: Loss 1.0858303308486938\n",
      "[Epoch 5] Training Batch [240/391]: Loss 0.8981285691261292\n",
      "[Epoch 5] Training Batch [241/391]: Loss 1.236542820930481\n",
      "[Epoch 5] Training Batch [242/391]: Loss 0.96397864818573\n",
      "[Epoch 5] Training Batch [243/391]: Loss 1.1701664924621582\n",
      "[Epoch 5] Training Batch [244/391]: Loss 0.9029501080513\n",
      "[Epoch 5] Training Batch [245/391]: Loss 1.159752368927002\n",
      "[Epoch 5] Training Batch [246/391]: Loss 1.22163987159729\n",
      "[Epoch 5] Training Batch [247/391]: Loss 1.015196681022644\n",
      "[Epoch 5] Training Batch [248/391]: Loss 1.1106733083724976\n",
      "[Epoch 5] Training Batch [249/391]: Loss 1.0719090700149536\n",
      "[Epoch 5] Training Batch [250/391]: Loss 1.2592295408248901\n",
      "[Epoch 5] Training Batch [251/391]: Loss 0.9957074522972107\n",
      "[Epoch 5] Training Batch [252/391]: Loss 1.0514492988586426\n",
      "[Epoch 5] Training Batch [253/391]: Loss 1.0303016901016235\n",
      "[Epoch 5] Training Batch [254/391]: Loss 1.037204384803772\n",
      "[Epoch 5] Training Batch [255/391]: Loss 1.082702875137329\n",
      "[Epoch 5] Training Batch [256/391]: Loss 0.9913201332092285\n",
      "[Epoch 5] Training Batch [257/391]: Loss 0.9864409565925598\n",
      "[Epoch 5] Training Batch [258/391]: Loss 1.0708547830581665\n",
      "[Epoch 5] Training Batch [259/391]: Loss 1.1176319122314453\n",
      "[Epoch 5] Training Batch [260/391]: Loss 1.1978065967559814\n",
      "[Epoch 5] Training Batch [261/391]: Loss 1.0956456661224365\n",
      "[Epoch 5] Training Batch [262/391]: Loss 1.1721034049987793\n",
      "[Epoch 5] Training Batch [263/391]: Loss 1.1054465770721436\n",
      "[Epoch 5] Training Batch [264/391]: Loss 1.1206592321395874\n",
      "[Epoch 5] Training Batch [265/391]: Loss 1.2815412282943726\n",
      "[Epoch 5] Training Batch [266/391]: Loss 1.0607566833496094\n",
      "[Epoch 5] Training Batch [267/391]: Loss 0.9763298034667969\n",
      "[Epoch 5] Training Batch [268/391]: Loss 1.07820463180542\n",
      "[Epoch 5] Training Batch [269/391]: Loss 1.2239567041397095\n",
      "[Epoch 5] Training Batch [270/391]: Loss 1.146509051322937\n",
      "[Epoch 5] Training Batch [271/391]: Loss 1.1369119882583618\n",
      "[Epoch 5] Training Batch [272/391]: Loss 1.0448940992355347\n",
      "[Epoch 5] Training Batch [273/391]: Loss 1.0701009035110474\n",
      "[Epoch 5] Training Batch [274/391]: Loss 1.0577657222747803\n",
      "[Epoch 5] Training Batch [275/391]: Loss 0.966532826423645\n",
      "[Epoch 5] Training Batch [276/391]: Loss 1.0324007272720337\n",
      "[Epoch 5] Training Batch [277/391]: Loss 0.9340519905090332\n",
      "[Epoch 5] Training Batch [278/391]: Loss 0.980480432510376\n",
      "[Epoch 5] Training Batch [279/391]: Loss 1.1933008432388306\n",
      "[Epoch 5] Training Batch [280/391]: Loss 0.9735612273216248\n",
      "[Epoch 5] Training Batch [281/391]: Loss 1.0554686784744263\n",
      "[Epoch 5] Training Batch [282/391]: Loss 1.3037893772125244\n",
      "[Epoch 5] Training Batch [283/391]: Loss 1.0577317476272583\n",
      "[Epoch 5] Training Batch [284/391]: Loss 1.2017594575881958\n",
      "[Epoch 5] Training Batch [285/391]: Loss 1.4428203105926514\n",
      "[Epoch 5] Training Batch [286/391]: Loss 1.2843310832977295\n",
      "[Epoch 5] Training Batch [287/391]: Loss 1.2336803674697876\n",
      "[Epoch 5] Training Batch [288/391]: Loss 1.27373206615448\n",
      "[Epoch 5] Training Batch [289/391]: Loss 1.1453428268432617\n",
      "[Epoch 5] Training Batch [290/391]: Loss 1.1619822978973389\n",
      "[Epoch 5] Training Batch [291/391]: Loss 1.1355229616165161\n",
      "[Epoch 5] Training Batch [292/391]: Loss 1.1298460960388184\n",
      "[Epoch 5] Training Batch [293/391]: Loss 1.253083348274231\n",
      "[Epoch 5] Training Batch [294/391]: Loss 1.0741748809814453\n",
      "[Epoch 5] Training Batch [295/391]: Loss 1.0198529958724976\n",
      "[Epoch 5] Training Batch [296/391]: Loss 1.0613718032836914\n",
      "[Epoch 5] Training Batch [297/391]: Loss 1.2112138271331787\n",
      "[Epoch 5] Training Batch [298/391]: Loss 1.2316538095474243\n",
      "[Epoch 5] Training Batch [299/391]: Loss 1.2079405784606934\n",
      "[Epoch 5] Training Batch [300/391]: Loss 1.1165142059326172\n",
      "[Epoch 5] Training Batch [301/391]: Loss 1.145291805267334\n",
      "[Epoch 5] Training Batch [302/391]: Loss 1.1573081016540527\n",
      "[Epoch 5] Training Batch [303/391]: Loss 1.003879189491272\n",
      "[Epoch 5] Training Batch [304/391]: Loss 1.1610004901885986\n",
      "[Epoch 5] Training Batch [305/391]: Loss 1.2170932292938232\n",
      "[Epoch 5] Training Batch [306/391]: Loss 1.0937070846557617\n",
      "[Epoch 5] Training Batch [307/391]: Loss 0.9579654335975647\n",
      "[Epoch 5] Training Batch [308/391]: Loss 1.115797519683838\n",
      "[Epoch 5] Training Batch [309/391]: Loss 1.2527016401290894\n",
      "[Epoch 5] Training Batch [310/391]: Loss 1.0300832986831665\n",
      "[Epoch 5] Training Batch [311/391]: Loss 1.2676918506622314\n",
      "[Epoch 5] Training Batch [312/391]: Loss 0.9872000813484192\n",
      "[Epoch 5] Training Batch [313/391]: Loss 1.0956426858901978\n",
      "[Epoch 5] Training Batch [314/391]: Loss 1.31130850315094\n",
      "[Epoch 5] Training Batch [315/391]: Loss 1.1032114028930664\n",
      "[Epoch 5] Training Batch [316/391]: Loss 0.9742271900177002\n",
      "[Epoch 5] Training Batch [317/391]: Loss 1.093984842300415\n",
      "[Epoch 5] Training Batch [318/391]: Loss 1.0976781845092773\n",
      "[Epoch 5] Training Batch [319/391]: Loss 1.197558045387268\n",
      "[Epoch 5] Training Batch [320/391]: Loss 1.133347988128662\n",
      "[Epoch 5] Training Batch [321/391]: Loss 1.23465895652771\n",
      "[Epoch 5] Training Batch [322/391]: Loss 1.1296223402023315\n",
      "[Epoch 5] Training Batch [323/391]: Loss 1.0231688022613525\n",
      "[Epoch 5] Training Batch [324/391]: Loss 1.1225996017456055\n",
      "[Epoch 5] Training Batch [325/391]: Loss 1.3235856294631958\n",
      "[Epoch 5] Training Batch [326/391]: Loss 1.1918519735336304\n",
      "[Epoch 5] Training Batch [327/391]: Loss 1.28300142288208\n",
      "[Epoch 5] Training Batch [328/391]: Loss 1.2638870477676392\n",
      "[Epoch 5] Training Batch [329/391]: Loss 1.0074294805526733\n",
      "[Epoch 5] Training Batch [330/391]: Loss 1.075799584388733\n",
      "[Epoch 5] Training Batch [331/391]: Loss 1.1198805570602417\n",
      "[Epoch 5] Training Batch [332/391]: Loss 1.2351659536361694\n",
      "[Epoch 5] Training Batch [333/391]: Loss 1.2125086784362793\n",
      "[Epoch 5] Training Batch [334/391]: Loss 1.1536246538162231\n",
      "[Epoch 5] Training Batch [335/391]: Loss 1.2459436655044556\n",
      "[Epoch 5] Training Batch [336/391]: Loss 1.183118224143982\n",
      "[Epoch 5] Training Batch [337/391]: Loss 1.3501222133636475\n",
      "[Epoch 5] Training Batch [338/391]: Loss 1.2288765907287598\n",
      "[Epoch 5] Training Batch [339/391]: Loss 1.1939698457717896\n",
      "[Epoch 5] Training Batch [340/391]: Loss 1.1021186113357544\n",
      "[Epoch 5] Training Batch [341/391]: Loss 1.1227467060089111\n",
      "[Epoch 5] Training Batch [342/391]: Loss 1.0284903049468994\n",
      "[Epoch 5] Training Batch [343/391]: Loss 1.1236575841903687\n",
      "[Epoch 5] Training Batch [344/391]: Loss 1.048449158668518\n",
      "[Epoch 5] Training Batch [345/391]: Loss 0.9690914154052734\n",
      "[Epoch 5] Training Batch [346/391]: Loss 0.8771719336509705\n",
      "[Epoch 5] Training Batch [347/391]: Loss 1.143692970275879\n",
      "[Epoch 5] Training Batch [348/391]: Loss 1.160921335220337\n",
      "[Epoch 5] Training Batch [349/391]: Loss 1.2335337400436401\n",
      "[Epoch 5] Training Batch [350/391]: Loss 1.0699418783187866\n",
      "[Epoch 5] Training Batch [351/391]: Loss 0.9384404420852661\n",
      "[Epoch 5] Training Batch [352/391]: Loss 1.187849998474121\n",
      "[Epoch 5] Training Batch [353/391]: Loss 1.2146978378295898\n",
      "[Epoch 5] Training Batch [354/391]: Loss 1.132293701171875\n",
      "[Epoch 5] Training Batch [355/391]: Loss 1.0650349855422974\n",
      "[Epoch 5] Training Batch [356/391]: Loss 1.0505255460739136\n",
      "[Epoch 5] Training Batch [357/391]: Loss 1.0874196290969849\n",
      "[Epoch 5] Training Batch [358/391]: Loss 1.1053059101104736\n",
      "[Epoch 5] Training Batch [359/391]: Loss 1.0865648984909058\n",
      "[Epoch 5] Training Batch [360/391]: Loss 1.1106947660446167\n",
      "[Epoch 5] Training Batch [361/391]: Loss 0.9614680409431458\n",
      "[Epoch 5] Training Batch [362/391]: Loss 1.1134238243103027\n",
      "[Epoch 5] Training Batch [363/391]: Loss 1.1171329021453857\n",
      "[Epoch 5] Training Batch [364/391]: Loss 1.094070553779602\n",
      "[Epoch 5] Training Batch [365/391]: Loss 1.1306588649749756\n",
      "[Epoch 5] Training Batch [366/391]: Loss 1.0314568281173706\n",
      "[Epoch 5] Training Batch [367/391]: Loss 1.1370398998260498\n",
      "[Epoch 5] Training Batch [368/391]: Loss 1.104091763496399\n",
      "[Epoch 5] Training Batch [369/391]: Loss 1.009221076965332\n",
      "[Epoch 5] Training Batch [370/391]: Loss 0.9586972594261169\n",
      "[Epoch 5] Training Batch [371/391]: Loss 1.0933315753936768\n",
      "[Epoch 5] Training Batch [372/391]: Loss 1.1725252866744995\n",
      "[Epoch 5] Training Batch [373/391]: Loss 1.1412094831466675\n",
      "[Epoch 5] Training Batch [374/391]: Loss 1.0773649215698242\n",
      "[Epoch 5] Training Batch [375/391]: Loss 1.0521337985992432\n",
      "[Epoch 5] Training Batch [376/391]: Loss 1.029949426651001\n",
      "[Epoch 5] Training Batch [377/391]: Loss 1.2379456758499146\n",
      "[Epoch 5] Training Batch [378/391]: Loss 1.0862314701080322\n",
      "[Epoch 5] Training Batch [379/391]: Loss 1.1759310960769653\n",
      "[Epoch 5] Training Batch [380/391]: Loss 0.8813030123710632\n",
      "[Epoch 5] Training Batch [381/391]: Loss 1.0807812213897705\n",
      "[Epoch 5] Training Batch [382/391]: Loss 1.0453969240188599\n",
      "[Epoch 5] Training Batch [383/391]: Loss 0.9486759901046753\n",
      "[Epoch 5] Training Batch [384/391]: Loss 1.0614124536514282\n",
      "[Epoch 5] Training Batch [385/391]: Loss 0.9583389759063721\n",
      "[Epoch 5] Training Batch [386/391]: Loss 1.1593440771102905\n",
      "[Epoch 5] Training Batch [387/391]: Loss 1.1011213064193726\n",
      "[Epoch 5] Training Batch [388/391]: Loss 1.074493646621704\n",
      "[Epoch 5] Training Batch [389/391]: Loss 1.2945436239242554\n",
      "[Epoch 5] Training Batch [390/391]: Loss 1.0464646816253662\n",
      "[Epoch 5] Training Batch [391/391]: Loss 1.2622755765914917\n",
      "Epoch 5 - Train Loss: 1.1200\n",
      "*********  Epoch 6/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Training Batch [1/391]: Loss 1.0080205202102661\n",
      "[Epoch 6] Training Batch [2/391]: Loss 0.9994816184043884\n",
      "[Epoch 6] Training Batch [3/391]: Loss 1.1089075803756714\n",
      "[Epoch 6] Training Batch [4/391]: Loss 1.117517113685608\n",
      "[Epoch 6] Training Batch [5/391]: Loss 1.0434726476669312\n",
      "[Epoch 6] Training Batch [6/391]: Loss 1.0470333099365234\n",
      "[Epoch 6] Training Batch [7/391]: Loss 0.945847749710083\n",
      "[Epoch 6] Training Batch [8/391]: Loss 0.8947212100028992\n",
      "[Epoch 6] Training Batch [9/391]: Loss 0.9699001908302307\n",
      "[Epoch 6] Training Batch [10/391]: Loss 1.1305736303329468\n",
      "[Epoch 6] Training Batch [11/391]: Loss 1.0989586114883423\n",
      "[Epoch 6] Training Batch [12/391]: Loss 1.0550801753997803\n",
      "[Epoch 6] Training Batch [13/391]: Loss 0.9522677659988403\n",
      "[Epoch 6] Training Batch [14/391]: Loss 1.0374261140823364\n",
      "[Epoch 6] Training Batch [15/391]: Loss 1.0158404111862183\n",
      "[Epoch 6] Training Batch [16/391]: Loss 1.1726561784744263\n",
      "[Epoch 6] Training Batch [17/391]: Loss 0.8479812741279602\n",
      "[Epoch 6] Training Batch [18/391]: Loss 0.9947472810745239\n",
      "[Epoch 6] Training Batch [19/391]: Loss 0.9643000960350037\n",
      "[Epoch 6] Training Batch [20/391]: Loss 1.0782753229141235\n",
      "[Epoch 6] Training Batch [21/391]: Loss 1.1813572645187378\n",
      "[Epoch 6] Training Batch [22/391]: Loss 0.8724124431610107\n",
      "[Epoch 6] Training Batch [23/391]: Loss 0.9622842073440552\n",
      "[Epoch 6] Training Batch [24/391]: Loss 0.9626432061195374\n",
      "[Epoch 6] Training Batch [25/391]: Loss 0.9537354707717896\n",
      "[Epoch 6] Training Batch [26/391]: Loss 1.0937772989273071\n",
      "[Epoch 6] Training Batch [27/391]: Loss 1.0404568910598755\n",
      "[Epoch 6] Training Batch [28/391]: Loss 0.787002682685852\n",
      "[Epoch 6] Training Batch [29/391]: Loss 0.924262523651123\n",
      "[Epoch 6] Training Batch [30/391]: Loss 0.9250431656837463\n",
      "[Epoch 6] Training Batch [31/391]: Loss 0.9509668350219727\n",
      "[Epoch 6] Training Batch [32/391]: Loss 0.9408628344535828\n",
      "[Epoch 6] Training Batch [33/391]: Loss 1.094517469406128\n",
      "[Epoch 6] Training Batch [34/391]: Loss 0.8831347227096558\n",
      "[Epoch 6] Training Batch [35/391]: Loss 1.021913766860962\n",
      "[Epoch 6] Training Batch [36/391]: Loss 1.0817492008209229\n",
      "[Epoch 6] Training Batch [37/391]: Loss 1.017372727394104\n",
      "[Epoch 6] Training Batch [38/391]: Loss 1.0123659372329712\n",
      "[Epoch 6] Training Batch [39/391]: Loss 0.8986347317695618\n",
      "[Epoch 6] Training Batch [40/391]: Loss 0.8096510767936707\n",
      "[Epoch 6] Training Batch [41/391]: Loss 1.1162296533584595\n",
      "[Epoch 6] Training Batch [42/391]: Loss 0.9199886322021484\n",
      "[Epoch 6] Training Batch [43/391]: Loss 0.8133144974708557\n",
      "[Epoch 6] Training Batch [44/391]: Loss 1.1059787273406982\n",
      "[Epoch 6] Training Batch [45/391]: Loss 0.9206417202949524\n",
      "[Epoch 6] Training Batch [46/391]: Loss 0.9508767127990723\n",
      "[Epoch 6] Training Batch [47/391]: Loss 0.8160899877548218\n",
      "[Epoch 6] Training Batch [48/391]: Loss 1.006522536277771\n",
      "[Epoch 6] Training Batch [49/391]: Loss 0.9308064579963684\n",
      "[Epoch 6] Training Batch [50/391]: Loss 0.8224455118179321\n",
      "[Epoch 6] Training Batch [51/391]: Loss 1.1132609844207764\n",
      "[Epoch 6] Training Batch [52/391]: Loss 0.8955445885658264\n",
      "[Epoch 6] Training Batch [53/391]: Loss 1.0284459590911865\n",
      "[Epoch 6] Training Batch [54/391]: Loss 0.9792841672897339\n",
      "[Epoch 6] Training Batch [55/391]: Loss 0.9448215961456299\n",
      "[Epoch 6] Training Batch [56/391]: Loss 0.9427555799484253\n",
      "[Epoch 6] Training Batch [57/391]: Loss 0.8804739117622375\n",
      "[Epoch 6] Training Batch [58/391]: Loss 1.1018893718719482\n",
      "[Epoch 6] Training Batch [59/391]: Loss 1.0119872093200684\n",
      "[Epoch 6] Training Batch [60/391]: Loss 1.1153620481491089\n",
      "[Epoch 6] Training Batch [61/391]: Loss 1.150439739227295\n",
      "[Epoch 6] Training Batch [62/391]: Loss 1.1155295372009277\n",
      "[Epoch 6] Training Batch [63/391]: Loss 1.1281102895736694\n",
      "[Epoch 6] Training Batch [64/391]: Loss 1.0261310338974\n",
      "[Epoch 6] Training Batch [65/391]: Loss 0.9974290728569031\n",
      "[Epoch 6] Training Batch [66/391]: Loss 1.1155802011489868\n",
      "[Epoch 6] Training Batch [67/391]: Loss 1.076479196548462\n",
      "[Epoch 6] Training Batch [68/391]: Loss 1.1129788160324097\n",
      "[Epoch 6] Training Batch [69/391]: Loss 1.0057274103164673\n",
      "[Epoch 6] Training Batch [70/391]: Loss 1.115393877029419\n",
      "[Epoch 6] Training Batch [71/391]: Loss 1.0423989295959473\n",
      "[Epoch 6] Training Batch [72/391]: Loss 0.8619385957717896\n",
      "[Epoch 6] Training Batch [73/391]: Loss 0.7759788632392883\n",
      "[Epoch 6] Training Batch [74/391]: Loss 0.9864879250526428\n",
      "[Epoch 6] Training Batch [75/391]: Loss 0.9695307016372681\n",
      "[Epoch 6] Training Batch [76/391]: Loss 1.0286962985992432\n",
      "[Epoch 6] Training Batch [77/391]: Loss 1.0536149740219116\n",
      "[Epoch 6] Training Batch [78/391]: Loss 1.0690600872039795\n",
      "[Epoch 6] Training Batch [79/391]: Loss 0.8862229585647583\n",
      "[Epoch 6] Training Batch [80/391]: Loss 1.0010954141616821\n",
      "[Epoch 6] Training Batch [81/391]: Loss 1.073851466178894\n",
      "[Epoch 6] Training Batch [82/391]: Loss 0.9868528246879578\n",
      "[Epoch 6] Training Batch [83/391]: Loss 1.1059989929199219\n",
      "[Epoch 6] Training Batch [84/391]: Loss 1.014450192451477\n",
      "[Epoch 6] Training Batch [85/391]: Loss 0.9752594232559204\n",
      "[Epoch 6] Training Batch [86/391]: Loss 1.0843607187271118\n",
      "[Epoch 6] Training Batch [87/391]: Loss 1.089301586151123\n",
      "[Epoch 6] Training Batch [88/391]: Loss 1.116903305053711\n",
      "[Epoch 6] Training Batch [89/391]: Loss 1.1153526306152344\n",
      "[Epoch 6] Training Batch [90/391]: Loss 1.0194138288497925\n",
      "[Epoch 6] Training Batch [91/391]: Loss 1.0048187971115112\n",
      "[Epoch 6] Training Batch [92/391]: Loss 1.2188735008239746\n",
      "[Epoch 6] Training Batch [93/391]: Loss 1.0951919555664062\n",
      "[Epoch 6] Training Batch [94/391]: Loss 0.9750275611877441\n",
      "[Epoch 6] Training Batch [95/391]: Loss 1.1582962274551392\n",
      "[Epoch 6] Training Batch [96/391]: Loss 1.0593764781951904\n",
      "[Epoch 6] Training Batch [97/391]: Loss 1.1567949056625366\n",
      "[Epoch 6] Training Batch [98/391]: Loss 1.1005475521087646\n",
      "[Epoch 6] Training Batch [99/391]: Loss 0.9952981472015381\n",
      "[Epoch 6] Training Batch [100/391]: Loss 1.0600969791412354\n",
      "[Epoch 6] Training Batch [101/391]: Loss 0.9855983853340149\n",
      "[Epoch 6] Training Batch [102/391]: Loss 1.0487291812896729\n",
      "[Epoch 6] Training Batch [103/391]: Loss 0.8667728900909424\n",
      "[Epoch 6] Training Batch [104/391]: Loss 0.9560961723327637\n",
      "[Epoch 6] Training Batch [105/391]: Loss 1.177712082862854\n",
      "[Epoch 6] Training Batch [106/391]: Loss 1.0551711320877075\n",
      "[Epoch 6] Training Batch [107/391]: Loss 1.1429057121276855\n",
      "[Epoch 6] Training Batch [108/391]: Loss 1.128893256187439\n",
      "[Epoch 6] Training Batch [109/391]: Loss 0.9683505892753601\n",
      "[Epoch 6] Training Batch [110/391]: Loss 0.9787554740905762\n",
      "[Epoch 6] Training Batch [111/391]: Loss 1.0656672716140747\n",
      "[Epoch 6] Training Batch [112/391]: Loss 1.0473207235336304\n",
      "[Epoch 6] Training Batch [113/391]: Loss 1.1469082832336426\n",
      "[Epoch 6] Training Batch [114/391]: Loss 0.9401959180831909\n",
      "[Epoch 6] Training Batch [115/391]: Loss 1.1144040822982788\n",
      "[Epoch 6] Training Batch [116/391]: Loss 0.9150514602661133\n",
      "[Epoch 6] Training Batch [117/391]: Loss 0.8643373250961304\n",
      "[Epoch 6] Training Batch [118/391]: Loss 0.996327817440033\n",
      "[Epoch 6] Training Batch [119/391]: Loss 0.9858336448669434\n",
      "[Epoch 6] Training Batch [120/391]: Loss 1.0260556936264038\n",
      "[Epoch 6] Training Batch [121/391]: Loss 0.9535366892814636\n",
      "[Epoch 6] Training Batch [122/391]: Loss 1.1364703178405762\n",
      "[Epoch 6] Training Batch [123/391]: Loss 0.9838694930076599\n",
      "[Epoch 6] Training Batch [124/391]: Loss 0.9273544549942017\n",
      "[Epoch 6] Training Batch [125/391]: Loss 1.047868013381958\n",
      "[Epoch 6] Training Batch [126/391]: Loss 0.8521059155464172\n",
      "[Epoch 6] Training Batch [127/391]: Loss 1.133140206336975\n",
      "[Epoch 6] Training Batch [128/391]: Loss 0.9415764212608337\n",
      "[Epoch 6] Training Batch [129/391]: Loss 1.012566089630127\n",
      "[Epoch 6] Training Batch [130/391]: Loss 0.8891534805297852\n",
      "[Epoch 6] Training Batch [131/391]: Loss 1.011346697807312\n",
      "[Epoch 6] Training Batch [132/391]: Loss 1.054558277130127\n",
      "[Epoch 6] Training Batch [133/391]: Loss 0.9265245795249939\n",
      "[Epoch 6] Training Batch [134/391]: Loss 1.169510841369629\n",
      "[Epoch 6] Training Batch [135/391]: Loss 1.2737692594528198\n",
      "[Epoch 6] Training Batch [136/391]: Loss 1.155932068824768\n",
      "[Epoch 6] Training Batch [137/391]: Loss 0.877163827419281\n",
      "[Epoch 6] Training Batch [138/391]: Loss 1.0190497636795044\n",
      "[Epoch 6] Training Batch [139/391]: Loss 1.2966638803482056\n",
      "[Epoch 6] Training Batch [140/391]: Loss 1.0670125484466553\n",
      "[Epoch 6] Training Batch [141/391]: Loss 1.051311731338501\n",
      "[Epoch 6] Training Batch [142/391]: Loss 0.8776390552520752\n",
      "[Epoch 6] Training Batch [143/391]: Loss 1.130935549736023\n",
      "[Epoch 6] Training Batch [144/391]: Loss 0.9277677536010742\n",
      "[Epoch 6] Training Batch [145/391]: Loss 1.1122037172317505\n",
      "[Epoch 6] Training Batch [146/391]: Loss 0.9644485116004944\n",
      "[Epoch 6] Training Batch [147/391]: Loss 1.1237523555755615\n",
      "[Epoch 6] Training Batch [148/391]: Loss 1.146362543106079\n",
      "[Epoch 6] Training Batch [149/391]: Loss 1.0241467952728271\n",
      "[Epoch 6] Training Batch [150/391]: Loss 1.0320854187011719\n",
      "[Epoch 6] Training Batch [151/391]: Loss 1.0192495584487915\n",
      "[Epoch 6] Training Batch [152/391]: Loss 1.0008509159088135\n",
      "[Epoch 6] Training Batch [153/391]: Loss 0.8169507384300232\n",
      "[Epoch 6] Training Batch [154/391]: Loss 0.9654096961021423\n",
      "[Epoch 6] Training Batch [155/391]: Loss 1.3053005933761597\n",
      "[Epoch 6] Training Batch [156/391]: Loss 1.1402456760406494\n",
      "[Epoch 6] Training Batch [157/391]: Loss 1.1723958253860474\n",
      "[Epoch 6] Training Batch [158/391]: Loss 1.077174425125122\n",
      "[Epoch 6] Training Batch [159/391]: Loss 0.9997257590293884\n",
      "[Epoch 6] Training Batch [160/391]: Loss 0.9470576643943787\n",
      "[Epoch 6] Training Batch [161/391]: Loss 0.9544581174850464\n",
      "[Epoch 6] Training Batch [162/391]: Loss 1.0136631727218628\n",
      "[Epoch 6] Training Batch [163/391]: Loss 0.9995506405830383\n",
      "[Epoch 6] Training Batch [164/391]: Loss 0.9434639811515808\n",
      "[Epoch 6] Training Batch [165/391]: Loss 0.9098698496818542\n",
      "[Epoch 6] Training Batch [166/391]: Loss 0.8877294659614563\n",
      "[Epoch 6] Training Batch [167/391]: Loss 0.8747166395187378\n",
      "[Epoch 6] Training Batch [168/391]: Loss 1.0021005868911743\n",
      "[Epoch 6] Training Batch [169/391]: Loss 1.0121232271194458\n",
      "[Epoch 6] Training Batch [170/391]: Loss 0.9287158846855164\n",
      "[Epoch 6] Training Batch [171/391]: Loss 1.0434355735778809\n",
      "[Epoch 6] Training Batch [172/391]: Loss 0.9132961630821228\n",
      "[Epoch 6] Training Batch [173/391]: Loss 1.2130012512207031\n",
      "[Epoch 6] Training Batch [174/391]: Loss 1.0191459655761719\n",
      "[Epoch 6] Training Batch [175/391]: Loss 1.0553120374679565\n",
      "[Epoch 6] Training Batch [176/391]: Loss 0.9472647309303284\n",
      "[Epoch 6] Training Batch [177/391]: Loss 1.0213302373886108\n",
      "[Epoch 6] Training Batch [178/391]: Loss 0.9938271641731262\n",
      "[Epoch 6] Training Batch [179/391]: Loss 0.9038270711898804\n",
      "[Epoch 6] Training Batch [180/391]: Loss 1.0100009441375732\n",
      "[Epoch 6] Training Batch [181/391]: Loss 0.9666141271591187\n",
      "[Epoch 6] Training Batch [182/391]: Loss 1.0299174785614014\n",
      "[Epoch 6] Training Batch [183/391]: Loss 0.9404144883155823\n",
      "[Epoch 6] Training Batch [184/391]: Loss 0.8261775970458984\n",
      "[Epoch 6] Training Batch [185/391]: Loss 1.0489561557769775\n",
      "[Epoch 6] Training Batch [186/391]: Loss 0.932110607624054\n",
      "[Epoch 6] Training Batch [187/391]: Loss 0.7543035745620728\n",
      "[Epoch 6] Training Batch [188/391]: Loss 1.031536340713501\n",
      "[Epoch 6] Training Batch [189/391]: Loss 1.023755431175232\n",
      "[Epoch 6] Training Batch [190/391]: Loss 1.2214292287826538\n",
      "[Epoch 6] Training Batch [191/391]: Loss 1.0702288150787354\n",
      "[Epoch 6] Training Batch [192/391]: Loss 1.084972858428955\n",
      "[Epoch 6] Training Batch [193/391]: Loss 1.061495065689087\n",
      "[Epoch 6] Training Batch [194/391]: Loss 0.9975380897521973\n",
      "[Epoch 6] Training Batch [195/391]: Loss 1.0261601209640503\n",
      "[Epoch 6] Training Batch [196/391]: Loss 1.1079027652740479\n",
      "[Epoch 6] Training Batch [197/391]: Loss 1.068725824356079\n",
      "[Epoch 6] Training Batch [198/391]: Loss 0.9707779884338379\n",
      "[Epoch 6] Training Batch [199/391]: Loss 0.9058628082275391\n",
      "[Epoch 6] Training Batch [200/391]: Loss 0.9936790466308594\n",
      "[Epoch 6] Training Batch [201/391]: Loss 0.9692997932434082\n",
      "[Epoch 6] Training Batch [202/391]: Loss 0.8784748911857605\n",
      "[Epoch 6] Training Batch [203/391]: Loss 1.0245444774627686\n",
      "[Epoch 6] Training Batch [204/391]: Loss 1.1692169904708862\n",
      "[Epoch 6] Training Batch [205/391]: Loss 0.9071253538131714\n",
      "[Epoch 6] Training Batch [206/391]: Loss 1.1112323999404907\n",
      "[Epoch 6] Training Batch [207/391]: Loss 0.8530202507972717\n",
      "[Epoch 6] Training Batch [208/391]: Loss 1.0773639678955078\n",
      "[Epoch 6] Training Batch [209/391]: Loss 0.9686036109924316\n",
      "[Epoch 6] Training Batch [210/391]: Loss 0.9184056520462036\n",
      "[Epoch 6] Training Batch [211/391]: Loss 0.9762446880340576\n",
      "[Epoch 6] Training Batch [212/391]: Loss 0.9525564908981323\n",
      "[Epoch 6] Training Batch [213/391]: Loss 0.9942355155944824\n",
      "[Epoch 6] Training Batch [214/391]: Loss 0.8787822127342224\n",
      "[Epoch 6] Training Batch [215/391]: Loss 0.9968944191932678\n",
      "[Epoch 6] Training Batch [216/391]: Loss 0.9089943170547485\n",
      "[Epoch 6] Training Batch [217/391]: Loss 0.916515588760376\n",
      "[Epoch 6] Training Batch [218/391]: Loss 1.1995712518692017\n",
      "[Epoch 6] Training Batch [219/391]: Loss 0.9555662274360657\n",
      "[Epoch 6] Training Batch [220/391]: Loss 1.048862338066101\n",
      "[Epoch 6] Training Batch [221/391]: Loss 0.907576858997345\n",
      "[Epoch 6] Training Batch [222/391]: Loss 1.0514720678329468\n",
      "[Epoch 6] Training Batch [223/391]: Loss 1.0632621049880981\n",
      "[Epoch 6] Training Batch [224/391]: Loss 0.9904178380966187\n",
      "[Epoch 6] Training Batch [225/391]: Loss 0.8062604069709778\n",
      "[Epoch 6] Training Batch [226/391]: Loss 0.7857511043548584\n",
      "[Epoch 6] Training Batch [227/391]: Loss 1.1018506288528442\n",
      "[Epoch 6] Training Batch [228/391]: Loss 1.018166184425354\n",
      "[Epoch 6] Training Batch [229/391]: Loss 0.8534207344055176\n",
      "[Epoch 6] Training Batch [230/391]: Loss 0.8815450072288513\n",
      "[Epoch 6] Training Batch [231/391]: Loss 0.856941819190979\n",
      "[Epoch 6] Training Batch [232/391]: Loss 0.9907084703445435\n",
      "[Epoch 6] Training Batch [233/391]: Loss 1.0690641403198242\n",
      "[Epoch 6] Training Batch [234/391]: Loss 1.0982576608657837\n",
      "[Epoch 6] Training Batch [235/391]: Loss 0.8771252632141113\n",
      "[Epoch 6] Training Batch [236/391]: Loss 1.0633974075317383\n",
      "[Epoch 6] Training Batch [237/391]: Loss 0.9527393579483032\n",
      "[Epoch 6] Training Batch [238/391]: Loss 1.0709731578826904\n",
      "[Epoch 6] Training Batch [239/391]: Loss 0.9878727197647095\n",
      "[Epoch 6] Training Batch [240/391]: Loss 0.8427395820617676\n",
      "[Epoch 6] Training Batch [241/391]: Loss 1.1255543231964111\n",
      "[Epoch 6] Training Batch [242/391]: Loss 0.9027634263038635\n",
      "[Epoch 6] Training Batch [243/391]: Loss 1.025814175605774\n",
      "[Epoch 6] Training Batch [244/391]: Loss 1.0438804626464844\n",
      "[Epoch 6] Training Batch [245/391]: Loss 0.9899814128875732\n",
      "[Epoch 6] Training Batch [246/391]: Loss 0.9247435927391052\n",
      "[Epoch 6] Training Batch [247/391]: Loss 1.0498889684677124\n",
      "[Epoch 6] Training Batch [248/391]: Loss 0.9054083824157715\n",
      "[Epoch 6] Training Batch [249/391]: Loss 1.0925310850143433\n",
      "[Epoch 6] Training Batch [250/391]: Loss 0.8550786375999451\n",
      "[Epoch 6] Training Batch [251/391]: Loss 1.0384821891784668\n",
      "[Epoch 6] Training Batch [252/391]: Loss 0.9766336679458618\n",
      "[Epoch 6] Training Batch [253/391]: Loss 0.9856655597686768\n",
      "[Epoch 6] Training Batch [254/391]: Loss 1.080237865447998\n",
      "[Epoch 6] Training Batch [255/391]: Loss 0.8821063041687012\n",
      "[Epoch 6] Training Batch [256/391]: Loss 1.0499577522277832\n",
      "[Epoch 6] Training Batch [257/391]: Loss 1.148963212966919\n",
      "[Epoch 6] Training Batch [258/391]: Loss 0.9378409385681152\n",
      "[Epoch 6] Training Batch [259/391]: Loss 1.1022424697875977\n",
      "[Epoch 6] Training Batch [260/391]: Loss 1.1174339056015015\n",
      "[Epoch 6] Training Batch [261/391]: Loss 1.0770549774169922\n",
      "[Epoch 6] Training Batch [262/391]: Loss 0.972734808921814\n",
      "[Epoch 6] Training Batch [263/391]: Loss 1.2374294996261597\n",
      "[Epoch 6] Training Batch [264/391]: Loss 0.9685870409011841\n",
      "[Epoch 6] Training Batch [265/391]: Loss 1.0124423503875732\n",
      "[Epoch 6] Training Batch [266/391]: Loss 0.9897258877754211\n",
      "[Epoch 6] Training Batch [267/391]: Loss 0.9637546539306641\n",
      "[Epoch 6] Training Batch [268/391]: Loss 0.9413675665855408\n",
      "[Epoch 6] Training Batch [269/391]: Loss 0.8730668425559998\n",
      "[Epoch 6] Training Batch [270/391]: Loss 1.0446231365203857\n",
      "[Epoch 6] Training Batch [271/391]: Loss 0.8038607835769653\n",
      "[Epoch 6] Training Batch [272/391]: Loss 1.023979663848877\n",
      "[Epoch 6] Training Batch [273/391]: Loss 1.145511507987976\n",
      "[Epoch 6] Training Batch [274/391]: Loss 0.8778415322303772\n",
      "[Epoch 6] Training Batch [275/391]: Loss 1.0509400367736816\n",
      "[Epoch 6] Training Batch [276/391]: Loss 0.9790350794792175\n",
      "[Epoch 6] Training Batch [277/391]: Loss 1.0279399156570435\n",
      "[Epoch 6] Training Batch [278/391]: Loss 0.9807868599891663\n",
      "[Epoch 6] Training Batch [279/391]: Loss 1.1047855615615845\n",
      "[Epoch 6] Training Batch [280/391]: Loss 0.9364508390426636\n",
      "[Epoch 6] Training Batch [281/391]: Loss 1.334781289100647\n",
      "[Epoch 6] Training Batch [282/391]: Loss 0.9845694899559021\n",
      "[Epoch 6] Training Batch [283/391]: Loss 1.211228370666504\n",
      "[Epoch 6] Training Batch [284/391]: Loss 0.9115614295005798\n",
      "[Epoch 6] Training Batch [285/391]: Loss 1.078183889389038\n",
      "[Epoch 6] Training Batch [286/391]: Loss 1.0437767505645752\n",
      "[Epoch 6] Training Batch [287/391]: Loss 0.9510481357574463\n",
      "[Epoch 6] Training Batch [288/391]: Loss 1.066270351409912\n",
      "[Epoch 6] Training Batch [289/391]: Loss 1.031032681465149\n",
      "[Epoch 6] Training Batch [290/391]: Loss 1.2353357076644897\n",
      "[Epoch 6] Training Batch [291/391]: Loss 1.1412646770477295\n",
      "[Epoch 6] Training Batch [292/391]: Loss 0.7455301880836487\n",
      "[Epoch 6] Training Batch [293/391]: Loss 1.177466869354248\n",
      "[Epoch 6] Training Batch [294/391]: Loss 1.0910594463348389\n",
      "[Epoch 6] Training Batch [295/391]: Loss 0.9187276363372803\n",
      "[Epoch 6] Training Batch [296/391]: Loss 1.0627999305725098\n",
      "[Epoch 6] Training Batch [297/391]: Loss 1.0044684410095215\n",
      "[Epoch 6] Training Batch [298/391]: Loss 1.0651640892028809\n",
      "[Epoch 6] Training Batch [299/391]: Loss 1.0011777877807617\n",
      "[Epoch 6] Training Batch [300/391]: Loss 1.1212286949157715\n",
      "[Epoch 6] Training Batch [301/391]: Loss 1.1445083618164062\n",
      "[Epoch 6] Training Batch [302/391]: Loss 1.146645188331604\n",
      "[Epoch 6] Training Batch [303/391]: Loss 0.9303642511367798\n",
      "[Epoch 6] Training Batch [304/391]: Loss 0.9563438296318054\n",
      "[Epoch 6] Training Batch [305/391]: Loss 0.9625067710876465\n",
      "[Epoch 6] Training Batch [306/391]: Loss 0.9623293280601501\n",
      "[Epoch 6] Training Batch [307/391]: Loss 0.8282092809677124\n",
      "[Epoch 6] Training Batch [308/391]: Loss 0.995680570602417\n",
      "[Epoch 6] Training Batch [309/391]: Loss 0.8748325705528259\n",
      "[Epoch 6] Training Batch [310/391]: Loss 0.99387127161026\n",
      "[Epoch 6] Training Batch [311/391]: Loss 0.868422269821167\n",
      "[Epoch 6] Training Batch [312/391]: Loss 1.0985207557678223\n",
      "[Epoch 6] Training Batch [313/391]: Loss 1.020159363746643\n",
      "[Epoch 6] Training Batch [314/391]: Loss 1.0686589479446411\n",
      "[Epoch 6] Training Batch [315/391]: Loss 1.0905165672302246\n",
      "[Epoch 6] Training Batch [316/391]: Loss 1.15626859664917\n",
      "[Epoch 6] Training Batch [317/391]: Loss 0.9409732222557068\n",
      "[Epoch 6] Training Batch [318/391]: Loss 1.017807126045227\n",
      "[Epoch 6] Training Batch [319/391]: Loss 1.1739287376403809\n",
      "[Epoch 6] Training Batch [320/391]: Loss 1.0209099054336548\n",
      "[Epoch 6] Training Batch [321/391]: Loss 0.8550042510032654\n",
      "[Epoch 6] Training Batch [322/391]: Loss 0.8870380520820618\n",
      "[Epoch 6] Training Batch [323/391]: Loss 0.8919305205345154\n",
      "[Epoch 6] Training Batch [324/391]: Loss 1.0964771509170532\n",
      "[Epoch 6] Training Batch [325/391]: Loss 0.9736794233322144\n",
      "[Epoch 6] Training Batch [326/391]: Loss 1.0482869148254395\n",
      "[Epoch 6] Training Batch [327/391]: Loss 1.1769753694534302\n",
      "[Epoch 6] Training Batch [328/391]: Loss 0.9411866068840027\n",
      "[Epoch 6] Training Batch [329/391]: Loss 0.9852549433708191\n",
      "[Epoch 6] Training Batch [330/391]: Loss 1.099194049835205\n",
      "[Epoch 6] Training Batch [331/391]: Loss 0.9819178581237793\n",
      "[Epoch 6] Training Batch [332/391]: Loss 1.0170122385025024\n",
      "[Epoch 6] Training Batch [333/391]: Loss 0.9119424819946289\n",
      "[Epoch 6] Training Batch [334/391]: Loss 1.0706796646118164\n",
      "[Epoch 6] Training Batch [335/391]: Loss 0.8327797651290894\n",
      "[Epoch 6] Training Batch [336/391]: Loss 1.0522691011428833\n",
      "[Epoch 6] Training Batch [337/391]: Loss 0.9962296485900879\n",
      "[Epoch 6] Training Batch [338/391]: Loss 1.1073076725006104\n",
      "[Epoch 6] Training Batch [339/391]: Loss 0.9914633631706238\n",
      "[Epoch 6] Training Batch [340/391]: Loss 1.01616632938385\n",
      "[Epoch 6] Training Batch [341/391]: Loss 1.1071796417236328\n",
      "[Epoch 6] Training Batch [342/391]: Loss 0.9072213768959045\n",
      "[Epoch 6] Training Batch [343/391]: Loss 1.139342188835144\n",
      "[Epoch 6] Training Batch [344/391]: Loss 1.128694772720337\n",
      "[Epoch 6] Training Batch [345/391]: Loss 0.9318726658821106\n",
      "[Epoch 6] Training Batch [346/391]: Loss 0.8233363628387451\n",
      "[Epoch 6] Training Batch [347/391]: Loss 0.7908288836479187\n",
      "[Epoch 6] Training Batch [348/391]: Loss 1.06571364402771\n",
      "[Epoch 6] Training Batch [349/391]: Loss 0.9205090999603271\n",
      "[Epoch 6] Training Batch [350/391]: Loss 1.1562168598175049\n",
      "[Epoch 6] Training Batch [351/391]: Loss 1.0255746841430664\n",
      "[Epoch 6] Training Batch [352/391]: Loss 0.9422628879547119\n",
      "[Epoch 6] Training Batch [353/391]: Loss 1.0260465145111084\n",
      "[Epoch 6] Training Batch [354/391]: Loss 0.9367311596870422\n",
      "[Epoch 6] Training Batch [355/391]: Loss 0.9289858341217041\n",
      "[Epoch 6] Training Batch [356/391]: Loss 1.0144548416137695\n",
      "[Epoch 6] Training Batch [357/391]: Loss 0.9277357459068298\n",
      "[Epoch 6] Training Batch [358/391]: Loss 0.7948216199874878\n",
      "[Epoch 6] Training Batch [359/391]: Loss 1.1265631914138794\n",
      "[Epoch 6] Training Batch [360/391]: Loss 0.9076414108276367\n",
      "[Epoch 6] Training Batch [361/391]: Loss 1.086287260055542\n",
      "[Epoch 6] Training Batch [362/391]: Loss 0.99286288022995\n",
      "[Epoch 6] Training Batch [363/391]: Loss 1.0870317220687866\n",
      "[Epoch 6] Training Batch [364/391]: Loss 0.9086338877677917\n",
      "[Epoch 6] Training Batch [365/391]: Loss 1.053736686706543\n",
      "[Epoch 6] Training Batch [366/391]: Loss 0.9949106574058533\n",
      "[Epoch 6] Training Batch [367/391]: Loss 0.9493497610092163\n",
      "[Epoch 6] Training Batch [368/391]: Loss 0.9109032154083252\n",
      "[Epoch 6] Training Batch [369/391]: Loss 0.964874804019928\n",
      "[Epoch 6] Training Batch [370/391]: Loss 1.1118367910385132\n",
      "[Epoch 6] Training Batch [371/391]: Loss 1.0519181489944458\n",
      "[Epoch 6] Training Batch [372/391]: Loss 0.907096266746521\n",
      "[Epoch 6] Training Batch [373/391]: Loss 1.1445026397705078\n",
      "[Epoch 6] Training Batch [374/391]: Loss 0.9301968216896057\n",
      "[Epoch 6] Training Batch [375/391]: Loss 1.0722261667251587\n",
      "[Epoch 6] Training Batch [376/391]: Loss 1.1008208990097046\n",
      "[Epoch 6] Training Batch [377/391]: Loss 1.0914186239242554\n",
      "[Epoch 6] Training Batch [378/391]: Loss 1.0654239654541016\n",
      "[Epoch 6] Training Batch [379/391]: Loss 0.8769503831863403\n",
      "[Epoch 6] Training Batch [380/391]: Loss 0.9312242269515991\n",
      "[Epoch 6] Training Batch [381/391]: Loss 0.9863499402999878\n",
      "[Epoch 6] Training Batch [382/391]: Loss 1.0361164808273315\n",
      "[Epoch 6] Training Batch [383/391]: Loss 1.0519239902496338\n",
      "[Epoch 6] Training Batch [384/391]: Loss 0.8347853422164917\n",
      "[Epoch 6] Training Batch [385/391]: Loss 1.1030941009521484\n",
      "[Epoch 6] Training Batch [386/391]: Loss 0.9792963266372681\n",
      "[Epoch 6] Training Batch [387/391]: Loss 0.9101396799087524\n",
      "[Epoch 6] Training Batch [388/391]: Loss 1.054242491722107\n",
      "[Epoch 6] Training Batch [389/391]: Loss 1.02201509475708\n",
      "[Epoch 6] Training Batch [390/391]: Loss 1.1208360195159912\n",
      "[Epoch 6] Training Batch [391/391]: Loss 0.9891654253005981\n",
      "Epoch 6 - Train Loss: 1.0088\n",
      "*********  Epoch 7/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Training Batch [1/391]: Loss 0.8005222678184509\n",
      "[Epoch 7] Training Batch [2/391]: Loss 0.7274554967880249\n",
      "[Epoch 7] Training Batch [3/391]: Loss 0.91389000415802\n",
      "[Epoch 7] Training Batch [4/391]: Loss 0.9041489362716675\n",
      "[Epoch 7] Training Batch [5/391]: Loss 0.8985210657119751\n",
      "[Epoch 7] Training Batch [6/391]: Loss 0.8238974809646606\n",
      "[Epoch 7] Training Batch [7/391]: Loss 0.924004316329956\n",
      "[Epoch 7] Training Batch [8/391]: Loss 0.9093354940414429\n",
      "[Epoch 7] Training Batch [9/391]: Loss 0.8107783794403076\n",
      "[Epoch 7] Training Batch [10/391]: Loss 0.9782531261444092\n",
      "[Epoch 7] Training Batch [11/391]: Loss 0.8974781036376953\n",
      "[Epoch 7] Training Batch [12/391]: Loss 0.9016931653022766\n",
      "[Epoch 7] Training Batch [13/391]: Loss 0.8502336740493774\n",
      "[Epoch 7] Training Batch [14/391]: Loss 0.7806856036186218\n",
      "[Epoch 7] Training Batch [15/391]: Loss 0.7061140537261963\n",
      "[Epoch 7] Training Batch [16/391]: Loss 0.9065248966217041\n",
      "[Epoch 7] Training Batch [17/391]: Loss 0.9249423146247864\n",
      "[Epoch 7] Training Batch [18/391]: Loss 0.8565528988838196\n",
      "[Epoch 7] Training Batch [19/391]: Loss 0.9266980886459351\n",
      "[Epoch 7] Training Batch [20/391]: Loss 0.8498526811599731\n",
      "[Epoch 7] Training Batch [21/391]: Loss 1.0133333206176758\n",
      "[Epoch 7] Training Batch [22/391]: Loss 0.9270986318588257\n",
      "[Epoch 7] Training Batch [23/391]: Loss 0.7608160376548767\n",
      "[Epoch 7] Training Batch [24/391]: Loss 0.9955283403396606\n",
      "[Epoch 7] Training Batch [25/391]: Loss 1.0228170156478882\n",
      "[Epoch 7] Training Batch [26/391]: Loss 0.9060656428337097\n",
      "[Epoch 7] Training Batch [27/391]: Loss 0.8412003517150879\n",
      "[Epoch 7] Training Batch [28/391]: Loss 0.8686321377754211\n",
      "[Epoch 7] Training Batch [29/391]: Loss 0.9368695020675659\n",
      "[Epoch 7] Training Batch [30/391]: Loss 0.990654468536377\n",
      "[Epoch 7] Training Batch [31/391]: Loss 0.9087965488433838\n",
      "[Epoch 7] Training Batch [32/391]: Loss 0.7988947033882141\n",
      "[Epoch 7] Training Batch [33/391]: Loss 0.8146387934684753\n",
      "[Epoch 7] Training Batch [34/391]: Loss 0.8925195932388306\n",
      "[Epoch 7] Training Batch [35/391]: Loss 0.9841265678405762\n",
      "[Epoch 7] Training Batch [36/391]: Loss 0.8325211405754089\n",
      "[Epoch 7] Training Batch [37/391]: Loss 0.9366235733032227\n",
      "[Epoch 7] Training Batch [38/391]: Loss 0.8153538703918457\n",
      "[Epoch 7] Training Batch [39/391]: Loss 0.86540687084198\n",
      "[Epoch 7] Training Batch [40/391]: Loss 0.8943601250648499\n",
      "[Epoch 7] Training Batch [41/391]: Loss 1.152231216430664\n",
      "[Epoch 7] Training Batch [42/391]: Loss 0.9796263575553894\n",
      "[Epoch 7] Training Batch [43/391]: Loss 1.082077145576477\n",
      "[Epoch 7] Training Batch [44/391]: Loss 0.8155561685562134\n",
      "[Epoch 7] Training Batch [45/391]: Loss 0.8761646151542664\n",
      "[Epoch 7] Training Batch [46/391]: Loss 0.7141076326370239\n",
      "[Epoch 7] Training Batch [47/391]: Loss 0.9914018511772156\n",
      "[Epoch 7] Training Batch [48/391]: Loss 0.8555052876472473\n",
      "[Epoch 7] Training Batch [49/391]: Loss 1.092514991760254\n",
      "[Epoch 7] Training Batch [50/391]: Loss 0.9459072947502136\n",
      "[Epoch 7] Training Batch [51/391]: Loss 0.9802135229110718\n",
      "[Epoch 7] Training Batch [52/391]: Loss 0.7960281372070312\n",
      "[Epoch 7] Training Batch [53/391]: Loss 0.8722327351570129\n",
      "[Epoch 7] Training Batch [54/391]: Loss 0.9284058809280396\n",
      "[Epoch 7] Training Batch [55/391]: Loss 0.7786747217178345\n",
      "[Epoch 7] Training Batch [56/391]: Loss 0.9173054695129395\n",
      "[Epoch 7] Training Batch [57/391]: Loss 0.9544590711593628\n",
      "[Epoch 7] Training Batch [58/391]: Loss 0.8829099535942078\n",
      "[Epoch 7] Training Batch [59/391]: Loss 0.8147305250167847\n",
      "[Epoch 7] Training Batch [60/391]: Loss 0.7994174957275391\n",
      "[Epoch 7] Training Batch [61/391]: Loss 0.9480434656143188\n",
      "[Epoch 7] Training Batch [62/391]: Loss 0.7959301471710205\n",
      "[Epoch 7] Training Batch [63/391]: Loss 0.9127053618431091\n",
      "[Epoch 7] Training Batch [64/391]: Loss 0.8460521697998047\n",
      "[Epoch 7] Training Batch [65/391]: Loss 0.8266737461090088\n",
      "[Epoch 7] Training Batch [66/391]: Loss 1.071825385093689\n",
      "[Epoch 7] Training Batch [67/391]: Loss 0.8471589088439941\n",
      "[Epoch 7] Training Batch [68/391]: Loss 0.7461152076721191\n",
      "[Epoch 7] Training Batch [69/391]: Loss 0.8700354099273682\n",
      "[Epoch 7] Training Batch [70/391]: Loss 0.9302852153778076\n",
      "[Epoch 7] Training Batch [71/391]: Loss 0.8004710078239441\n",
      "[Epoch 7] Training Batch [72/391]: Loss 0.9516522884368896\n",
      "[Epoch 7] Training Batch [73/391]: Loss 0.9210109710693359\n",
      "[Epoch 7] Training Batch [74/391]: Loss 0.9789255857467651\n",
      "[Epoch 7] Training Batch [75/391]: Loss 0.8878194689750671\n",
      "[Epoch 7] Training Batch [76/391]: Loss 0.8836843967437744\n",
      "[Epoch 7] Training Batch [77/391]: Loss 0.9077401757240295\n",
      "[Epoch 7] Training Batch [78/391]: Loss 1.0896698236465454\n",
      "[Epoch 7] Training Batch [79/391]: Loss 0.8802546262741089\n",
      "[Epoch 7] Training Batch [80/391]: Loss 0.9170957803726196\n",
      "[Epoch 7] Training Batch [81/391]: Loss 0.8638911247253418\n",
      "[Epoch 7] Training Batch [82/391]: Loss 0.9232318997383118\n",
      "[Epoch 7] Training Batch [83/391]: Loss 1.021937370300293\n",
      "[Epoch 7] Training Batch [84/391]: Loss 0.8425068259239197\n",
      "[Epoch 7] Training Batch [85/391]: Loss 0.8705044388771057\n",
      "[Epoch 7] Training Batch [86/391]: Loss 0.9887585639953613\n",
      "[Epoch 7] Training Batch [87/391]: Loss 0.9269080758094788\n",
      "[Epoch 7] Training Batch [88/391]: Loss 0.9305276870727539\n",
      "[Epoch 7] Training Batch [89/391]: Loss 0.8766419887542725\n",
      "[Epoch 7] Training Batch [90/391]: Loss 0.8492504358291626\n",
      "[Epoch 7] Training Batch [91/391]: Loss 0.7749575972557068\n",
      "[Epoch 7] Training Batch [92/391]: Loss 0.9436852335929871\n",
      "[Epoch 7] Training Batch [93/391]: Loss 1.226447343826294\n",
      "[Epoch 7] Training Batch [94/391]: Loss 0.829433798789978\n",
      "[Epoch 7] Training Batch [95/391]: Loss 0.9153378009796143\n",
      "[Epoch 7] Training Batch [96/391]: Loss 0.9703574776649475\n",
      "[Epoch 7] Training Batch [97/391]: Loss 0.9123955368995667\n",
      "[Epoch 7] Training Batch [98/391]: Loss 0.8964715003967285\n",
      "[Epoch 7] Training Batch [99/391]: Loss 0.9471309781074524\n",
      "[Epoch 7] Training Batch [100/391]: Loss 0.9932515621185303\n",
      "[Epoch 7] Training Batch [101/391]: Loss 0.8397156596183777\n",
      "[Epoch 7] Training Batch [102/391]: Loss 0.7413836717605591\n",
      "[Epoch 7] Training Batch [103/391]: Loss 1.0251399278640747\n",
      "[Epoch 7] Training Batch [104/391]: Loss 0.9743454456329346\n",
      "[Epoch 7] Training Batch [105/391]: Loss 0.8134860992431641\n",
      "[Epoch 7] Training Batch [106/391]: Loss 0.8866475224494934\n",
      "[Epoch 7] Training Batch [107/391]: Loss 0.9244343638420105\n",
      "[Epoch 7] Training Batch [108/391]: Loss 0.7048676609992981\n",
      "[Epoch 7] Training Batch [109/391]: Loss 1.0035494565963745\n",
      "[Epoch 7] Training Batch [110/391]: Loss 0.910057008266449\n",
      "[Epoch 7] Training Batch [111/391]: Loss 1.0882259607315063\n",
      "[Epoch 7] Training Batch [112/391]: Loss 0.946295976638794\n",
      "[Epoch 7] Training Batch [113/391]: Loss 0.7932702302932739\n",
      "[Epoch 7] Training Batch [114/391]: Loss 0.98161780834198\n",
      "[Epoch 7] Training Batch [115/391]: Loss 0.8597483038902283\n",
      "[Epoch 7] Training Batch [116/391]: Loss 0.8639277815818787\n",
      "[Epoch 7] Training Batch [117/391]: Loss 0.9883418083190918\n",
      "[Epoch 7] Training Batch [118/391]: Loss 0.8594387769699097\n",
      "[Epoch 7] Training Batch [119/391]: Loss 1.0289236307144165\n",
      "[Epoch 7] Training Batch [120/391]: Loss 0.9533106684684753\n",
      "[Epoch 7] Training Batch [121/391]: Loss 0.8327069282531738\n",
      "[Epoch 7] Training Batch [122/391]: Loss 0.9457860589027405\n",
      "[Epoch 7] Training Batch [123/391]: Loss 0.9197565913200378\n",
      "[Epoch 7] Training Batch [124/391]: Loss 0.880769670009613\n",
      "[Epoch 7] Training Batch [125/391]: Loss 1.0194231271743774\n",
      "[Epoch 7] Training Batch [126/391]: Loss 0.9274719953536987\n",
      "[Epoch 7] Training Batch [127/391]: Loss 0.9862611889839172\n",
      "[Epoch 7] Training Batch [128/391]: Loss 1.1406443119049072\n",
      "[Epoch 7] Training Batch [129/391]: Loss 0.7751281261444092\n",
      "[Epoch 7] Training Batch [130/391]: Loss 0.9354597926139832\n",
      "[Epoch 7] Training Batch [131/391]: Loss 0.9329153895378113\n",
      "[Epoch 7] Training Batch [132/391]: Loss 0.8843517303466797\n",
      "[Epoch 7] Training Batch [133/391]: Loss 0.8833436965942383\n",
      "[Epoch 7] Training Batch [134/391]: Loss 0.9534620046615601\n",
      "[Epoch 7] Training Batch [135/391]: Loss 0.9167949557304382\n",
      "[Epoch 7] Training Batch [136/391]: Loss 0.8201494216918945\n",
      "[Epoch 7] Training Batch [137/391]: Loss 0.8799005746841431\n",
      "[Epoch 7] Training Batch [138/391]: Loss 0.9894871711730957\n",
      "[Epoch 7] Training Batch [139/391]: Loss 0.8753418326377869\n",
      "[Epoch 7] Training Batch [140/391]: Loss 0.758449912071228\n",
      "[Epoch 7] Training Batch [141/391]: Loss 0.9303105473518372\n",
      "[Epoch 7] Training Batch [142/391]: Loss 0.891639769077301\n",
      "[Epoch 7] Training Batch [143/391]: Loss 0.8131833076477051\n",
      "[Epoch 7] Training Batch [144/391]: Loss 0.9022303819656372\n",
      "[Epoch 7] Training Batch [145/391]: Loss 0.9426537752151489\n",
      "[Epoch 7] Training Batch [146/391]: Loss 1.0519968271255493\n",
      "[Epoch 7] Training Batch [147/391]: Loss 0.7889693975448608\n",
      "[Epoch 7] Training Batch [148/391]: Loss 0.9965588450431824\n",
      "[Epoch 7] Training Batch [149/391]: Loss 0.8320309519767761\n",
      "[Epoch 7] Training Batch [150/391]: Loss 0.8745747804641724\n",
      "[Epoch 7] Training Batch [151/391]: Loss 1.0157506465911865\n",
      "[Epoch 7] Training Batch [152/391]: Loss 0.9334548115730286\n",
      "[Epoch 7] Training Batch [153/391]: Loss 0.5952123403549194\n",
      "[Epoch 7] Training Batch [154/391]: Loss 0.8899204134941101\n",
      "[Epoch 7] Training Batch [155/391]: Loss 0.8653383851051331\n",
      "[Epoch 7] Training Batch [156/391]: Loss 0.8294042348861694\n",
      "[Epoch 7] Training Batch [157/391]: Loss 0.8239768743515015\n",
      "[Epoch 7] Training Batch [158/391]: Loss 0.77371746301651\n",
      "[Epoch 7] Training Batch [159/391]: Loss 0.9141948223114014\n",
      "[Epoch 7] Training Batch [160/391]: Loss 0.6607503294944763\n",
      "[Epoch 7] Training Batch [161/391]: Loss 0.9381723999977112\n",
      "[Epoch 7] Training Batch [162/391]: Loss 0.8948997855186462\n",
      "[Epoch 7] Training Batch [163/391]: Loss 0.9240298271179199\n",
      "[Epoch 7] Training Batch [164/391]: Loss 1.0260579586029053\n",
      "[Epoch 7] Training Batch [165/391]: Loss 1.1234524250030518\n",
      "[Epoch 7] Training Batch [166/391]: Loss 0.9593935608863831\n",
      "[Epoch 7] Training Batch [167/391]: Loss 0.8985060453414917\n",
      "[Epoch 7] Training Batch [168/391]: Loss 1.026776671409607\n",
      "[Epoch 7] Training Batch [169/391]: Loss 0.8940888047218323\n",
      "[Epoch 7] Training Batch [170/391]: Loss 0.9493609070777893\n",
      "[Epoch 7] Training Batch [171/391]: Loss 0.8601043224334717\n",
      "[Epoch 7] Training Batch [172/391]: Loss 0.8730688095092773\n",
      "[Epoch 7] Training Batch [173/391]: Loss 0.8747172951698303\n",
      "[Epoch 7] Training Batch [174/391]: Loss 0.9913744926452637\n",
      "[Epoch 7] Training Batch [175/391]: Loss 0.9515324831008911\n",
      "[Epoch 7] Training Batch [176/391]: Loss 0.8253971934318542\n",
      "[Epoch 7] Training Batch [177/391]: Loss 0.8791001439094543\n",
      "[Epoch 7] Training Batch [178/391]: Loss 0.9522080421447754\n",
      "[Epoch 7] Training Batch [179/391]: Loss 1.0450843572616577\n",
      "[Epoch 7] Training Batch [180/391]: Loss 0.8165702223777771\n",
      "[Epoch 7] Training Batch [181/391]: Loss 0.8099853992462158\n",
      "[Epoch 7] Training Batch [182/391]: Loss 1.0248171091079712\n",
      "[Epoch 7] Training Batch [183/391]: Loss 0.9111225008964539\n",
      "[Epoch 7] Training Batch [184/391]: Loss 0.9349905252456665\n",
      "[Epoch 7] Training Batch [185/391]: Loss 0.8506085276603699\n",
      "[Epoch 7] Training Batch [186/391]: Loss 0.9572107791900635\n",
      "[Epoch 7] Training Batch [187/391]: Loss 0.8737266063690186\n",
      "[Epoch 7] Training Batch [188/391]: Loss 0.7701435685157776\n",
      "[Epoch 7] Training Batch [189/391]: Loss 0.8243786692619324\n",
      "[Epoch 7] Training Batch [190/391]: Loss 0.7973145842552185\n",
      "[Epoch 7] Training Batch [191/391]: Loss 0.892451822757721\n",
      "[Epoch 7] Training Batch [192/391]: Loss 0.7143757939338684\n",
      "[Epoch 7] Training Batch [193/391]: Loss 0.9446515440940857\n",
      "[Epoch 7] Training Batch [194/391]: Loss 0.7969365119934082\n",
      "[Epoch 7] Training Batch [195/391]: Loss 0.8172793984413147\n",
      "[Epoch 7] Training Batch [196/391]: Loss 0.9618799686431885\n",
      "[Epoch 7] Training Batch [197/391]: Loss 0.8354179263114929\n",
      "[Epoch 7] Training Batch [198/391]: Loss 0.8885307312011719\n",
      "[Epoch 7] Training Batch [199/391]: Loss 0.9697870016098022\n",
      "[Epoch 7] Training Batch [200/391]: Loss 0.935634195804596\n",
      "[Epoch 7] Training Batch [201/391]: Loss 1.0182944536209106\n",
      "[Epoch 7] Training Batch [202/391]: Loss 0.7933939099311829\n",
      "[Epoch 7] Training Batch [203/391]: Loss 0.8298523426055908\n",
      "[Epoch 7] Training Batch [204/391]: Loss 0.8896200060844421\n",
      "[Epoch 7] Training Batch [205/391]: Loss 0.8763285875320435\n",
      "[Epoch 7] Training Batch [206/391]: Loss 0.8779477477073669\n",
      "[Epoch 7] Training Batch [207/391]: Loss 0.9986215233802795\n",
      "[Epoch 7] Training Batch [208/391]: Loss 0.9997525215148926\n",
      "[Epoch 7] Training Batch [209/391]: Loss 0.8955244421958923\n",
      "[Epoch 7] Training Batch [210/391]: Loss 0.9025751352310181\n",
      "[Epoch 7] Training Batch [211/391]: Loss 0.9960264563560486\n",
      "[Epoch 7] Training Batch [212/391]: Loss 0.8264963626861572\n",
      "[Epoch 7] Training Batch [213/391]: Loss 1.0889029502868652\n",
      "[Epoch 7] Training Batch [214/391]: Loss 0.9079897403717041\n",
      "[Epoch 7] Training Batch [215/391]: Loss 0.7515164613723755\n",
      "[Epoch 7] Training Batch [216/391]: Loss 0.817984402179718\n",
      "[Epoch 7] Training Batch [217/391]: Loss 0.8368774056434631\n",
      "[Epoch 7] Training Batch [218/391]: Loss 0.8507755994796753\n",
      "[Epoch 7] Training Batch [219/391]: Loss 0.8542248606681824\n",
      "[Epoch 7] Training Batch [220/391]: Loss 0.9101064205169678\n",
      "[Epoch 7] Training Batch [221/391]: Loss 0.8427286744117737\n",
      "[Epoch 7] Training Batch [222/391]: Loss 0.9254817366600037\n",
      "[Epoch 7] Training Batch [223/391]: Loss 0.7612735033035278\n",
      "[Epoch 7] Training Batch [224/391]: Loss 1.000216007232666\n",
      "[Epoch 7] Training Batch [225/391]: Loss 0.8672504425048828\n",
      "[Epoch 7] Training Batch [226/391]: Loss 0.9191003441810608\n",
      "[Epoch 7] Training Batch [227/391]: Loss 0.9708537459373474\n",
      "[Epoch 7] Training Batch [228/391]: Loss 0.8161855936050415\n",
      "[Epoch 7] Training Batch [229/391]: Loss 0.8274085521697998\n",
      "[Epoch 7] Training Batch [230/391]: Loss 0.8712269067764282\n",
      "[Epoch 7] Training Batch [231/391]: Loss 0.8387772440910339\n",
      "[Epoch 7] Training Batch [232/391]: Loss 0.9100807905197144\n",
      "[Epoch 7] Training Batch [233/391]: Loss 0.9056571125984192\n",
      "[Epoch 7] Training Batch [234/391]: Loss 0.8511664867401123\n",
      "[Epoch 7] Training Batch [235/391]: Loss 0.9693461060523987\n",
      "[Epoch 7] Training Batch [236/391]: Loss 0.8909735083580017\n",
      "[Epoch 7] Training Batch [237/391]: Loss 1.027013897895813\n",
      "[Epoch 7] Training Batch [238/391]: Loss 0.8115004897117615\n",
      "[Epoch 7] Training Batch [239/391]: Loss 0.9193690419197083\n",
      "[Epoch 7] Training Batch [240/391]: Loss 0.8638719320297241\n",
      "[Epoch 7] Training Batch [241/391]: Loss 0.8921791315078735\n",
      "[Epoch 7] Training Batch [242/391]: Loss 0.9034784436225891\n",
      "[Epoch 7] Training Batch [243/391]: Loss 0.8807421326637268\n",
      "[Epoch 7] Training Batch [244/391]: Loss 0.905867338180542\n",
      "[Epoch 7] Training Batch [245/391]: Loss 0.9278073906898499\n",
      "[Epoch 7] Training Batch [246/391]: Loss 0.846770703792572\n",
      "[Epoch 7] Training Batch [247/391]: Loss 0.8303993940353394\n",
      "[Epoch 7] Training Batch [248/391]: Loss 0.9909564256668091\n",
      "[Epoch 7] Training Batch [249/391]: Loss 0.9028670787811279\n",
      "[Epoch 7] Training Batch [250/391]: Loss 0.9299532771110535\n",
      "[Epoch 7] Training Batch [251/391]: Loss 0.9835905432701111\n",
      "[Epoch 7] Training Batch [252/391]: Loss 0.7704457640647888\n",
      "[Epoch 7] Training Batch [253/391]: Loss 0.9284796118736267\n",
      "[Epoch 7] Training Batch [254/391]: Loss 1.0145256519317627\n",
      "[Epoch 7] Training Batch [255/391]: Loss 0.8793256282806396\n",
      "[Epoch 7] Training Batch [256/391]: Loss 1.120339035987854\n",
      "[Epoch 7] Training Batch [257/391]: Loss 0.8033874034881592\n",
      "[Epoch 7] Training Batch [258/391]: Loss 0.8252002596855164\n",
      "[Epoch 7] Training Batch [259/391]: Loss 0.868384838104248\n",
      "[Epoch 7] Training Batch [260/391]: Loss 1.022713541984558\n",
      "[Epoch 7] Training Batch [261/391]: Loss 1.0189669132232666\n",
      "[Epoch 7] Training Batch [262/391]: Loss 0.9595971703529358\n",
      "[Epoch 7] Training Batch [263/391]: Loss 0.8789428472518921\n",
      "[Epoch 7] Training Batch [264/391]: Loss 1.0134509801864624\n",
      "[Epoch 7] Training Batch [265/391]: Loss 1.0161272287368774\n",
      "[Epoch 7] Training Batch [266/391]: Loss 0.9154238700866699\n",
      "[Epoch 7] Training Batch [267/391]: Loss 0.9475775957107544\n",
      "[Epoch 7] Training Batch [268/391]: Loss 0.8611675500869751\n",
      "[Epoch 7] Training Batch [269/391]: Loss 0.961061418056488\n",
      "[Epoch 7] Training Batch [270/391]: Loss 1.0353740453720093\n",
      "[Epoch 7] Training Batch [271/391]: Loss 1.0031479597091675\n",
      "[Epoch 7] Training Batch [272/391]: Loss 0.8709367513656616\n",
      "[Epoch 7] Training Batch [273/391]: Loss 1.0286281108856201\n",
      "[Epoch 7] Training Batch [274/391]: Loss 0.8803027272224426\n",
      "[Epoch 7] Training Batch [275/391]: Loss 0.9442678093910217\n",
      "[Epoch 7] Training Batch [276/391]: Loss 0.9866460561752319\n",
      "[Epoch 7] Training Batch [277/391]: Loss 1.0274444818496704\n",
      "[Epoch 7] Training Batch [278/391]: Loss 0.8798457384109497\n",
      "[Epoch 7] Training Batch [279/391]: Loss 0.8745865225791931\n",
      "[Epoch 7] Training Batch [280/391]: Loss 0.7987796068191528\n",
      "[Epoch 7] Training Batch [281/391]: Loss 1.0312879085540771\n",
      "[Epoch 7] Training Batch [282/391]: Loss 1.0283846855163574\n",
      "[Epoch 7] Training Batch [283/391]: Loss 0.8397799730300903\n",
      "[Epoch 7] Training Batch [284/391]: Loss 0.8124130368232727\n",
      "[Epoch 7] Training Batch [285/391]: Loss 1.0112285614013672\n",
      "[Epoch 7] Training Batch [286/391]: Loss 0.9952723979949951\n",
      "[Epoch 7] Training Batch [287/391]: Loss 0.7120772004127502\n",
      "[Epoch 7] Training Batch [288/391]: Loss 0.8704541325569153\n",
      "[Epoch 7] Training Batch [289/391]: Loss 0.8054768443107605\n",
      "[Epoch 7] Training Batch [290/391]: Loss 0.9723852872848511\n",
      "[Epoch 7] Training Batch [291/391]: Loss 1.0943914651870728\n",
      "[Epoch 7] Training Batch [292/391]: Loss 0.7981820106506348\n",
      "[Epoch 7] Training Batch [293/391]: Loss 0.9521580934524536\n",
      "[Epoch 7] Training Batch [294/391]: Loss 0.7243574261665344\n",
      "[Epoch 7] Training Batch [295/391]: Loss 0.8425060510635376\n",
      "[Epoch 7] Training Batch [296/391]: Loss 1.0604255199432373\n",
      "[Epoch 7] Training Batch [297/391]: Loss 1.0076136589050293\n",
      "[Epoch 7] Training Batch [298/391]: Loss 0.9750726222991943\n",
      "[Epoch 7] Training Batch [299/391]: Loss 0.9000728726387024\n",
      "[Epoch 7] Training Batch [300/391]: Loss 0.9781861901283264\n",
      "[Epoch 7] Training Batch [301/391]: Loss 1.0153311491012573\n",
      "[Epoch 7] Training Batch [302/391]: Loss 0.7653804421424866\n",
      "[Epoch 7] Training Batch [303/391]: Loss 0.7991372346878052\n",
      "[Epoch 7] Training Batch [304/391]: Loss 0.8189392685890198\n",
      "[Epoch 7] Training Batch [305/391]: Loss 0.9442360401153564\n",
      "[Epoch 7] Training Batch [306/391]: Loss 0.8592158555984497\n",
      "[Epoch 7] Training Batch [307/391]: Loss 0.9265358448028564\n",
      "[Epoch 7] Training Batch [308/391]: Loss 0.9371783137321472\n",
      "[Epoch 7] Training Batch [309/391]: Loss 1.0416327714920044\n",
      "[Epoch 7] Training Batch [310/391]: Loss 1.0022037029266357\n",
      "[Epoch 7] Training Batch [311/391]: Loss 0.9490090608596802\n",
      "[Epoch 7] Training Batch [312/391]: Loss 0.9984649419784546\n",
      "[Epoch 7] Training Batch [313/391]: Loss 0.8653512001037598\n",
      "[Epoch 7] Training Batch [314/391]: Loss 0.814717710018158\n",
      "[Epoch 7] Training Batch [315/391]: Loss 0.8723568320274353\n",
      "[Epoch 7] Training Batch [316/391]: Loss 0.7749125361442566\n",
      "[Epoch 7] Training Batch [317/391]: Loss 0.9320815801620483\n",
      "[Epoch 7] Training Batch [318/391]: Loss 0.9162437915802002\n",
      "[Epoch 7] Training Batch [319/391]: Loss 1.0563911199569702\n",
      "[Epoch 7] Training Batch [320/391]: Loss 0.9855803847312927\n",
      "[Epoch 7] Training Batch [321/391]: Loss 0.8257058262825012\n",
      "[Epoch 7] Training Batch [322/391]: Loss 0.8913658857345581\n",
      "[Epoch 7] Training Batch [323/391]: Loss 0.754229724407196\n",
      "[Epoch 7] Training Batch [324/391]: Loss 0.9358326196670532\n",
      "[Epoch 7] Training Batch [325/391]: Loss 1.0642280578613281\n",
      "[Epoch 7] Training Batch [326/391]: Loss 0.9317463636398315\n",
      "[Epoch 7] Training Batch [327/391]: Loss 0.7985764145851135\n",
      "[Epoch 7] Training Batch [328/391]: Loss 0.9256594181060791\n",
      "[Epoch 7] Training Batch [329/391]: Loss 1.0448472499847412\n",
      "[Epoch 7] Training Batch [330/391]: Loss 0.7728065252304077\n",
      "[Epoch 7] Training Batch [331/391]: Loss 0.9982277750968933\n",
      "[Epoch 7] Training Batch [332/391]: Loss 0.9300360083580017\n",
      "[Epoch 7] Training Batch [333/391]: Loss 1.0137019157409668\n",
      "[Epoch 7] Training Batch [334/391]: Loss 0.975446879863739\n",
      "[Epoch 7] Training Batch [335/391]: Loss 0.7827105522155762\n",
      "[Epoch 7] Training Batch [336/391]: Loss 1.0127317905426025\n",
      "[Epoch 7] Training Batch [337/391]: Loss 1.0347869396209717\n",
      "[Epoch 7] Training Batch [338/391]: Loss 0.7330750823020935\n",
      "[Epoch 7] Training Batch [339/391]: Loss 0.8065603971481323\n",
      "[Epoch 7] Training Batch [340/391]: Loss 0.8500493168830872\n",
      "[Epoch 7] Training Batch [341/391]: Loss 1.0171144008636475\n",
      "[Epoch 7] Training Batch [342/391]: Loss 0.9364168047904968\n",
      "[Epoch 7] Training Batch [343/391]: Loss 0.8862016797065735\n",
      "[Epoch 7] Training Batch [344/391]: Loss 0.7967869639396667\n",
      "[Epoch 7] Training Batch [345/391]: Loss 0.7572100162506104\n",
      "[Epoch 7] Training Batch [346/391]: Loss 0.9632056951522827\n",
      "[Epoch 7] Training Batch [347/391]: Loss 0.7999929189682007\n",
      "[Epoch 7] Training Batch [348/391]: Loss 1.1216285228729248\n",
      "[Epoch 7] Training Batch [349/391]: Loss 0.7807439565658569\n",
      "[Epoch 7] Training Batch [350/391]: Loss 0.8825671672821045\n",
      "[Epoch 7] Training Batch [351/391]: Loss 0.9912847280502319\n",
      "[Epoch 7] Training Batch [352/391]: Loss 0.866483747959137\n",
      "[Epoch 7] Training Batch [353/391]: Loss 0.8466070890426636\n",
      "[Epoch 7] Training Batch [354/391]: Loss 0.8333140015602112\n",
      "[Epoch 7] Training Batch [355/391]: Loss 0.8698058128356934\n",
      "[Epoch 7] Training Batch [356/391]: Loss 0.7478291988372803\n",
      "[Epoch 7] Training Batch [357/391]: Loss 0.8721792101860046\n",
      "[Epoch 7] Training Batch [358/391]: Loss 0.786666214466095\n",
      "[Epoch 7] Training Batch [359/391]: Loss 0.9874354600906372\n",
      "[Epoch 7] Training Batch [360/391]: Loss 0.9564530253410339\n",
      "[Epoch 7] Training Batch [361/391]: Loss 0.8348757028579712\n",
      "[Epoch 7] Training Batch [362/391]: Loss 1.0035291910171509\n",
      "[Epoch 7] Training Batch [363/391]: Loss 1.0265456438064575\n",
      "[Epoch 7] Training Batch [364/391]: Loss 0.866602897644043\n",
      "[Epoch 7] Training Batch [365/391]: Loss 0.8589679002761841\n",
      "[Epoch 7] Training Batch [366/391]: Loss 0.8320757746696472\n",
      "[Epoch 7] Training Batch [367/391]: Loss 0.9163970351219177\n",
      "[Epoch 7] Training Batch [368/391]: Loss 1.0243126153945923\n",
      "[Epoch 7] Training Batch [369/391]: Loss 0.878464937210083\n",
      "[Epoch 7] Training Batch [370/391]: Loss 1.0270005464553833\n",
      "[Epoch 7] Training Batch [371/391]: Loss 1.1248979568481445\n",
      "[Epoch 7] Training Batch [372/391]: Loss 0.9091677665710449\n",
      "[Epoch 7] Training Batch [373/391]: Loss 0.9433010816574097\n",
      "[Epoch 7] Training Batch [374/391]: Loss 0.9790405035018921\n",
      "[Epoch 7] Training Batch [375/391]: Loss 0.8431575298309326\n",
      "[Epoch 7] Training Batch [376/391]: Loss 0.8515479564666748\n",
      "[Epoch 7] Training Batch [377/391]: Loss 0.9934962391853333\n",
      "[Epoch 7] Training Batch [378/391]: Loss 1.1543880701065063\n",
      "[Epoch 7] Training Batch [379/391]: Loss 0.9289692044258118\n",
      "[Epoch 7] Training Batch [380/391]: Loss 0.9656438231468201\n",
      "[Epoch 7] Training Batch [381/391]: Loss 1.0113450288772583\n",
      "[Epoch 7] Training Batch [382/391]: Loss 0.7558796405792236\n",
      "[Epoch 7] Training Batch [383/391]: Loss 0.9604914784431458\n",
      "[Epoch 7] Training Batch [384/391]: Loss 1.0766812562942505\n",
      "[Epoch 7] Training Batch [385/391]: Loss 1.0148515701293945\n",
      "[Epoch 7] Training Batch [386/391]: Loss 0.8954219222068787\n",
      "[Epoch 7] Training Batch [387/391]: Loss 1.0048657655715942\n",
      "[Epoch 7] Training Batch [388/391]: Loss 0.8516256809234619\n",
      "[Epoch 7] Training Batch [389/391]: Loss 0.9051347374916077\n",
      "[Epoch 7] Training Batch [390/391]: Loss 0.823488175868988\n",
      "[Epoch 7] Training Batch [391/391]: Loss 0.8309677243232727\n",
      "Epoch 7 - Train Loss: 0.9065\n",
      "*********  Epoch 8/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Training Batch [1/391]: Loss 0.8111995458602905\n",
      "[Epoch 8] Training Batch [2/391]: Loss 0.77215176820755\n",
      "[Epoch 8] Training Batch [3/391]: Loss 0.8638419508934021\n",
      "[Epoch 8] Training Batch [4/391]: Loss 0.6667277216911316\n",
      "[Epoch 8] Training Batch [5/391]: Loss 0.6346719264984131\n",
      "[Epoch 8] Training Batch [6/391]: Loss 0.7777184844017029\n",
      "[Epoch 8] Training Batch [7/391]: Loss 0.8061527013778687\n",
      "[Epoch 8] Training Batch [8/391]: Loss 0.7493369579315186\n",
      "[Epoch 8] Training Batch [9/391]: Loss 0.7582728862762451\n",
      "[Epoch 8] Training Batch [10/391]: Loss 0.7265280485153198\n",
      "[Epoch 8] Training Batch [11/391]: Loss 0.8179172277450562\n",
      "[Epoch 8] Training Batch [12/391]: Loss 0.7410017251968384\n",
      "[Epoch 8] Training Batch [13/391]: Loss 0.6426675319671631\n",
      "[Epoch 8] Training Batch [14/391]: Loss 0.827778160572052\n",
      "[Epoch 8] Training Batch [15/391]: Loss 0.6474661231040955\n",
      "[Epoch 8] Training Batch [16/391]: Loss 0.7816386222839355\n",
      "[Epoch 8] Training Batch [17/391]: Loss 0.7845315337181091\n",
      "[Epoch 8] Training Batch [18/391]: Loss 0.5597944855690002\n",
      "[Epoch 8] Training Batch [19/391]: Loss 0.635723352432251\n",
      "[Epoch 8] Training Batch [20/391]: Loss 0.761154055595398\n",
      "[Epoch 8] Training Batch [21/391]: Loss 0.7962557673454285\n",
      "[Epoch 8] Training Batch [22/391]: Loss 0.7115838527679443\n",
      "[Epoch 8] Training Batch [23/391]: Loss 0.7955904603004456\n",
      "[Epoch 8] Training Batch [24/391]: Loss 0.6310235857963562\n",
      "[Epoch 8] Training Batch [25/391]: Loss 0.7399437427520752\n",
      "[Epoch 8] Training Batch [26/391]: Loss 0.7738355398178101\n",
      "[Epoch 8] Training Batch [27/391]: Loss 0.655939519405365\n",
      "[Epoch 8] Training Batch [28/391]: Loss 0.7186122536659241\n",
      "[Epoch 8] Training Batch [29/391]: Loss 0.7841626405715942\n",
      "[Epoch 8] Training Batch [30/391]: Loss 0.8546858429908752\n",
      "[Epoch 8] Training Batch [31/391]: Loss 0.7654815912246704\n",
      "[Epoch 8] Training Batch [32/391]: Loss 0.6950823664665222\n",
      "[Epoch 8] Training Batch [33/391]: Loss 0.8407411575317383\n",
      "[Epoch 8] Training Batch [34/391]: Loss 0.7269427180290222\n",
      "[Epoch 8] Training Batch [35/391]: Loss 0.7266780138015747\n",
      "[Epoch 8] Training Batch [36/391]: Loss 0.8415197134017944\n",
      "[Epoch 8] Training Batch [37/391]: Loss 0.6906673908233643\n",
      "[Epoch 8] Training Batch [38/391]: Loss 0.9145591855049133\n",
      "[Epoch 8] Training Batch [39/391]: Loss 0.6798638105392456\n",
      "[Epoch 8] Training Batch [40/391]: Loss 0.6202035546302795\n",
      "[Epoch 8] Training Batch [41/391]: Loss 0.7041770815849304\n",
      "[Epoch 8] Training Batch [42/391]: Loss 0.7322912216186523\n",
      "[Epoch 8] Training Batch [43/391]: Loss 0.7091938257217407\n",
      "[Epoch 8] Training Batch [44/391]: Loss 0.7541832327842712\n",
      "[Epoch 8] Training Batch [45/391]: Loss 0.670728862285614\n",
      "[Epoch 8] Training Batch [46/391]: Loss 0.6616830229759216\n",
      "[Epoch 8] Training Batch [47/391]: Loss 0.8697564601898193\n",
      "[Epoch 8] Training Batch [48/391]: Loss 0.6691964864730835\n",
      "[Epoch 8] Training Batch [49/391]: Loss 0.76139235496521\n",
      "[Epoch 8] Training Batch [50/391]: Loss 0.6592251658439636\n",
      "[Epoch 8] Training Batch [51/391]: Loss 0.7594140768051147\n",
      "[Epoch 8] Training Batch [52/391]: Loss 0.6723633408546448\n",
      "[Epoch 8] Training Batch [53/391]: Loss 0.7116394639015198\n",
      "[Epoch 8] Training Batch [54/391]: Loss 0.672717809677124\n",
      "[Epoch 8] Training Batch [55/391]: Loss 0.7472726106643677\n",
      "[Epoch 8] Training Batch [56/391]: Loss 0.879216730594635\n",
      "[Epoch 8] Training Batch [57/391]: Loss 0.8821617960929871\n",
      "[Epoch 8] Training Batch [58/391]: Loss 0.9517917037010193\n",
      "[Epoch 8] Training Batch [59/391]: Loss 0.7582044005393982\n",
      "[Epoch 8] Training Batch [60/391]: Loss 0.7372841835021973\n",
      "[Epoch 8] Training Batch [61/391]: Loss 0.8431745767593384\n",
      "[Epoch 8] Training Batch [62/391]: Loss 0.7148005962371826\n",
      "[Epoch 8] Training Batch [63/391]: Loss 0.8096923828125\n",
      "[Epoch 8] Training Batch [64/391]: Loss 0.6891965866088867\n",
      "[Epoch 8] Training Batch [65/391]: Loss 0.8100283741950989\n",
      "[Epoch 8] Training Batch [66/391]: Loss 0.8509801030158997\n",
      "[Epoch 8] Training Batch [67/391]: Loss 0.7924190759658813\n",
      "[Epoch 8] Training Batch [68/391]: Loss 0.7452290654182434\n",
      "[Epoch 8] Training Batch [69/391]: Loss 0.6926340460777283\n",
      "[Epoch 8] Training Batch [70/391]: Loss 0.7724250555038452\n",
      "[Epoch 8] Training Batch [71/391]: Loss 0.8053065538406372\n",
      "[Epoch 8] Training Batch [72/391]: Loss 0.7976486682891846\n",
      "[Epoch 8] Training Batch [73/391]: Loss 0.848033607006073\n",
      "[Epoch 8] Training Batch [74/391]: Loss 0.7566908597946167\n",
      "[Epoch 8] Training Batch [75/391]: Loss 0.8646861910820007\n",
      "[Epoch 8] Training Batch [76/391]: Loss 0.8373519778251648\n",
      "[Epoch 8] Training Batch [77/391]: Loss 0.7890141606330872\n",
      "[Epoch 8] Training Batch [78/391]: Loss 0.7968472242355347\n",
      "[Epoch 8] Training Batch [79/391]: Loss 0.8371922969818115\n",
      "[Epoch 8] Training Batch [80/391]: Loss 0.8246399164199829\n",
      "[Epoch 8] Training Batch [81/391]: Loss 0.9786746501922607\n",
      "[Epoch 8] Training Batch [82/391]: Loss 0.8651221394538879\n",
      "[Epoch 8] Training Batch [83/391]: Loss 0.7958788871765137\n",
      "[Epoch 8] Training Batch [84/391]: Loss 0.8522657155990601\n",
      "[Epoch 8] Training Batch [85/391]: Loss 0.730476975440979\n",
      "[Epoch 8] Training Batch [86/391]: Loss 0.7325488328933716\n",
      "[Epoch 8] Training Batch [87/391]: Loss 0.8499512672424316\n",
      "[Epoch 8] Training Batch [88/391]: Loss 0.5990408658981323\n",
      "[Epoch 8] Training Batch [89/391]: Loss 0.7858073115348816\n",
      "[Epoch 8] Training Batch [90/391]: Loss 0.7190571427345276\n",
      "[Epoch 8] Training Batch [91/391]: Loss 0.7155095338821411\n",
      "[Epoch 8] Training Batch [92/391]: Loss 0.6271908283233643\n",
      "[Epoch 8] Training Batch [93/391]: Loss 0.8019821643829346\n",
      "[Epoch 8] Training Batch [94/391]: Loss 0.7136869430541992\n",
      "[Epoch 8] Training Batch [95/391]: Loss 0.7377398610115051\n",
      "[Epoch 8] Training Batch [96/391]: Loss 0.84784996509552\n",
      "[Epoch 8] Training Batch [97/391]: Loss 0.9326845407485962\n",
      "[Epoch 8] Training Batch [98/391]: Loss 0.8584933280944824\n",
      "[Epoch 8] Training Batch [99/391]: Loss 0.971254825592041\n",
      "[Epoch 8] Training Batch [100/391]: Loss 0.621404767036438\n",
      "[Epoch 8] Training Batch [101/391]: Loss 0.9090253114700317\n",
      "[Epoch 8] Training Batch [102/391]: Loss 0.8601088523864746\n",
      "[Epoch 8] Training Batch [103/391]: Loss 0.7060705423355103\n",
      "[Epoch 8] Training Batch [104/391]: Loss 0.7531830668449402\n",
      "[Epoch 8] Training Batch [105/391]: Loss 0.796146035194397\n",
      "[Epoch 8] Training Batch [106/391]: Loss 0.6729191541671753\n",
      "[Epoch 8] Training Batch [107/391]: Loss 0.665938138961792\n",
      "[Epoch 8] Training Batch [108/391]: Loss 0.8256880640983582\n",
      "[Epoch 8] Training Batch [109/391]: Loss 0.8420817852020264\n",
      "[Epoch 8] Training Batch [110/391]: Loss 0.8055678606033325\n",
      "[Epoch 8] Training Batch [111/391]: Loss 0.8231616616249084\n",
      "[Epoch 8] Training Batch [112/391]: Loss 0.810133159160614\n",
      "[Epoch 8] Training Batch [113/391]: Loss 0.8333594799041748\n",
      "[Epoch 8] Training Batch [114/391]: Loss 0.7273269891738892\n",
      "[Epoch 8] Training Batch [115/391]: Loss 0.6501836776733398\n",
      "[Epoch 8] Training Batch [116/391]: Loss 0.9615228772163391\n",
      "[Epoch 8] Training Batch [117/391]: Loss 0.7324496507644653\n",
      "[Epoch 8] Training Batch [118/391]: Loss 0.9649884700775146\n",
      "[Epoch 8] Training Batch [119/391]: Loss 0.8492221236228943\n",
      "[Epoch 8] Training Batch [120/391]: Loss 0.8156893849372864\n",
      "[Epoch 8] Training Batch [121/391]: Loss 0.7418266534805298\n",
      "[Epoch 8] Training Batch [122/391]: Loss 0.890217661857605\n",
      "[Epoch 8] Training Batch [123/391]: Loss 0.767081081867218\n",
      "[Epoch 8] Training Batch [124/391]: Loss 0.7695830464363098\n",
      "[Epoch 8] Training Batch [125/391]: Loss 1.006757378578186\n",
      "[Epoch 8] Training Batch [126/391]: Loss 0.7575539946556091\n",
      "[Epoch 8] Training Batch [127/391]: Loss 0.7833893299102783\n",
      "[Epoch 8] Training Batch [128/391]: Loss 0.7705516815185547\n",
      "[Epoch 8] Training Batch [129/391]: Loss 0.830938458442688\n",
      "[Epoch 8] Training Batch [130/391]: Loss 0.883255124092102\n",
      "[Epoch 8] Training Batch [131/391]: Loss 0.8845788836479187\n",
      "[Epoch 8] Training Batch [132/391]: Loss 0.6769973039627075\n",
      "[Epoch 8] Training Batch [133/391]: Loss 0.7558595538139343\n",
      "[Epoch 8] Training Batch [134/391]: Loss 0.802275538444519\n",
      "[Epoch 8] Training Batch [135/391]: Loss 0.9471795558929443\n",
      "[Epoch 8] Training Batch [136/391]: Loss 0.8311889171600342\n",
      "[Epoch 8] Training Batch [137/391]: Loss 0.7835829257965088\n",
      "[Epoch 8] Training Batch [138/391]: Loss 0.832325279712677\n",
      "[Epoch 8] Training Batch [139/391]: Loss 0.8796623349189758\n",
      "[Epoch 8] Training Batch [140/391]: Loss 0.8227246403694153\n",
      "[Epoch 8] Training Batch [141/391]: Loss 0.7632073760032654\n",
      "[Epoch 8] Training Batch [142/391]: Loss 0.7432425022125244\n",
      "[Epoch 8] Training Batch [143/391]: Loss 0.8798763751983643\n",
      "[Epoch 8] Training Batch [144/391]: Loss 0.9174309968948364\n",
      "[Epoch 8] Training Batch [145/391]: Loss 0.8483554720878601\n",
      "[Epoch 8] Training Batch [146/391]: Loss 0.9244203567504883\n",
      "[Epoch 8] Training Batch [147/391]: Loss 0.8468117117881775\n",
      "[Epoch 8] Training Batch [148/391]: Loss 0.8685786128044128\n",
      "[Epoch 8] Training Batch [149/391]: Loss 0.8744648694992065\n",
      "[Epoch 8] Training Batch [150/391]: Loss 0.9560449719429016\n",
      "[Epoch 8] Training Batch [151/391]: Loss 0.6400858759880066\n",
      "[Epoch 8] Training Batch [152/391]: Loss 0.8048274517059326\n",
      "[Epoch 8] Training Batch [153/391]: Loss 0.7483245134353638\n",
      "[Epoch 8] Training Batch [154/391]: Loss 0.836449384689331\n",
      "[Epoch 8] Training Batch [155/391]: Loss 0.7343574166297913\n",
      "[Epoch 8] Training Batch [156/391]: Loss 0.7347093820571899\n",
      "[Epoch 8] Training Batch [157/391]: Loss 0.8331071138381958\n",
      "[Epoch 8] Training Batch [158/391]: Loss 0.8299517035484314\n",
      "[Epoch 8] Training Batch [159/391]: Loss 0.7618495225906372\n",
      "[Epoch 8] Training Batch [160/391]: Loss 0.6583709120750427\n",
      "[Epoch 8] Training Batch [161/391]: Loss 0.6790547966957092\n",
      "[Epoch 8] Training Batch [162/391]: Loss 0.8370941877365112\n",
      "[Epoch 8] Training Batch [163/391]: Loss 0.7015417814254761\n",
      "[Epoch 8] Training Batch [164/391]: Loss 0.6730524897575378\n",
      "[Epoch 8] Training Batch [165/391]: Loss 0.7249512076377869\n",
      "[Epoch 8] Training Batch [166/391]: Loss 0.8183364272117615\n",
      "[Epoch 8] Training Batch [167/391]: Loss 0.8025244474411011\n",
      "[Epoch 8] Training Batch [168/391]: Loss 0.6643081903457642\n",
      "[Epoch 8] Training Batch [169/391]: Loss 0.9251294732093811\n",
      "[Epoch 8] Training Batch [170/391]: Loss 0.6059862971305847\n",
      "[Epoch 8] Training Batch [171/391]: Loss 0.7835769653320312\n",
      "[Epoch 8] Training Batch [172/391]: Loss 0.7222200036048889\n",
      "[Epoch 8] Training Batch [173/391]: Loss 0.7181079387664795\n",
      "[Epoch 8] Training Batch [174/391]: Loss 0.7203744053840637\n",
      "[Epoch 8] Training Batch [175/391]: Loss 0.7564370036125183\n",
      "[Epoch 8] Training Batch [176/391]: Loss 0.7511463761329651\n",
      "[Epoch 8] Training Batch [177/391]: Loss 0.7235351800918579\n",
      "[Epoch 8] Training Batch [178/391]: Loss 0.8494555950164795\n",
      "[Epoch 8] Training Batch [179/391]: Loss 1.1083804368972778\n",
      "[Epoch 8] Training Batch [180/391]: Loss 0.6966172456741333\n",
      "[Epoch 8] Training Batch [181/391]: Loss 0.6635896563529968\n",
      "[Epoch 8] Training Batch [182/391]: Loss 0.8199967741966248\n",
      "[Epoch 8] Training Batch [183/391]: Loss 0.9965609908103943\n",
      "[Epoch 8] Training Batch [184/391]: Loss 0.611130952835083\n",
      "[Epoch 8] Training Batch [185/391]: Loss 0.6758074164390564\n",
      "[Epoch 8] Training Batch [186/391]: Loss 0.7891186475753784\n",
      "[Epoch 8] Training Batch [187/391]: Loss 0.8763118982315063\n",
      "[Epoch 8] Training Batch [188/391]: Loss 0.9756019115447998\n",
      "[Epoch 8] Training Batch [189/391]: Loss 0.8937886357307434\n",
      "[Epoch 8] Training Batch [190/391]: Loss 0.9286777377128601\n",
      "[Epoch 8] Training Batch [191/391]: Loss 1.0612143278121948\n",
      "[Epoch 8] Training Batch [192/391]: Loss 0.6979857087135315\n",
      "[Epoch 8] Training Batch [193/391]: Loss 0.8098381161689758\n",
      "[Epoch 8] Training Batch [194/391]: Loss 0.7822462320327759\n",
      "[Epoch 8] Training Batch [195/391]: Loss 0.9431017637252808\n",
      "[Epoch 8] Training Batch [196/391]: Loss 0.5526671409606934\n",
      "[Epoch 8] Training Batch [197/391]: Loss 0.9893506169319153\n",
      "[Epoch 8] Training Batch [198/391]: Loss 0.7585867047309875\n",
      "[Epoch 8] Training Batch [199/391]: Loss 0.7805628180503845\n",
      "[Epoch 8] Training Batch [200/391]: Loss 0.862228274345398\n",
      "[Epoch 8] Training Batch [201/391]: Loss 0.8780384659767151\n",
      "[Epoch 8] Training Batch [202/391]: Loss 0.7334259152412415\n",
      "[Epoch 8] Training Batch [203/391]: Loss 0.666171669960022\n",
      "[Epoch 8] Training Batch [204/391]: Loss 0.8185428380966187\n",
      "[Epoch 8] Training Batch [205/391]: Loss 0.6564691066741943\n",
      "[Epoch 8] Training Batch [206/391]: Loss 0.8880199790000916\n",
      "[Epoch 8] Training Batch [207/391]: Loss 0.7568814158439636\n",
      "[Epoch 8] Training Batch [208/391]: Loss 0.8959507346153259\n",
      "[Epoch 8] Training Batch [209/391]: Loss 0.8625209331512451\n",
      "[Epoch 8] Training Batch [210/391]: Loss 0.755715548992157\n",
      "[Epoch 8] Training Batch [211/391]: Loss 0.6926220059394836\n",
      "[Epoch 8] Training Batch [212/391]: Loss 0.6664790511131287\n",
      "[Epoch 8] Training Batch [213/391]: Loss 0.9339439272880554\n",
      "[Epoch 8] Training Batch [214/391]: Loss 0.8827919363975525\n",
      "[Epoch 8] Training Batch [215/391]: Loss 0.9601932764053345\n",
      "[Epoch 8] Training Batch [216/391]: Loss 0.7849331498146057\n",
      "[Epoch 8] Training Batch [217/391]: Loss 0.6700911521911621\n",
      "[Epoch 8] Training Batch [218/391]: Loss 0.7545586228370667\n",
      "[Epoch 8] Training Batch [219/391]: Loss 0.8561455607414246\n",
      "[Epoch 8] Training Batch [220/391]: Loss 0.816684365272522\n",
      "[Epoch 8] Training Batch [221/391]: Loss 0.7701902389526367\n",
      "[Epoch 8] Training Batch [222/391]: Loss 0.9183732867240906\n",
      "[Epoch 8] Training Batch [223/391]: Loss 0.7022947072982788\n",
      "[Epoch 8] Training Batch [224/391]: Loss 0.9214655160903931\n",
      "[Epoch 8] Training Batch [225/391]: Loss 0.8855338096618652\n",
      "[Epoch 8] Training Batch [226/391]: Loss 0.7258817553520203\n",
      "[Epoch 8] Training Batch [227/391]: Loss 0.8460837602615356\n",
      "[Epoch 8] Training Batch [228/391]: Loss 0.931390106678009\n",
      "[Epoch 8] Training Batch [229/391]: Loss 0.8963559865951538\n",
      "[Epoch 8] Training Batch [230/391]: Loss 0.8817137479782104\n",
      "[Epoch 8] Training Batch [231/391]: Loss 0.8768309950828552\n",
      "[Epoch 8] Training Batch [232/391]: Loss 0.829901397228241\n",
      "[Epoch 8] Training Batch [233/391]: Loss 0.8380853533744812\n",
      "[Epoch 8] Training Batch [234/391]: Loss 0.843102753162384\n",
      "[Epoch 8] Training Batch [235/391]: Loss 0.9132959246635437\n",
      "[Epoch 8] Training Batch [236/391]: Loss 0.8895096778869629\n",
      "[Epoch 8] Training Batch [237/391]: Loss 0.873704731464386\n",
      "[Epoch 8] Training Batch [238/391]: Loss 0.6988859176635742\n",
      "[Epoch 8] Training Batch [239/391]: Loss 0.8379678726196289\n",
      "[Epoch 8] Training Batch [240/391]: Loss 0.6501540541648865\n",
      "[Epoch 8] Training Batch [241/391]: Loss 0.8143620491027832\n",
      "[Epoch 8] Training Batch [242/391]: Loss 0.8978790044784546\n",
      "[Epoch 8] Training Batch [243/391]: Loss 0.8081666231155396\n",
      "[Epoch 8] Training Batch [244/391]: Loss 0.7732263803482056\n",
      "[Epoch 8] Training Batch [245/391]: Loss 0.8314926028251648\n",
      "[Epoch 8] Training Batch [246/391]: Loss 0.639197826385498\n",
      "[Epoch 8] Training Batch [247/391]: Loss 0.6850146651268005\n",
      "[Epoch 8] Training Batch [248/391]: Loss 0.733845591545105\n",
      "[Epoch 8] Training Batch [249/391]: Loss 0.8735905289649963\n",
      "[Epoch 8] Training Batch [250/391]: Loss 0.832000195980072\n",
      "[Epoch 8] Training Batch [251/391]: Loss 0.7377189993858337\n",
      "[Epoch 8] Training Batch [252/391]: Loss 1.0686182975769043\n",
      "[Epoch 8] Training Batch [253/391]: Loss 0.7111305594444275\n",
      "[Epoch 8] Training Batch [254/391]: Loss 0.8223584890365601\n",
      "[Epoch 8] Training Batch [255/391]: Loss 0.7755990624427795\n",
      "[Epoch 8] Training Batch [256/391]: Loss 0.9435657262802124\n",
      "[Epoch 8] Training Batch [257/391]: Loss 0.7339586615562439\n",
      "[Epoch 8] Training Batch [258/391]: Loss 0.7972023487091064\n",
      "[Epoch 8] Training Batch [259/391]: Loss 0.8026374578475952\n",
      "[Epoch 8] Training Batch [260/391]: Loss 0.9328963160514832\n",
      "[Epoch 8] Training Batch [261/391]: Loss 0.8112049698829651\n",
      "[Epoch 8] Training Batch [262/391]: Loss 0.8540254235267639\n",
      "[Epoch 8] Training Batch [263/391]: Loss 0.8389255404472351\n",
      "[Epoch 8] Training Batch [264/391]: Loss 0.6469598412513733\n",
      "[Epoch 8] Training Batch [265/391]: Loss 0.8412249684333801\n",
      "[Epoch 8] Training Batch [266/391]: Loss 0.8239341378211975\n",
      "[Epoch 8] Training Batch [267/391]: Loss 1.1590272188186646\n",
      "[Epoch 8] Training Batch [268/391]: Loss 0.8182704448699951\n",
      "[Epoch 8] Training Batch [269/391]: Loss 0.8030134439468384\n",
      "[Epoch 8] Training Batch [270/391]: Loss 1.0165212154388428\n",
      "[Epoch 8] Training Batch [271/391]: Loss 0.7289867997169495\n",
      "[Epoch 8] Training Batch [272/391]: Loss 0.9005473256111145\n",
      "[Epoch 8] Training Batch [273/391]: Loss 0.7408567070960999\n",
      "[Epoch 8] Training Batch [274/391]: Loss 0.8191809058189392\n",
      "[Epoch 8] Training Batch [275/391]: Loss 0.9348147511482239\n",
      "[Epoch 8] Training Batch [276/391]: Loss 0.7330430746078491\n",
      "[Epoch 8] Training Batch [277/391]: Loss 0.8703383207321167\n",
      "[Epoch 8] Training Batch [278/391]: Loss 0.8668256998062134\n",
      "[Epoch 8] Training Batch [279/391]: Loss 0.9164971709251404\n",
      "[Epoch 8] Training Batch [280/391]: Loss 0.9004018306732178\n",
      "[Epoch 8] Training Batch [281/391]: Loss 0.7901992201805115\n",
      "[Epoch 8] Training Batch [282/391]: Loss 0.8063870072364807\n",
      "[Epoch 8] Training Batch [283/391]: Loss 0.6327294707298279\n",
      "[Epoch 8] Training Batch [284/391]: Loss 0.788553774356842\n",
      "[Epoch 8] Training Batch [285/391]: Loss 0.845043957233429\n",
      "[Epoch 8] Training Batch [286/391]: Loss 0.7851207852363586\n",
      "[Epoch 8] Training Batch [287/391]: Loss 0.846928596496582\n",
      "[Epoch 8] Training Batch [288/391]: Loss 0.8266180753707886\n",
      "[Epoch 8] Training Batch [289/391]: Loss 0.7982254028320312\n",
      "[Epoch 8] Training Batch [290/391]: Loss 0.6566123962402344\n",
      "[Epoch 8] Training Batch [291/391]: Loss 0.7872452735900879\n",
      "[Epoch 8] Training Batch [292/391]: Loss 0.7608999013900757\n",
      "[Epoch 8] Training Batch [293/391]: Loss 0.7070209383964539\n",
      "[Epoch 8] Training Batch [294/391]: Loss 0.7953957915306091\n",
      "[Epoch 8] Training Batch [295/391]: Loss 0.8638577461242676\n",
      "[Epoch 8] Training Batch [296/391]: Loss 0.8177603483200073\n",
      "[Epoch 8] Training Batch [297/391]: Loss 0.95823734998703\n",
      "[Epoch 8] Training Batch [298/391]: Loss 0.8130563497543335\n",
      "[Epoch 8] Training Batch [299/391]: Loss 0.8968228101730347\n",
      "[Epoch 8] Training Batch [300/391]: Loss 0.854446530342102\n",
      "[Epoch 8] Training Batch [301/391]: Loss 0.8982129693031311\n",
      "[Epoch 8] Training Batch [302/391]: Loss 0.8310168385505676\n",
      "[Epoch 8] Training Batch [303/391]: Loss 0.8394314646720886\n",
      "[Epoch 8] Training Batch [304/391]: Loss 0.7378894090652466\n",
      "[Epoch 8] Training Batch [305/391]: Loss 0.942435622215271\n",
      "[Epoch 8] Training Batch [306/391]: Loss 0.7152942419052124\n",
      "[Epoch 8] Training Batch [307/391]: Loss 0.8254705667495728\n",
      "[Epoch 8] Training Batch [308/391]: Loss 0.8065791130065918\n",
      "[Epoch 8] Training Batch [309/391]: Loss 0.7636209726333618\n",
      "[Epoch 8] Training Batch [310/391]: Loss 0.9554787278175354\n",
      "[Epoch 8] Training Batch [311/391]: Loss 0.8083145022392273\n",
      "[Epoch 8] Training Batch [312/391]: Loss 0.7900931239128113\n",
      "[Epoch 8] Training Batch [313/391]: Loss 0.7144055366516113\n",
      "[Epoch 8] Training Batch [314/391]: Loss 0.7152076363563538\n",
      "[Epoch 8] Training Batch [315/391]: Loss 0.8218706250190735\n",
      "[Epoch 8] Training Batch [316/391]: Loss 0.7530037760734558\n",
      "[Epoch 8] Training Batch [317/391]: Loss 0.9557774662971497\n",
      "[Epoch 8] Training Batch [318/391]: Loss 0.661190390586853\n",
      "[Epoch 8] Training Batch [319/391]: Loss 0.8391778469085693\n",
      "[Epoch 8] Training Batch [320/391]: Loss 0.6152923107147217\n",
      "[Epoch 8] Training Batch [321/391]: Loss 0.7738016247749329\n",
      "[Epoch 8] Training Batch [322/391]: Loss 0.7699629068374634\n",
      "[Epoch 8] Training Batch [323/391]: Loss 0.7503330111503601\n",
      "[Epoch 8] Training Batch [324/391]: Loss 0.8123747706413269\n",
      "[Epoch 8] Training Batch [325/391]: Loss 0.933134138584137\n",
      "[Epoch 8] Training Batch [326/391]: Loss 0.913704514503479\n",
      "[Epoch 8] Training Batch [327/391]: Loss 0.6672035455703735\n",
      "[Epoch 8] Training Batch [328/391]: Loss 0.8014369606971741\n",
      "[Epoch 8] Training Batch [329/391]: Loss 0.8337659239768982\n",
      "[Epoch 8] Training Batch [330/391]: Loss 0.8258365988731384\n",
      "[Epoch 8] Training Batch [331/391]: Loss 0.9896795153617859\n",
      "[Epoch 8] Training Batch [332/391]: Loss 0.7545840740203857\n",
      "[Epoch 8] Training Batch [333/391]: Loss 0.7224550247192383\n",
      "[Epoch 8] Training Batch [334/391]: Loss 0.854117751121521\n",
      "[Epoch 8] Training Batch [335/391]: Loss 0.6824925541877747\n",
      "[Epoch 8] Training Batch [336/391]: Loss 0.8004565238952637\n",
      "[Epoch 8] Training Batch [337/391]: Loss 0.6846132874488831\n",
      "[Epoch 8] Training Batch [338/391]: Loss 0.8555113673210144\n",
      "[Epoch 8] Training Batch [339/391]: Loss 0.8066622018814087\n",
      "[Epoch 8] Training Batch [340/391]: Loss 0.644467830657959\n",
      "[Epoch 8] Training Batch [341/391]: Loss 1.0034879446029663\n",
      "[Epoch 8] Training Batch [342/391]: Loss 0.9727287292480469\n",
      "[Epoch 8] Training Batch [343/391]: Loss 0.8680989742279053\n",
      "[Epoch 8] Training Batch [344/391]: Loss 1.0091431140899658\n",
      "[Epoch 8] Training Batch [345/391]: Loss 1.0272889137268066\n",
      "[Epoch 8] Training Batch [346/391]: Loss 0.7231003046035767\n",
      "[Epoch 8] Training Batch [347/391]: Loss 0.7717152833938599\n",
      "[Epoch 8] Training Batch [348/391]: Loss 0.683911919593811\n",
      "[Epoch 8] Training Batch [349/391]: Loss 0.6816689968109131\n",
      "[Epoch 8] Training Batch [350/391]: Loss 0.7320349812507629\n",
      "[Epoch 8] Training Batch [351/391]: Loss 0.7066603899002075\n",
      "[Epoch 8] Training Batch [352/391]: Loss 0.9248282313346863\n",
      "[Epoch 8] Training Batch [353/391]: Loss 0.7816689014434814\n",
      "[Epoch 8] Training Batch [354/391]: Loss 0.9291043877601624\n",
      "[Epoch 8] Training Batch [355/391]: Loss 0.7607122659683228\n",
      "[Epoch 8] Training Batch [356/391]: Loss 0.772420346736908\n",
      "[Epoch 8] Training Batch [357/391]: Loss 0.9754747748374939\n",
      "[Epoch 8] Training Batch [358/391]: Loss 0.7495644688606262\n",
      "[Epoch 8] Training Batch [359/391]: Loss 0.6950345039367676\n",
      "[Epoch 8] Training Batch [360/391]: Loss 0.8010143041610718\n",
      "[Epoch 8] Training Batch [361/391]: Loss 0.720671534538269\n",
      "[Epoch 8] Training Batch [362/391]: Loss 0.7098009586334229\n",
      "[Epoch 8] Training Batch [363/391]: Loss 0.9106027483940125\n",
      "[Epoch 8] Training Batch [364/391]: Loss 0.7395581603050232\n",
      "[Epoch 8] Training Batch [365/391]: Loss 0.8347545862197876\n",
      "[Epoch 8] Training Batch [366/391]: Loss 0.9035468101501465\n",
      "[Epoch 8] Training Batch [367/391]: Loss 0.8300611972808838\n",
      "[Epoch 8] Training Batch [368/391]: Loss 0.8411969542503357\n",
      "[Epoch 8] Training Batch [369/391]: Loss 0.965172290802002\n",
      "[Epoch 8] Training Batch [370/391]: Loss 0.9214800596237183\n",
      "[Epoch 8] Training Batch [371/391]: Loss 0.7493558526039124\n",
      "[Epoch 8] Training Batch [372/391]: Loss 0.8397137522697449\n",
      "[Epoch 8] Training Batch [373/391]: Loss 0.787287712097168\n",
      "[Epoch 8] Training Batch [374/391]: Loss 0.6936450004577637\n",
      "[Epoch 8] Training Batch [375/391]: Loss 0.8350327014923096\n",
      "[Epoch 8] Training Batch [376/391]: Loss 1.0147420167922974\n",
      "[Epoch 8] Training Batch [377/391]: Loss 0.9145739078521729\n",
      "[Epoch 8] Training Batch [378/391]: Loss 0.8431985378265381\n",
      "[Epoch 8] Training Batch [379/391]: Loss 0.856342077255249\n",
      "[Epoch 8] Training Batch [380/391]: Loss 0.7488707304000854\n",
      "[Epoch 8] Training Batch [381/391]: Loss 0.9103511571884155\n",
      "[Epoch 8] Training Batch [382/391]: Loss 0.6884254813194275\n",
      "[Epoch 8] Training Batch [383/391]: Loss 1.0007448196411133\n",
      "[Epoch 8] Training Batch [384/391]: Loss 0.8450040221214294\n",
      "[Epoch 8] Training Batch [385/391]: Loss 0.7390549182891846\n",
      "[Epoch 8] Training Batch [386/391]: Loss 0.8524024486541748\n",
      "[Epoch 8] Training Batch [387/391]: Loss 0.7740041613578796\n",
      "[Epoch 8] Training Batch [388/391]: Loss 0.7863705158233643\n",
      "[Epoch 8] Training Batch [389/391]: Loss 0.8146932125091553\n",
      "[Epoch 8] Training Batch [390/391]: Loss 0.7216349244117737\n",
      "[Epoch 8] Training Batch [391/391]: Loss 0.9186791181564331\n",
      "Epoch 8 - Train Loss: 0.7999\n",
      "*********  Epoch 9/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Training Batch [1/391]: Loss 0.5784754753112793\n",
      "[Epoch 9] Training Batch [2/391]: Loss 0.5821475386619568\n",
      "[Epoch 9] Training Batch [3/391]: Loss 0.9233373999595642\n",
      "[Epoch 9] Training Batch [4/391]: Loss 0.777725338935852\n",
      "[Epoch 9] Training Batch [5/391]: Loss 0.744808554649353\n",
      "[Epoch 9] Training Batch [6/391]: Loss 0.6728922128677368\n",
      "[Epoch 9] Training Batch [7/391]: Loss 0.7102153897285461\n",
      "[Epoch 9] Training Batch [8/391]: Loss 0.7013867497444153\n",
      "[Epoch 9] Training Batch [9/391]: Loss 0.5240636467933655\n",
      "[Epoch 9] Training Batch [10/391]: Loss 0.60550856590271\n",
      "[Epoch 9] Training Batch [11/391]: Loss 0.59844970703125\n",
      "[Epoch 9] Training Batch [12/391]: Loss 0.5093575119972229\n",
      "[Epoch 9] Training Batch [13/391]: Loss 0.8235239386558533\n",
      "[Epoch 9] Training Batch [14/391]: Loss 0.6092908382415771\n",
      "[Epoch 9] Training Batch [15/391]: Loss 0.6552091836929321\n",
      "[Epoch 9] Training Batch [16/391]: Loss 0.8064996600151062\n",
      "[Epoch 9] Training Batch [17/391]: Loss 0.587044358253479\n",
      "[Epoch 9] Training Batch [18/391]: Loss 0.6904357671737671\n",
      "[Epoch 9] Training Batch [19/391]: Loss 0.5963089466094971\n",
      "[Epoch 9] Training Batch [20/391]: Loss 0.7070690393447876\n",
      "[Epoch 9] Training Batch [21/391]: Loss 0.5647682547569275\n",
      "[Epoch 9] Training Batch [22/391]: Loss 0.6809675693511963\n",
      "[Epoch 9] Training Batch [23/391]: Loss 0.7641512751579285\n",
      "[Epoch 9] Training Batch [24/391]: Loss 0.5268134474754333\n",
      "[Epoch 9] Training Batch [25/391]: Loss 0.6463541388511658\n",
      "[Epoch 9] Training Batch [26/391]: Loss 0.7332286834716797\n",
      "[Epoch 9] Training Batch [27/391]: Loss 0.652813732624054\n",
      "[Epoch 9] Training Batch [28/391]: Loss 0.660519003868103\n",
      "[Epoch 9] Training Batch [29/391]: Loss 0.5697158575057983\n",
      "[Epoch 9] Training Batch [30/391]: Loss 0.5704149603843689\n",
      "[Epoch 9] Training Batch [31/391]: Loss 0.6320406198501587\n",
      "[Epoch 9] Training Batch [32/391]: Loss 0.7295761108398438\n",
      "[Epoch 9] Training Batch [33/391]: Loss 0.6967694759368896\n",
      "[Epoch 9] Training Batch [34/391]: Loss 0.5121874809265137\n",
      "[Epoch 9] Training Batch [35/391]: Loss 0.6254829168319702\n",
      "[Epoch 9] Training Batch [36/391]: Loss 0.4766721725463867\n",
      "[Epoch 9] Training Batch [37/391]: Loss 0.569839358329773\n",
      "[Epoch 9] Training Batch [38/391]: Loss 0.5812934041023254\n",
      "[Epoch 9] Training Batch [39/391]: Loss 0.6796842217445374\n",
      "[Epoch 9] Training Batch [40/391]: Loss 0.5730502605438232\n",
      "[Epoch 9] Training Batch [41/391]: Loss 0.50174480676651\n",
      "[Epoch 9] Training Batch [42/391]: Loss 0.6389946341514587\n",
      "[Epoch 9] Training Batch [43/391]: Loss 0.592113196849823\n",
      "[Epoch 9] Training Batch [44/391]: Loss 0.7156865000724792\n",
      "[Epoch 9] Training Batch [45/391]: Loss 0.6444679498672485\n",
      "[Epoch 9] Training Batch [46/391]: Loss 0.641935408115387\n",
      "[Epoch 9] Training Batch [47/391]: Loss 0.7746679186820984\n",
      "[Epoch 9] Training Batch [48/391]: Loss 0.653586745262146\n",
      "[Epoch 9] Training Batch [49/391]: Loss 0.5913591384887695\n",
      "[Epoch 9] Training Batch [50/391]: Loss 0.7196881175041199\n",
      "[Epoch 9] Training Batch [51/391]: Loss 0.626804769039154\n",
      "[Epoch 9] Training Batch [52/391]: Loss 0.5457314252853394\n",
      "[Epoch 9] Training Batch [53/391]: Loss 0.691865861415863\n",
      "[Epoch 9] Training Batch [54/391]: Loss 0.6378107070922852\n",
      "[Epoch 9] Training Batch [55/391]: Loss 0.6619235873222351\n",
      "[Epoch 9] Training Batch [56/391]: Loss 0.7863670587539673\n",
      "[Epoch 9] Training Batch [57/391]: Loss 0.5091639161109924\n",
      "[Epoch 9] Training Batch [58/391]: Loss 0.6092225909233093\n",
      "[Epoch 9] Training Batch [59/391]: Loss 0.5794283747673035\n",
      "[Epoch 9] Training Batch [60/391]: Loss 0.843090295791626\n",
      "[Epoch 9] Training Batch [61/391]: Loss 0.5947672128677368\n",
      "[Epoch 9] Training Batch [62/391]: Loss 0.6705295443534851\n",
      "[Epoch 9] Training Batch [63/391]: Loss 0.6619399785995483\n",
      "[Epoch 9] Training Batch [64/391]: Loss 0.6993309855461121\n",
      "[Epoch 9] Training Batch [65/391]: Loss 0.727829098701477\n",
      "[Epoch 9] Training Batch [66/391]: Loss 0.5735809803009033\n",
      "[Epoch 9] Training Batch [67/391]: Loss 0.6065481901168823\n",
      "[Epoch 9] Training Batch [68/391]: Loss 0.5489407777786255\n",
      "[Epoch 9] Training Batch [69/391]: Loss 0.5054882168769836\n",
      "[Epoch 9] Training Batch [70/391]: Loss 0.7861204743385315\n",
      "[Epoch 9] Training Batch [71/391]: Loss 0.5808753967285156\n",
      "[Epoch 9] Training Batch [72/391]: Loss 0.6855968236923218\n",
      "[Epoch 9] Training Batch [73/391]: Loss 0.5219203233718872\n",
      "[Epoch 9] Training Batch [74/391]: Loss 0.5882732272148132\n",
      "[Epoch 9] Training Batch [75/391]: Loss 0.6295942068099976\n",
      "[Epoch 9] Training Batch [76/391]: Loss 0.6904943585395813\n",
      "[Epoch 9] Training Batch [77/391]: Loss 0.4738712012767792\n",
      "[Epoch 9] Training Batch [78/391]: Loss 0.6175615787506104\n",
      "[Epoch 9] Training Batch [79/391]: Loss 0.5908976793289185\n",
      "[Epoch 9] Training Batch [80/391]: Loss 0.5665687322616577\n",
      "[Epoch 9] Training Batch [81/391]: Loss 0.6008188128471375\n",
      "[Epoch 9] Training Batch [82/391]: Loss 0.6572064161300659\n",
      "[Epoch 9] Training Batch [83/391]: Loss 0.5370631814002991\n",
      "[Epoch 9] Training Batch [84/391]: Loss 0.594219446182251\n",
      "[Epoch 9] Training Batch [85/391]: Loss 0.5729042291641235\n",
      "[Epoch 9] Training Batch [86/391]: Loss 0.6811822652816772\n",
      "[Epoch 9] Training Batch [87/391]: Loss 0.5649943351745605\n",
      "[Epoch 9] Training Batch [88/391]: Loss 0.5057429075241089\n",
      "[Epoch 9] Training Batch [89/391]: Loss 0.5704072117805481\n",
      "[Epoch 9] Training Batch [90/391]: Loss 0.5551713109016418\n",
      "[Epoch 9] Training Batch [91/391]: Loss 0.5836687684059143\n",
      "[Epoch 9] Training Batch [92/391]: Loss 0.8510312438011169\n",
      "[Epoch 9] Training Batch [93/391]: Loss 0.5714049339294434\n",
      "[Epoch 9] Training Batch [94/391]: Loss 0.8539453744888306\n",
      "[Epoch 9] Training Batch [95/391]: Loss 0.6467823386192322\n",
      "[Epoch 9] Training Batch [96/391]: Loss 0.7252493500709534\n",
      "[Epoch 9] Training Batch [97/391]: Loss 0.670272946357727\n",
      "[Epoch 9] Training Batch [98/391]: Loss 0.6307459473609924\n",
      "[Epoch 9] Training Batch [99/391]: Loss 0.5512049198150635\n",
      "[Epoch 9] Training Batch [100/391]: Loss 0.6034150719642639\n",
      "[Epoch 9] Training Batch [101/391]: Loss 0.6500459909439087\n",
      "[Epoch 9] Training Batch [102/391]: Loss 0.6293627023696899\n",
      "[Epoch 9] Training Batch [103/391]: Loss 0.7265328168869019\n",
      "[Epoch 9] Training Batch [104/391]: Loss 0.8042881488800049\n",
      "[Epoch 9] Training Batch [105/391]: Loss 0.6523758769035339\n",
      "[Epoch 9] Training Batch [106/391]: Loss 0.6165964603424072\n",
      "[Epoch 9] Training Batch [107/391]: Loss 0.6100123524665833\n",
      "[Epoch 9] Training Batch [108/391]: Loss 0.5654950737953186\n",
      "[Epoch 9] Training Batch [109/391]: Loss 0.7059299349784851\n",
      "[Epoch 9] Training Batch [110/391]: Loss 0.4969843029975891\n",
      "[Epoch 9] Training Batch [111/391]: Loss 0.6436933279037476\n",
      "[Epoch 9] Training Batch [112/391]: Loss 0.6210638284683228\n",
      "[Epoch 9] Training Batch [113/391]: Loss 0.5278079509735107\n",
      "[Epoch 9] Training Batch [114/391]: Loss 0.7015330791473389\n",
      "[Epoch 9] Training Batch [115/391]: Loss 0.6428613066673279\n",
      "[Epoch 9] Training Batch [116/391]: Loss 0.8108854293823242\n",
      "[Epoch 9] Training Batch [117/391]: Loss 0.6246595978736877\n",
      "[Epoch 9] Training Batch [118/391]: Loss 0.803845226764679\n",
      "[Epoch 9] Training Batch [119/391]: Loss 0.7210910320281982\n",
      "[Epoch 9] Training Batch [120/391]: Loss 0.7323606610298157\n",
      "[Epoch 9] Training Batch [121/391]: Loss 0.5907324552536011\n",
      "[Epoch 9] Training Batch [122/391]: Loss 0.7024874091148376\n",
      "[Epoch 9] Training Batch [123/391]: Loss 0.568156898021698\n",
      "[Epoch 9] Training Batch [124/391]: Loss 0.6732444167137146\n",
      "[Epoch 9] Training Batch [125/391]: Loss 0.6859452128410339\n",
      "[Epoch 9] Training Batch [126/391]: Loss 0.5816120505332947\n",
      "[Epoch 9] Training Batch [127/391]: Loss 0.6088531613349915\n",
      "[Epoch 9] Training Batch [128/391]: Loss 0.6236367225646973\n",
      "[Epoch 9] Training Batch [129/391]: Loss 0.5717597603797913\n",
      "[Epoch 9] Training Batch [130/391]: Loss 0.7685037851333618\n",
      "[Epoch 9] Training Batch [131/391]: Loss 0.619324266910553\n",
      "[Epoch 9] Training Batch [132/391]: Loss 0.6122525930404663\n",
      "[Epoch 9] Training Batch [133/391]: Loss 0.6689788103103638\n",
      "[Epoch 9] Training Batch [134/391]: Loss 0.807136595249176\n",
      "[Epoch 9] Training Batch [135/391]: Loss 0.5822254419326782\n",
      "[Epoch 9] Training Batch [136/391]: Loss 0.6844680905342102\n",
      "[Epoch 9] Training Batch [137/391]: Loss 0.6787042617797852\n",
      "[Epoch 9] Training Batch [138/391]: Loss 0.5585752129554749\n",
      "[Epoch 9] Training Batch [139/391]: Loss 0.6749050617218018\n",
      "[Epoch 9] Training Batch [140/391]: Loss 0.5404213070869446\n",
      "[Epoch 9] Training Batch [141/391]: Loss 0.6573063731193542\n",
      "[Epoch 9] Training Batch [142/391]: Loss 0.6483572125434875\n",
      "[Epoch 9] Training Batch [143/391]: Loss 0.6966709494590759\n",
      "[Epoch 9] Training Batch [144/391]: Loss 0.6278057098388672\n",
      "[Epoch 9] Training Batch [145/391]: Loss 0.7029128670692444\n",
      "[Epoch 9] Training Batch [146/391]: Loss 0.5707023739814758\n",
      "[Epoch 9] Training Batch [147/391]: Loss 0.7158578038215637\n",
      "[Epoch 9] Training Batch [148/391]: Loss 0.8417272567749023\n",
      "[Epoch 9] Training Batch [149/391]: Loss 0.7286991477012634\n",
      "[Epoch 9] Training Batch [150/391]: Loss 0.5608090162277222\n",
      "[Epoch 9] Training Batch [151/391]: Loss 0.7446383833885193\n",
      "[Epoch 9] Training Batch [152/391]: Loss 0.76120525598526\n",
      "[Epoch 9] Training Batch [153/391]: Loss 0.7231193780899048\n",
      "[Epoch 9] Training Batch [154/391]: Loss 0.5520257949829102\n",
      "[Epoch 9] Training Batch [155/391]: Loss 0.5311334729194641\n",
      "[Epoch 9] Training Batch [156/391]: Loss 0.6148096323013306\n",
      "[Epoch 9] Training Batch [157/391]: Loss 0.6528803706169128\n",
      "[Epoch 9] Training Batch [158/391]: Loss 0.6472790241241455\n",
      "[Epoch 9] Training Batch [159/391]: Loss 0.6073418259620667\n",
      "[Epoch 9] Training Batch [160/391]: Loss 0.5778942108154297\n",
      "[Epoch 9] Training Batch [161/391]: Loss 0.6690876483917236\n",
      "[Epoch 9] Training Batch [162/391]: Loss 0.6693176031112671\n",
      "[Epoch 9] Training Batch [163/391]: Loss 0.6512002348899841\n",
      "[Epoch 9] Training Batch [164/391]: Loss 0.7399749159812927\n",
      "[Epoch 9] Training Batch [165/391]: Loss 0.6641424298286438\n",
      "[Epoch 9] Training Batch [166/391]: Loss 0.6046922206878662\n",
      "[Epoch 9] Training Batch [167/391]: Loss 0.7725594639778137\n",
      "[Epoch 9] Training Batch [168/391]: Loss 0.6355145573616028\n",
      "[Epoch 9] Training Batch [169/391]: Loss 0.8620290756225586\n",
      "[Epoch 9] Training Batch [170/391]: Loss 0.6926069855690002\n",
      "[Epoch 9] Training Batch [171/391]: Loss 0.7087262868881226\n",
      "[Epoch 9] Training Batch [172/391]: Loss 0.518498420715332\n",
      "[Epoch 9] Training Batch [173/391]: Loss 0.8532959818840027\n",
      "[Epoch 9] Training Batch [174/391]: Loss 0.7913474440574646\n",
      "[Epoch 9] Training Batch [175/391]: Loss 0.5400370359420776\n",
      "[Epoch 9] Training Batch [176/391]: Loss 0.7095973491668701\n",
      "[Epoch 9] Training Batch [177/391]: Loss 0.7283780574798584\n",
      "[Epoch 9] Training Batch [178/391]: Loss 0.814929723739624\n",
      "[Epoch 9] Training Batch [179/391]: Loss 0.6730425357818604\n",
      "[Epoch 9] Training Batch [180/391]: Loss 0.6648797392845154\n",
      "[Epoch 9] Training Batch [181/391]: Loss 0.6971279978752136\n",
      "[Epoch 9] Training Batch [182/391]: Loss 0.5847606062889099\n",
      "[Epoch 9] Training Batch [183/391]: Loss 0.7408043146133423\n",
      "[Epoch 9] Training Batch [184/391]: Loss 0.6736319065093994\n",
      "[Epoch 9] Training Batch [185/391]: Loss 0.7582017779350281\n",
      "[Epoch 9] Training Batch [186/391]: Loss 0.6703601479530334\n",
      "[Epoch 9] Training Batch [187/391]: Loss 0.7771163582801819\n",
      "[Epoch 9] Training Batch [188/391]: Loss 0.8009774684906006\n",
      "[Epoch 9] Training Batch [189/391]: Loss 0.6432177424430847\n",
      "[Epoch 9] Training Batch [190/391]: Loss 0.6228407621383667\n",
      "[Epoch 9] Training Batch [191/391]: Loss 0.7318534255027771\n",
      "[Epoch 9] Training Batch [192/391]: Loss 0.7371220588684082\n",
      "[Epoch 9] Training Batch [193/391]: Loss 0.7256739139556885\n",
      "[Epoch 9] Training Batch [194/391]: Loss 0.7021490335464478\n",
      "[Epoch 9] Training Batch [195/391]: Loss 0.7097257971763611\n",
      "[Epoch 9] Training Batch [196/391]: Loss 0.5878974199295044\n",
      "[Epoch 9] Training Batch [197/391]: Loss 0.676053524017334\n",
      "[Epoch 9] Training Batch [198/391]: Loss 0.5843278765678406\n",
      "[Epoch 9] Training Batch [199/391]: Loss 0.7405150532722473\n",
      "[Epoch 9] Training Batch [200/391]: Loss 0.8019094467163086\n",
      "[Epoch 9] Training Batch [201/391]: Loss 0.5724546909332275\n",
      "[Epoch 9] Training Batch [202/391]: Loss 0.765583336353302\n",
      "[Epoch 9] Training Batch [203/391]: Loss 0.7213062644004822\n",
      "[Epoch 9] Training Batch [204/391]: Loss 0.6397459506988525\n",
      "[Epoch 9] Training Batch [205/391]: Loss 0.667273759841919\n",
      "[Epoch 9] Training Batch [206/391]: Loss 0.5398396253585815\n",
      "[Epoch 9] Training Batch [207/391]: Loss 0.6031426787376404\n",
      "[Epoch 9] Training Batch [208/391]: Loss 0.571659505367279\n",
      "[Epoch 9] Training Batch [209/391]: Loss 0.648790180683136\n",
      "[Epoch 9] Training Batch [210/391]: Loss 0.771604061126709\n",
      "[Epoch 9] Training Batch [211/391]: Loss 0.7938409447669983\n",
      "[Epoch 9] Training Batch [212/391]: Loss 0.5216753482818604\n",
      "[Epoch 9] Training Batch [213/391]: Loss 0.6660267114639282\n",
      "[Epoch 9] Training Batch [214/391]: Loss 0.6339933276176453\n",
      "[Epoch 9] Training Batch [215/391]: Loss 0.6074454188346863\n",
      "[Epoch 9] Training Batch [216/391]: Loss 0.7316128611564636\n",
      "[Epoch 9] Training Batch [217/391]: Loss 0.6066344976425171\n",
      "[Epoch 9] Training Batch [218/391]: Loss 0.8526729941368103\n",
      "[Epoch 9] Training Batch [219/391]: Loss 0.7615237236022949\n",
      "[Epoch 9] Training Batch [220/391]: Loss 0.6478410959243774\n",
      "[Epoch 9] Training Batch [221/391]: Loss 0.7043536305427551\n",
      "[Epoch 9] Training Batch [222/391]: Loss 0.8227542042732239\n",
      "[Epoch 9] Training Batch [223/391]: Loss 0.6150708198547363\n",
      "[Epoch 9] Training Batch [224/391]: Loss 0.5865107774734497\n",
      "[Epoch 9] Training Batch [225/391]: Loss 0.6281328201293945\n",
      "[Epoch 9] Training Batch [226/391]: Loss 0.8683133721351624\n",
      "[Epoch 9] Training Batch [227/391]: Loss 0.7015243172645569\n",
      "[Epoch 9] Training Batch [228/391]: Loss 0.6286505460739136\n",
      "[Epoch 9] Training Batch [229/391]: Loss 0.7358506917953491\n",
      "[Epoch 9] Training Batch [230/391]: Loss 0.6465327739715576\n",
      "[Epoch 9] Training Batch [231/391]: Loss 0.5982257127761841\n",
      "[Epoch 9] Training Batch [232/391]: Loss 0.6452404260635376\n",
      "[Epoch 9] Training Batch [233/391]: Loss 0.6313379406929016\n",
      "[Epoch 9] Training Batch [234/391]: Loss 0.5898035764694214\n",
      "[Epoch 9] Training Batch [235/391]: Loss 0.5484658479690552\n",
      "[Epoch 9] Training Batch [236/391]: Loss 0.5929323434829712\n",
      "[Epoch 9] Training Batch [237/391]: Loss 0.6136564612388611\n",
      "[Epoch 9] Training Batch [238/391]: Loss 0.5416560173034668\n",
      "[Epoch 9] Training Batch [239/391]: Loss 0.6961937546730042\n",
      "[Epoch 9] Training Batch [240/391]: Loss 0.6793359518051147\n",
      "[Epoch 9] Training Batch [241/391]: Loss 0.7200800180435181\n",
      "[Epoch 9] Training Batch [242/391]: Loss 0.6600754857063293\n",
      "[Epoch 9] Training Batch [243/391]: Loss 0.7042407989501953\n",
      "[Epoch 9] Training Batch [244/391]: Loss 0.799694299697876\n",
      "[Epoch 9] Training Batch [245/391]: Loss 0.6555982232093811\n",
      "[Epoch 9] Training Batch [246/391]: Loss 0.7270948886871338\n",
      "[Epoch 9] Training Batch [247/391]: Loss 0.7900365591049194\n",
      "[Epoch 9] Training Batch [248/391]: Loss 0.6477747559547424\n",
      "[Epoch 9] Training Batch [249/391]: Loss 0.7572357058525085\n",
      "[Epoch 9] Training Batch [250/391]: Loss 0.854077935218811\n",
      "[Epoch 9] Training Batch [251/391]: Loss 0.658649206161499\n",
      "[Epoch 9] Training Batch [252/391]: Loss 0.6324195861816406\n",
      "[Epoch 9] Training Batch [253/391]: Loss 0.561797022819519\n",
      "[Epoch 9] Training Batch [254/391]: Loss 0.7194446325302124\n",
      "[Epoch 9] Training Batch [255/391]: Loss 0.6626209020614624\n",
      "[Epoch 9] Training Batch [256/391]: Loss 0.7325170636177063\n",
      "[Epoch 9] Training Batch [257/391]: Loss 0.6596258282661438\n",
      "[Epoch 9] Training Batch [258/391]: Loss 0.6091893315315247\n",
      "[Epoch 9] Training Batch [259/391]: Loss 0.7518431544303894\n",
      "[Epoch 9] Training Batch [260/391]: Loss 0.9009285569190979\n",
      "[Epoch 9] Training Batch [261/391]: Loss 0.6033399105072021\n",
      "[Epoch 9] Training Batch [262/391]: Loss 0.5450184941291809\n",
      "[Epoch 9] Training Batch [263/391]: Loss 0.7704242467880249\n",
      "[Epoch 9] Training Batch [264/391]: Loss 0.8556026220321655\n",
      "[Epoch 9] Training Batch [265/391]: Loss 0.7166751623153687\n",
      "[Epoch 9] Training Batch [266/391]: Loss 0.7376227378845215\n",
      "[Epoch 9] Training Batch [267/391]: Loss 0.8095746040344238\n",
      "[Epoch 9] Training Batch [268/391]: Loss 0.7580328583717346\n",
      "[Epoch 9] Training Batch [269/391]: Loss 0.6852019429206848\n",
      "[Epoch 9] Training Batch [270/391]: Loss 0.5865193605422974\n",
      "[Epoch 9] Training Batch [271/391]: Loss 0.6656936407089233\n",
      "[Epoch 9] Training Batch [272/391]: Loss 0.8519667387008667\n",
      "[Epoch 9] Training Batch [273/391]: Loss 0.5842562913894653\n",
      "[Epoch 9] Training Batch [274/391]: Loss 0.78914475440979\n",
      "[Epoch 9] Training Batch [275/391]: Loss 0.5952339172363281\n",
      "[Epoch 9] Training Batch [276/391]: Loss 0.5783196091651917\n",
      "[Epoch 9] Training Batch [277/391]: Loss 0.6548099517822266\n",
      "[Epoch 9] Training Batch [278/391]: Loss 0.6073431372642517\n",
      "[Epoch 9] Training Batch [279/391]: Loss 0.6921125054359436\n",
      "[Epoch 9] Training Batch [280/391]: Loss 0.6075177788734436\n",
      "[Epoch 9] Training Batch [281/391]: Loss 0.5099529027938843\n",
      "[Epoch 9] Training Batch [282/391]: Loss 0.7683413624763489\n",
      "[Epoch 9] Training Batch [283/391]: Loss 0.8322723507881165\n",
      "[Epoch 9] Training Batch [284/391]: Loss 0.6687043905258179\n",
      "[Epoch 9] Training Batch [285/391]: Loss 0.7807313203811646\n",
      "[Epoch 9] Training Batch [286/391]: Loss 0.621830940246582\n",
      "[Epoch 9] Training Batch [287/391]: Loss 0.7631394863128662\n",
      "[Epoch 9] Training Batch [288/391]: Loss 0.7153297662734985\n",
      "[Epoch 9] Training Batch [289/391]: Loss 0.7811292409896851\n",
      "[Epoch 9] Training Batch [290/391]: Loss 0.7284596562385559\n",
      "[Epoch 9] Training Batch [291/391]: Loss 0.724051296710968\n",
      "[Epoch 9] Training Batch [292/391]: Loss 0.8183860182762146\n",
      "[Epoch 9] Training Batch [293/391]: Loss 0.694307267665863\n",
      "[Epoch 9] Training Batch [294/391]: Loss 0.7499993443489075\n",
      "[Epoch 9] Training Batch [295/391]: Loss 0.6843043565750122\n",
      "[Epoch 9] Training Batch [296/391]: Loss 0.5778641104698181\n",
      "[Epoch 9] Training Batch [297/391]: Loss 0.8106418251991272\n",
      "[Epoch 9] Training Batch [298/391]: Loss 0.595127284526825\n",
      "[Epoch 9] Training Batch [299/391]: Loss 0.7647876739501953\n",
      "[Epoch 9] Training Batch [300/391]: Loss 0.7963260412216187\n",
      "[Epoch 9] Training Batch [301/391]: Loss 0.9185377955436707\n",
      "[Epoch 9] Training Batch [302/391]: Loss 0.6040773391723633\n",
      "[Epoch 9] Training Batch [303/391]: Loss 0.7525675892829895\n",
      "[Epoch 9] Training Batch [304/391]: Loss 0.562722384929657\n",
      "[Epoch 9] Training Batch [305/391]: Loss 0.787390410900116\n",
      "[Epoch 9] Training Batch [306/391]: Loss 0.5926697850227356\n",
      "[Epoch 9] Training Batch [307/391]: Loss 0.701968252658844\n",
      "[Epoch 9] Training Batch [308/391]: Loss 0.6855226755142212\n",
      "[Epoch 9] Training Batch [309/391]: Loss 0.5648286938667297\n",
      "[Epoch 9] Training Batch [310/391]: Loss 0.7272866368293762\n",
      "[Epoch 9] Training Batch [311/391]: Loss 0.6610386371612549\n",
      "[Epoch 9] Training Batch [312/391]: Loss 0.7340232729911804\n",
      "[Epoch 9] Training Batch [313/391]: Loss 0.6997512578964233\n",
      "[Epoch 9] Training Batch [314/391]: Loss 0.6634028553962708\n",
      "[Epoch 9] Training Batch [315/391]: Loss 0.6148315668106079\n",
      "[Epoch 9] Training Batch [316/391]: Loss 0.6869892477989197\n",
      "[Epoch 9] Training Batch [317/391]: Loss 0.8231562376022339\n",
      "[Epoch 9] Training Batch [318/391]: Loss 0.6897645592689514\n",
      "[Epoch 9] Training Batch [319/391]: Loss 0.7165783643722534\n",
      "[Epoch 9] Training Batch [320/391]: Loss 0.6930279731750488\n",
      "[Epoch 9] Training Batch [321/391]: Loss 0.5559753775596619\n",
      "[Epoch 9] Training Batch [322/391]: Loss 0.7366617918014526\n",
      "[Epoch 9] Training Batch [323/391]: Loss 0.6902722716331482\n",
      "[Epoch 9] Training Batch [324/391]: Loss 0.7095355987548828\n",
      "[Epoch 9] Training Batch [325/391]: Loss 0.6963447332382202\n",
      "[Epoch 9] Training Batch [326/391]: Loss 0.637489914894104\n",
      "[Epoch 9] Training Batch [327/391]: Loss 0.6129535436630249\n",
      "[Epoch 9] Training Batch [328/391]: Loss 0.8212646842002869\n",
      "[Epoch 9] Training Batch [329/391]: Loss 0.5026162266731262\n",
      "[Epoch 9] Training Batch [330/391]: Loss 0.7376253008842468\n",
      "[Epoch 9] Training Batch [331/391]: Loss 0.5494875907897949\n",
      "[Epoch 9] Training Batch [332/391]: Loss 0.8973408341407776\n",
      "[Epoch 9] Training Batch [333/391]: Loss 0.6800268888473511\n",
      "[Epoch 9] Training Batch [334/391]: Loss 0.681615948677063\n",
      "[Epoch 9] Training Batch [335/391]: Loss 0.9022068977355957\n",
      "[Epoch 9] Training Batch [336/391]: Loss 0.8880213499069214\n",
      "[Epoch 9] Training Batch [337/391]: Loss 0.8290229439735413\n",
      "[Epoch 9] Training Batch [338/391]: Loss 0.4617694020271301\n",
      "[Epoch 9] Training Batch [339/391]: Loss 0.6740233302116394\n",
      "[Epoch 9] Training Batch [340/391]: Loss 0.7941988110542297\n",
      "[Epoch 9] Training Batch [341/391]: Loss 0.7530240416526794\n",
      "[Epoch 9] Training Batch [342/391]: Loss 0.7288883924484253\n",
      "[Epoch 9] Training Batch [343/391]: Loss 0.707205593585968\n",
      "[Epoch 9] Training Batch [344/391]: Loss 0.6529719233512878\n",
      "[Epoch 9] Training Batch [345/391]: Loss 0.6567747592926025\n",
      "[Epoch 9] Training Batch [346/391]: Loss 0.5945115089416504\n",
      "[Epoch 9] Training Batch [347/391]: Loss 0.8568360805511475\n",
      "[Epoch 9] Training Batch [348/391]: Loss 0.6018396615982056\n",
      "[Epoch 9] Training Batch [349/391]: Loss 0.6951032876968384\n",
      "[Epoch 9] Training Batch [350/391]: Loss 0.6720297932624817\n",
      "[Epoch 9] Training Batch [351/391]: Loss 0.6511864066123962\n",
      "[Epoch 9] Training Batch [352/391]: Loss 0.7018246054649353\n",
      "[Epoch 9] Training Batch [353/391]: Loss 0.6890515089035034\n",
      "[Epoch 9] Training Batch [354/391]: Loss 0.716920018196106\n",
      "[Epoch 9] Training Batch [355/391]: Loss 0.5309866666793823\n",
      "[Epoch 9] Training Batch [356/391]: Loss 0.7376381158828735\n",
      "[Epoch 9] Training Batch [357/391]: Loss 0.5824697017669678\n",
      "[Epoch 9] Training Batch [358/391]: Loss 0.7673038840293884\n",
      "[Epoch 9] Training Batch [359/391]: Loss 0.7192738652229309\n",
      "[Epoch 9] Training Batch [360/391]: Loss 0.7334457635879517\n",
      "[Epoch 9] Training Batch [361/391]: Loss 0.5334481000900269\n",
      "[Epoch 9] Training Batch [362/391]: Loss 0.6894788146018982\n",
      "[Epoch 9] Training Batch [363/391]: Loss 0.6429101228713989\n",
      "[Epoch 9] Training Batch [364/391]: Loss 0.7391088604927063\n",
      "[Epoch 9] Training Batch [365/391]: Loss 0.841562807559967\n",
      "[Epoch 9] Training Batch [366/391]: Loss 0.7181797027587891\n",
      "[Epoch 9] Training Batch [367/391]: Loss 0.7107861042022705\n",
      "[Epoch 9] Training Batch [368/391]: Loss 0.6376736760139465\n",
      "[Epoch 9] Training Batch [369/391]: Loss 0.5236639976501465\n",
      "[Epoch 9] Training Batch [370/391]: Loss 0.6119875311851501\n",
      "[Epoch 9] Training Batch [371/391]: Loss 0.6098949909210205\n",
      "[Epoch 9] Training Batch [372/391]: Loss 0.9132786989212036\n",
      "[Epoch 9] Training Batch [373/391]: Loss 0.6301608085632324\n",
      "[Epoch 9] Training Batch [374/391]: Loss 0.8565202951431274\n",
      "[Epoch 9] Training Batch [375/391]: Loss 0.5439121127128601\n",
      "[Epoch 9] Training Batch [376/391]: Loss 0.6704616546630859\n",
      "[Epoch 9] Training Batch [377/391]: Loss 0.8521215915679932\n",
      "[Epoch 9] Training Batch [378/391]: Loss 0.8071377277374268\n",
      "[Epoch 9] Training Batch [379/391]: Loss 0.7333147525787354\n",
      "[Epoch 9] Training Batch [380/391]: Loss 0.5855088233947754\n",
      "[Epoch 9] Training Batch [381/391]: Loss 0.7054028511047363\n",
      "[Epoch 9] Training Batch [382/391]: Loss 0.8547165393829346\n",
      "[Epoch 9] Training Batch [383/391]: Loss 0.7688420414924622\n",
      "[Epoch 9] Training Batch [384/391]: Loss 0.6150062084197998\n",
      "[Epoch 9] Training Batch [385/391]: Loss 0.7424365282058716\n",
      "[Epoch 9] Training Batch [386/391]: Loss 0.8111246824264526\n",
      "[Epoch 9] Training Batch [387/391]: Loss 0.7814271450042725\n",
      "[Epoch 9] Training Batch [388/391]: Loss 0.7153404355049133\n",
      "[Epoch 9] Training Batch [389/391]: Loss 0.8000122308731079\n",
      "[Epoch 9] Training Batch [390/391]: Loss 0.608248770236969\n",
      "[Epoch 9] Training Batch [391/391]: Loss 0.9249221086502075\n",
      "Epoch 9 - Train Loss: 0.6737\n",
      "*********  Epoch 10/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Training Batch [1/391]: Loss 0.500386655330658\n",
      "[Epoch 10] Training Batch [2/391]: Loss 0.5507825613021851\n",
      "[Epoch 10] Training Batch [3/391]: Loss 0.4546099305152893\n",
      "[Epoch 10] Training Batch [4/391]: Loss 0.5751065015792847\n",
      "[Epoch 10] Training Batch [5/391]: Loss 0.5745784640312195\n",
      "[Epoch 10] Training Batch [6/391]: Loss 0.5720474720001221\n",
      "[Epoch 10] Training Batch [7/391]: Loss 0.43374475836753845\n",
      "[Epoch 10] Training Batch [8/391]: Loss 0.6708759665489197\n",
      "[Epoch 10] Training Batch [9/391]: Loss 0.4794216752052307\n",
      "[Epoch 10] Training Batch [10/391]: Loss 0.490412175655365\n",
      "[Epoch 10] Training Batch [11/391]: Loss 0.4893864393234253\n",
      "[Epoch 10] Training Batch [12/391]: Loss 0.3681422173976898\n",
      "[Epoch 10] Training Batch [13/391]: Loss 0.5403014421463013\n",
      "[Epoch 10] Training Batch [14/391]: Loss 0.4782205820083618\n",
      "[Epoch 10] Training Batch [15/391]: Loss 0.3758096396923065\n",
      "[Epoch 10] Training Batch [16/391]: Loss 0.3781631290912628\n",
      "[Epoch 10] Training Batch [17/391]: Loss 0.6079239249229431\n",
      "[Epoch 10] Training Batch [18/391]: Loss 0.7207549214363098\n",
      "[Epoch 10] Training Batch [19/391]: Loss 0.5092182159423828\n",
      "[Epoch 10] Training Batch [20/391]: Loss 0.5345548391342163\n",
      "[Epoch 10] Training Batch [21/391]: Loss 0.35938408970832825\n",
      "[Epoch 10] Training Batch [22/391]: Loss 0.5695510506629944\n",
      "[Epoch 10] Training Batch [23/391]: Loss 0.4870619773864746\n",
      "[Epoch 10] Training Batch [24/391]: Loss 0.5047299265861511\n",
      "[Epoch 10] Training Batch [25/391]: Loss 0.4743197560310364\n",
      "[Epoch 10] Training Batch [26/391]: Loss 0.5143594145774841\n",
      "[Epoch 10] Training Batch [27/391]: Loss 0.5851513743400574\n",
      "[Epoch 10] Training Batch [28/391]: Loss 0.5490777492523193\n",
      "[Epoch 10] Training Batch [29/391]: Loss 0.5030274391174316\n",
      "[Epoch 10] Training Batch [30/391]: Loss 0.4256812632083893\n",
      "[Epoch 10] Training Batch [31/391]: Loss 0.5441862344741821\n",
      "[Epoch 10] Training Batch [32/391]: Loss 0.46890583634376526\n",
      "[Epoch 10] Training Batch [33/391]: Loss 0.5096211433410645\n",
      "[Epoch 10] Training Batch [34/391]: Loss 0.4601982831954956\n",
      "[Epoch 10] Training Batch [35/391]: Loss 0.47523021697998047\n",
      "[Epoch 10] Training Batch [36/391]: Loss 0.44823208451271057\n",
      "[Epoch 10] Training Batch [37/391]: Loss 0.5824349522590637\n",
      "[Epoch 10] Training Batch [38/391]: Loss 0.39003363251686096\n",
      "[Epoch 10] Training Batch [39/391]: Loss 0.46657201647758484\n",
      "[Epoch 10] Training Batch [40/391]: Loss 0.4558514654636383\n",
      "[Epoch 10] Training Batch [41/391]: Loss 0.5470707416534424\n",
      "[Epoch 10] Training Batch [42/391]: Loss 0.5053899884223938\n",
      "[Epoch 10] Training Batch [43/391]: Loss 0.8412346839904785\n",
      "[Epoch 10] Training Batch [44/391]: Loss 0.4390842318534851\n",
      "[Epoch 10] Training Batch [45/391]: Loss 0.4365972876548767\n",
      "[Epoch 10] Training Batch [46/391]: Loss 0.4891316890716553\n",
      "[Epoch 10] Training Batch [47/391]: Loss 0.625378429889679\n",
      "[Epoch 10] Training Batch [48/391]: Loss 0.4193599224090576\n",
      "[Epoch 10] Training Batch [49/391]: Loss 0.4518880844116211\n",
      "[Epoch 10] Training Batch [50/391]: Loss 0.41843944787979126\n",
      "[Epoch 10] Training Batch [51/391]: Loss 0.46397116780281067\n",
      "[Epoch 10] Training Batch [52/391]: Loss 0.5648924708366394\n",
      "[Epoch 10] Training Batch [53/391]: Loss 0.5005479454994202\n",
      "[Epoch 10] Training Batch [54/391]: Loss 0.4527948200702667\n",
      "[Epoch 10] Training Batch [55/391]: Loss 0.5102028250694275\n",
      "[Epoch 10] Training Batch [56/391]: Loss 0.3825867772102356\n",
      "[Epoch 10] Training Batch [57/391]: Loss 0.6033691763877869\n",
      "[Epoch 10] Training Batch [58/391]: Loss 0.4652901887893677\n",
      "[Epoch 10] Training Batch [59/391]: Loss 0.5119982957839966\n",
      "[Epoch 10] Training Batch [60/391]: Loss 0.560502827167511\n",
      "[Epoch 10] Training Batch [61/391]: Loss 0.37446779012680054\n",
      "[Epoch 10] Training Batch [62/391]: Loss 0.49089381098747253\n",
      "[Epoch 10] Training Batch [63/391]: Loss 0.4549809992313385\n",
      "[Epoch 10] Training Batch [64/391]: Loss 0.503768801689148\n",
      "[Epoch 10] Training Batch [65/391]: Loss 0.38808804750442505\n",
      "[Epoch 10] Training Batch [66/391]: Loss 0.6608002781867981\n",
      "[Epoch 10] Training Batch [67/391]: Loss 0.45008575916290283\n",
      "[Epoch 10] Training Batch [68/391]: Loss 0.3516465127468109\n",
      "[Epoch 10] Training Batch [69/391]: Loss 0.5202385783195496\n",
      "[Epoch 10] Training Batch [70/391]: Loss 0.4649754762649536\n",
      "[Epoch 10] Training Batch [71/391]: Loss 0.5380564332008362\n",
      "[Epoch 10] Training Batch [72/391]: Loss 0.49204790592193604\n",
      "[Epoch 10] Training Batch [73/391]: Loss 0.4963163137435913\n",
      "[Epoch 10] Training Batch [74/391]: Loss 0.5389602184295654\n",
      "[Epoch 10] Training Batch [75/391]: Loss 0.4982150197029114\n",
      "[Epoch 10] Training Batch [76/391]: Loss 0.40062955021858215\n",
      "[Epoch 10] Training Batch [77/391]: Loss 0.5155884027481079\n",
      "[Epoch 10] Training Batch [78/391]: Loss 0.6211450099945068\n",
      "[Epoch 10] Training Batch [79/391]: Loss 0.5946539044380188\n",
      "[Epoch 10] Training Batch [80/391]: Loss 0.4309781491756439\n",
      "[Epoch 10] Training Batch [81/391]: Loss 0.3922343850135803\n",
      "[Epoch 10] Training Batch [82/391]: Loss 0.5951070785522461\n",
      "[Epoch 10] Training Batch [83/391]: Loss 0.5606809258460999\n",
      "[Epoch 10] Training Batch [84/391]: Loss 0.47828325629234314\n",
      "[Epoch 10] Training Batch [85/391]: Loss 0.5565440654754639\n",
      "[Epoch 10] Training Batch [86/391]: Loss 0.5685462951660156\n",
      "[Epoch 10] Training Batch [87/391]: Loss 0.5486874580383301\n",
      "[Epoch 10] Training Batch [88/391]: Loss 0.5146908164024353\n",
      "[Epoch 10] Training Batch [89/391]: Loss 0.6000798940658569\n",
      "[Epoch 10] Training Batch [90/391]: Loss 0.49453482031822205\n",
      "[Epoch 10] Training Batch [91/391]: Loss 0.6956906318664551\n",
      "[Epoch 10] Training Batch [92/391]: Loss 0.5344309210777283\n",
      "[Epoch 10] Training Batch [93/391]: Loss 0.5622357130050659\n",
      "[Epoch 10] Training Batch [94/391]: Loss 0.3913101851940155\n",
      "[Epoch 10] Training Batch [95/391]: Loss 0.4906903803348541\n",
      "[Epoch 10] Training Batch [96/391]: Loss 0.6209757924079895\n",
      "[Epoch 10] Training Batch [97/391]: Loss 0.5161420702934265\n",
      "[Epoch 10] Training Batch [98/391]: Loss 0.4988674521446228\n",
      "[Epoch 10] Training Batch [99/391]: Loss 0.46317628026008606\n",
      "[Epoch 10] Training Batch [100/391]: Loss 0.4659348130226135\n",
      "[Epoch 10] Training Batch [101/391]: Loss 0.5470972061157227\n",
      "[Epoch 10] Training Batch [102/391]: Loss 0.5011395215988159\n",
      "[Epoch 10] Training Batch [103/391]: Loss 0.45600655674934387\n",
      "[Epoch 10] Training Batch [104/391]: Loss 0.5613077878952026\n",
      "[Epoch 10] Training Batch [105/391]: Loss 0.4767487645149231\n",
      "[Epoch 10] Training Batch [106/391]: Loss 0.46698659658432007\n",
      "[Epoch 10] Training Batch [107/391]: Loss 0.4103819727897644\n",
      "[Epoch 10] Training Batch [108/391]: Loss 0.5831631422042847\n",
      "[Epoch 10] Training Batch [109/391]: Loss 0.5594362616539001\n",
      "[Epoch 10] Training Batch [110/391]: Loss 0.45525410771369934\n",
      "[Epoch 10] Training Batch [111/391]: Loss 0.49914395809173584\n",
      "[Epoch 10] Training Batch [112/391]: Loss 0.5207827091217041\n",
      "[Epoch 10] Training Batch [113/391]: Loss 0.43743810057640076\n",
      "[Epoch 10] Training Batch [114/391]: Loss 0.4313971996307373\n",
      "[Epoch 10] Training Batch [115/391]: Loss 0.4283697009086609\n",
      "[Epoch 10] Training Batch [116/391]: Loss 0.5476580858230591\n",
      "[Epoch 10] Training Batch [117/391]: Loss 0.4667665362358093\n",
      "[Epoch 10] Training Batch [118/391]: Loss 0.5442087054252625\n",
      "[Epoch 10] Training Batch [119/391]: Loss 0.4769541323184967\n",
      "[Epoch 10] Training Batch [120/391]: Loss 0.41755959391593933\n",
      "[Epoch 10] Training Batch [121/391]: Loss 0.44124075770378113\n",
      "[Epoch 10] Training Batch [122/391]: Loss 0.5405247807502747\n",
      "[Epoch 10] Training Batch [123/391]: Loss 0.5612415075302124\n",
      "[Epoch 10] Training Batch [124/391]: Loss 0.5409804582595825\n",
      "[Epoch 10] Training Batch [125/391]: Loss 0.4587564170360565\n",
      "[Epoch 10] Training Batch [126/391]: Loss 0.4664158821105957\n",
      "[Epoch 10] Training Batch [127/391]: Loss 0.488415002822876\n",
      "[Epoch 10] Training Batch [128/391]: Loss 0.5133352279663086\n",
      "[Epoch 10] Training Batch [129/391]: Loss 0.49728482961654663\n",
      "[Epoch 10] Training Batch [130/391]: Loss 0.5205003619194031\n",
      "[Epoch 10] Training Batch [131/391]: Loss 0.5512590408325195\n",
      "[Epoch 10] Training Batch [132/391]: Loss 0.6130968332290649\n",
      "[Epoch 10] Training Batch [133/391]: Loss 0.521541178226471\n",
      "[Epoch 10] Training Batch [134/391]: Loss 0.4379998445510864\n",
      "[Epoch 10] Training Batch [135/391]: Loss 0.5605449676513672\n",
      "[Epoch 10] Training Batch [136/391]: Loss 0.5613142848014832\n",
      "[Epoch 10] Training Batch [137/391]: Loss 0.48010632395744324\n",
      "[Epoch 10] Training Batch [138/391]: Loss 0.6094369888305664\n",
      "[Epoch 10] Training Batch [139/391]: Loss 0.6117708683013916\n",
      "[Epoch 10] Training Batch [140/391]: Loss 0.5393412709236145\n",
      "[Epoch 10] Training Batch [141/391]: Loss 0.530454695224762\n",
      "[Epoch 10] Training Batch [142/391]: Loss 0.44358816742897034\n",
      "[Epoch 10] Training Batch [143/391]: Loss 0.5579321384429932\n",
      "[Epoch 10] Training Batch [144/391]: Loss 0.4765849709510803\n",
      "[Epoch 10] Training Batch [145/391]: Loss 0.7040985226631165\n",
      "[Epoch 10] Training Batch [146/391]: Loss 0.604952335357666\n",
      "[Epoch 10] Training Batch [147/391]: Loss 0.3819535970687866\n",
      "[Epoch 10] Training Batch [148/391]: Loss 0.4514484703540802\n",
      "[Epoch 10] Training Batch [149/391]: Loss 0.619498074054718\n",
      "[Epoch 10] Training Batch [150/391]: Loss 0.6091013550758362\n",
      "[Epoch 10] Training Batch [151/391]: Loss 0.40267932415008545\n",
      "[Epoch 10] Training Batch [152/391]: Loss 0.5880271792411804\n",
      "[Epoch 10] Training Batch [153/391]: Loss 0.4626595973968506\n",
      "[Epoch 10] Training Batch [154/391]: Loss 0.388120174407959\n",
      "[Epoch 10] Training Batch [155/391]: Loss 0.4450407028198242\n",
      "[Epoch 10] Training Batch [156/391]: Loss 0.6093115210533142\n",
      "[Epoch 10] Training Batch [157/391]: Loss 0.695894718170166\n",
      "[Epoch 10] Training Batch [158/391]: Loss 0.5182608962059021\n",
      "[Epoch 10] Training Batch [159/391]: Loss 0.43562477827072144\n",
      "[Epoch 10] Training Batch [160/391]: Loss 0.5132602453231812\n",
      "[Epoch 10] Training Batch [161/391]: Loss 0.6368035078048706\n",
      "[Epoch 10] Training Batch [162/391]: Loss 0.7338371276855469\n",
      "[Epoch 10] Training Batch [163/391]: Loss 0.579927384853363\n",
      "[Epoch 10] Training Batch [164/391]: Loss 0.5626517534255981\n",
      "[Epoch 10] Training Batch [165/391]: Loss 0.47792118787765503\n",
      "[Epoch 10] Training Batch [166/391]: Loss 0.4401121139526367\n",
      "[Epoch 10] Training Batch [167/391]: Loss 0.506752073764801\n",
      "[Epoch 10] Training Batch [168/391]: Loss 0.5388103723526001\n",
      "[Epoch 10] Training Batch [169/391]: Loss 0.5225645303726196\n",
      "[Epoch 10] Training Batch [170/391]: Loss 0.5899347066879272\n",
      "[Epoch 10] Training Batch [171/391]: Loss 0.578947901725769\n",
      "[Epoch 10] Training Batch [172/391]: Loss 0.6173564791679382\n",
      "[Epoch 10] Training Batch [173/391]: Loss 0.6635276675224304\n",
      "[Epoch 10] Training Batch [174/391]: Loss 0.4673253297805786\n",
      "[Epoch 10] Training Batch [175/391]: Loss 0.59546959400177\n",
      "[Epoch 10] Training Batch [176/391]: Loss 0.446956604719162\n",
      "[Epoch 10] Training Batch [177/391]: Loss 0.49666133522987366\n",
      "[Epoch 10] Training Batch [178/391]: Loss 0.5689920783042908\n",
      "[Epoch 10] Training Batch [179/391]: Loss 0.47253191471099854\n",
      "[Epoch 10] Training Batch [180/391]: Loss 0.5778487920761108\n",
      "[Epoch 10] Training Batch [181/391]: Loss 0.6628482341766357\n",
      "[Epoch 10] Training Batch [182/391]: Loss 0.6193993091583252\n",
      "[Epoch 10] Training Batch [183/391]: Loss 0.47885745763778687\n",
      "[Epoch 10] Training Batch [184/391]: Loss 0.42847052216529846\n",
      "[Epoch 10] Training Batch [185/391]: Loss 0.4841996431350708\n",
      "[Epoch 10] Training Batch [186/391]: Loss 0.44125592708587646\n",
      "[Epoch 10] Training Batch [187/391]: Loss 0.4624488055706024\n",
      "[Epoch 10] Training Batch [188/391]: Loss 0.5864795446395874\n",
      "[Epoch 10] Training Batch [189/391]: Loss 0.6065252423286438\n",
      "[Epoch 10] Training Batch [190/391]: Loss 0.5788546204566956\n",
      "[Epoch 10] Training Batch [191/391]: Loss 0.4075440466403961\n",
      "[Epoch 10] Training Batch [192/391]: Loss 0.4396813213825226\n",
      "[Epoch 10] Training Batch [193/391]: Loss 0.5025684833526611\n",
      "[Epoch 10] Training Batch [194/391]: Loss 0.6028361320495605\n",
      "[Epoch 10] Training Batch [195/391]: Loss 0.6003263592720032\n",
      "[Epoch 10] Training Batch [196/391]: Loss 0.5062436461448669\n",
      "[Epoch 10] Training Batch [197/391]: Loss 0.4965875744819641\n",
      "[Epoch 10] Training Batch [198/391]: Loss 0.506036639213562\n",
      "[Epoch 10] Training Batch [199/391]: Loss 0.4616801142692566\n",
      "[Epoch 10] Training Batch [200/391]: Loss 0.5702235102653503\n",
      "[Epoch 10] Training Batch [201/391]: Loss 0.471793532371521\n",
      "[Epoch 10] Training Batch [202/391]: Loss 0.6452072262763977\n",
      "[Epoch 10] Training Batch [203/391]: Loss 0.6094783544540405\n",
      "[Epoch 10] Training Batch [204/391]: Loss 0.6924129724502563\n",
      "[Epoch 10] Training Batch [205/391]: Loss 0.6297074556350708\n",
      "[Epoch 10] Training Batch [206/391]: Loss 0.6234061121940613\n",
      "[Epoch 10] Training Batch [207/391]: Loss 0.578552782535553\n",
      "[Epoch 10] Training Batch [208/391]: Loss 0.5794498324394226\n",
      "[Epoch 10] Training Batch [209/391]: Loss 0.5679969191551208\n",
      "[Epoch 10] Training Batch [210/391]: Loss 0.7681938409805298\n",
      "[Epoch 10] Training Batch [211/391]: Loss 0.6407269835472107\n",
      "[Epoch 10] Training Batch [212/391]: Loss 0.6201018691062927\n",
      "[Epoch 10] Training Batch [213/391]: Loss 0.6644501090049744\n",
      "[Epoch 10] Training Batch [214/391]: Loss 0.5217919945716858\n",
      "[Epoch 10] Training Batch [215/391]: Loss 0.47417140007019043\n",
      "[Epoch 10] Training Batch [216/391]: Loss 0.6685933470726013\n",
      "[Epoch 10] Training Batch [217/391]: Loss 0.6817438006401062\n",
      "[Epoch 10] Training Batch [218/391]: Loss 0.6802284717559814\n",
      "[Epoch 10] Training Batch [219/391]: Loss 0.6022138595581055\n",
      "[Epoch 10] Training Batch [220/391]: Loss 0.5920014381408691\n",
      "[Epoch 10] Training Batch [221/391]: Loss 0.4733390808105469\n",
      "[Epoch 10] Training Batch [222/391]: Loss 0.6086984276771545\n",
      "[Epoch 10] Training Batch [223/391]: Loss 0.5617774128913879\n",
      "[Epoch 10] Training Batch [224/391]: Loss 0.5251720547676086\n",
      "[Epoch 10] Training Batch [225/391]: Loss 0.4850618839263916\n",
      "[Epoch 10] Training Batch [226/391]: Loss 0.572741687297821\n",
      "[Epoch 10] Training Batch [227/391]: Loss 0.6182191967964172\n",
      "[Epoch 10] Training Batch [228/391]: Loss 0.524497389793396\n",
      "[Epoch 10] Training Batch [229/391]: Loss 0.7505043745040894\n",
      "[Epoch 10] Training Batch [230/391]: Loss 0.5384395718574524\n",
      "[Epoch 10] Training Batch [231/391]: Loss 0.48935845494270325\n",
      "[Epoch 10] Training Batch [232/391]: Loss 0.45031261444091797\n",
      "[Epoch 10] Training Batch [233/391]: Loss 0.4892770051956177\n",
      "[Epoch 10] Training Batch [234/391]: Loss 0.7166369557380676\n",
      "[Epoch 10] Training Batch [235/391]: Loss 0.4893503785133362\n",
      "[Epoch 10] Training Batch [236/391]: Loss 0.5425330996513367\n",
      "[Epoch 10] Training Batch [237/391]: Loss 0.6487688422203064\n",
      "[Epoch 10] Training Batch [238/391]: Loss 0.6442683935165405\n",
      "[Epoch 10] Training Batch [239/391]: Loss 0.5338754653930664\n",
      "[Epoch 10] Training Batch [240/391]: Loss 0.4839733839035034\n",
      "[Epoch 10] Training Batch [241/391]: Loss 0.5305622816085815\n",
      "[Epoch 10] Training Batch [242/391]: Loss 0.5718485116958618\n",
      "[Epoch 10] Training Batch [243/391]: Loss 0.4000314474105835\n",
      "[Epoch 10] Training Batch [244/391]: Loss 0.6251665353775024\n",
      "[Epoch 10] Training Batch [245/391]: Loss 0.6088024973869324\n",
      "[Epoch 10] Training Batch [246/391]: Loss 0.46875426173210144\n",
      "[Epoch 10] Training Batch [247/391]: Loss 0.6098799705505371\n",
      "[Epoch 10] Training Batch [248/391]: Loss 0.5245291590690613\n",
      "[Epoch 10] Training Batch [249/391]: Loss 0.5071536898612976\n",
      "[Epoch 10] Training Batch [250/391]: Loss 0.5578654408454895\n",
      "[Epoch 10] Training Batch [251/391]: Loss 0.5327637195587158\n",
      "[Epoch 10] Training Batch [252/391]: Loss 0.564816415309906\n",
      "[Epoch 10] Training Batch [253/391]: Loss 0.6352745890617371\n",
      "[Epoch 10] Training Batch [254/391]: Loss 0.6167494058609009\n",
      "[Epoch 10] Training Batch [255/391]: Loss 0.6581529378890991\n",
      "[Epoch 10] Training Batch [256/391]: Loss 0.4352756142616272\n",
      "[Epoch 10] Training Batch [257/391]: Loss 0.5380431413650513\n",
      "[Epoch 10] Training Batch [258/391]: Loss 0.49427127838134766\n",
      "[Epoch 10] Training Batch [259/391]: Loss 0.6435563564300537\n",
      "[Epoch 10] Training Batch [260/391]: Loss 0.6842669248580933\n",
      "[Epoch 10] Training Batch [261/391]: Loss 0.5008918642997742\n",
      "[Epoch 10] Training Batch [262/391]: Loss 0.4626554548740387\n",
      "[Epoch 10] Training Batch [263/391]: Loss 0.5794132351875305\n",
      "[Epoch 10] Training Batch [264/391]: Loss 0.5194292664527893\n",
      "[Epoch 10] Training Batch [265/391]: Loss 0.5638147592544556\n",
      "[Epoch 10] Training Batch [266/391]: Loss 0.592870831489563\n",
      "[Epoch 10] Training Batch [267/391]: Loss 0.4931720793247223\n",
      "[Epoch 10] Training Batch [268/391]: Loss 0.441956490278244\n",
      "[Epoch 10] Training Batch [269/391]: Loss 0.6297121047973633\n",
      "[Epoch 10] Training Batch [270/391]: Loss 0.5501848459243774\n",
      "[Epoch 10] Training Batch [271/391]: Loss 0.5312781929969788\n",
      "[Epoch 10] Training Batch [272/391]: Loss 0.7092609405517578\n",
      "[Epoch 10] Training Batch [273/391]: Loss 0.5912725925445557\n",
      "[Epoch 10] Training Batch [274/391]: Loss 0.4904699921607971\n",
      "[Epoch 10] Training Batch [275/391]: Loss 0.635606586933136\n",
      "[Epoch 10] Training Batch [276/391]: Loss 0.5398715138435364\n",
      "[Epoch 10] Training Batch [277/391]: Loss 0.6032120585441589\n",
      "[Epoch 10] Training Batch [278/391]: Loss 0.5799564123153687\n",
      "[Epoch 10] Training Batch [279/391]: Loss 0.4963335394859314\n",
      "[Epoch 10] Training Batch [280/391]: Loss 0.6508538722991943\n",
      "[Epoch 10] Training Batch [281/391]: Loss 0.5628355741500854\n",
      "[Epoch 10] Training Batch [282/391]: Loss 0.6161665320396423\n",
      "[Epoch 10] Training Batch [283/391]: Loss 0.4724220931529999\n",
      "[Epoch 10] Training Batch [284/391]: Loss 0.5511859059333801\n",
      "[Epoch 10] Training Batch [285/391]: Loss 0.5602803230285645\n",
      "[Epoch 10] Training Batch [286/391]: Loss 0.5757941007614136\n",
      "[Epoch 10] Training Batch [287/391]: Loss 0.48307204246520996\n",
      "[Epoch 10] Training Batch [288/391]: Loss 0.6772980093955994\n",
      "[Epoch 10] Training Batch [289/391]: Loss 0.5319735407829285\n",
      "[Epoch 10] Training Batch [290/391]: Loss 0.5742968320846558\n",
      "[Epoch 10] Training Batch [291/391]: Loss 0.5092042088508606\n",
      "[Epoch 10] Training Batch [292/391]: Loss 0.5777977108955383\n",
      "[Epoch 10] Training Batch [293/391]: Loss 0.5572028160095215\n",
      "[Epoch 10] Training Batch [294/391]: Loss 0.466816246509552\n",
      "[Epoch 10] Training Batch [295/391]: Loss 0.613801896572113\n",
      "[Epoch 10] Training Batch [296/391]: Loss 0.5737499594688416\n",
      "[Epoch 10] Training Batch [297/391]: Loss 0.5079790949821472\n",
      "[Epoch 10] Training Batch [298/391]: Loss 0.5703833103179932\n",
      "[Epoch 10] Training Batch [299/391]: Loss 0.5351788401603699\n",
      "[Epoch 10] Training Batch [300/391]: Loss 0.5753993391990662\n",
      "[Epoch 10] Training Batch [301/391]: Loss 0.6367599368095398\n",
      "[Epoch 10] Training Batch [302/391]: Loss 0.4865783452987671\n",
      "[Epoch 10] Training Batch [303/391]: Loss 0.48213768005371094\n",
      "[Epoch 10] Training Batch [304/391]: Loss 0.6122556924819946\n",
      "[Epoch 10] Training Batch [305/391]: Loss 0.6381270885467529\n",
      "[Epoch 10] Training Batch [306/391]: Loss 0.6895580887794495\n",
      "[Epoch 10] Training Batch [307/391]: Loss 0.4903503656387329\n",
      "[Epoch 10] Training Batch [308/391]: Loss 0.6992760300636292\n",
      "[Epoch 10] Training Batch [309/391]: Loss 0.5012367367744446\n",
      "[Epoch 10] Training Batch [310/391]: Loss 0.5368024110794067\n",
      "[Epoch 10] Training Batch [311/391]: Loss 0.6395286917686462\n",
      "[Epoch 10] Training Batch [312/391]: Loss 0.5685226917266846\n",
      "[Epoch 10] Training Batch [313/391]: Loss 0.5637695789337158\n",
      "[Epoch 10] Training Batch [314/391]: Loss 0.6932945251464844\n",
      "[Epoch 10] Training Batch [315/391]: Loss 0.74852454662323\n",
      "[Epoch 10] Training Batch [316/391]: Loss 0.7977150082588196\n",
      "[Epoch 10] Training Batch [317/391]: Loss 0.5353683233261108\n",
      "[Epoch 10] Training Batch [318/391]: Loss 0.6478829979896545\n",
      "[Epoch 10] Training Batch [319/391]: Loss 0.748048722743988\n",
      "[Epoch 10] Training Batch [320/391]: Loss 0.5353052020072937\n",
      "[Epoch 10] Training Batch [321/391]: Loss 0.5893943309783936\n",
      "[Epoch 10] Training Batch [322/391]: Loss 0.5384654402732849\n",
      "[Epoch 10] Training Batch [323/391]: Loss 0.5256988406181335\n",
      "[Epoch 10] Training Batch [324/391]: Loss 0.5838286876678467\n",
      "[Epoch 10] Training Batch [325/391]: Loss 0.4891461730003357\n",
      "[Epoch 10] Training Batch [326/391]: Loss 0.597454309463501\n",
      "[Epoch 10] Training Batch [327/391]: Loss 0.6051303148269653\n",
      "[Epoch 10] Training Batch [328/391]: Loss 0.5617586970329285\n",
      "[Epoch 10] Training Batch [329/391]: Loss 0.6551085710525513\n",
      "[Epoch 10] Training Batch [330/391]: Loss 0.609106183052063\n",
      "[Epoch 10] Training Batch [331/391]: Loss 0.6953821182250977\n",
      "[Epoch 10] Training Batch [332/391]: Loss 0.5788705945014954\n",
      "[Epoch 10] Training Batch [333/391]: Loss 0.540637731552124\n",
      "[Epoch 10] Training Batch [334/391]: Loss 0.6067189574241638\n",
      "[Epoch 10] Training Batch [335/391]: Loss 0.6408101916313171\n",
      "[Epoch 10] Training Batch [336/391]: Loss 0.6474173069000244\n",
      "[Epoch 10] Training Batch [337/391]: Loss 0.7006373405456543\n",
      "[Epoch 10] Training Batch [338/391]: Loss 0.5424853563308716\n",
      "[Epoch 10] Training Batch [339/391]: Loss 0.5496568083763123\n",
      "[Epoch 10] Training Batch [340/391]: Loss 0.5847315192222595\n",
      "[Epoch 10] Training Batch [341/391]: Loss 0.6482052803039551\n",
      "[Epoch 10] Training Batch [342/391]: Loss 0.6122319102287292\n",
      "[Epoch 10] Training Batch [343/391]: Loss 0.5913528203964233\n",
      "[Epoch 10] Training Batch [344/391]: Loss 0.45845311880111694\n",
      "[Epoch 10] Training Batch [345/391]: Loss 0.6065360307693481\n",
      "[Epoch 10] Training Batch [346/391]: Loss 0.48720940947532654\n",
      "[Epoch 10] Training Batch [347/391]: Loss 0.4916767179965973\n",
      "[Epoch 10] Training Batch [348/391]: Loss 0.6058621406555176\n",
      "[Epoch 10] Training Batch [349/391]: Loss 0.6277613043785095\n",
      "[Epoch 10] Training Batch [350/391]: Loss 0.6653618812561035\n",
      "[Epoch 10] Training Batch [351/391]: Loss 0.6879788041114807\n",
      "[Epoch 10] Training Batch [352/391]: Loss 0.41911935806274414\n",
      "[Epoch 10] Training Batch [353/391]: Loss 0.5558459758758545\n",
      "[Epoch 10] Training Batch [354/391]: Loss 0.6594049334526062\n",
      "[Epoch 10] Training Batch [355/391]: Loss 0.6014755964279175\n",
      "[Epoch 10] Training Batch [356/391]: Loss 0.5252973437309265\n",
      "[Epoch 10] Training Batch [357/391]: Loss 0.6350789070129395\n",
      "[Epoch 10] Training Batch [358/391]: Loss 0.42620551586151123\n",
      "[Epoch 10] Training Batch [359/391]: Loss 0.45707789063453674\n",
      "[Epoch 10] Training Batch [360/391]: Loss 0.4900974929332733\n",
      "[Epoch 10] Training Batch [361/391]: Loss 0.6065167188644409\n",
      "[Epoch 10] Training Batch [362/391]: Loss 0.48062506318092346\n",
      "[Epoch 10] Training Batch [363/391]: Loss 0.5391414165496826\n",
      "[Epoch 10] Training Batch [364/391]: Loss 0.5759186148643494\n",
      "[Epoch 10] Training Batch [365/391]: Loss 0.5335999727249146\n",
      "[Epoch 10] Training Batch [366/391]: Loss 0.5667309165000916\n",
      "[Epoch 10] Training Batch [367/391]: Loss 0.6110115051269531\n",
      "[Epoch 10] Training Batch [368/391]: Loss 0.4670546352863312\n",
      "[Epoch 10] Training Batch [369/391]: Loss 0.5544058680534363\n",
      "[Epoch 10] Training Batch [370/391]: Loss 0.6522637605667114\n",
      "[Epoch 10] Training Batch [371/391]: Loss 0.5043865442276001\n",
      "[Epoch 10] Training Batch [372/391]: Loss 0.5912047028541565\n",
      "[Epoch 10] Training Batch [373/391]: Loss 0.5369482040405273\n",
      "[Epoch 10] Training Batch [374/391]: Loss 0.566129744052887\n",
      "[Epoch 10] Training Batch [375/391]: Loss 0.538251519203186\n",
      "[Epoch 10] Training Batch [376/391]: Loss 0.5322911143302917\n",
      "[Epoch 10] Training Batch [377/391]: Loss 0.5898939967155457\n",
      "[Epoch 10] Training Batch [378/391]: Loss 0.5489301681518555\n",
      "[Epoch 10] Training Batch [379/391]: Loss 0.5123134255409241\n",
      "[Epoch 10] Training Batch [380/391]: Loss 0.6760110259056091\n",
      "[Epoch 10] Training Batch [381/391]: Loss 0.5297582745552063\n",
      "[Epoch 10] Training Batch [382/391]: Loss 0.4603055715560913\n",
      "[Epoch 10] Training Batch [383/391]: Loss 0.6274026036262512\n",
      "[Epoch 10] Training Batch [384/391]: Loss 0.6397958993911743\n",
      "[Epoch 10] Training Batch [385/391]: Loss 0.47846364974975586\n",
      "[Epoch 10] Training Batch [386/391]: Loss 0.5425902009010315\n",
      "[Epoch 10] Training Batch [387/391]: Loss 0.6815729737281799\n",
      "[Epoch 10] Training Batch [388/391]: Loss 0.644069254398346\n",
      "[Epoch 10] Training Batch [389/391]: Loss 0.6891103386878967\n",
      "[Epoch 10] Training Batch [390/391]: Loss 0.679261326789856\n",
      "[Epoch 10] Training Batch [391/391]: Loss 0.49842071533203125\n",
      "Epoch 10 - Train Loss: 0.5442\n",
      "*********  Epoch 11/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Training Batch [1/391]: Loss 0.36584192514419556\n",
      "[Epoch 11] Training Batch [2/391]: Loss 0.4197249710559845\n",
      "[Epoch 11] Training Batch [3/391]: Loss 0.3881753385066986\n",
      "[Epoch 11] Training Batch [4/391]: Loss 0.3751499056816101\n",
      "[Epoch 11] Training Batch [5/391]: Loss 0.4094559848308563\n",
      "[Epoch 11] Training Batch [6/391]: Loss 0.3656691014766693\n",
      "[Epoch 11] Training Batch [7/391]: Loss 0.5077350735664368\n",
      "[Epoch 11] Training Batch [8/391]: Loss 0.42855215072631836\n",
      "[Epoch 11] Training Batch [9/391]: Loss 0.4542821943759918\n",
      "[Epoch 11] Training Batch [10/391]: Loss 0.4378395974636078\n",
      "[Epoch 11] Training Batch [11/391]: Loss 0.43011584877967834\n",
      "[Epoch 11] Training Batch [12/391]: Loss 0.4617672264575958\n",
      "[Epoch 11] Training Batch [13/391]: Loss 0.26238569617271423\n",
      "[Epoch 11] Training Batch [14/391]: Loss 0.3477122485637665\n",
      "[Epoch 11] Training Batch [15/391]: Loss 0.37194448709487915\n",
      "[Epoch 11] Training Batch [16/391]: Loss 0.23598822951316833\n",
      "[Epoch 11] Training Batch [17/391]: Loss 0.3725527822971344\n",
      "[Epoch 11] Training Batch [18/391]: Loss 0.36857661604881287\n",
      "[Epoch 11] Training Batch [19/391]: Loss 0.45768678188323975\n",
      "[Epoch 11] Training Batch [20/391]: Loss 0.28431448340415955\n",
      "[Epoch 11] Training Batch [21/391]: Loss 0.38636356592178345\n",
      "[Epoch 11] Training Batch [22/391]: Loss 0.296051025390625\n",
      "[Epoch 11] Training Batch [23/391]: Loss 0.29709291458129883\n",
      "[Epoch 11] Training Batch [24/391]: Loss 0.3142849802970886\n",
      "[Epoch 11] Training Batch [25/391]: Loss 0.3471035659313202\n",
      "[Epoch 11] Training Batch [26/391]: Loss 0.38167256116867065\n",
      "[Epoch 11] Training Batch [27/391]: Loss 0.2893243730068207\n",
      "[Epoch 11] Training Batch [28/391]: Loss 0.37856534123420715\n",
      "[Epoch 11] Training Batch [29/391]: Loss 0.34621894359588623\n",
      "[Epoch 11] Training Batch [30/391]: Loss 0.4034876823425293\n",
      "[Epoch 11] Training Batch [31/391]: Loss 0.31432461738586426\n",
      "[Epoch 11] Training Batch [32/391]: Loss 0.3951851427555084\n",
      "[Epoch 11] Training Batch [33/391]: Loss 0.4128776788711548\n",
      "[Epoch 11] Training Batch [34/391]: Loss 0.3459811210632324\n",
      "[Epoch 11] Training Batch [35/391]: Loss 0.2811850905418396\n",
      "[Epoch 11] Training Batch [36/391]: Loss 0.33839577436447144\n",
      "[Epoch 11] Training Batch [37/391]: Loss 0.3182213604450226\n",
      "[Epoch 11] Training Batch [38/391]: Loss 0.2851196825504303\n",
      "[Epoch 11] Training Batch [39/391]: Loss 0.3573864996433258\n",
      "[Epoch 11] Training Batch [40/391]: Loss 0.2664685845375061\n",
      "[Epoch 11] Training Batch [41/391]: Loss 0.36686450242996216\n",
      "[Epoch 11] Training Batch [42/391]: Loss 0.40154480934143066\n",
      "[Epoch 11] Training Batch [43/391]: Loss 0.2874833345413208\n",
      "[Epoch 11] Training Batch [44/391]: Loss 0.3827084004878998\n",
      "[Epoch 11] Training Batch [45/391]: Loss 0.3296765089035034\n",
      "[Epoch 11] Training Batch [46/391]: Loss 0.3400973379611969\n",
      "[Epoch 11] Training Batch [47/391]: Loss 0.33962059020996094\n",
      "[Epoch 11] Training Batch [48/391]: Loss 0.33307087421417236\n",
      "[Epoch 11] Training Batch [49/391]: Loss 0.30939745903015137\n",
      "[Epoch 11] Training Batch [50/391]: Loss 0.3619632422924042\n",
      "[Epoch 11] Training Batch [51/391]: Loss 0.29927459359169006\n",
      "[Epoch 11] Training Batch [52/391]: Loss 0.36237266659736633\n",
      "[Epoch 11] Training Batch [53/391]: Loss 0.4042400121688843\n",
      "[Epoch 11] Training Batch [54/391]: Loss 0.3399522006511688\n",
      "[Epoch 11] Training Batch [55/391]: Loss 0.3653244376182556\n",
      "[Epoch 11] Training Batch [56/391]: Loss 0.42472267150878906\n",
      "[Epoch 11] Training Batch [57/391]: Loss 0.44406259059906006\n",
      "[Epoch 11] Training Batch [58/391]: Loss 0.395812451839447\n",
      "[Epoch 11] Training Batch [59/391]: Loss 0.3124050498008728\n",
      "[Epoch 11] Training Batch [60/391]: Loss 0.2113979160785675\n",
      "[Epoch 11] Training Batch [61/391]: Loss 0.26501354575157166\n",
      "[Epoch 11] Training Batch [62/391]: Loss 0.4672905504703522\n",
      "[Epoch 11] Training Batch [63/391]: Loss 0.2737219035625458\n",
      "[Epoch 11] Training Batch [64/391]: Loss 0.3130384087562561\n",
      "[Epoch 11] Training Batch [65/391]: Loss 0.43567192554473877\n",
      "[Epoch 11] Training Batch [66/391]: Loss 0.3112632632255554\n",
      "[Epoch 11] Training Batch [67/391]: Loss 0.3307490646839142\n",
      "[Epoch 11] Training Batch [68/391]: Loss 0.4287237226963043\n",
      "[Epoch 11] Training Batch [69/391]: Loss 0.3379177749156952\n",
      "[Epoch 11] Training Batch [70/391]: Loss 0.4197423458099365\n",
      "[Epoch 11] Training Batch [71/391]: Loss 0.3837694227695465\n",
      "[Epoch 11] Training Batch [72/391]: Loss 0.3523789048194885\n",
      "[Epoch 11] Training Batch [73/391]: Loss 0.3766697347164154\n",
      "[Epoch 11] Training Batch [74/391]: Loss 0.38758420944213867\n",
      "[Epoch 11] Training Batch [75/391]: Loss 0.35686618089675903\n",
      "[Epoch 11] Training Batch [76/391]: Loss 0.3648587465286255\n",
      "[Epoch 11] Training Batch [77/391]: Loss 0.3863796889781952\n",
      "[Epoch 11] Training Batch [78/391]: Loss 0.36688071489334106\n",
      "[Epoch 11] Training Batch [79/391]: Loss 0.4131416976451874\n",
      "[Epoch 11] Training Batch [80/391]: Loss 0.34864547848701477\n",
      "[Epoch 11] Training Batch [81/391]: Loss 0.3023020625114441\n",
      "[Epoch 11] Training Batch [82/391]: Loss 0.3830680251121521\n",
      "[Epoch 11] Training Batch [83/391]: Loss 0.39964544773101807\n",
      "[Epoch 11] Training Batch [84/391]: Loss 0.39651715755462646\n",
      "[Epoch 11] Training Batch [85/391]: Loss 0.29255422949790955\n",
      "[Epoch 11] Training Batch [86/391]: Loss 0.4588887095451355\n",
      "[Epoch 11] Training Batch [87/391]: Loss 0.36459365487098694\n",
      "[Epoch 11] Training Batch [88/391]: Loss 0.330469012260437\n",
      "[Epoch 11] Training Batch [89/391]: Loss 0.35602712631225586\n",
      "[Epoch 11] Training Batch [90/391]: Loss 0.3668428361415863\n",
      "[Epoch 11] Training Batch [91/391]: Loss 0.3583117127418518\n",
      "[Epoch 11] Training Batch [92/391]: Loss 0.2835301458835602\n",
      "[Epoch 11] Training Batch [93/391]: Loss 0.4352661967277527\n",
      "[Epoch 11] Training Batch [94/391]: Loss 0.262881338596344\n",
      "[Epoch 11] Training Batch [95/391]: Loss 0.3593985140323639\n",
      "[Epoch 11] Training Batch [96/391]: Loss 0.45086678862571716\n",
      "[Epoch 11] Training Batch [97/391]: Loss 0.49942296743392944\n",
      "[Epoch 11] Training Batch [98/391]: Loss 0.38738590478897095\n",
      "[Epoch 11] Training Batch [99/391]: Loss 0.5263926386833191\n",
      "[Epoch 11] Training Batch [100/391]: Loss 0.3377332389354706\n",
      "[Epoch 11] Training Batch [101/391]: Loss 0.33299344778060913\n",
      "[Epoch 11] Training Batch [102/391]: Loss 0.42932406067848206\n",
      "[Epoch 11] Training Batch [103/391]: Loss 0.43213850259780884\n",
      "[Epoch 11] Training Batch [104/391]: Loss 0.43061068654060364\n",
      "[Epoch 11] Training Batch [105/391]: Loss 0.4162489175796509\n",
      "[Epoch 11] Training Batch [106/391]: Loss 0.3662068843841553\n",
      "[Epoch 11] Training Batch [107/391]: Loss 0.3863232135772705\n",
      "[Epoch 11] Training Batch [108/391]: Loss 0.354318231344223\n",
      "[Epoch 11] Training Batch [109/391]: Loss 0.4016176164150238\n",
      "[Epoch 11] Training Batch [110/391]: Loss 0.4033874571323395\n",
      "[Epoch 11] Training Batch [111/391]: Loss 0.31441497802734375\n",
      "[Epoch 11] Training Batch [112/391]: Loss 0.2880363464355469\n",
      "[Epoch 11] Training Batch [113/391]: Loss 0.4802134335041046\n",
      "[Epoch 11] Training Batch [114/391]: Loss 0.618407130241394\n",
      "[Epoch 11] Training Batch [115/391]: Loss 0.3071208596229553\n",
      "[Epoch 11] Training Batch [116/391]: Loss 0.29574498534202576\n",
      "[Epoch 11] Training Batch [117/391]: Loss 0.2751837968826294\n",
      "[Epoch 11] Training Batch [118/391]: Loss 0.44092854857444763\n",
      "[Epoch 11] Training Batch [119/391]: Loss 0.2785657048225403\n",
      "[Epoch 11] Training Batch [120/391]: Loss 0.28273066878318787\n",
      "[Epoch 11] Training Batch [121/391]: Loss 0.46294060349464417\n",
      "[Epoch 11] Training Batch [122/391]: Loss 0.341801255941391\n",
      "[Epoch 11] Training Batch [123/391]: Loss 0.3755403757095337\n",
      "[Epoch 11] Training Batch [124/391]: Loss 0.36981552839279175\n",
      "[Epoch 11] Training Batch [125/391]: Loss 0.4480917751789093\n",
      "[Epoch 11] Training Batch [126/391]: Loss 0.39389267563819885\n",
      "[Epoch 11] Training Batch [127/391]: Loss 0.4855380952358246\n",
      "[Epoch 11] Training Batch [128/391]: Loss 0.33508622646331787\n",
      "[Epoch 11] Training Batch [129/391]: Loss 0.35201936960220337\n",
      "[Epoch 11] Training Batch [130/391]: Loss 0.32788246870040894\n",
      "[Epoch 11] Training Batch [131/391]: Loss 0.3044414520263672\n",
      "[Epoch 11] Training Batch [132/391]: Loss 0.3854939341545105\n",
      "[Epoch 11] Training Batch [133/391]: Loss 0.4120029807090759\n",
      "[Epoch 11] Training Batch [134/391]: Loss 0.42046770453453064\n",
      "[Epoch 11] Training Batch [135/391]: Loss 0.3734475374221802\n",
      "[Epoch 11] Training Batch [136/391]: Loss 0.4107598662376404\n",
      "[Epoch 11] Training Batch [137/391]: Loss 0.44030261039733887\n",
      "[Epoch 11] Training Batch [138/391]: Loss 0.44388994574546814\n",
      "[Epoch 11] Training Batch [139/391]: Loss 0.5248650312423706\n",
      "[Epoch 11] Training Batch [140/391]: Loss 0.30994388461112976\n",
      "[Epoch 11] Training Batch [141/391]: Loss 0.3962228298187256\n",
      "[Epoch 11] Training Batch [142/391]: Loss 0.5071314573287964\n",
      "[Epoch 11] Training Batch [143/391]: Loss 0.44303014874458313\n",
      "[Epoch 11] Training Batch [144/391]: Loss 0.3355662524700165\n",
      "[Epoch 11] Training Batch [145/391]: Loss 0.37986862659454346\n",
      "[Epoch 11] Training Batch [146/391]: Loss 0.4033855199813843\n",
      "[Epoch 11] Training Batch [147/391]: Loss 0.477065771818161\n",
      "[Epoch 11] Training Batch [148/391]: Loss 0.35866883397102356\n",
      "[Epoch 11] Training Batch [149/391]: Loss 0.4787219762802124\n",
      "[Epoch 11] Training Batch [150/391]: Loss 0.5051556825637817\n",
      "[Epoch 11] Training Batch [151/391]: Loss 0.3913083076477051\n",
      "[Epoch 11] Training Batch [152/391]: Loss 0.29734766483306885\n",
      "[Epoch 11] Training Batch [153/391]: Loss 0.39799514412879944\n",
      "[Epoch 11] Training Batch [154/391]: Loss 0.3961468040943146\n",
      "[Epoch 11] Training Batch [155/391]: Loss 0.2654089629650116\n",
      "[Epoch 11] Training Batch [156/391]: Loss 0.4467228651046753\n",
      "[Epoch 11] Training Batch [157/391]: Loss 0.30211779475212097\n",
      "[Epoch 11] Training Batch [158/391]: Loss 0.4278784394264221\n",
      "[Epoch 11] Training Batch [159/391]: Loss 0.5410463213920593\n",
      "[Epoch 11] Training Batch [160/391]: Loss 0.2742350697517395\n",
      "[Epoch 11] Training Batch [161/391]: Loss 0.4387832581996918\n",
      "[Epoch 11] Training Batch [162/391]: Loss 0.402185320854187\n",
      "[Epoch 11] Training Batch [163/391]: Loss 0.4290810525417328\n",
      "[Epoch 11] Training Batch [164/391]: Loss 0.36244913935661316\n",
      "[Epoch 11] Training Batch [165/391]: Loss 0.4882238209247589\n",
      "[Epoch 11] Training Batch [166/391]: Loss 0.31797143816947937\n",
      "[Epoch 11] Training Batch [167/391]: Loss 0.3212122917175293\n",
      "[Epoch 11] Training Batch [168/391]: Loss 0.4272095263004303\n",
      "[Epoch 11] Training Batch [169/391]: Loss 0.5153283476829529\n",
      "[Epoch 11] Training Batch [170/391]: Loss 0.4144881069660187\n",
      "[Epoch 11] Training Batch [171/391]: Loss 0.3169214427471161\n",
      "[Epoch 11] Training Batch [172/391]: Loss 0.36053481698036194\n",
      "[Epoch 11] Training Batch [173/391]: Loss 0.3304797410964966\n",
      "[Epoch 11] Training Batch [174/391]: Loss 0.37589311599731445\n",
      "[Epoch 11] Training Batch [175/391]: Loss 0.3586416244506836\n",
      "[Epoch 11] Training Batch [176/391]: Loss 0.43972551822662354\n",
      "[Epoch 11] Training Batch [177/391]: Loss 0.3370892107486725\n",
      "[Epoch 11] Training Batch [178/391]: Loss 0.3685300350189209\n",
      "[Epoch 11] Training Batch [179/391]: Loss 0.41769716143608093\n",
      "[Epoch 11] Training Batch [180/391]: Loss 0.4676317870616913\n",
      "[Epoch 11] Training Batch [181/391]: Loss 0.3885408043861389\n",
      "[Epoch 11] Training Batch [182/391]: Loss 0.42127010226249695\n",
      "[Epoch 11] Training Batch [183/391]: Loss 0.46165528893470764\n",
      "[Epoch 11] Training Batch [184/391]: Loss 0.5673887133598328\n",
      "[Epoch 11] Training Batch [185/391]: Loss 0.4141462743282318\n",
      "[Epoch 11] Training Batch [186/391]: Loss 0.46740415692329407\n",
      "[Epoch 11] Training Batch [187/391]: Loss 0.34293684363365173\n",
      "[Epoch 11] Training Batch [188/391]: Loss 0.4654296338558197\n",
      "[Epoch 11] Training Batch [189/391]: Loss 0.3854559063911438\n",
      "[Epoch 11] Training Batch [190/391]: Loss 0.4139702022075653\n",
      "[Epoch 11] Training Batch [191/391]: Loss 0.40178394317626953\n",
      "[Epoch 11] Training Batch [192/391]: Loss 0.4238460063934326\n",
      "[Epoch 11] Training Batch [193/391]: Loss 0.37240147590637207\n",
      "[Epoch 11] Training Batch [194/391]: Loss 0.35460084676742554\n",
      "[Epoch 11] Training Batch [195/391]: Loss 0.4578593075275421\n",
      "[Epoch 11] Training Batch [196/391]: Loss 0.46483683586120605\n",
      "[Epoch 11] Training Batch [197/391]: Loss 0.2379489541053772\n",
      "[Epoch 11] Training Batch [198/391]: Loss 0.4506723880767822\n",
      "[Epoch 11] Training Batch [199/391]: Loss 0.34558117389678955\n",
      "[Epoch 11] Training Batch [200/391]: Loss 0.5251175165176392\n",
      "[Epoch 11] Training Batch [201/391]: Loss 0.4402220547199249\n",
      "[Epoch 11] Training Batch [202/391]: Loss 0.32203778624534607\n",
      "[Epoch 11] Training Batch [203/391]: Loss 0.3399786353111267\n",
      "[Epoch 11] Training Batch [204/391]: Loss 0.41591838002204895\n",
      "[Epoch 11] Training Batch [205/391]: Loss 0.40135657787323\n",
      "[Epoch 11] Training Batch [206/391]: Loss 0.5132312774658203\n",
      "[Epoch 11] Training Batch [207/391]: Loss 0.3626366853713989\n",
      "[Epoch 11] Training Batch [208/391]: Loss 0.4641258418560028\n",
      "[Epoch 11] Training Batch [209/391]: Loss 0.32473257184028625\n",
      "[Epoch 11] Training Batch [210/391]: Loss 0.3294390141963959\n",
      "[Epoch 11] Training Batch [211/391]: Loss 0.42977315187454224\n",
      "[Epoch 11] Training Batch [212/391]: Loss 0.38630297780036926\n",
      "[Epoch 11] Training Batch [213/391]: Loss 0.45721402764320374\n",
      "[Epoch 11] Training Batch [214/391]: Loss 0.32540127635002136\n",
      "[Epoch 11] Training Batch [215/391]: Loss 0.4467323422431946\n",
      "[Epoch 11] Training Batch [216/391]: Loss 0.4700622856616974\n",
      "[Epoch 11] Training Batch [217/391]: Loss 0.625896155834198\n",
      "[Epoch 11] Training Batch [218/391]: Loss 0.4159635603427887\n",
      "[Epoch 11] Training Batch [219/391]: Loss 0.4552708566188812\n",
      "[Epoch 11] Training Batch [220/391]: Loss 0.3552184998989105\n",
      "[Epoch 11] Training Batch [221/391]: Loss 0.4346194863319397\n",
      "[Epoch 11] Training Batch [222/391]: Loss 0.38100212812423706\n",
      "[Epoch 11] Training Batch [223/391]: Loss 0.5095115900039673\n",
      "[Epoch 11] Training Batch [224/391]: Loss 0.412455290555954\n",
      "[Epoch 11] Training Batch [225/391]: Loss 0.5277561545372009\n",
      "[Epoch 11] Training Batch [226/391]: Loss 0.4793514907360077\n",
      "[Epoch 11] Training Batch [227/391]: Loss 0.4694176912307739\n",
      "[Epoch 11] Training Batch [228/391]: Loss 0.5726453065872192\n",
      "[Epoch 11] Training Batch [229/391]: Loss 0.3897956609725952\n",
      "[Epoch 11] Training Batch [230/391]: Loss 0.46518561244010925\n",
      "[Epoch 11] Training Batch [231/391]: Loss 0.4566269814968109\n",
      "[Epoch 11] Training Batch [232/391]: Loss 0.5134463310241699\n",
      "[Epoch 11] Training Batch [233/391]: Loss 0.48279470205307007\n",
      "[Epoch 11] Training Batch [234/391]: Loss 0.37547796964645386\n",
      "[Epoch 11] Training Batch [235/391]: Loss 0.4708978831768036\n",
      "[Epoch 11] Training Batch [236/391]: Loss 0.4830045998096466\n",
      "[Epoch 11] Training Batch [237/391]: Loss 0.4410392642021179\n",
      "[Epoch 11] Training Batch [238/391]: Loss 0.4241313338279724\n",
      "[Epoch 11] Training Batch [239/391]: Loss 0.4226078987121582\n",
      "[Epoch 11] Training Batch [240/391]: Loss 0.40861430764198303\n",
      "[Epoch 11] Training Batch [241/391]: Loss 0.4258672297000885\n",
      "[Epoch 11] Training Batch [242/391]: Loss 0.4102087914943695\n",
      "[Epoch 11] Training Batch [243/391]: Loss 0.41331246495246887\n",
      "[Epoch 11] Training Batch [244/391]: Loss 0.35892975330352783\n",
      "[Epoch 11] Training Batch [245/391]: Loss 0.41793113946914673\n",
      "[Epoch 11] Training Batch [246/391]: Loss 0.45194852352142334\n",
      "[Epoch 11] Training Batch [247/391]: Loss 0.3899255692958832\n",
      "[Epoch 11] Training Batch [248/391]: Loss 0.3521064221858978\n",
      "[Epoch 11] Training Batch [249/391]: Loss 0.388484925031662\n",
      "[Epoch 11] Training Batch [250/391]: Loss 0.4241122901439667\n",
      "[Epoch 11] Training Batch [251/391]: Loss 0.5034855008125305\n",
      "[Epoch 11] Training Batch [252/391]: Loss 0.41559478640556335\n",
      "[Epoch 11] Training Batch [253/391]: Loss 0.3710116446018219\n",
      "[Epoch 11] Training Batch [254/391]: Loss 0.3716060221195221\n",
      "[Epoch 11] Training Batch [255/391]: Loss 0.47192737460136414\n",
      "[Epoch 11] Training Batch [256/391]: Loss 0.43014371395111084\n",
      "[Epoch 11] Training Batch [257/391]: Loss 0.38540422916412354\n",
      "[Epoch 11] Training Batch [258/391]: Loss 0.38455653190612793\n",
      "[Epoch 11] Training Batch [259/391]: Loss 0.32754603028297424\n",
      "[Epoch 11] Training Batch [260/391]: Loss 0.48232901096343994\n",
      "[Epoch 11] Training Batch [261/391]: Loss 0.40235304832458496\n",
      "[Epoch 11] Training Batch [262/391]: Loss 0.5387657880783081\n",
      "[Epoch 11] Training Batch [263/391]: Loss 0.4129807949066162\n",
      "[Epoch 11] Training Batch [264/391]: Loss 0.392306923866272\n",
      "[Epoch 11] Training Batch [265/391]: Loss 0.5146769881248474\n",
      "[Epoch 11] Training Batch [266/391]: Loss 0.4101349115371704\n",
      "[Epoch 11] Training Batch [267/391]: Loss 0.5949301719665527\n",
      "[Epoch 11] Training Batch [268/391]: Loss 0.5166002511978149\n",
      "[Epoch 11] Training Batch [269/391]: Loss 0.2850317656993866\n",
      "[Epoch 11] Training Batch [270/391]: Loss 0.5341097712516785\n",
      "[Epoch 11] Training Batch [271/391]: Loss 0.5597168803215027\n",
      "[Epoch 11] Training Batch [272/391]: Loss 0.5133338570594788\n",
      "[Epoch 11] Training Batch [273/391]: Loss 0.47724151611328125\n",
      "[Epoch 11] Training Batch [274/391]: Loss 0.3921520709991455\n",
      "[Epoch 11] Training Batch [275/391]: Loss 0.5881601572036743\n",
      "[Epoch 11] Training Batch [276/391]: Loss 0.5436701774597168\n",
      "[Epoch 11] Training Batch [277/391]: Loss 0.31940189003944397\n",
      "[Epoch 11] Training Batch [278/391]: Loss 0.5129117965698242\n",
      "[Epoch 11] Training Batch [279/391]: Loss 0.4257936477661133\n",
      "[Epoch 11] Training Batch [280/391]: Loss 0.4578348398208618\n",
      "[Epoch 11] Training Batch [281/391]: Loss 0.39799824357032776\n",
      "[Epoch 11] Training Batch [282/391]: Loss 0.4205242395401001\n",
      "[Epoch 11] Training Batch [283/391]: Loss 0.47120290994644165\n",
      "[Epoch 11] Training Batch [284/391]: Loss 0.4745151698589325\n",
      "[Epoch 11] Training Batch [285/391]: Loss 0.5043681859970093\n",
      "[Epoch 11] Training Batch [286/391]: Loss 0.45672908425331116\n",
      "[Epoch 11] Training Batch [287/391]: Loss 0.4289061427116394\n",
      "[Epoch 11] Training Batch [288/391]: Loss 0.4809894859790802\n",
      "[Epoch 11] Training Batch [289/391]: Loss 0.41460320353507996\n",
      "[Epoch 11] Training Batch [290/391]: Loss 0.46263131499290466\n",
      "[Epoch 11] Training Batch [291/391]: Loss 0.4708269238471985\n",
      "[Epoch 11] Training Batch [292/391]: Loss 0.4830615818500519\n",
      "[Epoch 11] Training Batch [293/391]: Loss 0.36119648814201355\n",
      "[Epoch 11] Training Batch [294/391]: Loss 0.5210984945297241\n",
      "[Epoch 11] Training Batch [295/391]: Loss 0.3797755837440491\n",
      "[Epoch 11] Training Batch [296/391]: Loss 0.473156601190567\n",
      "[Epoch 11] Training Batch [297/391]: Loss 0.45379313826560974\n",
      "[Epoch 11] Training Batch [298/391]: Loss 0.34595391154289246\n",
      "[Epoch 11] Training Batch [299/391]: Loss 0.3406544029712677\n",
      "[Epoch 11] Training Batch [300/391]: Loss 0.40402913093566895\n",
      "[Epoch 11] Training Batch [301/391]: Loss 0.3992043137550354\n",
      "[Epoch 11] Training Batch [302/391]: Loss 0.3414754569530487\n",
      "[Epoch 11] Training Batch [303/391]: Loss 0.39652925729751587\n",
      "[Epoch 11] Training Batch [304/391]: Loss 0.3313661515712738\n",
      "[Epoch 11] Training Batch [305/391]: Loss 0.43070125579833984\n",
      "[Epoch 11] Training Batch [306/391]: Loss 0.4657454788684845\n",
      "[Epoch 11] Training Batch [307/391]: Loss 0.5305895209312439\n",
      "[Epoch 11] Training Batch [308/391]: Loss 0.42002880573272705\n",
      "[Epoch 11] Training Batch [309/391]: Loss 0.37405505776405334\n",
      "[Epoch 11] Training Batch [310/391]: Loss 0.5113556385040283\n",
      "[Epoch 11] Training Batch [311/391]: Loss 0.4228186011314392\n",
      "[Epoch 11] Training Batch [312/391]: Loss 0.4308307468891144\n",
      "[Epoch 11] Training Batch [313/391]: Loss 0.4224323034286499\n",
      "[Epoch 11] Training Batch [314/391]: Loss 0.5359019637107849\n",
      "[Epoch 11] Training Batch [315/391]: Loss 0.3689759075641632\n",
      "[Epoch 11] Training Batch [316/391]: Loss 0.45877406001091003\n",
      "[Epoch 11] Training Batch [317/391]: Loss 0.43433672189712524\n",
      "[Epoch 11] Training Batch [318/391]: Loss 0.5228436589241028\n",
      "[Epoch 11] Training Batch [319/391]: Loss 0.5350940227508545\n",
      "[Epoch 11] Training Batch [320/391]: Loss 0.4717033803462982\n",
      "[Epoch 11] Training Batch [321/391]: Loss 0.5566171407699585\n",
      "[Epoch 11] Training Batch [322/391]: Loss 0.47048255801200867\n",
      "[Epoch 11] Training Batch [323/391]: Loss 0.42175543308258057\n",
      "[Epoch 11] Training Batch [324/391]: Loss 0.47337979078292847\n",
      "[Epoch 11] Training Batch [325/391]: Loss 0.5899412035942078\n",
      "[Epoch 11] Training Batch [326/391]: Loss 0.4872726500034332\n",
      "[Epoch 11] Training Batch [327/391]: Loss 0.542813777923584\n",
      "[Epoch 11] Training Batch [328/391]: Loss 0.4695690870285034\n",
      "[Epoch 11] Training Batch [329/391]: Loss 0.6098554730415344\n",
      "[Epoch 11] Training Batch [330/391]: Loss 0.3894380033016205\n",
      "[Epoch 11] Training Batch [331/391]: Loss 0.44754159450531006\n",
      "[Epoch 11] Training Batch [332/391]: Loss 0.49172431230545044\n",
      "[Epoch 11] Training Batch [333/391]: Loss 0.37610745429992676\n",
      "[Epoch 11] Training Batch [334/391]: Loss 0.35807695984840393\n",
      "[Epoch 11] Training Batch [335/391]: Loss 0.4739895462989807\n",
      "[Epoch 11] Training Batch [336/391]: Loss 0.3099275529384613\n",
      "[Epoch 11] Training Batch [337/391]: Loss 0.467909038066864\n",
      "[Epoch 11] Training Batch [338/391]: Loss 0.3837890923023224\n",
      "[Epoch 11] Training Batch [339/391]: Loss 0.6162428855895996\n",
      "[Epoch 11] Training Batch [340/391]: Loss 0.49622300267219543\n",
      "[Epoch 11] Training Batch [341/391]: Loss 0.5088008642196655\n",
      "[Epoch 11] Training Batch [342/391]: Loss 0.49679404497146606\n",
      "[Epoch 11] Training Batch [343/391]: Loss 0.4872724413871765\n",
      "[Epoch 11] Training Batch [344/391]: Loss 0.33318012952804565\n",
      "[Epoch 11] Training Batch [345/391]: Loss 0.49690237641334534\n",
      "[Epoch 11] Training Batch [346/391]: Loss 0.5703867673873901\n",
      "[Epoch 11] Training Batch [347/391]: Loss 0.46103018522262573\n",
      "[Epoch 11] Training Batch [348/391]: Loss 0.4470767676830292\n",
      "[Epoch 11] Training Batch [349/391]: Loss 0.41415664553642273\n",
      "[Epoch 11] Training Batch [350/391]: Loss 0.43385136127471924\n",
      "[Epoch 11] Training Batch [351/391]: Loss 0.42718255519866943\n",
      "[Epoch 11] Training Batch [352/391]: Loss 0.4453805387020111\n",
      "[Epoch 11] Training Batch [353/391]: Loss 0.43386000394821167\n",
      "[Epoch 11] Training Batch [354/391]: Loss 0.5851818919181824\n",
      "[Epoch 11] Training Batch [355/391]: Loss 0.4465557634830475\n",
      "[Epoch 11] Training Batch [356/391]: Loss 0.3861112892627716\n",
      "[Epoch 11] Training Batch [357/391]: Loss 0.40610581636428833\n",
      "[Epoch 11] Training Batch [358/391]: Loss 0.42342647910118103\n",
      "[Epoch 11] Training Batch [359/391]: Loss 0.44308847188949585\n",
      "[Epoch 11] Training Batch [360/391]: Loss 0.37729471921920776\n",
      "[Epoch 11] Training Batch [361/391]: Loss 0.5232553482055664\n",
      "[Epoch 11] Training Batch [362/391]: Loss 0.5056352019309998\n",
      "[Epoch 11] Training Batch [363/391]: Loss 0.5252630114555359\n",
      "[Epoch 11] Training Batch [364/391]: Loss 0.3021332025527954\n",
      "[Epoch 11] Training Batch [365/391]: Loss 0.5189606547355652\n",
      "[Epoch 11] Training Batch [366/391]: Loss 0.4854719936847687\n",
      "[Epoch 11] Training Batch [367/391]: Loss 0.3182084262371063\n",
      "[Epoch 11] Training Batch [368/391]: Loss 0.4814915359020233\n",
      "[Epoch 11] Training Batch [369/391]: Loss 0.5117743611335754\n",
      "[Epoch 11] Training Batch [370/391]: Loss 0.410143107175827\n",
      "[Epoch 11] Training Batch [371/391]: Loss 0.5264989137649536\n",
      "[Epoch 11] Training Batch [372/391]: Loss 0.3683124780654907\n",
      "[Epoch 11] Training Batch [373/391]: Loss 0.4196060597896576\n",
      "[Epoch 11] Training Batch [374/391]: Loss 0.48751166462898254\n",
      "[Epoch 11] Training Batch [375/391]: Loss 0.3730682134628296\n",
      "[Epoch 11] Training Batch [376/391]: Loss 0.4451727569103241\n",
      "[Epoch 11] Training Batch [377/391]: Loss 0.4599060118198395\n",
      "[Epoch 11] Training Batch [378/391]: Loss 0.3146156966686249\n",
      "[Epoch 11] Training Batch [379/391]: Loss 0.38872694969177246\n",
      "[Epoch 11] Training Batch [380/391]: Loss 0.43129563331604004\n",
      "[Epoch 11] Training Batch [381/391]: Loss 0.5130614638328552\n",
      "[Epoch 11] Training Batch [382/391]: Loss 0.4007212817668915\n",
      "[Epoch 11] Training Batch [383/391]: Loss 0.37573033571243286\n",
      "[Epoch 11] Training Batch [384/391]: Loss 0.4704734683036804\n",
      "[Epoch 11] Training Batch [385/391]: Loss 0.4909096360206604\n",
      "[Epoch 11] Training Batch [386/391]: Loss 0.4291413426399231\n",
      "[Epoch 11] Training Batch [387/391]: Loss 0.4733825623989105\n",
      "[Epoch 11] Training Batch [388/391]: Loss 0.49174705147743225\n",
      "[Epoch 11] Training Batch [389/391]: Loss 0.5996032953262329\n",
      "[Epoch 11] Training Batch [390/391]: Loss 0.5020147562026978\n",
      "[Epoch 11] Training Batch [391/391]: Loss 0.4942973554134369\n",
      "Epoch 11 - Train Loss: 0.4121\n",
      "*********  Epoch 12/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Training Batch [1/391]: Loss 0.3186119496822357\n",
      "[Epoch 12] Training Batch [2/391]: Loss 0.3156477212905884\n",
      "[Epoch 12] Training Batch [3/391]: Loss 0.26486530900001526\n",
      "[Epoch 12] Training Batch [4/391]: Loss 0.27468112111091614\n",
      "[Epoch 12] Training Batch [5/391]: Loss 0.2627268433570862\n",
      "[Epoch 12] Training Batch [6/391]: Loss 0.23429535329341888\n",
      "[Epoch 12] Training Batch [7/391]: Loss 0.3554045855998993\n",
      "[Epoch 12] Training Batch [8/391]: Loss 0.2565765380859375\n",
      "[Epoch 12] Training Batch [9/391]: Loss 0.29742157459259033\n",
      "[Epoch 12] Training Batch [10/391]: Loss 0.24461548030376434\n",
      "[Epoch 12] Training Batch [11/391]: Loss 0.2778744697570801\n",
      "[Epoch 12] Training Batch [12/391]: Loss 0.3960740268230438\n",
      "[Epoch 12] Training Batch [13/391]: Loss 0.25632163882255554\n",
      "[Epoch 12] Training Batch [14/391]: Loss 0.2953110337257385\n",
      "[Epoch 12] Training Batch [15/391]: Loss 0.2884427309036255\n",
      "[Epoch 12] Training Batch [16/391]: Loss 0.3382451832294464\n",
      "[Epoch 12] Training Batch [17/391]: Loss 0.22987480461597443\n",
      "[Epoch 12] Training Batch [18/391]: Loss 0.2846290171146393\n",
      "[Epoch 12] Training Batch [19/391]: Loss 0.22422122955322266\n",
      "[Epoch 12] Training Batch [20/391]: Loss 0.20050303637981415\n",
      "[Epoch 12] Training Batch [21/391]: Loss 0.27148476243019104\n",
      "[Epoch 12] Training Batch [22/391]: Loss 0.30774155259132385\n",
      "[Epoch 12] Training Batch [23/391]: Loss 0.2730773389339447\n",
      "[Epoch 12] Training Batch [24/391]: Loss 0.21663716435432434\n",
      "[Epoch 12] Training Batch [25/391]: Loss 0.2374275177717209\n",
      "[Epoch 12] Training Batch [26/391]: Loss 0.3529207706451416\n",
      "[Epoch 12] Training Batch [27/391]: Loss 0.25037527084350586\n",
      "[Epoch 12] Training Batch [28/391]: Loss 0.2563101649284363\n",
      "[Epoch 12] Training Batch [29/391]: Loss 0.2154434770345688\n",
      "[Epoch 12] Training Batch [30/391]: Loss 0.29319408535957336\n",
      "[Epoch 12] Training Batch [31/391]: Loss 0.22594429552555084\n",
      "[Epoch 12] Training Batch [32/391]: Loss 0.2656487822532654\n",
      "[Epoch 12] Training Batch [33/391]: Loss 0.18473738431930542\n",
      "[Epoch 12] Training Batch [34/391]: Loss 0.2262423038482666\n",
      "[Epoch 12] Training Batch [35/391]: Loss 0.1811712384223938\n",
      "[Epoch 12] Training Batch [36/391]: Loss 0.27368494868278503\n",
      "[Epoch 12] Training Batch [37/391]: Loss 0.2055080682039261\n",
      "[Epoch 12] Training Batch [38/391]: Loss 0.2271459400653839\n",
      "[Epoch 12] Training Batch [39/391]: Loss 0.31501537561416626\n",
      "[Epoch 12] Training Batch [40/391]: Loss 0.2552516460418701\n",
      "[Epoch 12] Training Batch [41/391]: Loss 0.20249682664871216\n",
      "[Epoch 12] Training Batch [42/391]: Loss 0.21158453822135925\n",
      "[Epoch 12] Training Batch [43/391]: Loss 0.28459304571151733\n",
      "[Epoch 12] Training Batch [44/391]: Loss 0.19854578375816345\n",
      "[Epoch 12] Training Batch [45/391]: Loss 0.27231404185295105\n",
      "[Epoch 12] Training Batch [46/391]: Loss 0.2606393098831177\n",
      "[Epoch 12] Training Batch [47/391]: Loss 0.22926971316337585\n",
      "[Epoch 12] Training Batch [48/391]: Loss 0.269212007522583\n",
      "[Epoch 12] Training Batch [49/391]: Loss 0.31976449489593506\n",
      "[Epoch 12] Training Batch [50/391]: Loss 0.33540722727775574\n",
      "[Epoch 12] Training Batch [51/391]: Loss 0.34636855125427246\n",
      "[Epoch 12] Training Batch [52/391]: Loss 0.29605990648269653\n",
      "[Epoch 12] Training Batch [53/391]: Loss 0.303350567817688\n",
      "[Epoch 12] Training Batch [54/391]: Loss 0.1358419805765152\n",
      "[Epoch 12] Training Batch [55/391]: Loss 0.2699303925037384\n",
      "[Epoch 12] Training Batch [56/391]: Loss 0.33152249455451965\n",
      "[Epoch 12] Training Batch [57/391]: Loss 0.23217874765396118\n",
      "[Epoch 12] Training Batch [58/391]: Loss 0.2546820044517517\n",
      "[Epoch 12] Training Batch [59/391]: Loss 0.36154815554618835\n",
      "[Epoch 12] Training Batch [60/391]: Loss 0.236181378364563\n",
      "[Epoch 12] Training Batch [61/391]: Loss 0.28524696826934814\n",
      "[Epoch 12] Training Batch [62/391]: Loss 0.2765031158924103\n",
      "[Epoch 12] Training Batch [63/391]: Loss 0.31026768684387207\n",
      "[Epoch 12] Training Batch [64/391]: Loss 0.21070988476276398\n",
      "[Epoch 12] Training Batch [65/391]: Loss 0.2759420871734619\n",
      "[Epoch 12] Training Batch [66/391]: Loss 0.24981637299060822\n",
      "[Epoch 12] Training Batch [67/391]: Loss 0.1475342959165573\n",
      "[Epoch 12] Training Batch [68/391]: Loss 0.30868202447891235\n",
      "[Epoch 12] Training Batch [69/391]: Loss 0.21641162037849426\n",
      "[Epoch 12] Training Batch [70/391]: Loss 0.24837546050548553\n",
      "[Epoch 12] Training Batch [71/391]: Loss 0.2318154275417328\n",
      "[Epoch 12] Training Batch [72/391]: Loss 0.25902608036994934\n",
      "[Epoch 12] Training Batch [73/391]: Loss 0.29701000452041626\n",
      "[Epoch 12] Training Batch [74/391]: Loss 0.23902302980422974\n",
      "[Epoch 12] Training Batch [75/391]: Loss 0.292018324136734\n",
      "[Epoch 12] Training Batch [76/391]: Loss 0.1609952747821808\n",
      "[Epoch 12] Training Batch [77/391]: Loss 0.3491530120372772\n",
      "[Epoch 12] Training Batch [78/391]: Loss 0.17149071395397186\n",
      "[Epoch 12] Training Batch [79/391]: Loss 0.3351547122001648\n",
      "[Epoch 12] Training Batch [80/391]: Loss 0.2716808319091797\n",
      "[Epoch 12] Training Batch [81/391]: Loss 0.2917766869068146\n",
      "[Epoch 12] Training Batch [82/391]: Loss 0.18803897500038147\n",
      "[Epoch 12] Training Batch [83/391]: Loss 0.23066692054271698\n",
      "[Epoch 12] Training Batch [84/391]: Loss 0.22763097286224365\n",
      "[Epoch 12] Training Batch [85/391]: Loss 0.20534589886665344\n",
      "[Epoch 12] Training Batch [86/391]: Loss 0.2929033637046814\n",
      "[Epoch 12] Training Batch [87/391]: Loss 0.2093120962381363\n",
      "[Epoch 12] Training Batch [88/391]: Loss 0.2721205949783325\n",
      "[Epoch 12] Training Batch [89/391]: Loss 0.17810805141925812\n",
      "[Epoch 12] Training Batch [90/391]: Loss 0.2484927624464035\n",
      "[Epoch 12] Training Batch [91/391]: Loss 0.20816238224506378\n",
      "[Epoch 12] Training Batch [92/391]: Loss 0.291858047246933\n",
      "[Epoch 12] Training Batch [93/391]: Loss 0.2696329355239868\n",
      "[Epoch 12] Training Batch [94/391]: Loss 0.2216411679983139\n",
      "[Epoch 12] Training Batch [95/391]: Loss 0.21502965688705444\n",
      "[Epoch 12] Training Batch [96/391]: Loss 0.2079629898071289\n",
      "[Epoch 12] Training Batch [97/391]: Loss 0.22222429513931274\n",
      "[Epoch 12] Training Batch [98/391]: Loss 0.2276257574558258\n",
      "[Epoch 12] Training Batch [99/391]: Loss 0.3014785647392273\n",
      "[Epoch 12] Training Batch [100/391]: Loss 0.18358607590198517\n",
      "[Epoch 12] Training Batch [101/391]: Loss 0.1793748140335083\n",
      "[Epoch 12] Training Batch [102/391]: Loss 0.24775493144989014\n",
      "[Epoch 12] Training Batch [103/391]: Loss 0.20931994915008545\n",
      "[Epoch 12] Training Batch [104/391]: Loss 0.16834497451782227\n",
      "[Epoch 12] Training Batch [105/391]: Loss 0.20364327728748322\n",
      "[Epoch 12] Training Batch [106/391]: Loss 0.3340883255004883\n",
      "[Epoch 12] Training Batch [107/391]: Loss 0.2226327508687973\n",
      "[Epoch 12] Training Batch [108/391]: Loss 0.3431297838687897\n",
      "[Epoch 12] Training Batch [109/391]: Loss 0.24888859689235687\n",
      "[Epoch 12] Training Batch [110/391]: Loss 0.32000184059143066\n",
      "[Epoch 12] Training Batch [111/391]: Loss 0.2942773997783661\n",
      "[Epoch 12] Training Batch [112/391]: Loss 0.14427058398723602\n",
      "[Epoch 12] Training Batch [113/391]: Loss 0.36798062920570374\n",
      "[Epoch 12] Training Batch [114/391]: Loss 0.3279746472835541\n",
      "[Epoch 12] Training Batch [115/391]: Loss 0.20361237227916718\n",
      "[Epoch 12] Training Batch [116/391]: Loss 0.2834917902946472\n",
      "[Epoch 12] Training Batch [117/391]: Loss 0.2168935239315033\n",
      "[Epoch 12] Training Batch [118/391]: Loss 0.22862425446510315\n",
      "[Epoch 12] Training Batch [119/391]: Loss 0.41893067955970764\n",
      "[Epoch 12] Training Batch [120/391]: Loss 0.26317062973976135\n",
      "[Epoch 12] Training Batch [121/391]: Loss 0.23921658098697662\n",
      "[Epoch 12] Training Batch [122/391]: Loss 0.2294640690088272\n",
      "[Epoch 12] Training Batch [123/391]: Loss 0.2494303584098816\n",
      "[Epoch 12] Training Batch [124/391]: Loss 0.23644284904003143\n",
      "[Epoch 12] Training Batch [125/391]: Loss 0.27702245116233826\n",
      "[Epoch 12] Training Batch [126/391]: Loss 0.2601422667503357\n",
      "[Epoch 12] Training Batch [127/391]: Loss 0.19972127676010132\n",
      "[Epoch 12] Training Batch [128/391]: Loss 0.2167387306690216\n",
      "[Epoch 12] Training Batch [129/391]: Loss 0.21937872469425201\n",
      "[Epoch 12] Training Batch [130/391]: Loss 0.39252135157585144\n",
      "[Epoch 12] Training Batch [131/391]: Loss 0.24048911035060883\n",
      "[Epoch 12] Training Batch [132/391]: Loss 0.20046840608119965\n",
      "[Epoch 12] Training Batch [133/391]: Loss 0.2442895919084549\n",
      "[Epoch 12] Training Batch [134/391]: Loss 0.2959158718585968\n",
      "[Epoch 12] Training Batch [135/391]: Loss 0.27029454708099365\n",
      "[Epoch 12] Training Batch [136/391]: Loss 0.33580800890922546\n",
      "[Epoch 12] Training Batch [137/391]: Loss 0.32937800884246826\n",
      "[Epoch 12] Training Batch [138/391]: Loss 0.30872103571891785\n",
      "[Epoch 12] Training Batch [139/391]: Loss 0.18121656775474548\n",
      "[Epoch 12] Training Batch [140/391]: Loss 0.2615116536617279\n",
      "[Epoch 12] Training Batch [141/391]: Loss 0.2728821933269501\n",
      "[Epoch 12] Training Batch [142/391]: Loss 0.18962180614471436\n",
      "[Epoch 12] Training Batch [143/391]: Loss 0.21773166954517365\n",
      "[Epoch 12] Training Batch [144/391]: Loss 0.3508204221725464\n",
      "[Epoch 12] Training Batch [145/391]: Loss 0.2721700966358185\n",
      "[Epoch 12] Training Batch [146/391]: Loss 0.1647268533706665\n",
      "[Epoch 12] Training Batch [147/391]: Loss 0.2817743718624115\n",
      "[Epoch 12] Training Batch [148/391]: Loss 0.1926877349615097\n",
      "[Epoch 12] Training Batch [149/391]: Loss 0.27838268876075745\n",
      "[Epoch 12] Training Batch [150/391]: Loss 0.2694363594055176\n",
      "[Epoch 12] Training Batch [151/391]: Loss 0.22572582960128784\n",
      "[Epoch 12] Training Batch [152/391]: Loss 0.3091658651828766\n",
      "[Epoch 12] Training Batch [153/391]: Loss 0.2946312129497528\n",
      "[Epoch 12] Training Batch [154/391]: Loss 0.2715883255004883\n",
      "[Epoch 12] Training Batch [155/391]: Loss 0.27841243147850037\n",
      "[Epoch 12] Training Batch [156/391]: Loss 0.21828047931194305\n",
      "[Epoch 12] Training Batch [157/391]: Loss 0.20849832892417908\n",
      "[Epoch 12] Training Batch [158/391]: Loss 0.20611132681369781\n",
      "[Epoch 12] Training Batch [159/391]: Loss 0.25134554505348206\n",
      "[Epoch 12] Training Batch [160/391]: Loss 0.29333731532096863\n",
      "[Epoch 12] Training Batch [161/391]: Loss 0.29271137714385986\n",
      "[Epoch 12] Training Batch [162/391]: Loss 0.13870443403720856\n",
      "[Epoch 12] Training Batch [163/391]: Loss 0.2003297656774521\n",
      "[Epoch 12] Training Batch [164/391]: Loss 0.24224567413330078\n",
      "[Epoch 12] Training Batch [165/391]: Loss 0.28318914771080017\n",
      "[Epoch 12] Training Batch [166/391]: Loss 0.19747145473957062\n",
      "[Epoch 12] Training Batch [167/391]: Loss 0.2107032984495163\n",
      "[Epoch 12] Training Batch [168/391]: Loss 0.26800772547721863\n",
      "[Epoch 12] Training Batch [169/391]: Loss 0.457515150308609\n",
      "[Epoch 12] Training Batch [170/391]: Loss 0.24678504467010498\n",
      "[Epoch 12] Training Batch [171/391]: Loss 0.23440980911254883\n",
      "[Epoch 12] Training Batch [172/391]: Loss 0.2421424686908722\n",
      "[Epoch 12] Training Batch [173/391]: Loss 0.23741702735424042\n",
      "[Epoch 12] Training Batch [174/391]: Loss 0.2634921967983246\n",
      "[Epoch 12] Training Batch [175/391]: Loss 0.25672081112861633\n",
      "[Epoch 12] Training Batch [176/391]: Loss 0.34196528792381287\n",
      "[Epoch 12] Training Batch [177/391]: Loss 0.31479454040527344\n",
      "[Epoch 12] Training Batch [178/391]: Loss 0.24412642419338226\n",
      "[Epoch 12] Training Batch [179/391]: Loss 0.33511409163475037\n",
      "[Epoch 12] Training Batch [180/391]: Loss 0.3627501428127289\n",
      "[Epoch 12] Training Batch [181/391]: Loss 0.2741710841655731\n",
      "[Epoch 12] Training Batch [182/391]: Loss 0.32865357398986816\n",
      "[Epoch 12] Training Batch [183/391]: Loss 0.36510786414146423\n",
      "[Epoch 12] Training Batch [184/391]: Loss 0.2715417444705963\n",
      "[Epoch 12] Training Batch [185/391]: Loss 0.1986539661884308\n",
      "[Epoch 12] Training Batch [186/391]: Loss 0.293588250875473\n",
      "[Epoch 12] Training Batch [187/391]: Loss 0.33689364790916443\n",
      "[Epoch 12] Training Batch [188/391]: Loss 0.3033589720726013\n",
      "[Epoch 12] Training Batch [189/391]: Loss 0.20313873887062073\n",
      "[Epoch 12] Training Batch [190/391]: Loss 0.22804103791713715\n",
      "[Epoch 12] Training Batch [191/391]: Loss 0.3243641257286072\n",
      "[Epoch 12] Training Batch [192/391]: Loss 0.25322964787483215\n",
      "[Epoch 12] Training Batch [193/391]: Loss 0.2703563868999481\n",
      "[Epoch 12] Training Batch [194/391]: Loss 0.26034489274024963\n",
      "[Epoch 12] Training Batch [195/391]: Loss 0.27763617038726807\n",
      "[Epoch 12] Training Batch [196/391]: Loss 0.2897380292415619\n",
      "[Epoch 12] Training Batch [197/391]: Loss 0.24466943740844727\n",
      "[Epoch 12] Training Batch [198/391]: Loss 0.2964799106121063\n",
      "[Epoch 12] Training Batch [199/391]: Loss 0.2141060084104538\n",
      "[Epoch 12] Training Batch [200/391]: Loss 0.22820553183555603\n",
      "[Epoch 12] Training Batch [201/391]: Loss 0.3496330976486206\n",
      "[Epoch 12] Training Batch [202/391]: Loss 0.3518843650817871\n",
      "[Epoch 12] Training Batch [203/391]: Loss 0.23221994936466217\n",
      "[Epoch 12] Training Batch [204/391]: Loss 0.3918854594230652\n",
      "[Epoch 12] Training Batch [205/391]: Loss 0.26978716254234314\n",
      "[Epoch 12] Training Batch [206/391]: Loss 0.2841799557209015\n",
      "[Epoch 12] Training Batch [207/391]: Loss 0.2953750193119049\n",
      "[Epoch 12] Training Batch [208/391]: Loss 0.35305386781692505\n",
      "[Epoch 12] Training Batch [209/391]: Loss 0.2123488336801529\n",
      "[Epoch 12] Training Batch [210/391]: Loss 0.2090545892715454\n",
      "[Epoch 12] Training Batch [211/391]: Loss 0.32425931096076965\n",
      "[Epoch 12] Training Batch [212/391]: Loss 0.3162309229373932\n",
      "[Epoch 12] Training Batch [213/391]: Loss 0.2700972259044647\n",
      "[Epoch 12] Training Batch [214/391]: Loss 0.20751656591892242\n",
      "[Epoch 12] Training Batch [215/391]: Loss 0.28114286065101624\n",
      "[Epoch 12] Training Batch [216/391]: Loss 0.238116055727005\n",
      "[Epoch 12] Training Batch [217/391]: Loss 0.2832592725753784\n",
      "[Epoch 12] Training Batch [218/391]: Loss 0.2940157949924469\n",
      "[Epoch 12] Training Batch [219/391]: Loss 0.25765547156333923\n",
      "[Epoch 12] Training Batch [220/391]: Loss 0.33993327617645264\n",
      "[Epoch 12] Training Batch [221/391]: Loss 0.30188924074172974\n",
      "[Epoch 12] Training Batch [222/391]: Loss 0.2754746377468109\n",
      "[Epoch 12] Training Batch [223/391]: Loss 0.2769499719142914\n",
      "[Epoch 12] Training Batch [224/391]: Loss 0.20268823206424713\n",
      "[Epoch 12] Training Batch [225/391]: Loss 0.22868327796459198\n",
      "[Epoch 12] Training Batch [226/391]: Loss 0.29267358779907227\n",
      "[Epoch 12] Training Batch [227/391]: Loss 0.24366487562656403\n",
      "[Epoch 12] Training Batch [228/391]: Loss 0.3804003596305847\n",
      "[Epoch 12] Training Batch [229/391]: Loss 0.35948145389556885\n",
      "[Epoch 12] Training Batch [230/391]: Loss 0.2695547938346863\n",
      "[Epoch 12] Training Batch [231/391]: Loss 0.3498607873916626\n",
      "[Epoch 12] Training Batch [232/391]: Loss 0.25536978244781494\n",
      "[Epoch 12] Training Batch [233/391]: Loss 0.275113046169281\n",
      "[Epoch 12] Training Batch [234/391]: Loss 0.27001065015792847\n",
      "[Epoch 12] Training Batch [235/391]: Loss 0.2989698052406311\n",
      "[Epoch 12] Training Batch [236/391]: Loss 0.21614645421504974\n",
      "[Epoch 12] Training Batch [237/391]: Loss 0.3069170117378235\n",
      "[Epoch 12] Training Batch [238/391]: Loss 0.43998122215270996\n",
      "[Epoch 12] Training Batch [239/391]: Loss 0.34937378764152527\n",
      "[Epoch 12] Training Batch [240/391]: Loss 0.22194239497184753\n",
      "[Epoch 12] Training Batch [241/391]: Loss 0.23094284534454346\n",
      "[Epoch 12] Training Batch [242/391]: Loss 0.3565758466720581\n",
      "[Epoch 12] Training Batch [243/391]: Loss 0.2478996068239212\n",
      "[Epoch 12] Training Batch [244/391]: Loss 0.3088301122188568\n",
      "[Epoch 12] Training Batch [245/391]: Loss 0.28528067469596863\n",
      "[Epoch 12] Training Batch [246/391]: Loss 0.3141827881336212\n",
      "[Epoch 12] Training Batch [247/391]: Loss 0.32764169573783875\n",
      "[Epoch 12] Training Batch [248/391]: Loss 0.2765570282936096\n",
      "[Epoch 12] Training Batch [249/391]: Loss 0.3270666301250458\n",
      "[Epoch 12] Training Batch [250/391]: Loss 0.3169403076171875\n",
      "[Epoch 12] Training Batch [251/391]: Loss 0.30325037240982056\n",
      "[Epoch 12] Training Batch [252/391]: Loss 0.4358196258544922\n",
      "[Epoch 12] Training Batch [253/391]: Loss 0.27299582958221436\n",
      "[Epoch 12] Training Batch [254/391]: Loss 0.25083476305007935\n",
      "[Epoch 12] Training Batch [255/391]: Loss 0.33212926983833313\n",
      "[Epoch 12] Training Batch [256/391]: Loss 0.2790006101131439\n",
      "[Epoch 12] Training Batch [257/391]: Loss 0.2873181402683258\n",
      "[Epoch 12] Training Batch [258/391]: Loss 0.2888570725917816\n",
      "[Epoch 12] Training Batch [259/391]: Loss 0.2998715043067932\n",
      "[Epoch 12] Training Batch [260/391]: Loss 0.29669317603111267\n",
      "[Epoch 12] Training Batch [261/391]: Loss 0.3340042233467102\n",
      "[Epoch 12] Training Batch [262/391]: Loss 0.23222233355045319\n",
      "[Epoch 12] Training Batch [263/391]: Loss 0.26044297218322754\n",
      "[Epoch 12] Training Batch [264/391]: Loss 0.26977109909057617\n",
      "[Epoch 12] Training Batch [265/391]: Loss 0.27079758048057556\n",
      "[Epoch 12] Training Batch [266/391]: Loss 0.36966097354888916\n",
      "[Epoch 12] Training Batch [267/391]: Loss 0.31485986709594727\n",
      "[Epoch 12] Training Batch [268/391]: Loss 0.2306256890296936\n",
      "[Epoch 12] Training Batch [269/391]: Loss 0.268102765083313\n",
      "[Epoch 12] Training Batch [270/391]: Loss 0.29217422008514404\n",
      "[Epoch 12] Training Batch [271/391]: Loss 0.38822880387306213\n",
      "[Epoch 12] Training Batch [272/391]: Loss 0.43113023042678833\n",
      "[Epoch 12] Training Batch [273/391]: Loss 0.30112454295158386\n",
      "[Epoch 12] Training Batch [274/391]: Loss 0.2765849530696869\n",
      "[Epoch 12] Training Batch [275/391]: Loss 0.3129784166812897\n",
      "[Epoch 12] Training Batch [276/391]: Loss 0.41725772619247437\n",
      "[Epoch 12] Training Batch [277/391]: Loss 0.34905359148979187\n",
      "[Epoch 12] Training Batch [278/391]: Loss 0.3886287808418274\n",
      "[Epoch 12] Training Batch [279/391]: Loss 0.34431591629981995\n",
      "[Epoch 12] Training Batch [280/391]: Loss 0.2806988060474396\n",
      "[Epoch 12] Training Batch [281/391]: Loss 0.3157035708427429\n",
      "[Epoch 12] Training Batch [282/391]: Loss 0.296026349067688\n",
      "[Epoch 12] Training Batch [283/391]: Loss 0.3735232651233673\n",
      "[Epoch 12] Training Batch [284/391]: Loss 0.2808767855167389\n",
      "[Epoch 12] Training Batch [285/391]: Loss 0.2510240375995636\n",
      "[Epoch 12] Training Batch [286/391]: Loss 0.23827101290225983\n",
      "[Epoch 12] Training Batch [287/391]: Loss 0.32057374715805054\n",
      "[Epoch 12] Training Batch [288/391]: Loss 0.27089923620224\n",
      "[Epoch 12] Training Batch [289/391]: Loss 0.40702715516090393\n",
      "[Epoch 12] Training Batch [290/391]: Loss 0.32689568400382996\n",
      "[Epoch 12] Training Batch [291/391]: Loss 0.2953410744667053\n",
      "[Epoch 12] Training Batch [292/391]: Loss 0.3734983503818512\n",
      "[Epoch 12] Training Batch [293/391]: Loss 0.41279569268226624\n",
      "[Epoch 12] Training Batch [294/391]: Loss 0.3149550259113312\n",
      "[Epoch 12] Training Batch [295/391]: Loss 0.3346565365791321\n",
      "[Epoch 12] Training Batch [296/391]: Loss 0.392368346452713\n",
      "[Epoch 12] Training Batch [297/391]: Loss 0.31388458609580994\n",
      "[Epoch 12] Training Batch [298/391]: Loss 0.3110460638999939\n",
      "[Epoch 12] Training Batch [299/391]: Loss 0.28479230403900146\n",
      "[Epoch 12] Training Batch [300/391]: Loss 0.30135276913642883\n",
      "[Epoch 12] Training Batch [301/391]: Loss 0.34572678804397583\n",
      "[Epoch 12] Training Batch [302/391]: Loss 0.2583242356777191\n",
      "[Epoch 12] Training Batch [303/391]: Loss 0.43245843052864075\n",
      "[Epoch 12] Training Batch [304/391]: Loss 0.3717215657234192\n",
      "[Epoch 12] Training Batch [305/391]: Loss 0.21903559565544128\n",
      "[Epoch 12] Training Batch [306/391]: Loss 0.2572993338108063\n",
      "[Epoch 12] Training Batch [307/391]: Loss 0.3248027265071869\n",
      "[Epoch 12] Training Batch [308/391]: Loss 0.40403321385383606\n",
      "[Epoch 12] Training Batch [309/391]: Loss 0.43473324179649353\n",
      "[Epoch 12] Training Batch [310/391]: Loss 0.2975563704967499\n",
      "[Epoch 12] Training Batch [311/391]: Loss 0.22945155203342438\n",
      "[Epoch 12] Training Batch [312/391]: Loss 0.30819931626319885\n",
      "[Epoch 12] Training Batch [313/391]: Loss 0.28124651312828064\n",
      "[Epoch 12] Training Batch [314/391]: Loss 0.2922380566596985\n",
      "[Epoch 12] Training Batch [315/391]: Loss 0.33477112650871277\n",
      "[Epoch 12] Training Batch [316/391]: Loss 0.3188323676586151\n",
      "[Epoch 12] Training Batch [317/391]: Loss 0.2640427350997925\n",
      "[Epoch 12] Training Batch [318/391]: Loss 0.22203975915908813\n",
      "[Epoch 12] Training Batch [319/391]: Loss 0.4127311706542969\n",
      "[Epoch 12] Training Batch [320/391]: Loss 0.2884644865989685\n",
      "[Epoch 12] Training Batch [321/391]: Loss 0.24281878769397736\n",
      "[Epoch 12] Training Batch [322/391]: Loss 0.4329332113265991\n",
      "[Epoch 12] Training Batch [323/391]: Loss 0.46950680017471313\n",
      "[Epoch 12] Training Batch [324/391]: Loss 0.338468462228775\n",
      "[Epoch 12] Training Batch [325/391]: Loss 0.22852379083633423\n",
      "[Epoch 12] Training Batch [326/391]: Loss 0.36612027883529663\n",
      "[Epoch 12] Training Batch [327/391]: Loss 0.3219462037086487\n",
      "[Epoch 12] Training Batch [328/391]: Loss 0.31116369366645813\n",
      "[Epoch 12] Training Batch [329/391]: Loss 0.27032285928726196\n",
      "[Epoch 12] Training Batch [330/391]: Loss 0.3311549723148346\n",
      "[Epoch 12] Training Batch [331/391]: Loss 0.36609965562820435\n",
      "[Epoch 12] Training Batch [332/391]: Loss 0.28295859694480896\n",
      "[Epoch 12] Training Batch [333/391]: Loss 0.4704184830188751\n",
      "[Epoch 12] Training Batch [334/391]: Loss 0.5361658334732056\n",
      "[Epoch 12] Training Batch [335/391]: Loss 0.3352814316749573\n",
      "[Epoch 12] Training Batch [336/391]: Loss 0.34832072257995605\n",
      "[Epoch 12] Training Batch [337/391]: Loss 0.32809218764305115\n",
      "[Epoch 12] Training Batch [338/391]: Loss 0.2912787199020386\n",
      "[Epoch 12] Training Batch [339/391]: Loss 0.3356747329235077\n",
      "[Epoch 12] Training Batch [340/391]: Loss 0.3107541501522064\n",
      "[Epoch 12] Training Batch [341/391]: Loss 0.2141764760017395\n",
      "[Epoch 12] Training Batch [342/391]: Loss 0.3491385579109192\n",
      "[Epoch 12] Training Batch [343/391]: Loss 0.22899353504180908\n",
      "[Epoch 12] Training Batch [344/391]: Loss 0.2591691315174103\n",
      "[Epoch 12] Training Batch [345/391]: Loss 0.23348772525787354\n",
      "[Epoch 12] Training Batch [346/391]: Loss 0.3472088575363159\n",
      "[Epoch 12] Training Batch [347/391]: Loss 0.3593117892742157\n",
      "[Epoch 12] Training Batch [348/391]: Loss 0.25142791867256165\n",
      "[Epoch 12] Training Batch [349/391]: Loss 0.29934972524642944\n",
      "[Epoch 12] Training Batch [350/391]: Loss 0.38758936524391174\n",
      "[Epoch 12] Training Batch [351/391]: Loss 0.29571932554244995\n",
      "[Epoch 12] Training Batch [352/391]: Loss 0.4245450496673584\n",
      "[Epoch 12] Training Batch [353/391]: Loss 0.3472747206687927\n",
      "[Epoch 12] Training Batch [354/391]: Loss 0.3201022148132324\n",
      "[Epoch 12] Training Batch [355/391]: Loss 0.35472387075424194\n",
      "[Epoch 12] Training Batch [356/391]: Loss 0.43242964148521423\n",
      "[Epoch 12] Training Batch [357/391]: Loss 0.44218701124191284\n",
      "[Epoch 12] Training Batch [358/391]: Loss 0.3391987681388855\n",
      "[Epoch 12] Training Batch [359/391]: Loss 0.3556559681892395\n",
      "[Epoch 12] Training Batch [360/391]: Loss 0.22493040561676025\n",
      "[Epoch 12] Training Batch [361/391]: Loss 0.21609146893024445\n",
      "[Epoch 12] Training Batch [362/391]: Loss 0.3080041706562042\n",
      "[Epoch 12] Training Batch [363/391]: Loss 0.41506797075271606\n",
      "[Epoch 12] Training Batch [364/391]: Loss 0.3041977286338806\n",
      "[Epoch 12] Training Batch [365/391]: Loss 0.19670787453651428\n",
      "[Epoch 12] Training Batch [366/391]: Loss 0.45272403955459595\n",
      "[Epoch 12] Training Batch [367/391]: Loss 0.4186003506183624\n",
      "[Epoch 12] Training Batch [368/391]: Loss 0.27891024947166443\n",
      "[Epoch 12] Training Batch [369/391]: Loss 0.35312071442604065\n",
      "[Epoch 12] Training Batch [370/391]: Loss 0.4191628098487854\n",
      "[Epoch 12] Training Batch [371/391]: Loss 0.275613933801651\n",
      "[Epoch 12] Training Batch [372/391]: Loss 0.3429906964302063\n",
      "[Epoch 12] Training Batch [373/391]: Loss 0.3037656843662262\n",
      "[Epoch 12] Training Batch [374/391]: Loss 0.3871327340602875\n",
      "[Epoch 12] Training Batch [375/391]: Loss 0.36308053135871887\n",
      "[Epoch 12] Training Batch [376/391]: Loss 0.3601832985877991\n",
      "[Epoch 12] Training Batch [377/391]: Loss 0.3215843439102173\n",
      "[Epoch 12] Training Batch [378/391]: Loss 0.3564658463001251\n",
      "[Epoch 12] Training Batch [379/391]: Loss 0.32051846385002136\n",
      "[Epoch 12] Training Batch [380/391]: Loss 0.39698082208633423\n",
      "[Epoch 12] Training Batch [381/391]: Loss 0.34331658482551575\n",
      "[Epoch 12] Training Batch [382/391]: Loss 0.31173399090766907\n",
      "[Epoch 12] Training Batch [383/391]: Loss 0.3399036228656769\n",
      "[Epoch 12] Training Batch [384/391]: Loss 0.4089807868003845\n",
      "[Epoch 12] Training Batch [385/391]: Loss 0.3516116142272949\n",
      "[Epoch 12] Training Batch [386/391]: Loss 0.3178655803203583\n",
      "[Epoch 12] Training Batch [387/391]: Loss 0.3464014232158661\n",
      "[Epoch 12] Training Batch [388/391]: Loss 0.3923310339450836\n",
      "[Epoch 12] Training Batch [389/391]: Loss 0.29177993535995483\n",
      "[Epoch 12] Training Batch [390/391]: Loss 0.3704969882965088\n",
      "[Epoch 12] Training Batch [391/391]: Loss 0.26118558645248413\n",
      "Epoch 12 - Train Loss: 0.2882\n",
      "*********  Epoch 13/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Training Batch [1/391]: Loss 0.2767435610294342\n",
      "[Epoch 13] Training Batch [2/391]: Loss 0.19772011041641235\n",
      "[Epoch 13] Training Batch [3/391]: Loss 0.17127177119255066\n",
      "[Epoch 13] Training Batch [4/391]: Loss 0.18533627688884735\n",
      "[Epoch 13] Training Batch [5/391]: Loss 0.19947059452533722\n",
      "[Epoch 13] Training Batch [6/391]: Loss 0.24485641717910767\n",
      "[Epoch 13] Training Batch [7/391]: Loss 0.22169248759746552\n",
      "[Epoch 13] Training Batch [8/391]: Loss 0.22879746556282043\n",
      "[Epoch 13] Training Batch [9/391]: Loss 0.11215084791183472\n",
      "[Epoch 13] Training Batch [10/391]: Loss 0.24086163938045502\n",
      "[Epoch 13] Training Batch [11/391]: Loss 0.15929193794727325\n",
      "[Epoch 13] Training Batch [12/391]: Loss 0.15442928671836853\n",
      "[Epoch 13] Training Batch [13/391]: Loss 0.22383375465869904\n",
      "[Epoch 13] Training Batch [14/391]: Loss 0.14631812274456024\n",
      "[Epoch 13] Training Batch [15/391]: Loss 0.18617992103099823\n",
      "[Epoch 13] Training Batch [16/391]: Loss 0.15707868337631226\n",
      "[Epoch 13] Training Batch [17/391]: Loss 0.18278194963932037\n",
      "[Epoch 13] Training Batch [18/391]: Loss 0.17530907690525055\n",
      "[Epoch 13] Training Batch [19/391]: Loss 0.16627471148967743\n",
      "[Epoch 13] Training Batch [20/391]: Loss 0.14392079412937164\n",
      "[Epoch 13] Training Batch [21/391]: Loss 0.2018953412771225\n",
      "[Epoch 13] Training Batch [22/391]: Loss 0.14489783346652985\n",
      "[Epoch 13] Training Batch [23/391]: Loss 0.1772167682647705\n",
      "[Epoch 13] Training Batch [24/391]: Loss 0.1650318205356598\n",
      "[Epoch 13] Training Batch [25/391]: Loss 0.13409188389778137\n",
      "[Epoch 13] Training Batch [26/391]: Loss 0.20831920206546783\n",
      "[Epoch 13] Training Batch [27/391]: Loss 0.13600218296051025\n",
      "[Epoch 13] Training Batch [28/391]: Loss 0.26524609327316284\n",
      "[Epoch 13] Training Batch [29/391]: Loss 0.12233802676200867\n",
      "[Epoch 13] Training Batch [30/391]: Loss 0.2288774996995926\n",
      "[Epoch 13] Training Batch [31/391]: Loss 0.1389479786157608\n",
      "[Epoch 13] Training Batch [32/391]: Loss 0.20301830768585205\n",
      "[Epoch 13] Training Batch [33/391]: Loss 0.15182682871818542\n",
      "[Epoch 13] Training Batch [34/391]: Loss 0.09807968139648438\n",
      "[Epoch 13] Training Batch [35/391]: Loss 0.2258850634098053\n",
      "[Epoch 13] Training Batch [36/391]: Loss 0.1291753500699997\n",
      "[Epoch 13] Training Batch [37/391]: Loss 0.21016888320446014\n",
      "[Epoch 13] Training Batch [38/391]: Loss 0.17497852444648743\n",
      "[Epoch 13] Training Batch [39/391]: Loss 0.11750006675720215\n",
      "[Epoch 13] Training Batch [40/391]: Loss 0.14026740193367004\n",
      "[Epoch 13] Training Batch [41/391]: Loss 0.20358878374099731\n",
      "[Epoch 13] Training Batch [42/391]: Loss 0.0877847671508789\n",
      "[Epoch 13] Training Batch [43/391]: Loss 0.18560780584812164\n",
      "[Epoch 13] Training Batch [44/391]: Loss 0.1417894959449768\n",
      "[Epoch 13] Training Batch [45/391]: Loss 0.2547856569290161\n",
      "[Epoch 13] Training Batch [46/391]: Loss 0.14658057689666748\n",
      "[Epoch 13] Training Batch [47/391]: Loss 0.19957533478736877\n",
      "[Epoch 13] Training Batch [48/391]: Loss 0.1417287141084671\n",
      "[Epoch 13] Training Batch [49/391]: Loss 0.10442021489143372\n",
      "[Epoch 13] Training Batch [50/391]: Loss 0.1885252594947815\n",
      "[Epoch 13] Training Batch [51/391]: Loss 0.1775018274784088\n",
      "[Epoch 13] Training Batch [52/391]: Loss 0.1241719201207161\n",
      "[Epoch 13] Training Batch [53/391]: Loss 0.12936021387577057\n",
      "[Epoch 13] Training Batch [54/391]: Loss 0.1051323339343071\n",
      "[Epoch 13] Training Batch [55/391]: Loss 0.18659989535808563\n",
      "[Epoch 13] Training Batch [56/391]: Loss 0.22659829258918762\n",
      "[Epoch 13] Training Batch [57/391]: Loss 0.13879752159118652\n",
      "[Epoch 13] Training Batch [58/391]: Loss 0.16708922386169434\n",
      "[Epoch 13] Training Batch [59/391]: Loss 0.17866456508636475\n",
      "[Epoch 13] Training Batch [60/391]: Loss 0.14439110457897186\n",
      "[Epoch 13] Training Batch [61/391]: Loss 0.17664089798927307\n",
      "[Epoch 13] Training Batch [62/391]: Loss 0.1682901531457901\n",
      "[Epoch 13] Training Batch [63/391]: Loss 0.21812115609645844\n",
      "[Epoch 13] Training Batch [64/391]: Loss 0.12306235730648041\n",
      "[Epoch 13] Training Batch [65/391]: Loss 0.14744244515895844\n",
      "[Epoch 13] Training Batch [66/391]: Loss 0.10788993537425995\n",
      "[Epoch 13] Training Batch [67/391]: Loss 0.1443345695734024\n",
      "[Epoch 13] Training Batch [68/391]: Loss 0.1788337081670761\n",
      "[Epoch 13] Training Batch [69/391]: Loss 0.15297213196754456\n",
      "[Epoch 13] Training Batch [70/391]: Loss 0.16931240260601044\n",
      "[Epoch 13] Training Batch [71/391]: Loss 0.17462077736854553\n",
      "[Epoch 13] Training Batch [72/391]: Loss 0.14015759527683258\n",
      "[Epoch 13] Training Batch [73/391]: Loss 0.10860971361398697\n",
      "[Epoch 13] Training Batch [74/391]: Loss 0.20765036344528198\n",
      "[Epoch 13] Training Batch [75/391]: Loss 0.10162659734487534\n",
      "[Epoch 13] Training Batch [76/391]: Loss 0.13089706003665924\n",
      "[Epoch 13] Training Batch [77/391]: Loss 0.11534218490123749\n",
      "[Epoch 13] Training Batch [78/391]: Loss 0.18783970177173615\n",
      "[Epoch 13] Training Batch [79/391]: Loss 0.2279108464717865\n",
      "[Epoch 13] Training Batch [80/391]: Loss 0.19660240411758423\n",
      "[Epoch 13] Training Batch [81/391]: Loss 0.12295684963464737\n",
      "[Epoch 13] Training Batch [82/391]: Loss 0.13605345785617828\n",
      "[Epoch 13] Training Batch [83/391]: Loss 0.16145756840705872\n",
      "[Epoch 13] Training Batch [84/391]: Loss 0.16031256318092346\n",
      "[Epoch 13] Training Batch [85/391]: Loss 0.18334580957889557\n",
      "[Epoch 13] Training Batch [86/391]: Loss 0.13538683950901031\n",
      "[Epoch 13] Training Batch [87/391]: Loss 0.17955327033996582\n",
      "[Epoch 13] Training Batch [88/391]: Loss 0.17123627662658691\n",
      "[Epoch 13] Training Batch [89/391]: Loss 0.15227966010570526\n",
      "[Epoch 13] Training Batch [90/391]: Loss 0.13013501465320587\n",
      "[Epoch 13] Training Batch [91/391]: Loss 0.13905110955238342\n",
      "[Epoch 13] Training Batch [92/391]: Loss 0.1517244428396225\n",
      "[Epoch 13] Training Batch [93/391]: Loss 0.11296261847019196\n",
      "[Epoch 13] Training Batch [94/391]: Loss 0.12885460257530212\n",
      "[Epoch 13] Training Batch [95/391]: Loss 0.11756329983472824\n",
      "[Epoch 13] Training Batch [96/391]: Loss 0.13290245831012726\n",
      "[Epoch 13] Training Batch [97/391]: Loss 0.21455490589141846\n",
      "[Epoch 13] Training Batch [98/391]: Loss 0.18334418535232544\n",
      "[Epoch 13] Training Batch [99/391]: Loss 0.19962750375270844\n",
      "[Epoch 13] Training Batch [100/391]: Loss 0.09018614143133163\n",
      "[Epoch 13] Training Batch [101/391]: Loss 0.21682243049144745\n",
      "[Epoch 13] Training Batch [102/391]: Loss 0.11351261287927628\n",
      "[Epoch 13] Training Batch [103/391]: Loss 0.1493484079837799\n",
      "[Epoch 13] Training Batch [104/391]: Loss 0.20515258610248566\n",
      "[Epoch 13] Training Batch [105/391]: Loss 0.148581400513649\n",
      "[Epoch 13] Training Batch [106/391]: Loss 0.16653358936309814\n",
      "[Epoch 13] Training Batch [107/391]: Loss 0.21750134229660034\n",
      "[Epoch 13] Training Batch [108/391]: Loss 0.18452158570289612\n",
      "[Epoch 13] Training Batch [109/391]: Loss 0.08865729719400406\n",
      "[Epoch 13] Training Batch [110/391]: Loss 0.12689125537872314\n",
      "[Epoch 13] Training Batch [111/391]: Loss 0.15910422801971436\n",
      "[Epoch 13] Training Batch [112/391]: Loss 0.16174668073654175\n",
      "[Epoch 13] Training Batch [113/391]: Loss 0.2451982945203781\n",
      "[Epoch 13] Training Batch [114/391]: Loss 0.16508299112319946\n",
      "[Epoch 13] Training Batch [115/391]: Loss 0.216216042637825\n",
      "[Epoch 13] Training Batch [116/391]: Loss 0.17793023586273193\n",
      "[Epoch 13] Training Batch [117/391]: Loss 0.1338212639093399\n",
      "[Epoch 13] Training Batch [118/391]: Loss 0.18090903759002686\n",
      "[Epoch 13] Training Batch [119/391]: Loss 0.2044583410024643\n",
      "[Epoch 13] Training Batch [120/391]: Loss 0.17130456864833832\n",
      "[Epoch 13] Training Batch [121/391]: Loss 0.07374134659767151\n",
      "[Epoch 13] Training Batch [122/391]: Loss 0.23866088688373566\n",
      "[Epoch 13] Training Batch [123/391]: Loss 0.20664241909980774\n",
      "[Epoch 13] Training Batch [124/391]: Loss 0.2058444768190384\n",
      "[Epoch 13] Training Batch [125/391]: Loss 0.22444313764572144\n",
      "[Epoch 13] Training Batch [126/391]: Loss 0.16157779097557068\n",
      "[Epoch 13] Training Batch [127/391]: Loss 0.1453678160905838\n",
      "[Epoch 13] Training Batch [128/391]: Loss 0.18612739443778992\n",
      "[Epoch 13] Training Batch [129/391]: Loss 0.10760775208473206\n",
      "[Epoch 13] Training Batch [130/391]: Loss 0.18111465871334076\n",
      "[Epoch 13] Training Batch [131/391]: Loss 0.19778570532798767\n",
      "[Epoch 13] Training Batch [132/391]: Loss 0.1771978884935379\n",
      "[Epoch 13] Training Batch [133/391]: Loss 0.13382810354232788\n",
      "[Epoch 13] Training Batch [134/391]: Loss 0.14581328630447388\n",
      "[Epoch 13] Training Batch [135/391]: Loss 0.1804613620042801\n",
      "[Epoch 13] Training Batch [136/391]: Loss 0.11666548997163773\n",
      "[Epoch 13] Training Batch [137/391]: Loss 0.12532848119735718\n",
      "[Epoch 13] Training Batch [138/391]: Loss 0.13364841043949127\n",
      "[Epoch 13] Training Batch [139/391]: Loss 0.1589096337556839\n",
      "[Epoch 13] Training Batch [140/391]: Loss 0.16033543646335602\n",
      "[Epoch 13] Training Batch [141/391]: Loss 0.14542357623577118\n",
      "[Epoch 13] Training Batch [142/391]: Loss 0.227105513215065\n",
      "[Epoch 13] Training Batch [143/391]: Loss 0.13831818103790283\n",
      "[Epoch 13] Training Batch [144/391]: Loss 0.23019173741340637\n",
      "[Epoch 13] Training Batch [145/391]: Loss 0.1411294937133789\n",
      "[Epoch 13] Training Batch [146/391]: Loss 0.1348302811384201\n",
      "[Epoch 13] Training Batch [147/391]: Loss 0.19320693612098694\n",
      "[Epoch 13] Training Batch [148/391]: Loss 0.16256703436374664\n",
      "[Epoch 13] Training Batch [149/391]: Loss 0.14220887422561646\n",
      "[Epoch 13] Training Batch [150/391]: Loss 0.17931458353996277\n",
      "[Epoch 13] Training Batch [151/391]: Loss 0.18110279738903046\n",
      "[Epoch 13] Training Batch [152/391]: Loss 0.2458954006433487\n",
      "[Epoch 13] Training Batch [153/391]: Loss 0.15763254463672638\n",
      "[Epoch 13] Training Batch [154/391]: Loss 0.12530064582824707\n",
      "[Epoch 13] Training Batch [155/391]: Loss 0.25823384523391724\n",
      "[Epoch 13] Training Batch [156/391]: Loss 0.28438472747802734\n",
      "[Epoch 13] Training Batch [157/391]: Loss 0.16118542850017548\n",
      "[Epoch 13] Training Batch [158/391]: Loss 0.14537973701953888\n",
      "[Epoch 13] Training Batch [159/391]: Loss 0.12498054653406143\n",
      "[Epoch 13] Training Batch [160/391]: Loss 0.13133937120437622\n",
      "[Epoch 13] Training Batch [161/391]: Loss 0.19141030311584473\n",
      "[Epoch 13] Training Batch [162/391]: Loss 0.11276277899742126\n",
      "[Epoch 13] Training Batch [163/391]: Loss 0.19023285806179047\n",
      "[Epoch 13] Training Batch [164/391]: Loss 0.17733454704284668\n",
      "[Epoch 13] Training Batch [165/391]: Loss 0.19315646588802338\n",
      "[Epoch 13] Training Batch [166/391]: Loss 0.22494518756866455\n",
      "[Epoch 13] Training Batch [167/391]: Loss 0.20147371292114258\n",
      "[Epoch 13] Training Batch [168/391]: Loss 0.29587751626968384\n",
      "[Epoch 13] Training Batch [169/391]: Loss 0.1597067415714264\n",
      "[Epoch 13] Training Batch [170/391]: Loss 0.18275421857833862\n",
      "[Epoch 13] Training Batch [171/391]: Loss 0.18451479077339172\n",
      "[Epoch 13] Training Batch [172/391]: Loss 0.1837584525346756\n",
      "[Epoch 13] Training Batch [173/391]: Loss 0.15342506766319275\n",
      "[Epoch 13] Training Batch [174/391]: Loss 0.2012041211128235\n",
      "[Epoch 13] Training Batch [175/391]: Loss 0.15806768834590912\n",
      "[Epoch 13] Training Batch [176/391]: Loss 0.22968415915966034\n",
      "[Epoch 13] Training Batch [177/391]: Loss 0.1709350198507309\n",
      "[Epoch 13] Training Batch [178/391]: Loss 0.2633872926235199\n",
      "[Epoch 13] Training Batch [179/391]: Loss 0.13656869530677795\n",
      "[Epoch 13] Training Batch [180/391]: Loss 0.30919820070266724\n",
      "[Epoch 13] Training Batch [181/391]: Loss 0.12648239731788635\n",
      "[Epoch 13] Training Batch [182/391]: Loss 0.21169739961624146\n",
      "[Epoch 13] Training Batch [183/391]: Loss 0.18971818685531616\n",
      "[Epoch 13] Training Batch [184/391]: Loss 0.18586453795433044\n",
      "[Epoch 13] Training Batch [185/391]: Loss 0.2721116244792938\n",
      "[Epoch 13] Training Batch [186/391]: Loss 0.1822347790002823\n",
      "[Epoch 13] Training Batch [187/391]: Loss 0.2138012945652008\n",
      "[Epoch 13] Training Batch [188/391]: Loss 0.15226905047893524\n",
      "[Epoch 13] Training Batch [189/391]: Loss 0.1714385449886322\n",
      "[Epoch 13] Training Batch [190/391]: Loss 0.26650387048721313\n",
      "[Epoch 13] Training Batch [191/391]: Loss 0.12424880266189575\n",
      "[Epoch 13] Training Batch [192/391]: Loss 0.1985260546207428\n",
      "[Epoch 13] Training Batch [193/391]: Loss 0.2141583263874054\n",
      "[Epoch 13] Training Batch [194/391]: Loss 0.16812479496002197\n",
      "[Epoch 13] Training Batch [195/391]: Loss 0.17530298233032227\n",
      "[Epoch 13] Training Batch [196/391]: Loss 0.14385150372982025\n",
      "[Epoch 13] Training Batch [197/391]: Loss 0.24847589433193207\n",
      "[Epoch 13] Training Batch [198/391]: Loss 0.20188705623149872\n",
      "[Epoch 13] Training Batch [199/391]: Loss 0.23478226363658905\n",
      "[Epoch 13] Training Batch [200/391]: Loss 0.2133348435163498\n",
      "[Epoch 13] Training Batch [201/391]: Loss 0.2660757601261139\n",
      "[Epoch 13] Training Batch [202/391]: Loss 0.23936887085437775\n",
      "[Epoch 13] Training Batch [203/391]: Loss 0.1926962435245514\n",
      "[Epoch 13] Training Batch [204/391]: Loss 0.2450772225856781\n",
      "[Epoch 13] Training Batch [205/391]: Loss 0.1456325799226761\n",
      "[Epoch 13] Training Batch [206/391]: Loss 0.1985936462879181\n",
      "[Epoch 13] Training Batch [207/391]: Loss 0.1593184471130371\n",
      "[Epoch 13] Training Batch [208/391]: Loss 0.21778813004493713\n",
      "[Epoch 13] Training Batch [209/391]: Loss 0.1656085103750229\n",
      "[Epoch 13] Training Batch [210/391]: Loss 0.20486973226070404\n",
      "[Epoch 13] Training Batch [211/391]: Loss 0.18535365164279938\n",
      "[Epoch 13] Training Batch [212/391]: Loss 0.2507118284702301\n",
      "[Epoch 13] Training Batch [213/391]: Loss 0.19395025074481964\n",
      "[Epoch 13] Training Batch [214/391]: Loss 0.16685009002685547\n",
      "[Epoch 13] Training Batch [215/391]: Loss 0.19865170121192932\n",
      "[Epoch 13] Training Batch [216/391]: Loss 0.281258761882782\n",
      "[Epoch 13] Training Batch [217/391]: Loss 0.21520650386810303\n",
      "[Epoch 13] Training Batch [218/391]: Loss 0.13779371976852417\n",
      "[Epoch 13] Training Batch [219/391]: Loss 0.23517221212387085\n",
      "[Epoch 13] Training Batch [220/391]: Loss 0.18582113087177277\n",
      "[Epoch 13] Training Batch [221/391]: Loss 0.20535783469676971\n",
      "[Epoch 13] Training Batch [222/391]: Loss 0.25839343667030334\n",
      "[Epoch 13] Training Batch [223/391]: Loss 0.18565602600574493\n",
      "[Epoch 13] Training Batch [224/391]: Loss 0.09881038218736649\n",
      "[Epoch 13] Training Batch [225/391]: Loss 0.24237248301506042\n",
      "[Epoch 13] Training Batch [226/391]: Loss 0.2530686855316162\n",
      "[Epoch 13] Training Batch [227/391]: Loss 0.18579544126987457\n",
      "[Epoch 13] Training Batch [228/391]: Loss 0.20820333063602448\n",
      "[Epoch 13] Training Batch [229/391]: Loss 0.19713282585144043\n",
      "[Epoch 13] Training Batch [230/391]: Loss 0.1904507726430893\n",
      "[Epoch 13] Training Batch [231/391]: Loss 0.30323734879493713\n",
      "[Epoch 13] Training Batch [232/391]: Loss 0.17911437153816223\n",
      "[Epoch 13] Training Batch [233/391]: Loss 0.223633274435997\n",
      "[Epoch 13] Training Batch [234/391]: Loss 0.2519037425518036\n",
      "[Epoch 13] Training Batch [235/391]: Loss 0.19632510840892792\n",
      "[Epoch 13] Training Batch [236/391]: Loss 0.24865217506885529\n",
      "[Epoch 13] Training Batch [237/391]: Loss 0.2043815702199936\n",
      "[Epoch 13] Training Batch [238/391]: Loss 0.16251875460147858\n",
      "[Epoch 13] Training Batch [239/391]: Loss 0.31840550899505615\n",
      "[Epoch 13] Training Batch [240/391]: Loss 0.1402360200881958\n",
      "[Epoch 13] Training Batch [241/391]: Loss 0.11072807013988495\n",
      "[Epoch 13] Training Batch [242/391]: Loss 0.16706952452659607\n",
      "[Epoch 13] Training Batch [243/391]: Loss 0.21159952878952026\n",
      "[Epoch 13] Training Batch [244/391]: Loss 0.1983087658882141\n",
      "[Epoch 13] Training Batch [245/391]: Loss 0.21876640617847443\n",
      "[Epoch 13] Training Batch [246/391]: Loss 0.13816864788532257\n",
      "[Epoch 13] Training Batch [247/391]: Loss 0.1565692126750946\n",
      "[Epoch 13] Training Batch [248/391]: Loss 0.16130445897579193\n",
      "[Epoch 13] Training Batch [249/391]: Loss 0.21732217073440552\n",
      "[Epoch 13] Training Batch [250/391]: Loss 0.16020292043685913\n",
      "[Epoch 13] Training Batch [251/391]: Loss 0.17572028934955597\n",
      "[Epoch 13] Training Batch [252/391]: Loss 0.13203004002571106\n",
      "[Epoch 13] Training Batch [253/391]: Loss 0.16244351863861084\n",
      "[Epoch 13] Training Batch [254/391]: Loss 0.1697722226381302\n",
      "[Epoch 13] Training Batch [255/391]: Loss 0.2168889045715332\n",
      "[Epoch 13] Training Batch [256/391]: Loss 0.24069394171237946\n",
      "[Epoch 13] Training Batch [257/391]: Loss 0.19155775010585785\n",
      "[Epoch 13] Training Batch [258/391]: Loss 0.1601562649011612\n",
      "[Epoch 13] Training Batch [259/391]: Loss 0.1318112313747406\n",
      "[Epoch 13] Training Batch [260/391]: Loss 0.16191235184669495\n",
      "[Epoch 13] Training Batch [261/391]: Loss 0.13919048011302948\n",
      "[Epoch 13] Training Batch [262/391]: Loss 0.1503942757844925\n",
      "[Epoch 13] Training Batch [263/391]: Loss 0.1643025130033493\n",
      "[Epoch 13] Training Batch [264/391]: Loss 0.2374761700630188\n",
      "[Epoch 13] Training Batch [265/391]: Loss 0.17214450240135193\n",
      "[Epoch 13] Training Batch [266/391]: Loss 0.186715766787529\n",
      "[Epoch 13] Training Batch [267/391]: Loss 0.166265070438385\n",
      "[Epoch 13] Training Batch [268/391]: Loss 0.19203956425189972\n",
      "[Epoch 13] Training Batch [269/391]: Loss 0.1634099930524826\n",
      "[Epoch 13] Training Batch [270/391]: Loss 0.22947098314762115\n",
      "[Epoch 13] Training Batch [271/391]: Loss 0.2102721929550171\n",
      "[Epoch 13] Training Batch [272/391]: Loss 0.13876771926879883\n",
      "[Epoch 13] Training Batch [273/391]: Loss 0.1757277399301529\n",
      "[Epoch 13] Training Batch [274/391]: Loss 0.24612273275852203\n",
      "[Epoch 13] Training Batch [275/391]: Loss 0.2555626928806305\n",
      "[Epoch 13] Training Batch [276/391]: Loss 0.2235700786113739\n",
      "[Epoch 13] Training Batch [277/391]: Loss 0.15879078209400177\n",
      "[Epoch 13] Training Batch [278/391]: Loss 0.22823934257030487\n",
      "[Epoch 13] Training Batch [279/391]: Loss 0.18974488973617554\n",
      "[Epoch 13] Training Batch [280/391]: Loss 0.18260131776332855\n",
      "[Epoch 13] Training Batch [281/391]: Loss 0.14283932745456696\n",
      "[Epoch 13] Training Batch [282/391]: Loss 0.225130096077919\n",
      "[Epoch 13] Training Batch [283/391]: Loss 0.2410939633846283\n",
      "[Epoch 13] Training Batch [284/391]: Loss 0.23955093324184418\n",
      "[Epoch 13] Training Batch [285/391]: Loss 0.24947263300418854\n",
      "[Epoch 13] Training Batch [286/391]: Loss 0.2431444674730301\n",
      "[Epoch 13] Training Batch [287/391]: Loss 0.22291621565818787\n",
      "[Epoch 13] Training Batch [288/391]: Loss 0.2172580361366272\n",
      "[Epoch 13] Training Batch [289/391]: Loss 0.2530163526535034\n",
      "[Epoch 13] Training Batch [290/391]: Loss 0.1411907821893692\n",
      "[Epoch 13] Training Batch [291/391]: Loss 0.303602933883667\n",
      "[Epoch 13] Training Batch [292/391]: Loss 0.13796047866344452\n",
      "[Epoch 13] Training Batch [293/391]: Loss 0.19292235374450684\n",
      "[Epoch 13] Training Batch [294/391]: Loss 0.2436351329088211\n",
      "[Epoch 13] Training Batch [295/391]: Loss 0.26816219091415405\n",
      "[Epoch 13] Training Batch [296/391]: Loss 0.33191630244255066\n",
      "[Epoch 13] Training Batch [297/391]: Loss 0.2743089199066162\n",
      "[Epoch 13] Training Batch [298/391]: Loss 0.29532450437545776\n",
      "[Epoch 13] Training Batch [299/391]: Loss 0.3222756087779999\n",
      "[Epoch 13] Training Batch [300/391]: Loss 0.22570429742336273\n",
      "[Epoch 13] Training Batch [301/391]: Loss 0.21750572323799133\n",
      "[Epoch 13] Training Batch [302/391]: Loss 0.21537692844867706\n",
      "[Epoch 13] Training Batch [303/391]: Loss 0.3000577688217163\n",
      "[Epoch 13] Training Batch [304/391]: Loss 0.2670988142490387\n",
      "[Epoch 13] Training Batch [305/391]: Loss 0.25663596391677856\n",
      "[Epoch 13] Training Batch [306/391]: Loss 0.17830690741539001\n",
      "[Epoch 13] Training Batch [307/391]: Loss 0.13525007665157318\n",
      "[Epoch 13] Training Batch [308/391]: Loss 0.3217211365699768\n",
      "[Epoch 13] Training Batch [309/391]: Loss 0.31935837864875793\n",
      "[Epoch 13] Training Batch [310/391]: Loss 0.3285890817642212\n",
      "[Epoch 13] Training Batch [311/391]: Loss 0.1660109907388687\n",
      "[Epoch 13] Training Batch [312/391]: Loss 0.244390070438385\n",
      "[Epoch 13] Training Batch [313/391]: Loss 0.19451388716697693\n",
      "[Epoch 13] Training Batch [314/391]: Loss 0.246621236205101\n",
      "[Epoch 13] Training Batch [315/391]: Loss 0.254202663898468\n",
      "[Epoch 13] Training Batch [316/391]: Loss 0.17271311581134796\n",
      "[Epoch 13] Training Batch [317/391]: Loss 0.26479628682136536\n",
      "[Epoch 13] Training Batch [318/391]: Loss 0.3222273886203766\n",
      "[Epoch 13] Training Batch [319/391]: Loss 0.27033692598342896\n",
      "[Epoch 13] Training Batch [320/391]: Loss 0.18506461381912231\n",
      "[Epoch 13] Training Batch [321/391]: Loss 0.30903953313827515\n",
      "[Epoch 13] Training Batch [322/391]: Loss 0.21922633051872253\n",
      "[Epoch 13] Training Batch [323/391]: Loss 0.19991105794906616\n",
      "[Epoch 13] Training Batch [324/391]: Loss 0.15267451107501984\n",
      "[Epoch 13] Training Batch [325/391]: Loss 0.20897696912288666\n",
      "[Epoch 13] Training Batch [326/391]: Loss 0.27209699153900146\n",
      "[Epoch 13] Training Batch [327/391]: Loss 0.2933771312236786\n",
      "[Epoch 13] Training Batch [328/391]: Loss 0.2709079384803772\n",
      "[Epoch 13] Training Batch [329/391]: Loss 0.36300432682037354\n",
      "[Epoch 13] Training Batch [330/391]: Loss 0.1449933648109436\n",
      "[Epoch 13] Training Batch [331/391]: Loss 0.2596476376056671\n",
      "[Epoch 13] Training Batch [332/391]: Loss 0.21463294327259064\n",
      "[Epoch 13] Training Batch [333/391]: Loss 0.3415965437889099\n",
      "[Epoch 13] Training Batch [334/391]: Loss 0.23036113381385803\n",
      "[Epoch 13] Training Batch [335/391]: Loss 0.25581100583076477\n",
      "[Epoch 13] Training Batch [336/391]: Loss 0.24906909465789795\n",
      "[Epoch 13] Training Batch [337/391]: Loss 0.20838822424411774\n",
      "[Epoch 13] Training Batch [338/391]: Loss 0.25192081928253174\n",
      "[Epoch 13] Training Batch [339/391]: Loss 0.16078685224056244\n",
      "[Epoch 13] Training Batch [340/391]: Loss 0.21038827300071716\n",
      "[Epoch 13] Training Batch [341/391]: Loss 0.29464125633239746\n",
      "[Epoch 13] Training Batch [342/391]: Loss 0.1894485503435135\n",
      "[Epoch 13] Training Batch [343/391]: Loss 0.25692248344421387\n",
      "[Epoch 13] Training Batch [344/391]: Loss 0.2986728250980377\n",
      "[Epoch 13] Training Batch [345/391]: Loss 0.15552787482738495\n",
      "[Epoch 13] Training Batch [346/391]: Loss 0.22970111668109894\n",
      "[Epoch 13] Training Batch [347/391]: Loss 0.22300727665424347\n",
      "[Epoch 13] Training Batch [348/391]: Loss 0.2454385906457901\n",
      "[Epoch 13] Training Batch [349/391]: Loss 0.19364102184772491\n",
      "[Epoch 13] Training Batch [350/391]: Loss 0.2329387068748474\n",
      "[Epoch 13] Training Batch [351/391]: Loss 0.27866098284721375\n",
      "[Epoch 13] Training Batch [352/391]: Loss 0.1736464649438858\n",
      "[Epoch 13] Training Batch [353/391]: Loss 0.25004225969314575\n",
      "[Epoch 13] Training Batch [354/391]: Loss 0.24262553453445435\n",
      "[Epoch 13] Training Batch [355/391]: Loss 0.2926751673221588\n",
      "[Epoch 13] Training Batch [356/391]: Loss 0.2171914428472519\n",
      "[Epoch 13] Training Batch [357/391]: Loss 0.22780562937259674\n",
      "[Epoch 13] Training Batch [358/391]: Loss 0.15603892505168915\n",
      "[Epoch 13] Training Batch [359/391]: Loss 0.20290689170360565\n",
      "[Epoch 13] Training Batch [360/391]: Loss 0.25621846318244934\n",
      "[Epoch 13] Training Batch [361/391]: Loss 0.3401726186275482\n",
      "[Epoch 13] Training Batch [362/391]: Loss 0.16710489988327026\n",
      "[Epoch 13] Training Batch [363/391]: Loss 0.2896933853626251\n",
      "[Epoch 13] Training Batch [364/391]: Loss 0.19739025831222534\n",
      "[Epoch 13] Training Batch [365/391]: Loss 0.2625769376754761\n",
      "[Epoch 13] Training Batch [366/391]: Loss 0.19057412445545197\n",
      "[Epoch 13] Training Batch [367/391]: Loss 0.2571282684803009\n",
      "[Epoch 13] Training Batch [368/391]: Loss 0.24364900588989258\n",
      "[Epoch 13] Training Batch [369/391]: Loss 0.23447518050670624\n",
      "[Epoch 13] Training Batch [370/391]: Loss 0.1364966630935669\n",
      "[Epoch 13] Training Batch [371/391]: Loss 0.26860442757606506\n",
      "[Epoch 13] Training Batch [372/391]: Loss 0.1993207335472107\n",
      "[Epoch 13] Training Batch [373/391]: Loss 0.2635343670845032\n",
      "[Epoch 13] Training Batch [374/391]: Loss 0.24866923689842224\n",
      "[Epoch 13] Training Batch [375/391]: Loss 0.14737185835838318\n",
      "[Epoch 13] Training Batch [376/391]: Loss 0.26430341601371765\n",
      "[Epoch 13] Training Batch [377/391]: Loss 0.22642523050308228\n",
      "[Epoch 13] Training Batch [378/391]: Loss 0.2444806843996048\n",
      "[Epoch 13] Training Batch [379/391]: Loss 0.2946024537086487\n",
      "[Epoch 13] Training Batch [380/391]: Loss 0.3191946744918823\n",
      "[Epoch 13] Training Batch [381/391]: Loss 0.20583999156951904\n",
      "[Epoch 13] Training Batch [382/391]: Loss 0.2846274673938751\n",
      "[Epoch 13] Training Batch [383/391]: Loss 0.2705404460430145\n",
      "[Epoch 13] Training Batch [384/391]: Loss 0.3296319246292114\n",
      "[Epoch 13] Training Batch [385/391]: Loss 0.389413058757782\n",
      "[Epoch 13] Training Batch [386/391]: Loss 0.3788560628890991\n",
      "[Epoch 13] Training Batch [387/391]: Loss 0.22656281292438507\n",
      "[Epoch 13] Training Batch [388/391]: Loss 0.2707115411758423\n",
      "[Epoch 13] Training Batch [389/391]: Loss 0.269187867641449\n",
      "[Epoch 13] Training Batch [390/391]: Loss 0.3019965887069702\n",
      "[Epoch 13] Training Batch [391/391]: Loss 0.22903268039226532\n",
      "Epoch 13 - Train Loss: 0.1975\n",
      "*********  Epoch 14/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Training Batch [1/391]: Loss 0.15841686725616455\n",
      "[Epoch 14] Training Batch [2/391]: Loss 0.13790588080883026\n",
      "[Epoch 14] Training Batch [3/391]: Loss 0.16071461141109467\n",
      "[Epoch 14] Training Batch [4/391]: Loss 0.15973550081253052\n",
      "[Epoch 14] Training Batch [5/391]: Loss 0.1524011641740799\n",
      "[Epoch 14] Training Batch [6/391]: Loss 0.19381257891654968\n",
      "[Epoch 14] Training Batch [7/391]: Loss 0.14181068539619446\n",
      "[Epoch 14] Training Batch [8/391]: Loss 0.15576618909835815\n",
      "[Epoch 14] Training Batch [9/391]: Loss 0.1197042465209961\n",
      "[Epoch 14] Training Batch [10/391]: Loss 0.11591769754886627\n",
      "[Epoch 14] Training Batch [11/391]: Loss 0.26950156688690186\n",
      "[Epoch 14] Training Batch [12/391]: Loss 0.14873571693897247\n",
      "[Epoch 14] Training Batch [13/391]: Loss 0.08815205097198486\n",
      "[Epoch 14] Training Batch [14/391]: Loss 0.1078106090426445\n",
      "[Epoch 14] Training Batch [15/391]: Loss 0.11874443292617798\n",
      "[Epoch 14] Training Batch [16/391]: Loss 0.1738172471523285\n",
      "[Epoch 14] Training Batch [17/391]: Loss 0.2011352777481079\n",
      "[Epoch 14] Training Batch [18/391]: Loss 0.17021062970161438\n",
      "[Epoch 14] Training Batch [19/391]: Loss 0.13700610399246216\n",
      "[Epoch 14] Training Batch [20/391]: Loss 0.11256954073905945\n",
      "[Epoch 14] Training Batch [21/391]: Loss 0.14288261532783508\n",
      "[Epoch 14] Training Batch [22/391]: Loss 0.08746841549873352\n",
      "[Epoch 14] Training Batch [23/391]: Loss 0.12314455956220627\n",
      "[Epoch 14] Training Batch [24/391]: Loss 0.11803437024354935\n",
      "[Epoch 14] Training Batch [25/391]: Loss 0.14330638945102692\n",
      "[Epoch 14] Training Batch [26/391]: Loss 0.16244234144687653\n",
      "[Epoch 14] Training Batch [27/391]: Loss 0.1435464322566986\n",
      "[Epoch 14] Training Batch [28/391]: Loss 0.13432113826274872\n",
      "[Epoch 14] Training Batch [29/391]: Loss 0.13298681378364563\n",
      "[Epoch 14] Training Batch [30/391]: Loss 0.1133691817522049\n",
      "[Epoch 14] Training Batch [31/391]: Loss 0.17306098341941833\n",
      "[Epoch 14] Training Batch [32/391]: Loss 0.13996939361095428\n",
      "[Epoch 14] Training Batch [33/391]: Loss 0.08508334308862686\n",
      "[Epoch 14] Training Batch [34/391]: Loss 0.10393553227186203\n",
      "[Epoch 14] Training Batch [35/391]: Loss 0.13806551694869995\n",
      "[Epoch 14] Training Batch [36/391]: Loss 0.12373935431241989\n",
      "[Epoch 14] Training Batch [37/391]: Loss 0.06341876089572906\n",
      "[Epoch 14] Training Batch [38/391]: Loss 0.14055299758911133\n",
      "[Epoch 14] Training Batch [39/391]: Loss 0.20470385253429413\n",
      "[Epoch 14] Training Batch [40/391]: Loss 0.19222940504550934\n",
      "[Epoch 14] Training Batch [41/391]: Loss 0.07867834717035294\n",
      "[Epoch 14] Training Batch [42/391]: Loss 0.11307072639465332\n",
      "[Epoch 14] Training Batch [43/391]: Loss 0.15043756365776062\n",
      "[Epoch 14] Training Batch [44/391]: Loss 0.150734543800354\n",
      "[Epoch 14] Training Batch [45/391]: Loss 0.21865840256214142\n",
      "[Epoch 14] Training Batch [46/391]: Loss 0.11184113472700119\n",
      "[Epoch 14] Training Batch [47/391]: Loss 0.17324861884117126\n",
      "[Epoch 14] Training Batch [48/391]: Loss 0.2384362369775772\n",
      "[Epoch 14] Training Batch [49/391]: Loss 0.1610725075006485\n",
      "[Epoch 14] Training Batch [50/391]: Loss 0.10171028971672058\n",
      "[Epoch 14] Training Batch [51/391]: Loss 0.16904042661190033\n",
      "[Epoch 14] Training Batch [52/391]: Loss 0.14746898412704468\n",
      "[Epoch 14] Training Batch [53/391]: Loss 0.1495271623134613\n",
      "[Epoch 14] Training Batch [54/391]: Loss 0.18251368403434753\n",
      "[Epoch 14] Training Batch [55/391]: Loss 0.11340662837028503\n",
      "[Epoch 14] Training Batch [56/391]: Loss 0.18564997613430023\n",
      "[Epoch 14] Training Batch [57/391]: Loss 0.10511136054992676\n",
      "[Epoch 14] Training Batch [58/391]: Loss 0.13021522760391235\n",
      "[Epoch 14] Training Batch [59/391]: Loss 0.12781336903572083\n",
      "[Epoch 14] Training Batch [60/391]: Loss 0.0847935751080513\n",
      "[Epoch 14] Training Batch [61/391]: Loss 0.08917548507452011\n",
      "[Epoch 14] Training Batch [62/391]: Loss 0.11849445104598999\n",
      "[Epoch 14] Training Batch [63/391]: Loss 0.16638188064098358\n",
      "[Epoch 14] Training Batch [64/391]: Loss 0.1430305689573288\n",
      "[Epoch 14] Training Batch [65/391]: Loss 0.13798242807388306\n",
      "[Epoch 14] Training Batch [66/391]: Loss 0.07213679701089859\n",
      "[Epoch 14] Training Batch [67/391]: Loss 0.14214088022708893\n",
      "[Epoch 14] Training Batch [68/391]: Loss 0.2594407796859741\n",
      "[Epoch 14] Training Batch [69/391]: Loss 0.10972778499126434\n",
      "[Epoch 14] Training Batch [70/391]: Loss 0.06626247614622116\n",
      "[Epoch 14] Training Batch [71/391]: Loss 0.10310418158769608\n",
      "[Epoch 14] Training Batch [72/391]: Loss 0.1453646719455719\n",
      "[Epoch 14] Training Batch [73/391]: Loss 0.08483248949050903\n",
      "[Epoch 14] Training Batch [74/391]: Loss 0.10947729647159576\n",
      "[Epoch 14] Training Batch [75/391]: Loss 0.10947497934103012\n",
      "[Epoch 14] Training Batch [76/391]: Loss 0.08351395279169083\n",
      "[Epoch 14] Training Batch [77/391]: Loss 0.11189495772123337\n",
      "[Epoch 14] Training Batch [78/391]: Loss 0.0936005711555481\n",
      "[Epoch 14] Training Batch [79/391]: Loss 0.13464252650737762\n",
      "[Epoch 14] Training Batch [80/391]: Loss 0.09974022954702377\n",
      "[Epoch 14] Training Batch [81/391]: Loss 0.1393647938966751\n",
      "[Epoch 14] Training Batch [82/391]: Loss 0.14317788183689117\n",
      "[Epoch 14] Training Batch [83/391]: Loss 0.1440889835357666\n",
      "[Epoch 14] Training Batch [84/391]: Loss 0.09600432962179184\n",
      "[Epoch 14] Training Batch [85/391]: Loss 0.11777748912572861\n",
      "[Epoch 14] Training Batch [86/391]: Loss 0.09891727566719055\n",
      "[Epoch 14] Training Batch [87/391]: Loss 0.11535433679819107\n",
      "[Epoch 14] Training Batch [88/391]: Loss 0.11696526408195496\n",
      "[Epoch 14] Training Batch [89/391]: Loss 0.08346065133810043\n",
      "[Epoch 14] Training Batch [90/391]: Loss 0.1281658560037613\n",
      "[Epoch 14] Training Batch [91/391]: Loss 0.12296199798583984\n",
      "[Epoch 14] Training Batch [92/391]: Loss 0.1223810538649559\n",
      "[Epoch 14] Training Batch [93/391]: Loss 0.09326757490634918\n",
      "[Epoch 14] Training Batch [94/391]: Loss 0.11122862249612808\n",
      "[Epoch 14] Training Batch [95/391]: Loss 0.11386489123106003\n",
      "[Epoch 14] Training Batch [96/391]: Loss 0.11730008572340012\n",
      "[Epoch 14] Training Batch [97/391]: Loss 0.2029753178358078\n",
      "[Epoch 14] Training Batch [98/391]: Loss 0.18479596078395844\n",
      "[Epoch 14] Training Batch [99/391]: Loss 0.10799580812454224\n",
      "[Epoch 14] Training Batch [100/391]: Loss 0.08585834503173828\n",
      "[Epoch 14] Training Batch [101/391]: Loss 0.14786796271800995\n",
      "[Epoch 14] Training Batch [102/391]: Loss 0.15298479795455933\n",
      "[Epoch 14] Training Batch [103/391]: Loss 0.09868060797452927\n",
      "[Epoch 14] Training Batch [104/391]: Loss 0.0891060009598732\n",
      "[Epoch 14] Training Batch [105/391]: Loss 0.07320645451545715\n",
      "[Epoch 14] Training Batch [106/391]: Loss 0.12716808915138245\n",
      "[Epoch 14] Training Batch [107/391]: Loss 0.1222294494509697\n",
      "[Epoch 14] Training Batch [108/391]: Loss 0.1074923500418663\n",
      "[Epoch 14] Training Batch [109/391]: Loss 0.111371248960495\n",
      "[Epoch 14] Training Batch [110/391]: Loss 0.08401834964752197\n",
      "[Epoch 14] Training Batch [111/391]: Loss 0.11178183555603027\n",
      "[Epoch 14] Training Batch [112/391]: Loss 0.08645226061344147\n",
      "[Epoch 14] Training Batch [113/391]: Loss 0.12094977498054504\n",
      "[Epoch 14] Training Batch [114/391]: Loss 0.13253721594810486\n",
      "[Epoch 14] Training Batch [115/391]: Loss 0.1296062171459198\n",
      "[Epoch 14] Training Batch [116/391]: Loss 0.1984521746635437\n",
      "[Epoch 14] Training Batch [117/391]: Loss 0.10821522772312164\n",
      "[Epoch 14] Training Batch [118/391]: Loss 0.10759679973125458\n",
      "[Epoch 14] Training Batch [119/391]: Loss 0.17052532732486725\n",
      "[Epoch 14] Training Batch [120/391]: Loss 0.15050482749938965\n",
      "[Epoch 14] Training Batch [121/391]: Loss 0.12873442471027374\n",
      "[Epoch 14] Training Batch [122/391]: Loss 0.11025973409414291\n",
      "[Epoch 14] Training Batch [123/391]: Loss 0.06755787134170532\n",
      "[Epoch 14] Training Batch [124/391]: Loss 0.10412371903657913\n",
      "[Epoch 14] Training Batch [125/391]: Loss 0.07218226790428162\n",
      "[Epoch 14] Training Batch [126/391]: Loss 0.10433417558670044\n",
      "[Epoch 14] Training Batch [127/391]: Loss 0.1219477578997612\n",
      "[Epoch 14] Training Batch [128/391]: Loss 0.13132111728191376\n",
      "[Epoch 14] Training Batch [129/391]: Loss 0.15825453400611877\n",
      "[Epoch 14] Training Batch [130/391]: Loss 0.14926481246948242\n",
      "[Epoch 14] Training Batch [131/391]: Loss 0.1307784616947174\n",
      "[Epoch 14] Training Batch [132/391]: Loss 0.13295306265354156\n",
      "[Epoch 14] Training Batch [133/391]: Loss 0.1306171417236328\n",
      "[Epoch 14] Training Batch [134/391]: Loss 0.18037211894989014\n",
      "[Epoch 14] Training Batch [135/391]: Loss 0.059161003679037094\n",
      "[Epoch 14] Training Batch [136/391]: Loss 0.13557417690753937\n",
      "[Epoch 14] Training Batch [137/391]: Loss 0.1157371774315834\n",
      "[Epoch 14] Training Batch [138/391]: Loss 0.13172772526741028\n",
      "[Epoch 14] Training Batch [139/391]: Loss 0.149789497256279\n",
      "[Epoch 14] Training Batch [140/391]: Loss 0.13275747001171112\n",
      "[Epoch 14] Training Batch [141/391]: Loss 0.2091044783592224\n",
      "[Epoch 14] Training Batch [142/391]: Loss 0.07579974085092545\n",
      "[Epoch 14] Training Batch [143/391]: Loss 0.11294448375701904\n",
      "[Epoch 14] Training Batch [144/391]: Loss 0.13441641628742218\n",
      "[Epoch 14] Training Batch [145/391]: Loss 0.08257170766592026\n",
      "[Epoch 14] Training Batch [146/391]: Loss 0.12100746482610703\n",
      "[Epoch 14] Training Batch [147/391]: Loss 0.15928302705287933\n",
      "[Epoch 14] Training Batch [148/391]: Loss 0.13907262682914734\n",
      "[Epoch 14] Training Batch [149/391]: Loss 0.08383741974830627\n",
      "[Epoch 14] Training Batch [150/391]: Loss 0.09420942515134811\n",
      "[Epoch 14] Training Batch [151/391]: Loss 0.10015273094177246\n",
      "[Epoch 14] Training Batch [152/391]: Loss 0.12373858690261841\n",
      "[Epoch 14] Training Batch [153/391]: Loss 0.12125498801469803\n",
      "[Epoch 14] Training Batch [154/391]: Loss 0.15544404089450836\n",
      "[Epoch 14] Training Batch [155/391]: Loss 0.06981033831834793\n",
      "[Epoch 14] Training Batch [156/391]: Loss 0.17020544409751892\n",
      "[Epoch 14] Training Batch [157/391]: Loss 0.11788629740476608\n",
      "[Epoch 14] Training Batch [158/391]: Loss 0.12836985290050507\n",
      "[Epoch 14] Training Batch [159/391]: Loss 0.11284805834293365\n",
      "[Epoch 14] Training Batch [160/391]: Loss 0.1354960948228836\n",
      "[Epoch 14] Training Batch [161/391]: Loss 0.09490427374839783\n",
      "[Epoch 14] Training Batch [162/391]: Loss 0.10071046650409698\n",
      "[Epoch 14] Training Batch [163/391]: Loss 0.1083282083272934\n",
      "[Epoch 14] Training Batch [164/391]: Loss 0.13353416323661804\n",
      "[Epoch 14] Training Batch [165/391]: Loss 0.1779957115650177\n",
      "[Epoch 14] Training Batch [166/391]: Loss 0.22524666786193848\n",
      "[Epoch 14] Training Batch [167/391]: Loss 0.07908303290605545\n",
      "[Epoch 14] Training Batch [168/391]: Loss 0.08286423981189728\n",
      "[Epoch 14] Training Batch [169/391]: Loss 0.06993334740400314\n",
      "[Epoch 14] Training Batch [170/391]: Loss 0.1434951275587082\n",
      "[Epoch 14] Training Batch [171/391]: Loss 0.12485264241695404\n",
      "[Epoch 14] Training Batch [172/391]: Loss 0.2286987453699112\n",
      "[Epoch 14] Training Batch [173/391]: Loss 0.09013096988201141\n",
      "[Epoch 14] Training Batch [174/391]: Loss 0.1446690410375595\n",
      "[Epoch 14] Training Batch [175/391]: Loss 0.1356651932001114\n",
      "[Epoch 14] Training Batch [176/391]: Loss 0.11117381602525711\n",
      "[Epoch 14] Training Batch [177/391]: Loss 0.059822194278240204\n",
      "[Epoch 14] Training Batch [178/391]: Loss 0.10144156962633133\n",
      "[Epoch 14] Training Batch [179/391]: Loss 0.08059542626142502\n",
      "[Epoch 14] Training Batch [180/391]: Loss 0.1124601885676384\n",
      "[Epoch 14] Training Batch [181/391]: Loss 0.11386771500110626\n",
      "[Epoch 14] Training Batch [182/391]: Loss 0.10692982375621796\n",
      "[Epoch 14] Training Batch [183/391]: Loss 0.12606041133403778\n",
      "[Epoch 14] Training Batch [184/391]: Loss 0.13319161534309387\n",
      "[Epoch 14] Training Batch [185/391]: Loss 0.051551319658756256\n",
      "[Epoch 14] Training Batch [186/391]: Loss 0.1561131328344345\n",
      "[Epoch 14] Training Batch [187/391]: Loss 0.13069462776184082\n",
      "[Epoch 14] Training Batch [188/391]: Loss 0.1567094326019287\n",
      "[Epoch 14] Training Batch [189/391]: Loss 0.10586478561162949\n",
      "[Epoch 14] Training Batch [190/391]: Loss 0.164169579744339\n",
      "[Epoch 14] Training Batch [191/391]: Loss 0.16215072572231293\n",
      "[Epoch 14] Training Batch [192/391]: Loss 0.09271202981472015\n",
      "[Epoch 14] Training Batch [193/391]: Loss 0.1222589835524559\n",
      "[Epoch 14] Training Batch [194/391]: Loss 0.1162002682685852\n",
      "[Epoch 14] Training Batch [195/391]: Loss 0.0975089967250824\n",
      "[Epoch 14] Training Batch [196/391]: Loss 0.1108725517988205\n",
      "[Epoch 14] Training Batch [197/391]: Loss 0.14959682524204254\n",
      "[Epoch 14] Training Batch [198/391]: Loss 0.16356922686100006\n",
      "[Epoch 14] Training Batch [199/391]: Loss 0.1539679616689682\n",
      "[Epoch 14] Training Batch [200/391]: Loss 0.155442476272583\n",
      "[Epoch 14] Training Batch [201/391]: Loss 0.129646435379982\n",
      "[Epoch 14] Training Batch [202/391]: Loss 0.04616911709308624\n",
      "[Epoch 14] Training Batch [203/391]: Loss 0.17405752837657928\n",
      "[Epoch 14] Training Batch [204/391]: Loss 0.175075501203537\n",
      "[Epoch 14] Training Batch [205/391]: Loss 0.06741097569465637\n",
      "[Epoch 14] Training Batch [206/391]: Loss 0.09610871970653534\n",
      "[Epoch 14] Training Batch [207/391]: Loss 0.10860523581504822\n",
      "[Epoch 14] Training Batch [208/391]: Loss 0.10612879693508148\n",
      "[Epoch 14] Training Batch [209/391]: Loss 0.12742766737937927\n",
      "[Epoch 14] Training Batch [210/391]: Loss 0.13152454793453217\n",
      "[Epoch 14] Training Batch [211/391]: Loss 0.11712097376585007\n",
      "[Epoch 14] Training Batch [212/391]: Loss 0.16736696660518646\n",
      "[Epoch 14] Training Batch [213/391]: Loss 0.15448467433452606\n",
      "[Epoch 14] Training Batch [214/391]: Loss 0.10430100560188293\n",
      "[Epoch 14] Training Batch [215/391]: Loss 0.05857204273343086\n",
      "[Epoch 14] Training Batch [216/391]: Loss 0.14010246098041534\n",
      "[Epoch 14] Training Batch [217/391]: Loss 0.17169107496738434\n",
      "[Epoch 14] Training Batch [218/391]: Loss 0.06493283063173294\n",
      "[Epoch 14] Training Batch [219/391]: Loss 0.12539315223693848\n",
      "[Epoch 14] Training Batch [220/391]: Loss 0.19357028603553772\n",
      "[Epoch 14] Training Batch [221/391]: Loss 0.10791504383087158\n",
      "[Epoch 14] Training Batch [222/391]: Loss 0.1098874881863594\n",
      "[Epoch 14] Training Batch [223/391]: Loss 0.11558686941862106\n",
      "[Epoch 14] Training Batch [224/391]: Loss 0.13844361901283264\n",
      "[Epoch 14] Training Batch [225/391]: Loss 0.10203628987073898\n",
      "[Epoch 14] Training Batch [226/391]: Loss 0.09515449404716492\n",
      "[Epoch 14] Training Batch [227/391]: Loss 0.13101184368133545\n",
      "[Epoch 14] Training Batch [228/391]: Loss 0.18154576420783997\n",
      "[Epoch 14] Training Batch [229/391]: Loss 0.08181694149971008\n",
      "[Epoch 14] Training Batch [230/391]: Loss 0.10954305529594421\n",
      "[Epoch 14] Training Batch [231/391]: Loss 0.1213909462094307\n",
      "[Epoch 14] Training Batch [232/391]: Loss 0.09371611475944519\n",
      "[Epoch 14] Training Batch [233/391]: Loss 0.11680970340967178\n",
      "[Epoch 14] Training Batch [234/391]: Loss 0.14467626810073853\n",
      "[Epoch 14] Training Batch [235/391]: Loss 0.15906274318695068\n",
      "[Epoch 14] Training Batch [236/391]: Loss 0.13319745659828186\n",
      "[Epoch 14] Training Batch [237/391]: Loss 0.16308461129665375\n",
      "[Epoch 14] Training Batch [238/391]: Loss 0.16853858530521393\n",
      "[Epoch 14] Training Batch [239/391]: Loss 0.09210363775491714\n",
      "[Epoch 14] Training Batch [240/391]: Loss 0.10867873579263687\n",
      "[Epoch 14] Training Batch [241/391]: Loss 0.09982584416866302\n",
      "[Epoch 14] Training Batch [242/391]: Loss 0.12048894166946411\n",
      "[Epoch 14] Training Batch [243/391]: Loss 0.13941453397274017\n",
      "[Epoch 14] Training Batch [244/391]: Loss 0.10232504457235336\n",
      "[Epoch 14] Training Batch [245/391]: Loss 0.08420873433351517\n",
      "[Epoch 14] Training Batch [246/391]: Loss 0.12042925506830215\n",
      "[Epoch 14] Training Batch [247/391]: Loss 0.17375421524047852\n",
      "[Epoch 14] Training Batch [248/391]: Loss 0.1130712628364563\n",
      "[Epoch 14] Training Batch [249/391]: Loss 0.11281181871891022\n",
      "[Epoch 14] Training Batch [250/391]: Loss 0.1793489307165146\n",
      "[Epoch 14] Training Batch [251/391]: Loss 0.1782320737838745\n",
      "[Epoch 14] Training Batch [252/391]: Loss 0.23547686636447906\n",
      "[Epoch 14] Training Batch [253/391]: Loss 0.1446119099855423\n",
      "[Epoch 14] Training Batch [254/391]: Loss 0.11342590302228928\n",
      "[Epoch 14] Training Batch [255/391]: Loss 0.1912664771080017\n",
      "[Epoch 14] Training Batch [256/391]: Loss 0.1817772090435028\n",
      "[Epoch 14] Training Batch [257/391]: Loss 0.12838570773601532\n",
      "[Epoch 14] Training Batch [258/391]: Loss 0.1513129621744156\n",
      "[Epoch 14] Training Batch [259/391]: Loss 0.11490059643983841\n",
      "[Epoch 14] Training Batch [260/391]: Loss 0.15509557723999023\n",
      "[Epoch 14] Training Batch [261/391]: Loss 0.18311432003974915\n",
      "[Epoch 14] Training Batch [262/391]: Loss 0.1352689415216446\n",
      "[Epoch 14] Training Batch [263/391]: Loss 0.16389696300029755\n",
      "[Epoch 14] Training Batch [264/391]: Loss 0.1391691118478775\n",
      "[Epoch 14] Training Batch [265/391]: Loss 0.23362204432487488\n",
      "[Epoch 14] Training Batch [266/391]: Loss 0.1336473971605301\n",
      "[Epoch 14] Training Batch [267/391]: Loss 0.20368534326553345\n",
      "[Epoch 14] Training Batch [268/391]: Loss 0.13495925068855286\n",
      "[Epoch 14] Training Batch [269/391]: Loss 0.16853049397468567\n",
      "[Epoch 14] Training Batch [270/391]: Loss 0.1103740856051445\n",
      "[Epoch 14] Training Batch [271/391]: Loss 0.21834279596805573\n",
      "[Epoch 14] Training Batch [272/391]: Loss 0.19856905937194824\n",
      "[Epoch 14] Training Batch [273/391]: Loss 0.10722985863685608\n",
      "[Epoch 14] Training Batch [274/391]: Loss 0.18868152797222137\n",
      "[Epoch 14] Training Batch [275/391]: Loss 0.1900782287120819\n",
      "[Epoch 14] Training Batch [276/391]: Loss 0.12831366062164307\n",
      "[Epoch 14] Training Batch [277/391]: Loss 0.08980903774499893\n",
      "[Epoch 14] Training Batch [278/391]: Loss 0.11458960920572281\n",
      "[Epoch 14] Training Batch [279/391]: Loss 0.16433607041835785\n",
      "[Epoch 14] Training Batch [280/391]: Loss 0.18443810939788818\n",
      "[Epoch 14] Training Batch [281/391]: Loss 0.09726819396018982\n",
      "[Epoch 14] Training Batch [282/391]: Loss 0.1561301201581955\n",
      "[Epoch 14] Training Batch [283/391]: Loss 0.149911567568779\n",
      "[Epoch 14] Training Batch [284/391]: Loss 0.20606862008571625\n",
      "[Epoch 14] Training Batch [285/391]: Loss 0.11771644651889801\n",
      "[Epoch 14] Training Batch [286/391]: Loss 0.12743063271045685\n",
      "[Epoch 14] Training Batch [287/391]: Loss 0.13219085335731506\n",
      "[Epoch 14] Training Batch [288/391]: Loss 0.18260557949543\n",
      "[Epoch 14] Training Batch [289/391]: Loss 0.12043006718158722\n",
      "[Epoch 14] Training Batch [290/391]: Loss 0.17583143711090088\n",
      "[Epoch 14] Training Batch [291/391]: Loss 0.1517307162284851\n",
      "[Epoch 14] Training Batch [292/391]: Loss 0.061638057231903076\n",
      "[Epoch 14] Training Batch [293/391]: Loss 0.12445105612277985\n",
      "[Epoch 14] Training Batch [294/391]: Loss 0.1619277149438858\n",
      "[Epoch 14] Training Batch [295/391]: Loss 0.11725674569606781\n",
      "[Epoch 14] Training Batch [296/391]: Loss 0.1663360297679901\n",
      "[Epoch 14] Training Batch [297/391]: Loss 0.07121049612760544\n",
      "[Epoch 14] Training Batch [298/391]: Loss 0.1606045812368393\n",
      "[Epoch 14] Training Batch [299/391]: Loss 0.08391726016998291\n",
      "[Epoch 14] Training Batch [300/391]: Loss 0.13545683026313782\n",
      "[Epoch 14] Training Batch [301/391]: Loss 0.1297782063484192\n",
      "[Epoch 14] Training Batch [302/391]: Loss 0.1132495254278183\n",
      "[Epoch 14] Training Batch [303/391]: Loss 0.11701785773038864\n",
      "[Epoch 14] Training Batch [304/391]: Loss 0.13825467228889465\n",
      "[Epoch 14] Training Batch [305/391]: Loss 0.16580438613891602\n",
      "[Epoch 14] Training Batch [306/391]: Loss 0.12182743102312088\n",
      "[Epoch 14] Training Batch [307/391]: Loss 0.10357461869716644\n",
      "[Epoch 14] Training Batch [308/391]: Loss 0.1062159314751625\n",
      "[Epoch 14] Training Batch [309/391]: Loss 0.1319238841533661\n",
      "[Epoch 14] Training Batch [310/391]: Loss 0.12551802396774292\n",
      "[Epoch 14] Training Batch [311/391]: Loss 0.18205592036247253\n",
      "[Epoch 14] Training Batch [312/391]: Loss 0.09864386171102524\n",
      "[Epoch 14] Training Batch [313/391]: Loss 0.13873063027858734\n",
      "[Epoch 14] Training Batch [314/391]: Loss 0.1605352759361267\n",
      "[Epoch 14] Training Batch [315/391]: Loss 0.148297518491745\n",
      "[Epoch 14] Training Batch [316/391]: Loss 0.13936881721019745\n",
      "[Epoch 14] Training Batch [317/391]: Loss 0.1081312745809555\n",
      "[Epoch 14] Training Batch [318/391]: Loss 0.1839849352836609\n",
      "[Epoch 14] Training Batch [319/391]: Loss 0.1490096151828766\n",
      "[Epoch 14] Training Batch [320/391]: Loss 0.18981647491455078\n",
      "[Epoch 14] Training Batch [321/391]: Loss 0.15594185888767242\n",
      "[Epoch 14] Training Batch [322/391]: Loss 0.10631474107503891\n",
      "[Epoch 14] Training Batch [323/391]: Loss 0.17139354348182678\n",
      "[Epoch 14] Training Batch [324/391]: Loss 0.16904963552951813\n",
      "[Epoch 14] Training Batch [325/391]: Loss 0.18665683269500732\n",
      "[Epoch 14] Training Batch [326/391]: Loss 0.12845058739185333\n",
      "[Epoch 14] Training Batch [327/391]: Loss 0.15075209736824036\n",
      "[Epoch 14] Training Batch [328/391]: Loss 0.20296689867973328\n",
      "[Epoch 14] Training Batch [329/391]: Loss 0.11309103667736053\n",
      "[Epoch 14] Training Batch [330/391]: Loss 0.10195576399564743\n",
      "[Epoch 14] Training Batch [331/391]: Loss 0.10269862413406372\n",
      "[Epoch 14] Training Batch [332/391]: Loss 0.11194117367267609\n",
      "[Epoch 14] Training Batch [333/391]: Loss 0.16333907842636108\n",
      "[Epoch 14] Training Batch [334/391]: Loss 0.15482929348945618\n",
      "[Epoch 14] Training Batch [335/391]: Loss 0.13614009320735931\n",
      "[Epoch 14] Training Batch [336/391]: Loss 0.1776534765958786\n",
      "[Epoch 14] Training Batch [337/391]: Loss 0.08461297303438187\n",
      "[Epoch 14] Training Batch [338/391]: Loss 0.25712382793426514\n",
      "[Epoch 14] Training Batch [339/391]: Loss 0.12040633708238602\n",
      "[Epoch 14] Training Batch [340/391]: Loss 0.21850930154323578\n",
      "[Epoch 14] Training Batch [341/391]: Loss 0.1692931056022644\n",
      "[Epoch 14] Training Batch [342/391]: Loss 0.20011116564273834\n",
      "[Epoch 14] Training Batch [343/391]: Loss 0.13627633452415466\n",
      "[Epoch 14] Training Batch [344/391]: Loss 0.26304948329925537\n",
      "[Epoch 14] Training Batch [345/391]: Loss 0.24626225233078003\n",
      "[Epoch 14] Training Batch [346/391]: Loss 0.15668734908103943\n",
      "[Epoch 14] Training Batch [347/391]: Loss 0.10588784515857697\n",
      "[Epoch 14] Training Batch [348/391]: Loss 0.2163054347038269\n",
      "[Epoch 14] Training Batch [349/391]: Loss 0.1784312129020691\n",
      "[Epoch 14] Training Batch [350/391]: Loss 0.13307707011699677\n",
      "[Epoch 14] Training Batch [351/391]: Loss 0.17874109745025635\n",
      "[Epoch 14] Training Batch [352/391]: Loss 0.11895181983709335\n",
      "[Epoch 14] Training Batch [353/391]: Loss 0.11942100524902344\n",
      "[Epoch 14] Training Batch [354/391]: Loss 0.17644764482975006\n",
      "[Epoch 14] Training Batch [355/391]: Loss 0.19854862987995148\n",
      "[Epoch 14] Training Batch [356/391]: Loss 0.27567070722579956\n",
      "[Epoch 14] Training Batch [357/391]: Loss 0.19599851965904236\n",
      "[Epoch 14] Training Batch [358/391]: Loss 0.1373489946126938\n",
      "[Epoch 14] Training Batch [359/391]: Loss 0.17008429765701294\n",
      "[Epoch 14] Training Batch [360/391]: Loss 0.24382057785987854\n",
      "[Epoch 14] Training Batch [361/391]: Loss 0.18181747198104858\n",
      "[Epoch 14] Training Batch [362/391]: Loss 0.2070121169090271\n",
      "[Epoch 14] Training Batch [363/391]: Loss 0.181161567568779\n",
      "[Epoch 14] Training Batch [364/391]: Loss 0.1752554029226303\n",
      "[Epoch 14] Training Batch [365/391]: Loss 0.2376762330532074\n",
      "[Epoch 14] Training Batch [366/391]: Loss 0.22351917624473572\n",
      "[Epoch 14] Training Batch [367/391]: Loss 0.1403033286333084\n",
      "[Epoch 14] Training Batch [368/391]: Loss 0.21422484517097473\n",
      "[Epoch 14] Training Batch [369/391]: Loss 0.1964862197637558\n",
      "[Epoch 14] Training Batch [370/391]: Loss 0.2602214515209198\n",
      "[Epoch 14] Training Batch [371/391]: Loss 0.236094132065773\n",
      "[Epoch 14] Training Batch [372/391]: Loss 0.16039232909679413\n",
      "[Epoch 14] Training Batch [373/391]: Loss 0.10493341833353043\n",
      "[Epoch 14] Training Batch [374/391]: Loss 0.16627992689609528\n",
      "[Epoch 14] Training Batch [375/391]: Loss 0.11866996437311172\n",
      "[Epoch 14] Training Batch [376/391]: Loss 0.3313678801059723\n",
      "[Epoch 14] Training Batch [377/391]: Loss 0.23535409569740295\n",
      "[Epoch 14] Training Batch [378/391]: Loss 0.17069503664970398\n",
      "[Epoch 14] Training Batch [379/391]: Loss 0.20096361637115479\n",
      "[Epoch 14] Training Batch [380/391]: Loss 0.1666250377893448\n",
      "[Epoch 14] Training Batch [381/391]: Loss 0.23078525066375732\n",
      "[Epoch 14] Training Batch [382/391]: Loss 0.18603001534938812\n",
      "[Epoch 14] Training Batch [383/391]: Loss 0.2598240375518799\n",
      "[Epoch 14] Training Batch [384/391]: Loss 0.2254122644662857\n",
      "[Epoch 14] Training Batch [385/391]: Loss 0.15127256512641907\n",
      "[Epoch 14] Training Batch [386/391]: Loss 0.11985020339488983\n",
      "[Epoch 14] Training Batch [387/391]: Loss 0.26344332098960876\n",
      "[Epoch 14] Training Batch [388/391]: Loss 0.2565596401691437\n",
      "[Epoch 14] Training Batch [389/391]: Loss 0.11191974580287933\n",
      "[Epoch 14] Training Batch [390/391]: Loss 0.13599368929862976\n",
      "[Epoch 14] Training Batch [391/391]: Loss 0.25010114908218384\n",
      "Epoch 14 - Train Loss: 0.1403\n",
      "*********  Epoch 15/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Training Batch [1/391]: Loss 0.15912266075611115\n",
      "[Epoch 15] Training Batch [2/391]: Loss 0.09048539400100708\n",
      "[Epoch 15] Training Batch [3/391]: Loss 0.12007459253072739\n",
      "[Epoch 15] Training Batch [4/391]: Loss 0.06581619381904602\n",
      "[Epoch 15] Training Batch [5/391]: Loss 0.1185002326965332\n",
      "[Epoch 15] Training Batch [6/391]: Loss 0.08180119097232819\n",
      "[Epoch 15] Training Batch [7/391]: Loss 0.1962413787841797\n",
      "[Epoch 15] Training Batch [8/391]: Loss 0.09527090191841125\n",
      "[Epoch 15] Training Batch [9/391]: Loss 0.11329980194568634\n",
      "[Epoch 15] Training Batch [10/391]: Loss 0.09027640521526337\n",
      "[Epoch 15] Training Batch [11/391]: Loss 0.13583128154277802\n",
      "[Epoch 15] Training Batch [12/391]: Loss 0.07528124749660492\n",
      "[Epoch 15] Training Batch [13/391]: Loss 0.10143575072288513\n",
      "[Epoch 15] Training Batch [14/391]: Loss 0.11351998895406723\n",
      "[Epoch 15] Training Batch [15/391]: Loss 0.08804120123386383\n",
      "[Epoch 15] Training Batch [16/391]: Loss 0.07062996923923492\n",
      "[Epoch 15] Training Batch [17/391]: Loss 0.0766834244132042\n",
      "[Epoch 15] Training Batch [18/391]: Loss 0.09179435670375824\n",
      "[Epoch 15] Training Batch [19/391]: Loss 0.13853169977664948\n",
      "[Epoch 15] Training Batch [20/391]: Loss 0.08340493589639664\n",
      "[Epoch 15] Training Batch [21/391]: Loss 0.10666114091873169\n",
      "[Epoch 15] Training Batch [22/391]: Loss 0.1475260704755783\n",
      "[Epoch 15] Training Batch [23/391]: Loss 0.11555089056491852\n",
      "[Epoch 15] Training Batch [24/391]: Loss 0.05793933942914009\n",
      "[Epoch 15] Training Batch [25/391]: Loss 0.04549615457653999\n",
      "[Epoch 15] Training Batch [26/391]: Loss 0.06684990972280502\n",
      "[Epoch 15] Training Batch [27/391]: Loss 0.09139864891767502\n",
      "[Epoch 15] Training Batch [28/391]: Loss 0.0901404395699501\n",
      "[Epoch 15] Training Batch [29/391]: Loss 0.10480202734470367\n",
      "[Epoch 15] Training Batch [30/391]: Loss 0.09562526643276215\n",
      "[Epoch 15] Training Batch [31/391]: Loss 0.08220643550157547\n",
      "[Epoch 15] Training Batch [32/391]: Loss 0.08564256131649017\n",
      "[Epoch 15] Training Batch [33/391]: Loss 0.08948009461164474\n",
      "[Epoch 15] Training Batch [34/391]: Loss 0.05924033001065254\n",
      "[Epoch 15] Training Batch [35/391]: Loss 0.10893050581216812\n",
      "[Epoch 15] Training Batch [36/391]: Loss 0.0850108191370964\n",
      "[Epoch 15] Training Batch [37/391]: Loss 0.0455867238342762\n",
      "[Epoch 15] Training Batch [38/391]: Loss 0.07488198578357697\n",
      "[Epoch 15] Training Batch [39/391]: Loss 0.0814262330532074\n",
      "[Epoch 15] Training Batch [40/391]: Loss 0.04829507693648338\n",
      "[Epoch 15] Training Batch [41/391]: Loss 0.048927366733551025\n",
      "[Epoch 15] Training Batch [42/391]: Loss 0.1672951579093933\n",
      "[Epoch 15] Training Batch [43/391]: Loss 0.09927811473608017\n",
      "[Epoch 15] Training Batch [44/391]: Loss 0.20869037508964539\n",
      "[Epoch 15] Training Batch [45/391]: Loss 0.055644575506448746\n",
      "[Epoch 15] Training Batch [46/391]: Loss 0.05927009508013725\n",
      "[Epoch 15] Training Batch [47/391]: Loss 0.11772002279758453\n",
      "[Epoch 15] Training Batch [48/391]: Loss 0.11085796356201172\n",
      "[Epoch 15] Training Batch [49/391]: Loss 0.0916505977511406\n",
      "[Epoch 15] Training Batch [50/391]: Loss 0.07214713096618652\n",
      "[Epoch 15] Training Batch [51/391]: Loss 0.07500812411308289\n",
      "[Epoch 15] Training Batch [52/391]: Loss 0.07653217762708664\n",
      "[Epoch 15] Training Batch [53/391]: Loss 0.07086120545864105\n",
      "[Epoch 15] Training Batch [54/391]: Loss 0.06294406205415726\n",
      "[Epoch 15] Training Batch [55/391]: Loss 0.0497639924287796\n",
      "[Epoch 15] Training Batch [56/391]: Loss 0.12553724646568298\n",
      "[Epoch 15] Training Batch [57/391]: Loss 0.06746171414852142\n",
      "[Epoch 15] Training Batch [58/391]: Loss 0.09452181309461594\n",
      "[Epoch 15] Training Batch [59/391]: Loss 0.05255584791302681\n",
      "[Epoch 15] Training Batch [60/391]: Loss 0.10890688002109528\n",
      "[Epoch 15] Training Batch [61/391]: Loss 0.1099100187420845\n",
      "[Epoch 15] Training Batch [62/391]: Loss 0.08075679838657379\n",
      "[Epoch 15] Training Batch [63/391]: Loss 0.06736423820257187\n",
      "[Epoch 15] Training Batch [64/391]: Loss 0.06952737271785736\n",
      "[Epoch 15] Training Batch [65/391]: Loss 0.09978572279214859\n",
      "[Epoch 15] Training Batch [66/391]: Loss 0.06966850161552429\n",
      "[Epoch 15] Training Batch [67/391]: Loss 0.09932535886764526\n",
      "[Epoch 15] Training Batch [68/391]: Loss 0.10661142319440842\n",
      "[Epoch 15] Training Batch [69/391]: Loss 0.10543861985206604\n",
      "[Epoch 15] Training Batch [70/391]: Loss 0.0763712152838707\n",
      "[Epoch 15] Training Batch [71/391]: Loss 0.03203228861093521\n",
      "[Epoch 15] Training Batch [72/391]: Loss 0.049146171659231186\n",
      "[Epoch 15] Training Batch [73/391]: Loss 0.052261702716350555\n",
      "[Epoch 15] Training Batch [74/391]: Loss 0.062001071870326996\n",
      "[Epoch 15] Training Batch [75/391]: Loss 0.15757796168327332\n",
      "[Epoch 15] Training Batch [76/391]: Loss 0.0585041418671608\n",
      "[Epoch 15] Training Batch [77/391]: Loss 0.1099884882569313\n",
      "[Epoch 15] Training Batch [78/391]: Loss 0.036221496760845184\n",
      "[Epoch 15] Training Batch [79/391]: Loss 0.0728340670466423\n",
      "[Epoch 15] Training Batch [80/391]: Loss 0.1010061651468277\n",
      "[Epoch 15] Training Batch [81/391]: Loss 0.04440901428461075\n",
      "[Epoch 15] Training Batch [82/391]: Loss 0.10997231304645538\n",
      "[Epoch 15] Training Batch [83/391]: Loss 0.11249732226133347\n",
      "[Epoch 15] Training Batch [84/391]: Loss 0.0472356379032135\n",
      "[Epoch 15] Training Batch [85/391]: Loss 0.05057136341929436\n",
      "[Epoch 15] Training Batch [86/391]: Loss 0.16007816791534424\n",
      "[Epoch 15] Training Batch [87/391]: Loss 0.04807009547948837\n",
      "[Epoch 15] Training Batch [88/391]: Loss 0.10870425403118134\n",
      "[Epoch 15] Training Batch [89/391]: Loss 0.0916052833199501\n",
      "[Epoch 15] Training Batch [90/391]: Loss 0.07528319209814072\n",
      "[Epoch 15] Training Batch [91/391]: Loss 0.06339861452579498\n",
      "[Epoch 15] Training Batch [92/391]: Loss 0.11303101480007172\n",
      "[Epoch 15] Training Batch [93/391]: Loss 0.04450007528066635\n",
      "[Epoch 15] Training Batch [94/391]: Loss 0.056163180619478226\n",
      "[Epoch 15] Training Batch [95/391]: Loss 0.12469913810491562\n",
      "[Epoch 15] Training Batch [96/391]: Loss 0.09073792397975922\n",
      "[Epoch 15] Training Batch [97/391]: Loss 0.06765294075012207\n",
      "[Epoch 15] Training Batch [98/391]: Loss 0.08425315469503403\n",
      "[Epoch 15] Training Batch [99/391]: Loss 0.06762515753507614\n",
      "[Epoch 15] Training Batch [100/391]: Loss 0.12426803261041641\n",
      "[Epoch 15] Training Batch [101/391]: Loss 0.03481333330273628\n",
      "[Epoch 15] Training Batch [102/391]: Loss 0.06674575060606003\n",
      "[Epoch 15] Training Batch [103/391]: Loss 0.08902103453874588\n",
      "[Epoch 15] Training Batch [104/391]: Loss 0.06823364645242691\n",
      "[Epoch 15] Training Batch [105/391]: Loss 0.03650861606001854\n",
      "[Epoch 15] Training Batch [106/391]: Loss 0.15420162677764893\n",
      "[Epoch 15] Training Batch [107/391]: Loss 0.08684766292572021\n",
      "[Epoch 15] Training Batch [108/391]: Loss 0.040099307894706726\n",
      "[Epoch 15] Training Batch [109/391]: Loss 0.06373440474271774\n",
      "[Epoch 15] Training Batch [110/391]: Loss 0.054734934121370316\n",
      "[Epoch 15] Training Batch [111/391]: Loss 0.06749480962753296\n",
      "[Epoch 15] Training Batch [112/391]: Loss 0.06065469980239868\n",
      "[Epoch 15] Training Batch [113/391]: Loss 0.07101103663444519\n",
      "[Epoch 15] Training Batch [114/391]: Loss 0.10662224888801575\n",
      "[Epoch 15] Training Batch [115/391]: Loss 0.05127678066492081\n",
      "[Epoch 15] Training Batch [116/391]: Loss 0.1909133642911911\n",
      "[Epoch 15] Training Batch [117/391]: Loss 0.13387899100780487\n",
      "[Epoch 15] Training Batch [118/391]: Loss 0.03837504982948303\n",
      "[Epoch 15] Training Batch [119/391]: Loss 0.06154564395546913\n",
      "[Epoch 15] Training Batch [120/391]: Loss 0.07542841881513596\n",
      "[Epoch 15] Training Batch [121/391]: Loss 0.09834513068199158\n",
      "[Epoch 15] Training Batch [122/391]: Loss 0.025397222489118576\n",
      "[Epoch 15] Training Batch [123/391]: Loss 0.09021817147731781\n",
      "[Epoch 15] Training Batch [124/391]: Loss 0.07277585566043854\n",
      "[Epoch 15] Training Batch [125/391]: Loss 0.07468453794717789\n",
      "[Epoch 15] Training Batch [126/391]: Loss 0.0650995671749115\n",
      "[Epoch 15] Training Batch [127/391]: Loss 0.09318961948156357\n",
      "[Epoch 15] Training Batch [128/391]: Loss 0.0491776242852211\n",
      "[Epoch 15] Training Batch [129/391]: Loss 0.04707012698054314\n",
      "[Epoch 15] Training Batch [130/391]: Loss 0.05039070174098015\n",
      "[Epoch 15] Training Batch [131/391]: Loss 0.09271809458732605\n",
      "[Epoch 15] Training Batch [132/391]: Loss 0.052831850945949554\n",
      "[Epoch 15] Training Batch [133/391]: Loss 0.10946007072925568\n",
      "[Epoch 15] Training Batch [134/391]: Loss 0.05371576547622681\n",
      "[Epoch 15] Training Batch [135/391]: Loss 0.09514093399047852\n",
      "[Epoch 15] Training Batch [136/391]: Loss 0.06829972565174103\n",
      "[Epoch 15] Training Batch [137/391]: Loss 0.06656497716903687\n",
      "[Epoch 15] Training Batch [138/391]: Loss 0.04491850733757019\n",
      "[Epoch 15] Training Batch [139/391]: Loss 0.03499322757124901\n",
      "[Epoch 15] Training Batch [140/391]: Loss 0.07264509052038193\n",
      "[Epoch 15] Training Batch [141/391]: Loss 0.06809110939502716\n",
      "[Epoch 15] Training Batch [142/391]: Loss 0.12990227341651917\n",
      "[Epoch 15] Training Batch [143/391]: Loss 0.07109429687261581\n",
      "[Epoch 15] Training Batch [144/391]: Loss 0.03466209024190903\n",
      "[Epoch 15] Training Batch [145/391]: Loss 0.06918621063232422\n",
      "[Epoch 15] Training Batch [146/391]: Loss 0.10805652290582657\n",
      "[Epoch 15] Training Batch [147/391]: Loss 0.07181445509195328\n",
      "[Epoch 15] Training Batch [148/391]: Loss 0.06585665792226791\n",
      "[Epoch 15] Training Batch [149/391]: Loss 0.05601530149579048\n",
      "[Epoch 15] Training Batch [150/391]: Loss 0.06067119911313057\n",
      "[Epoch 15] Training Batch [151/391]: Loss 0.1048700138926506\n",
      "[Epoch 15] Training Batch [152/391]: Loss 0.07225804775953293\n",
      "[Epoch 15] Training Batch [153/391]: Loss 0.09311094135046005\n",
      "[Epoch 15] Training Batch [154/391]: Loss 0.09155121445655823\n",
      "[Epoch 15] Training Batch [155/391]: Loss 0.043844643980264664\n",
      "[Epoch 15] Training Batch [156/391]: Loss 0.07756678014993668\n",
      "[Epoch 15] Training Batch [157/391]: Loss 0.09165362268686295\n",
      "[Epoch 15] Training Batch [158/391]: Loss 0.11655706912279129\n",
      "[Epoch 15] Training Batch [159/391]: Loss 0.0648263692855835\n",
      "[Epoch 15] Training Batch [160/391]: Loss 0.08982187509536743\n",
      "[Epoch 15] Training Batch [161/391]: Loss 0.10111305117607117\n",
      "[Epoch 15] Training Batch [162/391]: Loss 0.07909328490495682\n",
      "[Epoch 15] Training Batch [163/391]: Loss 0.08625635504722595\n",
      "[Epoch 15] Training Batch [164/391]: Loss 0.08159028738737106\n",
      "[Epoch 15] Training Batch [165/391]: Loss 0.05970093607902527\n",
      "[Epoch 15] Training Batch [166/391]: Loss 0.14077475666999817\n",
      "[Epoch 15] Training Batch [167/391]: Loss 0.10787082463502884\n",
      "[Epoch 15] Training Batch [168/391]: Loss 0.05349043756723404\n",
      "[Epoch 15] Training Batch [169/391]: Loss 0.08713096380233765\n",
      "[Epoch 15] Training Batch [170/391]: Loss 0.06131424382328987\n",
      "[Epoch 15] Training Batch [171/391]: Loss 0.1293993890285492\n",
      "[Epoch 15] Training Batch [172/391]: Loss 0.10446353256702423\n",
      "[Epoch 15] Training Batch [173/391]: Loss 0.08540323376655579\n",
      "[Epoch 15] Training Batch [174/391]: Loss 0.11519373953342438\n",
      "[Epoch 15] Training Batch [175/391]: Loss 0.17496635019779205\n",
      "[Epoch 15] Training Batch [176/391]: Loss 0.04406735301017761\n",
      "[Epoch 15] Training Batch [177/391]: Loss 0.150207057595253\n",
      "[Epoch 15] Training Batch [178/391]: Loss 0.10513512790203094\n",
      "[Epoch 15] Training Batch [179/391]: Loss 0.056629542261362076\n",
      "[Epoch 15] Training Batch [180/391]: Loss 0.18110471963882446\n",
      "[Epoch 15] Training Batch [181/391]: Loss 0.14183740317821503\n",
      "[Epoch 15] Training Batch [182/391]: Loss 0.11054950952529907\n",
      "[Epoch 15] Training Batch [183/391]: Loss 0.021197235211730003\n",
      "[Epoch 15] Training Batch [184/391]: Loss 0.12156534940004349\n",
      "[Epoch 15] Training Batch [185/391]: Loss 0.11142974346876144\n",
      "[Epoch 15] Training Batch [186/391]: Loss 0.09023388475179672\n",
      "[Epoch 15] Training Batch [187/391]: Loss 0.10885875672101974\n",
      "[Epoch 15] Training Batch [188/391]: Loss 0.06898168474435806\n",
      "[Epoch 15] Training Batch [189/391]: Loss 0.047493334859609604\n",
      "[Epoch 15] Training Batch [190/391]: Loss 0.06572041660547256\n",
      "[Epoch 15] Training Batch [191/391]: Loss 0.21577292680740356\n",
      "[Epoch 15] Training Batch [192/391]: Loss 0.1153629943728447\n",
      "[Epoch 15] Training Batch [193/391]: Loss 0.15220844745635986\n",
      "[Epoch 15] Training Batch [194/391]: Loss 0.12252657860517502\n",
      "[Epoch 15] Training Batch [195/391]: Loss 0.06491042673587799\n",
      "[Epoch 15] Training Batch [196/391]: Loss 0.06939321011304855\n",
      "[Epoch 15] Training Batch [197/391]: Loss 0.07559745013713837\n",
      "[Epoch 15] Training Batch [198/391]: Loss 0.06209776550531387\n",
      "[Epoch 15] Training Batch [199/391]: Loss 0.07874347269535065\n",
      "[Epoch 15] Training Batch [200/391]: Loss 0.08602961152791977\n",
      "[Epoch 15] Training Batch [201/391]: Loss 0.060019660741090775\n",
      "[Epoch 15] Training Batch [202/391]: Loss 0.12253810465335846\n",
      "[Epoch 15] Training Batch [203/391]: Loss 0.04139866307377815\n",
      "[Epoch 15] Training Batch [204/391]: Loss 0.05848079174757004\n",
      "[Epoch 15] Training Batch [205/391]: Loss 0.11720370501279831\n",
      "[Epoch 15] Training Batch [206/391]: Loss 0.08192449808120728\n",
      "[Epoch 15] Training Batch [207/391]: Loss 0.08201593160629272\n",
      "[Epoch 15] Training Batch [208/391]: Loss 0.10735036432743073\n",
      "[Epoch 15] Training Batch [209/391]: Loss 0.05359407141804695\n",
      "[Epoch 15] Training Batch [210/391]: Loss 0.06265690922737122\n",
      "[Epoch 15] Training Batch [211/391]: Loss 0.05215172842144966\n",
      "[Epoch 15] Training Batch [212/391]: Loss 0.08560884743928909\n",
      "[Epoch 15] Training Batch [213/391]: Loss 0.0986245721578598\n",
      "[Epoch 15] Training Batch [214/391]: Loss 0.09413854777812958\n",
      "[Epoch 15] Training Batch [215/391]: Loss 0.08676300942897797\n",
      "[Epoch 15] Training Batch [216/391]: Loss 0.08037343621253967\n",
      "[Epoch 15] Training Batch [217/391]: Loss 0.13405968248844147\n",
      "[Epoch 15] Training Batch [218/391]: Loss 0.15021322667598724\n",
      "[Epoch 15] Training Batch [219/391]: Loss 0.11647271364927292\n",
      "[Epoch 15] Training Batch [220/391]: Loss 0.08982080221176147\n",
      "[Epoch 15] Training Batch [221/391]: Loss 0.12787963449954987\n",
      "[Epoch 15] Training Batch [222/391]: Loss 0.06546381860971451\n",
      "[Epoch 15] Training Batch [223/391]: Loss 0.04584476724267006\n",
      "[Epoch 15] Training Batch [224/391]: Loss 0.11743547022342682\n",
      "[Epoch 15] Training Batch [225/391]: Loss 0.08964316546916962\n",
      "[Epoch 15] Training Batch [226/391]: Loss 0.0377458855509758\n",
      "[Epoch 15] Training Batch [227/391]: Loss 0.0640268474817276\n",
      "[Epoch 15] Training Batch [228/391]: Loss 0.08794670552015305\n",
      "[Epoch 15] Training Batch [229/391]: Loss 0.1211526021361351\n",
      "[Epoch 15] Training Batch [230/391]: Loss 0.14861629903316498\n",
      "[Epoch 15] Training Batch [231/391]: Loss 0.17403389513492584\n",
      "[Epoch 15] Training Batch [232/391]: Loss 0.06791674345731735\n",
      "[Epoch 15] Training Batch [233/391]: Loss 0.08009619265794754\n",
      "[Epoch 15] Training Batch [234/391]: Loss 0.11898589134216309\n",
      "[Epoch 15] Training Batch [235/391]: Loss 0.10825085639953613\n",
      "[Epoch 15] Training Batch [236/391]: Loss 0.0767606571316719\n",
      "[Epoch 15] Training Batch [237/391]: Loss 0.1286596953868866\n",
      "[Epoch 15] Training Batch [238/391]: Loss 0.24143145978450775\n",
      "[Epoch 15] Training Batch [239/391]: Loss 0.08734485507011414\n",
      "[Epoch 15] Training Batch [240/391]: Loss 0.11396379768848419\n",
      "[Epoch 15] Training Batch [241/391]: Loss 0.10663633793592453\n",
      "[Epoch 15] Training Batch [242/391]: Loss 0.09075216948986053\n",
      "[Epoch 15] Training Batch [243/391]: Loss 0.1328631192445755\n",
      "[Epoch 15] Training Batch [244/391]: Loss 0.1649743914604187\n",
      "[Epoch 15] Training Batch [245/391]: Loss 0.11259336769580841\n",
      "[Epoch 15] Training Batch [246/391]: Loss 0.10071241110563278\n",
      "[Epoch 15] Training Batch [247/391]: Loss 0.062227584421634674\n",
      "[Epoch 15] Training Batch [248/391]: Loss 0.10452833771705627\n",
      "[Epoch 15] Training Batch [249/391]: Loss 0.11983582377433777\n",
      "[Epoch 15] Training Batch [250/391]: Loss 0.09160889685153961\n",
      "[Epoch 15] Training Batch [251/391]: Loss 0.1829424500465393\n",
      "[Epoch 15] Training Batch [252/391]: Loss 0.10675700753927231\n",
      "[Epoch 15] Training Batch [253/391]: Loss 0.07433523237705231\n",
      "[Epoch 15] Training Batch [254/391]: Loss 0.17428374290466309\n",
      "[Epoch 15] Training Batch [255/391]: Loss 0.10769031941890717\n",
      "[Epoch 15] Training Batch [256/391]: Loss 0.05828933045268059\n",
      "[Epoch 15] Training Batch [257/391]: Loss 0.17322218418121338\n",
      "[Epoch 15] Training Batch [258/391]: Loss 0.12019792199134827\n",
      "[Epoch 15] Training Batch [259/391]: Loss 0.09706748276948929\n",
      "[Epoch 15] Training Batch [260/391]: Loss 0.0777428150177002\n",
      "[Epoch 15] Training Batch [261/391]: Loss 0.09580215811729431\n",
      "[Epoch 15] Training Batch [262/391]: Loss 0.12705637514591217\n",
      "[Epoch 15] Training Batch [263/391]: Loss 0.16680778563022614\n",
      "[Epoch 15] Training Batch [264/391]: Loss 0.0933016762137413\n",
      "[Epoch 15] Training Batch [265/391]: Loss 0.10137628763914108\n",
      "[Epoch 15] Training Batch [266/391]: Loss 0.07895759493112564\n",
      "[Epoch 15] Training Batch [267/391]: Loss 0.13027088344097137\n",
      "[Epoch 15] Training Batch [268/391]: Loss 0.11185339093208313\n",
      "[Epoch 15] Training Batch [269/391]: Loss 0.07390012592077255\n",
      "[Epoch 15] Training Batch [270/391]: Loss 0.07948890328407288\n",
      "[Epoch 15] Training Batch [271/391]: Loss 0.09084269404411316\n",
      "[Epoch 15] Training Batch [272/391]: Loss 0.17881914973258972\n",
      "[Epoch 15] Training Batch [273/391]: Loss 0.15417376160621643\n",
      "[Epoch 15] Training Batch [274/391]: Loss 0.05983865261077881\n",
      "[Epoch 15] Training Batch [275/391]: Loss 0.08203694969415665\n",
      "[Epoch 15] Training Batch [276/391]: Loss 0.1106560081243515\n",
      "[Epoch 15] Training Batch [277/391]: Loss 0.20227569341659546\n",
      "[Epoch 15] Training Batch [278/391]: Loss 0.09955829381942749\n",
      "[Epoch 15] Training Batch [279/391]: Loss 0.14502498507499695\n",
      "[Epoch 15] Training Batch [280/391]: Loss 0.06977836787700653\n",
      "[Epoch 15] Training Batch [281/391]: Loss 0.11361666023731232\n",
      "[Epoch 15] Training Batch [282/391]: Loss 0.09690845012664795\n",
      "[Epoch 15] Training Batch [283/391]: Loss 0.09591660648584366\n",
      "[Epoch 15] Training Batch [284/391]: Loss 0.11498484015464783\n",
      "[Epoch 15] Training Batch [285/391]: Loss 0.118073970079422\n",
      "[Epoch 15] Training Batch [286/391]: Loss 0.06508119404315948\n",
      "[Epoch 15] Training Batch [287/391]: Loss 0.12299664318561554\n",
      "[Epoch 15] Training Batch [288/391]: Loss 0.09779107570648193\n",
      "[Epoch 15] Training Batch [289/391]: Loss 0.12426143139600754\n",
      "[Epoch 15] Training Batch [290/391]: Loss 0.1267169862985611\n",
      "[Epoch 15] Training Batch [291/391]: Loss 0.11860180646181107\n",
      "[Epoch 15] Training Batch [292/391]: Loss 0.0680035725235939\n",
      "[Epoch 15] Training Batch [293/391]: Loss 0.09956660866737366\n",
      "[Epoch 15] Training Batch [294/391]: Loss 0.1004076674580574\n",
      "[Epoch 15] Training Batch [295/391]: Loss 0.11843784153461456\n",
      "[Epoch 15] Training Batch [296/391]: Loss 0.07962661981582642\n",
      "[Epoch 15] Training Batch [297/391]: Loss 0.05558903142809868\n",
      "[Epoch 15] Training Batch [298/391]: Loss 0.0964396595954895\n",
      "[Epoch 15] Training Batch [299/391]: Loss 0.12982147932052612\n",
      "[Epoch 15] Training Batch [300/391]: Loss 0.07329435646533966\n",
      "[Epoch 15] Training Batch [301/391]: Loss 0.07341928035020828\n",
      "[Epoch 15] Training Batch [302/391]: Loss 0.14158526062965393\n",
      "[Epoch 15] Training Batch [303/391]: Loss 0.1105538159608841\n",
      "[Epoch 15] Training Batch [304/391]: Loss 0.08324034512042999\n",
      "[Epoch 15] Training Batch [305/391]: Loss 0.0897015854716301\n",
      "[Epoch 15] Training Batch [306/391]: Loss 0.11304929107427597\n",
      "[Epoch 15] Training Batch [307/391]: Loss 0.1474784016609192\n",
      "[Epoch 15] Training Batch [308/391]: Loss 0.09411346167325974\n",
      "[Epoch 15] Training Batch [309/391]: Loss 0.12494570016860962\n",
      "[Epoch 15] Training Batch [310/391]: Loss 0.1718091517686844\n",
      "[Epoch 15] Training Batch [311/391]: Loss 0.06390631198883057\n",
      "[Epoch 15] Training Batch [312/391]: Loss 0.13906972110271454\n",
      "[Epoch 15] Training Batch [313/391]: Loss 0.09251100569963455\n",
      "[Epoch 15] Training Batch [314/391]: Loss 0.1190563216805458\n",
      "[Epoch 15] Training Batch [315/391]: Loss 0.07114605605602264\n",
      "[Epoch 15] Training Batch [316/391]: Loss 0.09384214133024216\n",
      "[Epoch 15] Training Batch [317/391]: Loss 0.17918026447296143\n",
      "[Epoch 15] Training Batch [318/391]: Loss 0.11476360261440277\n",
      "[Epoch 15] Training Batch [319/391]: Loss 0.09038154780864716\n",
      "[Epoch 15] Training Batch [320/391]: Loss 0.09574079513549805\n",
      "[Epoch 15] Training Batch [321/391]: Loss 0.11772230267524719\n",
      "[Epoch 15] Training Batch [322/391]: Loss 0.14233258366584778\n",
      "[Epoch 15] Training Batch [323/391]: Loss 0.23641127347946167\n",
      "[Epoch 15] Training Batch [324/391]: Loss 0.16015437245368958\n",
      "[Epoch 15] Training Batch [325/391]: Loss 0.06538446247577667\n",
      "[Epoch 15] Training Batch [326/391]: Loss 0.10346411913633347\n",
      "[Epoch 15] Training Batch [327/391]: Loss 0.13562366366386414\n",
      "[Epoch 15] Training Batch [328/391]: Loss 0.10857558995485306\n",
      "[Epoch 15] Training Batch [329/391]: Loss 0.1107349768280983\n",
      "[Epoch 15] Training Batch [330/391]: Loss 0.10137627273797989\n",
      "[Epoch 15] Training Batch [331/391]: Loss 0.14379934966564178\n",
      "[Epoch 15] Training Batch [332/391]: Loss 0.1642235517501831\n",
      "[Epoch 15] Training Batch [333/391]: Loss 0.07523011416196823\n",
      "[Epoch 15] Training Batch [334/391]: Loss 0.11314857751131058\n",
      "[Epoch 15] Training Batch [335/391]: Loss 0.08839372545480728\n",
      "[Epoch 15] Training Batch [336/391]: Loss 0.10038706660270691\n",
      "[Epoch 15] Training Batch [337/391]: Loss 0.20927196741104126\n",
      "[Epoch 15] Training Batch [338/391]: Loss 0.1280820220708847\n",
      "[Epoch 15] Training Batch [339/391]: Loss 0.20819653570652008\n",
      "[Epoch 15] Training Batch [340/391]: Loss 0.07966804504394531\n",
      "[Epoch 15] Training Batch [341/391]: Loss 0.1179787889122963\n",
      "[Epoch 15] Training Batch [342/391]: Loss 0.08656857162714005\n",
      "[Epoch 15] Training Batch [343/391]: Loss 0.05619407072663307\n",
      "[Epoch 15] Training Batch [344/391]: Loss 0.11863145977258682\n",
      "[Epoch 15] Training Batch [345/391]: Loss 0.17517203092575073\n",
      "[Epoch 15] Training Batch [346/391]: Loss 0.15816174447536469\n",
      "[Epoch 15] Training Batch [347/391]: Loss 0.2033587545156479\n",
      "[Epoch 15] Training Batch [348/391]: Loss 0.19751977920532227\n",
      "[Epoch 15] Training Batch [349/391]: Loss 0.11031290888786316\n",
      "[Epoch 15] Training Batch [350/391]: Loss 0.13027578592300415\n",
      "[Epoch 15] Training Batch [351/391]: Loss 0.15904057025909424\n",
      "[Epoch 15] Training Batch [352/391]: Loss 0.22555986046791077\n",
      "[Epoch 15] Training Batch [353/391]: Loss 0.1627894937992096\n",
      "[Epoch 15] Training Batch [354/391]: Loss 0.16473284363746643\n",
      "[Epoch 15] Training Batch [355/391]: Loss 0.12418434023857117\n",
      "[Epoch 15] Training Batch [356/391]: Loss 0.09432591497898102\n",
      "[Epoch 15] Training Batch [357/391]: Loss 0.18450087308883667\n",
      "[Epoch 15] Training Batch [358/391]: Loss 0.18435384333133698\n",
      "[Epoch 15] Training Batch [359/391]: Loss 0.13464468717575073\n",
      "[Epoch 15] Training Batch [360/391]: Loss 0.060592036694288254\n",
      "[Epoch 15] Training Batch [361/391]: Loss 0.11932600289583206\n",
      "[Epoch 15] Training Batch [362/391]: Loss 0.13428844511508942\n",
      "[Epoch 15] Training Batch [363/391]: Loss 0.12353094667196274\n",
      "[Epoch 15] Training Batch [364/391]: Loss 0.15867289900779724\n",
      "[Epoch 15] Training Batch [365/391]: Loss 0.11017867177724838\n",
      "[Epoch 15] Training Batch [366/391]: Loss 0.1006174311041832\n",
      "[Epoch 15] Training Batch [367/391]: Loss 0.08686025440692902\n",
      "[Epoch 15] Training Batch [368/391]: Loss 0.09966026991605759\n",
      "[Epoch 15] Training Batch [369/391]: Loss 0.10676839202642441\n",
      "[Epoch 15] Training Batch [370/391]: Loss 0.11586931347846985\n",
      "[Epoch 15] Training Batch [371/391]: Loss 0.07727742195129395\n",
      "[Epoch 15] Training Batch [372/391]: Loss 0.1670750230550766\n",
      "[Epoch 15] Training Batch [373/391]: Loss 0.19252127408981323\n",
      "[Epoch 15] Training Batch [374/391]: Loss 0.16872696578502655\n",
      "[Epoch 15] Training Batch [375/391]: Loss 0.11282187700271606\n",
      "[Epoch 15] Training Batch [376/391]: Loss 0.1310810148715973\n",
      "[Epoch 15] Training Batch [377/391]: Loss 0.1445324867963791\n",
      "[Epoch 15] Training Batch [378/391]: Loss 0.1503949910402298\n",
      "[Epoch 15] Training Batch [379/391]: Loss 0.08839215338230133\n",
      "[Epoch 15] Training Batch [380/391]: Loss 0.1899816393852234\n",
      "[Epoch 15] Training Batch [381/391]: Loss 0.08039174228906631\n",
      "[Epoch 15] Training Batch [382/391]: Loss 0.08687371760606766\n",
      "[Epoch 15] Training Batch [383/391]: Loss 0.1524549424648285\n",
      "[Epoch 15] Training Batch [384/391]: Loss 0.1538275182247162\n",
      "[Epoch 15] Training Batch [385/391]: Loss 0.09480313211679459\n",
      "[Epoch 15] Training Batch [386/391]: Loss 0.12328489869832993\n",
      "[Epoch 15] Training Batch [387/391]: Loss 0.1164243146777153\n",
      "[Epoch 15] Training Batch [388/391]: Loss 0.121692955493927\n",
      "[Epoch 15] Training Batch [389/391]: Loss 0.10166550427675247\n",
      "[Epoch 15] Training Batch [390/391]: Loss 0.11353404819965363\n",
      "[Epoch 15] Training Batch [391/391]: Loss 0.2264728844165802\n",
      "Epoch 15 - Train Loss: 0.1006\n",
      "*********  Epoch 16/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Training Batch [1/391]: Loss 0.06739587336778641\n",
      "[Epoch 16] Training Batch [2/391]: Loss 0.05412084981799126\n",
      "[Epoch 16] Training Batch [3/391]: Loss 0.10397162288427353\n",
      "[Epoch 16] Training Batch [4/391]: Loss 0.1278293877840042\n",
      "[Epoch 16] Training Batch [5/391]: Loss 0.079594187438488\n",
      "[Epoch 16] Training Batch [6/391]: Loss 0.09774287045001984\n",
      "[Epoch 16] Training Batch [7/391]: Loss 0.06108584627509117\n",
      "[Epoch 16] Training Batch [8/391]: Loss 0.10947678983211517\n",
      "[Epoch 16] Training Batch [9/391]: Loss 0.09977570921182632\n",
      "[Epoch 16] Training Batch [10/391]: Loss 0.15840817987918854\n",
      "[Epoch 16] Training Batch [11/391]: Loss 0.07161542773246765\n",
      "[Epoch 16] Training Batch [12/391]: Loss 0.10495984554290771\n",
      "[Epoch 16] Training Batch [13/391]: Loss 0.19715748727321625\n",
      "[Epoch 16] Training Batch [14/391]: Loss 0.1394309103488922\n",
      "[Epoch 16] Training Batch [15/391]: Loss 0.0584852434694767\n",
      "[Epoch 16] Training Batch [16/391]: Loss 0.05212635546922684\n",
      "[Epoch 16] Training Batch [17/391]: Loss 0.05158945545554161\n",
      "[Epoch 16] Training Batch [18/391]: Loss 0.10273656994104385\n",
      "[Epoch 16] Training Batch [19/391]: Loss 0.08728539943695068\n",
      "[Epoch 16] Training Batch [20/391]: Loss 0.07431765645742416\n",
      "[Epoch 16] Training Batch [21/391]: Loss 0.1087997779250145\n",
      "[Epoch 16] Training Batch [22/391]: Loss 0.05044770613312721\n",
      "[Epoch 16] Training Batch [23/391]: Loss 0.18454639613628387\n",
      "[Epoch 16] Training Batch [24/391]: Loss 0.04002061113715172\n",
      "[Epoch 16] Training Batch [25/391]: Loss 0.08379075676202774\n",
      "[Epoch 16] Training Batch [26/391]: Loss 0.07165280729532242\n",
      "[Epoch 16] Training Batch [27/391]: Loss 0.053116969764232635\n",
      "[Epoch 16] Training Batch [28/391]: Loss 0.08745284378528595\n",
      "[Epoch 16] Training Batch [29/391]: Loss 0.06408797949552536\n",
      "[Epoch 16] Training Batch [30/391]: Loss 0.12243188917636871\n",
      "[Epoch 16] Training Batch [31/391]: Loss 0.040499016642570496\n",
      "[Epoch 16] Training Batch [32/391]: Loss 0.066318079829216\n",
      "[Epoch 16] Training Batch [33/391]: Loss 0.08649037778377533\n",
      "[Epoch 16] Training Batch [34/391]: Loss 0.05251554027199745\n",
      "[Epoch 16] Training Batch [35/391]: Loss 0.05318669229745865\n",
      "[Epoch 16] Training Batch [36/391]: Loss 0.1234150379896164\n",
      "[Epoch 16] Training Batch [37/391]: Loss 0.03545018658041954\n",
      "[Epoch 16] Training Batch [38/391]: Loss 0.03909003734588623\n",
      "[Epoch 16] Training Batch [39/391]: Loss 0.09789354354143143\n",
      "[Epoch 16] Training Batch [40/391]: Loss 0.07243102043867111\n",
      "[Epoch 16] Training Batch [41/391]: Loss 0.10702712833881378\n",
      "[Epoch 16] Training Batch [42/391]: Loss 0.08840634673833847\n",
      "[Epoch 16] Training Batch [43/391]: Loss 0.052554186433553696\n",
      "[Epoch 16] Training Batch [44/391]: Loss 0.1260962039232254\n",
      "[Epoch 16] Training Batch [45/391]: Loss 0.07329512387514114\n",
      "[Epoch 16] Training Batch [46/391]: Loss 0.06397052109241486\n",
      "[Epoch 16] Training Batch [47/391]: Loss 0.07873106002807617\n",
      "[Epoch 16] Training Batch [48/391]: Loss 0.03671522065997124\n",
      "[Epoch 16] Training Batch [49/391]: Loss 0.09921976178884506\n",
      "[Epoch 16] Training Batch [50/391]: Loss 0.06806600093841553\n",
      "[Epoch 16] Training Batch [51/391]: Loss 0.046875935047864914\n",
      "[Epoch 16] Training Batch [52/391]: Loss 0.13797807693481445\n",
      "[Epoch 16] Training Batch [53/391]: Loss 0.09074665606021881\n",
      "[Epoch 16] Training Batch [54/391]: Loss 0.06501887738704681\n",
      "[Epoch 16] Training Batch [55/391]: Loss 0.06782632321119308\n",
      "[Epoch 16] Training Batch [56/391]: Loss 0.08856511861085892\n",
      "[Epoch 16] Training Batch [57/391]: Loss 0.0803237333893776\n",
      "[Epoch 16] Training Batch [58/391]: Loss 0.10143100470304489\n",
      "[Epoch 16] Training Batch [59/391]: Loss 0.1281459629535675\n",
      "[Epoch 16] Training Batch [60/391]: Loss 0.0312323160469532\n",
      "[Epoch 16] Training Batch [61/391]: Loss 0.1173517256975174\n",
      "[Epoch 16] Training Batch [62/391]: Loss 0.058284156024456024\n",
      "[Epoch 16] Training Batch [63/391]: Loss 0.08871495723724365\n",
      "[Epoch 16] Training Batch [64/391]: Loss 0.04812353476881981\n",
      "[Epoch 16] Training Batch [65/391]: Loss 0.12636487185955048\n",
      "[Epoch 16] Training Batch [66/391]: Loss 0.048747070133686066\n",
      "[Epoch 16] Training Batch [67/391]: Loss 0.04148348048329353\n",
      "[Epoch 16] Training Batch [68/391]: Loss 0.05290537327528\n",
      "[Epoch 16] Training Batch [69/391]: Loss 0.09144299477338791\n",
      "[Epoch 16] Training Batch [70/391]: Loss 0.045177385210990906\n",
      "[Epoch 16] Training Batch [71/391]: Loss 0.05727941915392876\n",
      "[Epoch 16] Training Batch [72/391]: Loss 0.05859684199094772\n",
      "[Epoch 16] Training Batch [73/391]: Loss 0.07236745208501816\n",
      "[Epoch 16] Training Batch [74/391]: Loss 0.052661240100860596\n",
      "[Epoch 16] Training Batch [75/391]: Loss 0.10406196862459183\n",
      "[Epoch 16] Training Batch [76/391]: Loss 0.054006945341825485\n",
      "[Epoch 16] Training Batch [77/391]: Loss 0.05023834481835365\n",
      "[Epoch 16] Training Batch [78/391]: Loss 0.10212432593107224\n",
      "[Epoch 16] Training Batch [79/391]: Loss 0.05720638483762741\n",
      "[Epoch 16] Training Batch [80/391]: Loss 0.10995975136756897\n",
      "[Epoch 16] Training Batch [81/391]: Loss 0.055835772305727005\n",
      "[Epoch 16] Training Batch [82/391]: Loss 0.060397714376449585\n",
      "[Epoch 16] Training Batch [83/391]: Loss 0.07347682118415833\n",
      "[Epoch 16] Training Batch [84/391]: Loss 0.07779306173324585\n",
      "[Epoch 16] Training Batch [85/391]: Loss 0.03889507055282593\n",
      "[Epoch 16] Training Batch [86/391]: Loss 0.058787181973457336\n",
      "[Epoch 16] Training Batch [87/391]: Loss 0.08049431443214417\n",
      "[Epoch 16] Training Batch [88/391]: Loss 0.08100372552871704\n",
      "[Epoch 16] Training Batch [89/391]: Loss 0.10167120397090912\n",
      "[Epoch 16] Training Batch [90/391]: Loss 0.09043668210506439\n",
      "[Epoch 16] Training Batch [91/391]: Loss 0.07349678874015808\n",
      "[Epoch 16] Training Batch [92/391]: Loss 0.13172979652881622\n",
      "[Epoch 16] Training Batch [93/391]: Loss 0.056505411863327026\n",
      "[Epoch 16] Training Batch [94/391]: Loss 0.04232585430145264\n",
      "[Epoch 16] Training Batch [95/391]: Loss 0.037077538669109344\n",
      "[Epoch 16] Training Batch [96/391]: Loss 0.04869763180613518\n",
      "[Epoch 16] Training Batch [97/391]: Loss 0.04767639562487602\n",
      "[Epoch 16] Training Batch [98/391]: Loss 0.07988008856773376\n",
      "[Epoch 16] Training Batch [99/391]: Loss 0.05078786611557007\n",
      "[Epoch 16] Training Batch [100/391]: Loss 0.03231434524059296\n",
      "[Epoch 16] Training Batch [101/391]: Loss 0.053263790905475616\n",
      "[Epoch 16] Training Batch [102/391]: Loss 0.08708561956882477\n",
      "[Epoch 16] Training Batch [103/391]: Loss 0.044579848647117615\n",
      "[Epoch 16] Training Batch [104/391]: Loss 0.07473739236593246\n",
      "[Epoch 16] Training Batch [105/391]: Loss 0.07035575807094574\n",
      "[Epoch 16] Training Batch [106/391]: Loss 0.11428174376487732\n",
      "[Epoch 16] Training Batch [107/391]: Loss 0.05698072165250778\n",
      "[Epoch 16] Training Batch [108/391]: Loss 0.07598593086004257\n",
      "[Epoch 16] Training Batch [109/391]: Loss 0.04406166076660156\n",
      "[Epoch 16] Training Batch [110/391]: Loss 0.12002608180046082\n",
      "[Epoch 16] Training Batch [111/391]: Loss 0.05990023538470268\n",
      "[Epoch 16] Training Batch [112/391]: Loss 0.06978438794612885\n",
      "[Epoch 16] Training Batch [113/391]: Loss 0.04161824658513069\n",
      "[Epoch 16] Training Batch [114/391]: Loss 0.08911988139152527\n",
      "[Epoch 16] Training Batch [115/391]: Loss 0.07434795051813126\n",
      "[Epoch 16] Training Batch [116/391]: Loss 0.08496256917715073\n",
      "[Epoch 16] Training Batch [117/391]: Loss 0.04951617121696472\n",
      "[Epoch 16] Training Batch [118/391]: Loss 0.06920449435710907\n",
      "[Epoch 16] Training Batch [119/391]: Loss 0.08268281072378159\n",
      "[Epoch 16] Training Batch [120/391]: Loss 0.04917353391647339\n",
      "[Epoch 16] Training Batch [121/391]: Loss 0.06329969316720963\n",
      "[Epoch 16] Training Batch [122/391]: Loss 0.08438829332590103\n",
      "[Epoch 16] Training Batch [123/391]: Loss 0.06191325560212135\n",
      "[Epoch 16] Training Batch [124/391]: Loss 0.11763449758291245\n",
      "[Epoch 16] Training Batch [125/391]: Loss 0.0759739801287651\n",
      "[Epoch 16] Training Batch [126/391]: Loss 0.03864063695073128\n",
      "[Epoch 16] Training Batch [127/391]: Loss 0.023789186030626297\n",
      "[Epoch 16] Training Batch [128/391]: Loss 0.045435287058353424\n",
      "[Epoch 16] Training Batch [129/391]: Loss 0.06043529510498047\n",
      "[Epoch 16] Training Batch [130/391]: Loss 0.059750765562057495\n",
      "[Epoch 16] Training Batch [131/391]: Loss 0.06494349241256714\n",
      "[Epoch 16] Training Batch [132/391]: Loss 0.056656528264284134\n",
      "[Epoch 16] Training Batch [133/391]: Loss 0.03969326615333557\n",
      "[Epoch 16] Training Batch [134/391]: Loss 0.09200448542833328\n",
      "[Epoch 16] Training Batch [135/391]: Loss 0.08832909911870956\n",
      "[Epoch 16] Training Batch [136/391]: Loss 0.07290519773960114\n",
      "[Epoch 16] Training Batch [137/391]: Loss 0.08418861031532288\n",
      "[Epoch 16] Training Batch [138/391]: Loss 0.16479170322418213\n",
      "[Epoch 16] Training Batch [139/391]: Loss 0.10279862582683563\n",
      "[Epoch 16] Training Batch [140/391]: Loss 0.05723648890852928\n",
      "[Epoch 16] Training Batch [141/391]: Loss 0.10134096443653107\n",
      "[Epoch 16] Training Batch [142/391]: Loss 0.06770267337560654\n",
      "[Epoch 16] Training Batch [143/391]: Loss 0.08074988424777985\n",
      "[Epoch 16] Training Batch [144/391]: Loss 0.09744846075773239\n",
      "[Epoch 16] Training Batch [145/391]: Loss 0.09969231486320496\n",
      "[Epoch 16] Training Batch [146/391]: Loss 0.026351790875196457\n",
      "[Epoch 16] Training Batch [147/391]: Loss 0.11228422820568085\n",
      "[Epoch 16] Training Batch [148/391]: Loss 0.06758112460374832\n",
      "[Epoch 16] Training Batch [149/391]: Loss 0.0433666929602623\n",
      "[Epoch 16] Training Batch [150/391]: Loss 0.03566361963748932\n",
      "[Epoch 16] Training Batch [151/391]: Loss 0.07070551067590714\n",
      "[Epoch 16] Training Batch [152/391]: Loss 0.13783755898475647\n",
      "[Epoch 16] Training Batch [153/391]: Loss 0.10941402614116669\n",
      "[Epoch 16] Training Batch [154/391]: Loss 0.10764352232217789\n",
      "[Epoch 16] Training Batch [155/391]: Loss 0.0638212114572525\n",
      "[Epoch 16] Training Batch [156/391]: Loss 0.07042025029659271\n",
      "[Epoch 16] Training Batch [157/391]: Loss 0.06401747465133667\n",
      "[Epoch 16] Training Batch [158/391]: Loss 0.11040709912776947\n",
      "[Epoch 16] Training Batch [159/391]: Loss 0.10466036945581436\n",
      "[Epoch 16] Training Batch [160/391]: Loss 0.1703958958387375\n",
      "[Epoch 16] Training Batch [161/391]: Loss 0.10203509032726288\n",
      "[Epoch 16] Training Batch [162/391]: Loss 0.0991831049323082\n",
      "[Epoch 16] Training Batch [163/391]: Loss 0.04952925071120262\n",
      "[Epoch 16] Training Batch [164/391]: Loss 0.07829967886209488\n",
      "[Epoch 16] Training Batch [165/391]: Loss 0.08092799782752991\n",
      "[Epoch 16] Training Batch [166/391]: Loss 0.11755385994911194\n",
      "[Epoch 16] Training Batch [167/391]: Loss 0.055860165506601334\n",
      "[Epoch 16] Training Batch [168/391]: Loss 0.056648146361112595\n",
      "[Epoch 16] Training Batch [169/391]: Loss 0.12495024502277374\n",
      "[Epoch 16] Training Batch [170/391]: Loss 0.07566168904304504\n",
      "[Epoch 16] Training Batch [171/391]: Loss 0.06141611188650131\n",
      "[Epoch 16] Training Batch [172/391]: Loss 0.09068389236927032\n",
      "[Epoch 16] Training Batch [173/391]: Loss 0.06025973707437515\n",
      "[Epoch 16] Training Batch [174/391]: Loss 0.04166271910071373\n",
      "[Epoch 16] Training Batch [175/391]: Loss 0.10853641480207443\n",
      "[Epoch 16] Training Batch [176/391]: Loss 0.08858700096607208\n",
      "[Epoch 16] Training Batch [177/391]: Loss 0.07666553556919098\n",
      "[Epoch 16] Training Batch [178/391]: Loss 0.06746600568294525\n",
      "[Epoch 16] Training Batch [179/391]: Loss 0.07255911082029343\n",
      "[Epoch 16] Training Batch [180/391]: Loss 0.06361247599124908\n",
      "[Epoch 16] Training Batch [181/391]: Loss 0.056147295981645584\n",
      "[Epoch 16] Training Batch [182/391]: Loss 0.06473024934530258\n",
      "[Epoch 16] Training Batch [183/391]: Loss 0.08201145380735397\n",
      "[Epoch 16] Training Batch [184/391]: Loss 0.03263212367892265\n",
      "[Epoch 16] Training Batch [185/391]: Loss 0.0752558559179306\n",
      "[Epoch 16] Training Batch [186/391]: Loss 0.07589294016361237\n",
      "[Epoch 16] Training Batch [187/391]: Loss 0.05213958024978638\n",
      "[Epoch 16] Training Batch [188/391]: Loss 0.11062312126159668\n",
      "[Epoch 16] Training Batch [189/391]: Loss 0.05721166357398033\n",
      "[Epoch 16] Training Batch [190/391]: Loss 0.04745278134942055\n",
      "[Epoch 16] Training Batch [191/391]: Loss 0.08781643956899643\n",
      "[Epoch 16] Training Batch [192/391]: Loss 0.09404338151216507\n",
      "[Epoch 16] Training Batch [193/391]: Loss 0.05607903003692627\n",
      "[Epoch 16] Training Batch [194/391]: Loss 0.1156964898109436\n",
      "[Epoch 16] Training Batch [195/391]: Loss 0.054182324558496475\n",
      "[Epoch 16] Training Batch [196/391]: Loss 0.06228141859173775\n",
      "[Epoch 16] Training Batch [197/391]: Loss 0.11619578301906586\n",
      "[Epoch 16] Training Batch [198/391]: Loss 0.03825164958834648\n",
      "[Epoch 16] Training Batch [199/391]: Loss 0.07950130105018616\n",
      "[Epoch 16] Training Batch [200/391]: Loss 0.09751971811056137\n",
      "[Epoch 16] Training Batch [201/391]: Loss 0.09186457842588425\n",
      "[Epoch 16] Training Batch [202/391]: Loss 0.11813516914844513\n",
      "[Epoch 16] Training Batch [203/391]: Loss 0.03316955268383026\n",
      "[Epoch 16] Training Batch [204/391]: Loss 0.072840116918087\n",
      "[Epoch 16] Training Batch [205/391]: Loss 0.0599830336868763\n",
      "[Epoch 16] Training Batch [206/391]: Loss 0.12236883491277695\n",
      "[Epoch 16] Training Batch [207/391]: Loss 0.08549481630325317\n",
      "[Epoch 16] Training Batch [208/391]: Loss 0.06745132058858871\n",
      "[Epoch 16] Training Batch [209/391]: Loss 0.06471830606460571\n",
      "[Epoch 16] Training Batch [210/391]: Loss 0.04191236570477486\n",
      "[Epoch 16] Training Batch [211/391]: Loss 0.06556989252567291\n",
      "[Epoch 16] Training Batch [212/391]: Loss 0.04914446920156479\n",
      "[Epoch 16] Training Batch [213/391]: Loss 0.04496553912758827\n",
      "[Epoch 16] Training Batch [214/391]: Loss 0.06628761440515518\n",
      "[Epoch 16] Training Batch [215/391]: Loss 0.04090236872434616\n",
      "[Epoch 16] Training Batch [216/391]: Loss 0.036092229187488556\n",
      "[Epoch 16] Training Batch [217/391]: Loss 0.08391766250133514\n",
      "[Epoch 16] Training Batch [218/391]: Loss 0.1275509148836136\n",
      "[Epoch 16] Training Batch [219/391]: Loss 0.04960298910737038\n",
      "[Epoch 16] Training Batch [220/391]: Loss 0.02477455511689186\n",
      "[Epoch 16] Training Batch [221/391]: Loss 0.03511010482907295\n",
      "[Epoch 16] Training Batch [222/391]: Loss 0.09070099890232086\n",
      "[Epoch 16] Training Batch [223/391]: Loss 0.10906031727790833\n",
      "[Epoch 16] Training Batch [224/391]: Loss 0.041472434997558594\n",
      "[Epoch 16] Training Batch [225/391]: Loss 0.03055223450064659\n",
      "[Epoch 16] Training Batch [226/391]: Loss 0.11891226470470428\n",
      "[Epoch 16] Training Batch [227/391]: Loss 0.04621189460158348\n",
      "[Epoch 16] Training Batch [228/391]: Loss 0.07431550323963165\n",
      "[Epoch 16] Training Batch [229/391]: Loss 0.04556377977132797\n",
      "[Epoch 16] Training Batch [230/391]: Loss 0.05599645525217056\n",
      "[Epoch 16] Training Batch [231/391]: Loss 0.07560387998819351\n",
      "[Epoch 16] Training Batch [232/391]: Loss 0.03505788743495941\n",
      "[Epoch 16] Training Batch [233/391]: Loss 0.041320912539958954\n",
      "[Epoch 16] Training Batch [234/391]: Loss 0.05051453784108162\n",
      "[Epoch 16] Training Batch [235/391]: Loss 0.04065951332449913\n",
      "[Epoch 16] Training Batch [236/391]: Loss 0.10949055105447769\n",
      "[Epoch 16] Training Batch [237/391]: Loss 0.06544461846351624\n",
      "[Epoch 16] Training Batch [238/391]: Loss 0.04005727171897888\n",
      "[Epoch 16] Training Batch [239/391]: Loss 0.10910332947969437\n",
      "[Epoch 16] Training Batch [240/391]: Loss 0.10400194674730301\n",
      "[Epoch 16] Training Batch [241/391]: Loss 0.10949987173080444\n",
      "[Epoch 16] Training Batch [242/391]: Loss 0.0711338147521019\n",
      "[Epoch 16] Training Batch [243/391]: Loss 0.036936476826667786\n",
      "[Epoch 16] Training Batch [244/391]: Loss 0.046800337731838226\n",
      "[Epoch 16] Training Batch [245/391]: Loss 0.09664615988731384\n",
      "[Epoch 16] Training Batch [246/391]: Loss 0.09410040080547333\n",
      "[Epoch 16] Training Batch [247/391]: Loss 0.08024457097053528\n",
      "[Epoch 16] Training Batch [248/391]: Loss 0.0852007195353508\n",
      "[Epoch 16] Training Batch [249/391]: Loss 0.054943740367889404\n",
      "[Epoch 16] Training Batch [250/391]: Loss 0.05371317267417908\n",
      "[Epoch 16] Training Batch [251/391]: Loss 0.035451535135507584\n",
      "[Epoch 16] Training Batch [252/391]: Loss 0.07398349791765213\n",
      "[Epoch 16] Training Batch [253/391]: Loss 0.060060691088438034\n",
      "[Epoch 16] Training Batch [254/391]: Loss 0.08317922800779343\n",
      "[Epoch 16] Training Batch [255/391]: Loss 0.038542695343494415\n",
      "[Epoch 16] Training Batch [256/391]: Loss 0.10514664649963379\n",
      "[Epoch 16] Training Batch [257/391]: Loss 0.15908606350421906\n",
      "[Epoch 16] Training Batch [258/391]: Loss 0.08243758976459503\n",
      "[Epoch 16] Training Batch [259/391]: Loss 0.07495724409818649\n",
      "[Epoch 16] Training Batch [260/391]: Loss 0.11573301255702972\n",
      "[Epoch 16] Training Batch [261/391]: Loss 0.0776042640209198\n",
      "[Epoch 16] Training Batch [262/391]: Loss 0.09454110264778137\n",
      "[Epoch 16] Training Batch [263/391]: Loss 0.09357984364032745\n",
      "[Epoch 16] Training Batch [264/391]: Loss 0.112247996032238\n",
      "[Epoch 16] Training Batch [265/391]: Loss 0.055804308503866196\n",
      "[Epoch 16] Training Batch [266/391]: Loss 0.06998977065086365\n",
      "[Epoch 16] Training Batch [267/391]: Loss 0.14299948513507843\n",
      "[Epoch 16] Training Batch [268/391]: Loss 0.05454792082309723\n",
      "[Epoch 16] Training Batch [269/391]: Loss 0.08223739266395569\n",
      "[Epoch 16] Training Batch [270/391]: Loss 0.043990686535835266\n",
      "[Epoch 16] Training Batch [271/391]: Loss 0.033946890383958817\n",
      "[Epoch 16] Training Batch [272/391]: Loss 0.06231211498379707\n",
      "[Epoch 16] Training Batch [273/391]: Loss 0.07738185673952103\n",
      "[Epoch 16] Training Batch [274/391]: Loss 0.07284598797559738\n",
      "[Epoch 16] Training Batch [275/391]: Loss 0.052816327661275864\n",
      "[Epoch 16] Training Batch [276/391]: Loss 0.06136293336749077\n",
      "[Epoch 16] Training Batch [277/391]: Loss 0.024822909384965897\n",
      "[Epoch 16] Training Batch [278/391]: Loss 0.13979285955429077\n",
      "[Epoch 16] Training Batch [279/391]: Loss 0.0729755312204361\n",
      "[Epoch 16] Training Batch [280/391]: Loss 0.07021157443523407\n",
      "[Epoch 16] Training Batch [281/391]: Loss 0.027551576495170593\n",
      "[Epoch 16] Training Batch [282/391]: Loss 0.05791941657662392\n",
      "[Epoch 16] Training Batch [283/391]: Loss 0.10220453143119812\n",
      "[Epoch 16] Training Batch [284/391]: Loss 0.08467605710029602\n",
      "[Epoch 16] Training Batch [285/391]: Loss 0.0638953372836113\n",
      "[Epoch 16] Training Batch [286/391]: Loss 0.1020250916481018\n",
      "[Epoch 16] Training Batch [287/391]: Loss 0.05490022897720337\n",
      "[Epoch 16] Training Batch [288/391]: Loss 0.040290046483278275\n",
      "[Epoch 16] Training Batch [289/391]: Loss 0.05964920297265053\n",
      "[Epoch 16] Training Batch [290/391]: Loss 0.07654888927936554\n",
      "[Epoch 16] Training Batch [291/391]: Loss 0.09065798670053482\n",
      "[Epoch 16] Training Batch [292/391]: Loss 0.05362408235669136\n",
      "[Epoch 16] Training Batch [293/391]: Loss 0.03558864817023277\n",
      "[Epoch 16] Training Batch [294/391]: Loss 0.0682167112827301\n",
      "[Epoch 16] Training Batch [295/391]: Loss 0.06966859102249146\n",
      "[Epoch 16] Training Batch [296/391]: Loss 0.03801397979259491\n",
      "[Epoch 16] Training Batch [297/391]: Loss 0.04303160682320595\n",
      "[Epoch 16] Training Batch [298/391]: Loss 0.06786350905895233\n",
      "[Epoch 16] Training Batch [299/391]: Loss 0.04769008979201317\n",
      "[Epoch 16] Training Batch [300/391]: Loss 0.0641864538192749\n",
      "[Epoch 16] Training Batch [301/391]: Loss 0.06036816164851189\n",
      "[Epoch 16] Training Batch [302/391]: Loss 0.01778089813888073\n",
      "[Epoch 16] Training Batch [303/391]: Loss 0.05070921778678894\n",
      "[Epoch 16] Training Batch [304/391]: Loss 0.09291539341211319\n",
      "[Epoch 16] Training Batch [305/391]: Loss 0.10359925776720047\n",
      "[Epoch 16] Training Batch [306/391]: Loss 0.06552428007125854\n",
      "[Epoch 16] Training Batch [307/391]: Loss 0.0865497887134552\n",
      "[Epoch 16] Training Batch [308/391]: Loss 0.08849097788333893\n",
      "[Epoch 16] Training Batch [309/391]: Loss 0.0787201002240181\n",
      "[Epoch 16] Training Batch [310/391]: Loss 0.07598519325256348\n",
      "[Epoch 16] Training Batch [311/391]: Loss 0.032582055777311325\n",
      "[Epoch 16] Training Batch [312/391]: Loss 0.047362297773361206\n",
      "[Epoch 16] Training Batch [313/391]: Loss 0.03052695281803608\n",
      "[Epoch 16] Training Batch [314/391]: Loss 0.05338001251220703\n",
      "[Epoch 16] Training Batch [315/391]: Loss 0.08081094175577164\n",
      "[Epoch 16] Training Batch [316/391]: Loss 0.05553750321269035\n",
      "[Epoch 16] Training Batch [317/391]: Loss 0.08859153836965561\n",
      "[Epoch 16] Training Batch [318/391]: Loss 0.05774347484111786\n",
      "[Epoch 16] Training Batch [319/391]: Loss 0.08058896660804749\n",
      "[Epoch 16] Training Batch [320/391]: Loss 0.11277830600738525\n",
      "[Epoch 16] Training Batch [321/391]: Loss 0.1239294484257698\n",
      "[Epoch 16] Training Batch [322/391]: Loss 0.053381599485874176\n",
      "[Epoch 16] Training Batch [323/391]: Loss 0.07507655024528503\n",
      "[Epoch 16] Training Batch [324/391]: Loss 0.06362783908843994\n",
      "[Epoch 16] Training Batch [325/391]: Loss 0.09572912007570267\n",
      "[Epoch 16] Training Batch [326/391]: Loss 0.05207967013120651\n",
      "[Epoch 16] Training Batch [327/391]: Loss 0.12242262065410614\n",
      "[Epoch 16] Training Batch [328/391]: Loss 0.11369703710079193\n",
      "[Epoch 16] Training Batch [329/391]: Loss 0.09465313702821732\n",
      "[Epoch 16] Training Batch [330/391]: Loss 0.07283540815114975\n",
      "[Epoch 16] Training Batch [331/391]: Loss 0.1375286877155304\n",
      "[Epoch 16] Training Batch [332/391]: Loss 0.11840929090976715\n",
      "[Epoch 16] Training Batch [333/391]: Loss 0.032872460782527924\n",
      "[Epoch 16] Training Batch [334/391]: Loss 0.08977887779474258\n",
      "[Epoch 16] Training Batch [335/391]: Loss 0.10142809897661209\n",
      "[Epoch 16] Training Batch [336/391]: Loss 0.09651679545640945\n",
      "[Epoch 16] Training Batch [337/391]: Loss 0.10461227595806122\n",
      "[Epoch 16] Training Batch [338/391]: Loss 0.08553110063076019\n",
      "[Epoch 16] Training Batch [339/391]: Loss 0.06129994988441467\n",
      "[Epoch 16] Training Batch [340/391]: Loss 0.09504396468400955\n",
      "[Epoch 16] Training Batch [341/391]: Loss 0.08444575220346451\n",
      "[Epoch 16] Training Batch [342/391]: Loss 0.16368409991264343\n",
      "[Epoch 16] Training Batch [343/391]: Loss 0.06842619180679321\n",
      "[Epoch 16] Training Batch [344/391]: Loss 0.08113062381744385\n",
      "[Epoch 16] Training Batch [345/391]: Loss 0.09967032074928284\n",
      "[Epoch 16] Training Batch [346/391]: Loss 0.04830954968929291\n",
      "[Epoch 16] Training Batch [347/391]: Loss 0.05909476429224014\n",
      "[Epoch 16] Training Batch [348/391]: Loss 0.09818330407142639\n",
      "[Epoch 16] Training Batch [349/391]: Loss 0.13436895608901978\n",
      "[Epoch 16] Training Batch [350/391]: Loss 0.17143826186656952\n",
      "[Epoch 16] Training Batch [351/391]: Loss 0.06691527366638184\n",
      "[Epoch 16] Training Batch [352/391]: Loss 0.08848924189805984\n",
      "[Epoch 16] Training Batch [353/391]: Loss 0.0841493085026741\n",
      "[Epoch 16] Training Batch [354/391]: Loss 0.1554415374994278\n",
      "[Epoch 16] Training Batch [355/391]: Loss 0.055791180580854416\n",
      "[Epoch 16] Training Batch [356/391]: Loss 0.07358007878065109\n",
      "[Epoch 16] Training Batch [357/391]: Loss 0.075149767100811\n",
      "[Epoch 16] Training Batch [358/391]: Loss 0.06126957759261131\n",
      "[Epoch 16] Training Batch [359/391]: Loss 0.07090975344181061\n",
      "[Epoch 16] Training Batch [360/391]: Loss 0.03530100733041763\n",
      "[Epoch 16] Training Batch [361/391]: Loss 0.22379393875598907\n",
      "[Epoch 16] Training Batch [362/391]: Loss 0.08614189177751541\n",
      "[Epoch 16] Training Batch [363/391]: Loss 0.12652161717414856\n",
      "[Epoch 16] Training Batch [364/391]: Loss 0.0856834128499031\n",
      "[Epoch 16] Training Batch [365/391]: Loss 0.13893292844295502\n",
      "[Epoch 16] Training Batch [366/391]: Loss 0.08548746258020401\n",
      "[Epoch 16] Training Batch [367/391]: Loss 0.0687788650393486\n",
      "[Epoch 16] Training Batch [368/391]: Loss 0.10096278786659241\n",
      "[Epoch 16] Training Batch [369/391]: Loss 0.07281972467899323\n",
      "[Epoch 16] Training Batch [370/391]: Loss 0.08711907267570496\n",
      "[Epoch 16] Training Batch [371/391]: Loss 0.08927356451749802\n",
      "[Epoch 16] Training Batch [372/391]: Loss 0.03713478147983551\n",
      "[Epoch 16] Training Batch [373/391]: Loss 0.08424465358257294\n",
      "[Epoch 16] Training Batch [374/391]: Loss 0.08386342227458954\n",
      "[Epoch 16] Training Batch [375/391]: Loss 0.09748093038797379\n",
      "[Epoch 16] Training Batch [376/391]: Loss 0.04268547147512436\n",
      "[Epoch 16] Training Batch [377/391]: Loss 0.12042217701673508\n",
      "[Epoch 16] Training Batch [378/391]: Loss 0.06063995510339737\n",
      "[Epoch 16] Training Batch [379/391]: Loss 0.07385871559381485\n",
      "[Epoch 16] Training Batch [380/391]: Loss 0.11741183698177338\n",
      "[Epoch 16] Training Batch [381/391]: Loss 0.12107957154512405\n",
      "[Epoch 16] Training Batch [382/391]: Loss 0.07055746018886566\n",
      "[Epoch 16] Training Batch [383/391]: Loss 0.04288540408015251\n",
      "[Epoch 16] Training Batch [384/391]: Loss 0.13555586338043213\n",
      "[Epoch 16] Training Batch [385/391]: Loss 0.06568337976932526\n",
      "[Epoch 16] Training Batch [386/391]: Loss 0.0707770586013794\n",
      "[Epoch 16] Training Batch [387/391]: Loss 0.11391473561525345\n",
      "[Epoch 16] Training Batch [388/391]: Loss 0.07938052713871002\n",
      "[Epoch 16] Training Batch [389/391]: Loss 0.11995134502649307\n",
      "[Epoch 16] Training Batch [390/391]: Loss 0.14370696246623993\n",
      "[Epoch 16] Training Batch [391/391]: Loss 0.08274151384830475\n",
      "Epoch 16 - Train Loss: 0.0774\n",
      "*********  Epoch 17/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Training Batch [1/391]: Loss 0.04371481388807297\n",
      "[Epoch 17] Training Batch [2/391]: Loss 0.04141947999596596\n",
      "[Epoch 17] Training Batch [3/391]: Loss 0.1572439819574356\n",
      "[Epoch 17] Training Batch [4/391]: Loss 0.06798053532838821\n",
      "[Epoch 17] Training Batch [5/391]: Loss 0.033392611891031265\n",
      "[Epoch 17] Training Batch [6/391]: Loss 0.047431860119104385\n",
      "[Epoch 17] Training Batch [7/391]: Loss 0.06399460881948471\n",
      "[Epoch 17] Training Batch [8/391]: Loss 0.05043826252222061\n",
      "[Epoch 17] Training Batch [9/391]: Loss 0.06817791610956192\n",
      "[Epoch 17] Training Batch [10/391]: Loss 0.0477786548435688\n",
      "[Epoch 17] Training Batch [11/391]: Loss 0.06144389137625694\n",
      "[Epoch 17] Training Batch [12/391]: Loss 0.14570797979831696\n",
      "[Epoch 17] Training Batch [13/391]: Loss 0.06265470385551453\n",
      "[Epoch 17] Training Batch [14/391]: Loss 0.07035751640796661\n",
      "[Epoch 17] Training Batch [15/391]: Loss 0.05307637155056\n",
      "[Epoch 17] Training Batch [16/391]: Loss 0.13282033801078796\n",
      "[Epoch 17] Training Batch [17/391]: Loss 0.04877239838242531\n",
      "[Epoch 17] Training Batch [18/391]: Loss 0.06268707662820816\n",
      "[Epoch 17] Training Batch [19/391]: Loss 0.054175667464733124\n",
      "[Epoch 17] Training Batch [20/391]: Loss 0.08560159802436829\n",
      "[Epoch 17] Training Batch [21/391]: Loss 0.06818830966949463\n",
      "[Epoch 17] Training Batch [22/391]: Loss 0.11169354617595673\n",
      "[Epoch 17] Training Batch [23/391]: Loss 0.04774235934019089\n",
      "[Epoch 17] Training Batch [24/391]: Loss 0.1384861320257187\n",
      "[Epoch 17] Training Batch [25/391]: Loss 0.06360986828804016\n",
      "[Epoch 17] Training Batch [26/391]: Loss 0.06379065662622452\n",
      "[Epoch 17] Training Batch [27/391]: Loss 0.02223087288439274\n",
      "[Epoch 17] Training Batch [28/391]: Loss 0.053343262523412704\n",
      "[Epoch 17] Training Batch [29/391]: Loss 0.029284611344337463\n",
      "[Epoch 17] Training Batch [30/391]: Loss 0.040109917521476746\n",
      "[Epoch 17] Training Batch [31/391]: Loss 0.025927729904651642\n",
      "[Epoch 17] Training Batch [32/391]: Loss 0.08186574280261993\n",
      "[Epoch 17] Training Batch [33/391]: Loss 0.07449842989444733\n",
      "[Epoch 17] Training Batch [34/391]: Loss 0.060619208961725235\n",
      "[Epoch 17] Training Batch [35/391]: Loss 0.056572869420051575\n",
      "[Epoch 17] Training Batch [36/391]: Loss 0.024588776752352715\n",
      "[Epoch 17] Training Batch [37/391]: Loss 0.04043429344892502\n",
      "[Epoch 17] Training Batch [38/391]: Loss 0.03654901310801506\n",
      "[Epoch 17] Training Batch [39/391]: Loss 0.10017827153205872\n",
      "[Epoch 17] Training Batch [40/391]: Loss 0.09643951803445816\n",
      "[Epoch 17] Training Batch [41/391]: Loss 0.03557995706796646\n",
      "[Epoch 17] Training Batch [42/391]: Loss 0.06855721771717072\n",
      "[Epoch 17] Training Batch [43/391]: Loss 0.02860582247376442\n",
      "[Epoch 17] Training Batch [44/391]: Loss 0.041290007531642914\n",
      "[Epoch 17] Training Batch [45/391]: Loss 0.03511594980955124\n",
      "[Epoch 17] Training Batch [46/391]: Loss 0.1287679374217987\n",
      "[Epoch 17] Training Batch [47/391]: Loss 0.0671738013625145\n",
      "[Epoch 17] Training Batch [48/391]: Loss 0.09908942878246307\n",
      "[Epoch 17] Training Batch [49/391]: Loss 0.04828836768865585\n",
      "[Epoch 17] Training Batch [50/391]: Loss 0.018880747258663177\n",
      "[Epoch 17] Training Batch [51/391]: Loss 0.03662123158574104\n",
      "[Epoch 17] Training Batch [52/391]: Loss 0.04798198863863945\n",
      "[Epoch 17] Training Batch [53/391]: Loss 0.10481807589530945\n",
      "[Epoch 17] Training Batch [54/391]: Loss 0.05729590356349945\n",
      "[Epoch 17] Training Batch [55/391]: Loss 0.046219516545534134\n",
      "[Epoch 17] Training Batch [56/391]: Loss 0.05528917908668518\n",
      "[Epoch 17] Training Batch [57/391]: Loss 0.0407969169318676\n",
      "[Epoch 17] Training Batch [58/391]: Loss 0.0878114402294159\n",
      "[Epoch 17] Training Batch [59/391]: Loss 0.050615593791007996\n",
      "[Epoch 17] Training Batch [60/391]: Loss 0.04329737648367882\n",
      "[Epoch 17] Training Batch [61/391]: Loss 0.0563335157930851\n",
      "[Epoch 17] Training Batch [62/391]: Loss 0.07441864162683487\n",
      "[Epoch 17] Training Batch [63/391]: Loss 0.06102776899933815\n",
      "[Epoch 17] Training Batch [64/391]: Loss 0.09483151137828827\n",
      "[Epoch 17] Training Batch [65/391]: Loss 0.08902253210544586\n",
      "[Epoch 17] Training Batch [66/391]: Loss 0.06525688618421555\n",
      "[Epoch 17] Training Batch [67/391]: Loss 0.034070275723934174\n",
      "[Epoch 17] Training Batch [68/391]: Loss 0.04963774234056473\n",
      "[Epoch 17] Training Batch [69/391]: Loss 0.08739828318357468\n",
      "[Epoch 17] Training Batch [70/391]: Loss 0.06292012333869934\n",
      "[Epoch 17] Training Batch [71/391]: Loss 0.03941170498728752\n",
      "[Epoch 17] Training Batch [72/391]: Loss 0.08481523394584656\n",
      "[Epoch 17] Training Batch [73/391]: Loss 0.07339050620794296\n",
      "[Epoch 17] Training Batch [74/391]: Loss 0.1419936716556549\n",
      "[Epoch 17] Training Batch [75/391]: Loss 0.09765610843896866\n",
      "[Epoch 17] Training Batch [76/391]: Loss 0.05735093727707863\n",
      "[Epoch 17] Training Batch [77/391]: Loss 0.026993662118911743\n",
      "[Epoch 17] Training Batch [78/391]: Loss 0.0373731330037117\n",
      "[Epoch 17] Training Batch [79/391]: Loss 0.06773822754621506\n",
      "[Epoch 17] Training Batch [80/391]: Loss 0.03855231776833534\n",
      "[Epoch 17] Training Batch [81/391]: Loss 0.06741322576999664\n",
      "[Epoch 17] Training Batch [82/391]: Loss 0.049854379147291183\n",
      "[Epoch 17] Training Batch [83/391]: Loss 0.09225676208734512\n",
      "[Epoch 17] Training Batch [84/391]: Loss 0.016291970387101173\n",
      "[Epoch 17] Training Batch [85/391]: Loss 0.039191555231809616\n",
      "[Epoch 17] Training Batch [86/391]: Loss 0.032597512006759644\n",
      "[Epoch 17] Training Batch [87/391]: Loss 0.028987199068069458\n",
      "[Epoch 17] Training Batch [88/391]: Loss 0.08368761837482452\n",
      "[Epoch 17] Training Batch [89/391]: Loss 0.11444311589002609\n",
      "[Epoch 17] Training Batch [90/391]: Loss 0.07330680638551712\n",
      "[Epoch 17] Training Batch [91/391]: Loss 0.05987070873379707\n",
      "[Epoch 17] Training Batch [92/391]: Loss 0.1086149513721466\n",
      "[Epoch 17] Training Batch [93/391]: Loss 0.114808589220047\n",
      "[Epoch 17] Training Batch [94/391]: Loss 0.03350668400526047\n",
      "[Epoch 17] Training Batch [95/391]: Loss 0.0661218911409378\n",
      "[Epoch 17] Training Batch [96/391]: Loss 0.07427430897951126\n",
      "[Epoch 17] Training Batch [97/391]: Loss 0.035032209008932114\n",
      "[Epoch 17] Training Batch [98/391]: Loss 0.058178529143333435\n",
      "[Epoch 17] Training Batch [99/391]: Loss 0.1097729280591011\n",
      "[Epoch 17] Training Batch [100/391]: Loss 0.1392388939857483\n",
      "[Epoch 17] Training Batch [101/391]: Loss 0.0734323263168335\n",
      "[Epoch 17] Training Batch [102/391]: Loss 0.08368486166000366\n",
      "[Epoch 17] Training Batch [103/391]: Loss 0.10250888019800186\n",
      "[Epoch 17] Training Batch [104/391]: Loss 0.04427287355065346\n",
      "[Epoch 17] Training Batch [105/391]: Loss 0.040839195251464844\n",
      "[Epoch 17] Training Batch [106/391]: Loss 0.03826884552836418\n",
      "[Epoch 17] Training Batch [107/391]: Loss 0.0588366836309433\n",
      "[Epoch 17] Training Batch [108/391]: Loss 0.09022576361894608\n",
      "[Epoch 17] Training Batch [109/391]: Loss 0.03989830240607262\n",
      "[Epoch 17] Training Batch [110/391]: Loss 0.027737164869904518\n",
      "[Epoch 17] Training Batch [111/391]: Loss 0.06828954070806503\n",
      "[Epoch 17] Training Batch [112/391]: Loss 0.09737175703048706\n",
      "[Epoch 17] Training Batch [113/391]: Loss 0.03351298347115517\n",
      "[Epoch 17] Training Batch [114/391]: Loss 0.031383153051137924\n",
      "[Epoch 17] Training Batch [115/391]: Loss 0.03654690459370613\n",
      "[Epoch 17] Training Batch [116/391]: Loss 0.05547039583325386\n",
      "[Epoch 17] Training Batch [117/391]: Loss 0.04431871697306633\n",
      "[Epoch 17] Training Batch [118/391]: Loss 0.05731802061200142\n",
      "[Epoch 17] Training Batch [119/391]: Loss 0.08102314919233322\n",
      "[Epoch 17] Training Batch [120/391]: Loss 0.07275162637233734\n",
      "[Epoch 17] Training Batch [121/391]: Loss 0.08643767237663269\n",
      "[Epoch 17] Training Batch [122/391]: Loss 0.06008698418736458\n",
      "[Epoch 17] Training Batch [123/391]: Loss 0.09005661308765411\n",
      "[Epoch 17] Training Batch [124/391]: Loss 0.03619851544499397\n",
      "[Epoch 17] Training Batch [125/391]: Loss 0.016541611403226852\n",
      "[Epoch 17] Training Batch [126/391]: Loss 0.06398898363113403\n",
      "[Epoch 17] Training Batch [127/391]: Loss 0.08324053883552551\n",
      "[Epoch 17] Training Batch [128/391]: Loss 0.06440578401088715\n",
      "[Epoch 17] Training Batch [129/391]: Loss 0.07873174548149109\n",
      "[Epoch 17] Training Batch [130/391]: Loss 0.1024453416466713\n",
      "[Epoch 17] Training Batch [131/391]: Loss 0.03146951645612717\n",
      "[Epoch 17] Training Batch [132/391]: Loss 0.062200143933296204\n",
      "[Epoch 17] Training Batch [133/391]: Loss 0.12278598546981812\n",
      "[Epoch 17] Training Batch [134/391]: Loss 0.08705107867717743\n",
      "[Epoch 17] Training Batch [135/391]: Loss 0.022295545786619186\n",
      "[Epoch 17] Training Batch [136/391]: Loss 0.03905843570828438\n",
      "[Epoch 17] Training Batch [137/391]: Loss 0.025814257562160492\n",
      "[Epoch 17] Training Batch [138/391]: Loss 0.06382856518030167\n",
      "[Epoch 17] Training Batch [139/391]: Loss 0.03840026631951332\n",
      "[Epoch 17] Training Batch [140/391]: Loss 0.06119850277900696\n",
      "[Epoch 17] Training Batch [141/391]: Loss 0.05405019596219063\n",
      "[Epoch 17] Training Batch [142/391]: Loss 0.042810771614313126\n",
      "[Epoch 17] Training Batch [143/391]: Loss 0.08106999099254608\n",
      "[Epoch 17] Training Batch [144/391]: Loss 0.04955314099788666\n",
      "[Epoch 17] Training Batch [145/391]: Loss 0.0441955104470253\n",
      "[Epoch 17] Training Batch [146/391]: Loss 0.12502220273017883\n",
      "[Epoch 17] Training Batch [147/391]: Loss 0.05525561422109604\n",
      "[Epoch 17] Training Batch [148/391]: Loss 0.07876129448413849\n",
      "[Epoch 17] Training Batch [149/391]: Loss 0.13809113204479218\n",
      "[Epoch 17] Training Batch [150/391]: Loss 0.038941945880651474\n",
      "[Epoch 17] Training Batch [151/391]: Loss 0.03840846195816994\n",
      "[Epoch 17] Training Batch [152/391]: Loss 0.09360415488481522\n",
      "[Epoch 17] Training Batch [153/391]: Loss 0.07058815658092499\n",
      "[Epoch 17] Training Batch [154/391]: Loss 0.031233353540301323\n",
      "[Epoch 17] Training Batch [155/391]: Loss 0.05951424688100815\n",
      "[Epoch 17] Training Batch [156/391]: Loss 0.10483137518167496\n",
      "[Epoch 17] Training Batch [157/391]: Loss 0.04744522646069527\n",
      "[Epoch 17] Training Batch [158/391]: Loss 0.02478877454996109\n",
      "[Epoch 17] Training Batch [159/391]: Loss 0.07654064893722534\n",
      "[Epoch 17] Training Batch [160/391]: Loss 0.03578467294573784\n",
      "[Epoch 17] Training Batch [161/391]: Loss 0.061101846396923065\n",
      "[Epoch 17] Training Batch [162/391]: Loss 0.06336504966020584\n",
      "[Epoch 17] Training Batch [163/391]: Loss 0.12201855331659317\n",
      "[Epoch 17] Training Batch [164/391]: Loss 0.022912725806236267\n",
      "[Epoch 17] Training Batch [165/391]: Loss 0.056464824825525284\n",
      "[Epoch 17] Training Batch [166/391]: Loss 0.0624757744371891\n",
      "[Epoch 17] Training Batch [167/391]: Loss 0.05054013803601265\n",
      "[Epoch 17] Training Batch [168/391]: Loss 0.07812003791332245\n",
      "[Epoch 17] Training Batch [169/391]: Loss 0.031769800931215286\n",
      "[Epoch 17] Training Batch [170/391]: Loss 0.06819440424442291\n",
      "[Epoch 17] Training Batch [171/391]: Loss 0.11675864458084106\n",
      "[Epoch 17] Training Batch [172/391]: Loss 0.08427515625953674\n",
      "[Epoch 17] Training Batch [173/391]: Loss 0.056103579699993134\n",
      "[Epoch 17] Training Batch [174/391]: Loss 0.06108333170413971\n",
      "[Epoch 17] Training Batch [175/391]: Loss 0.1055677980184555\n",
      "[Epoch 17] Training Batch [176/391]: Loss 0.042657315731048584\n",
      "[Epoch 17] Training Batch [177/391]: Loss 0.056722141802310944\n",
      "[Epoch 17] Training Batch [178/391]: Loss 0.04693785309791565\n",
      "[Epoch 17] Training Batch [179/391]: Loss 0.024090968072414398\n",
      "[Epoch 17] Training Batch [180/391]: Loss 0.05564243718981743\n",
      "[Epoch 17] Training Batch [181/391]: Loss 0.10304756462574005\n",
      "[Epoch 17] Training Batch [182/391]: Loss 0.05382475256919861\n",
      "[Epoch 17] Training Batch [183/391]: Loss 0.04800930246710777\n",
      "[Epoch 17] Training Batch [184/391]: Loss 0.05530552566051483\n",
      "[Epoch 17] Training Batch [185/391]: Loss 0.04693346843123436\n",
      "[Epoch 17] Training Batch [186/391]: Loss 0.110112264752388\n",
      "[Epoch 17] Training Batch [187/391]: Loss 0.04767882451415062\n",
      "[Epoch 17] Training Batch [188/391]: Loss 0.015256148763000965\n",
      "[Epoch 17] Training Batch [189/391]: Loss 0.06203600391745567\n",
      "[Epoch 17] Training Batch [190/391]: Loss 0.07061363756656647\n",
      "[Epoch 17] Training Batch [191/391]: Loss 0.11567812412977219\n",
      "[Epoch 17] Training Batch [192/391]: Loss 0.05356884375214577\n",
      "[Epoch 17] Training Batch [193/391]: Loss 0.09786432236433029\n",
      "[Epoch 17] Training Batch [194/391]: Loss 0.053425565361976624\n",
      "[Epoch 17] Training Batch [195/391]: Loss 0.15298426151275635\n",
      "[Epoch 17] Training Batch [196/391]: Loss 0.09465067088603973\n",
      "[Epoch 17] Training Batch [197/391]: Loss 0.08724872022867203\n",
      "[Epoch 17] Training Batch [198/391]: Loss 0.052606504410505295\n",
      "[Epoch 17] Training Batch [199/391]: Loss 0.13652606308460236\n",
      "[Epoch 17] Training Batch [200/391]: Loss 0.06095859408378601\n",
      "[Epoch 17] Training Batch [201/391]: Loss 0.06247032806277275\n",
      "[Epoch 17] Training Batch [202/391]: Loss 0.06259974837303162\n",
      "[Epoch 17] Training Batch [203/391]: Loss 0.09374896436929703\n",
      "[Epoch 17] Training Batch [204/391]: Loss 0.10919267684221268\n",
      "[Epoch 17] Training Batch [205/391]: Loss 0.05164377763867378\n",
      "[Epoch 17] Training Batch [206/391]: Loss 0.16238541901111603\n",
      "[Epoch 17] Training Batch [207/391]: Loss 0.10280641913414001\n",
      "[Epoch 17] Training Batch [208/391]: Loss 0.053683582693338394\n",
      "[Epoch 17] Training Batch [209/391]: Loss 0.10713936388492584\n",
      "[Epoch 17] Training Batch [210/391]: Loss 0.08843035995960236\n",
      "[Epoch 17] Training Batch [211/391]: Loss 0.03180433064699173\n",
      "[Epoch 17] Training Batch [212/391]: Loss 0.0671435073018074\n",
      "[Epoch 17] Training Batch [213/391]: Loss 0.17091050744056702\n",
      "[Epoch 17] Training Batch [214/391]: Loss 0.03408399969339371\n",
      "[Epoch 17] Training Batch [215/391]: Loss 0.0819002315402031\n",
      "[Epoch 17] Training Batch [216/391]: Loss 0.09291747212409973\n",
      "[Epoch 17] Training Batch [217/391]: Loss 0.0934966579079628\n",
      "[Epoch 17] Training Batch [218/391]: Loss 0.0639628916978836\n",
      "[Epoch 17] Training Batch [219/391]: Loss 0.14282459020614624\n",
      "[Epoch 17] Training Batch [220/391]: Loss 0.07074492424726486\n",
      "[Epoch 17] Training Batch [221/391]: Loss 0.09732180833816528\n",
      "[Epoch 17] Training Batch [222/391]: Loss 0.13153806328773499\n",
      "[Epoch 17] Training Batch [223/391]: Loss 0.09564588218927383\n",
      "[Epoch 17] Training Batch [224/391]: Loss 0.09215477108955383\n",
      "[Epoch 17] Training Batch [225/391]: Loss 0.04707074165344238\n",
      "[Epoch 17] Training Batch [226/391]: Loss 0.09248974919319153\n",
      "[Epoch 17] Training Batch [227/391]: Loss 0.11985381692647934\n",
      "[Epoch 17] Training Batch [228/391]: Loss 0.09643025696277618\n",
      "[Epoch 17] Training Batch [229/391]: Loss 0.15556831657886505\n",
      "[Epoch 17] Training Batch [230/391]: Loss 0.07113061100244522\n",
      "[Epoch 17] Training Batch [231/391]: Loss 0.10224418342113495\n",
      "[Epoch 17] Training Batch [232/391]: Loss 0.07223853468894958\n",
      "[Epoch 17] Training Batch [233/391]: Loss 0.03632359579205513\n",
      "[Epoch 17] Training Batch [234/391]: Loss 0.11603882163763046\n",
      "[Epoch 17] Training Batch [235/391]: Loss 0.0830620527267456\n",
      "[Epoch 17] Training Batch [236/391]: Loss 0.10963120311498642\n",
      "[Epoch 17] Training Batch [237/391]: Loss 0.06252183020114899\n",
      "[Epoch 17] Training Batch [238/391]: Loss 0.10769017040729523\n",
      "[Epoch 17] Training Batch [239/391]: Loss 0.07385028153657913\n",
      "[Epoch 17] Training Batch [240/391]: Loss 0.0900983139872551\n",
      "[Epoch 17] Training Batch [241/391]: Loss 0.10463648289442062\n",
      "[Epoch 17] Training Batch [242/391]: Loss 0.08408802002668381\n",
      "[Epoch 17] Training Batch [243/391]: Loss 0.0695604756474495\n",
      "[Epoch 17] Training Batch [244/391]: Loss 0.1177949458360672\n",
      "[Epoch 17] Training Batch [245/391]: Loss 0.12170279026031494\n",
      "[Epoch 17] Training Batch [246/391]: Loss 0.09478487074375153\n",
      "[Epoch 17] Training Batch [247/391]: Loss 0.04722428694367409\n",
      "[Epoch 17] Training Batch [248/391]: Loss 0.09416994452476501\n",
      "[Epoch 17] Training Batch [249/391]: Loss 0.15762855112552643\n",
      "[Epoch 17] Training Batch [250/391]: Loss 0.067952461540699\n",
      "[Epoch 17] Training Batch [251/391]: Loss 0.13079804182052612\n",
      "[Epoch 17] Training Batch [252/391]: Loss 0.17771752178668976\n",
      "[Epoch 17] Training Batch [253/391]: Loss 0.08467689901590347\n",
      "[Epoch 17] Training Batch [254/391]: Loss 0.10666213929653168\n",
      "[Epoch 17] Training Batch [255/391]: Loss 0.08329607546329498\n",
      "[Epoch 17] Training Batch [256/391]: Loss 0.06045001372694969\n",
      "[Epoch 17] Training Batch [257/391]: Loss 0.08268532156944275\n",
      "[Epoch 17] Training Batch [258/391]: Loss 0.050607312470674515\n",
      "[Epoch 17] Training Batch [259/391]: Loss 0.04608485847711563\n",
      "[Epoch 17] Training Batch [260/391]: Loss 0.08293759822845459\n",
      "[Epoch 17] Training Batch [261/391]: Loss 0.0820290595293045\n",
      "[Epoch 17] Training Batch [262/391]: Loss 0.06211577355861664\n",
      "[Epoch 17] Training Batch [263/391]: Loss 0.09752706438302994\n",
      "[Epoch 17] Training Batch [264/391]: Loss 0.026238998398184776\n",
      "[Epoch 17] Training Batch [265/391]: Loss 0.06385044753551483\n",
      "[Epoch 17] Training Batch [266/391]: Loss 0.06439279019832611\n",
      "[Epoch 17] Training Batch [267/391]: Loss 0.07360801100730896\n",
      "[Epoch 17] Training Batch [268/391]: Loss 0.06639827787876129\n",
      "[Epoch 17] Training Batch [269/391]: Loss 0.08549868315458298\n",
      "[Epoch 17] Training Batch [270/391]: Loss 0.05566197633743286\n",
      "[Epoch 17] Training Batch [271/391]: Loss 0.12340591847896576\n",
      "[Epoch 17] Training Batch [272/391]: Loss 0.1335408240556717\n",
      "[Epoch 17] Training Batch [273/391]: Loss 0.11312466859817505\n",
      "[Epoch 17] Training Batch [274/391]: Loss 0.1523338258266449\n",
      "[Epoch 17] Training Batch [275/391]: Loss 0.0786280483007431\n",
      "[Epoch 17] Training Batch [276/391]: Loss 0.09057803452014923\n",
      "[Epoch 17] Training Batch [277/391]: Loss 0.09292837232351303\n",
      "[Epoch 17] Training Batch [278/391]: Loss 0.06141986325383186\n",
      "[Epoch 17] Training Batch [279/391]: Loss 0.08470381796360016\n",
      "[Epoch 17] Training Batch [280/391]: Loss 0.05326217785477638\n",
      "[Epoch 17] Training Batch [281/391]: Loss 0.1812441647052765\n",
      "[Epoch 17] Training Batch [282/391]: Loss 0.04985663667321205\n",
      "[Epoch 17] Training Batch [283/391]: Loss 0.1050650104880333\n",
      "[Epoch 17] Training Batch [284/391]: Loss 0.09514684975147247\n",
      "[Epoch 17] Training Batch [285/391]: Loss 0.09190316498279572\n",
      "[Epoch 17] Training Batch [286/391]: Loss 0.09626996517181396\n",
      "[Epoch 17] Training Batch [287/391]: Loss 0.1674000471830368\n",
      "[Epoch 17] Training Batch [288/391]: Loss 0.07255110144615173\n",
      "[Epoch 17] Training Batch [289/391]: Loss 0.08614548295736313\n",
      "[Epoch 17] Training Batch [290/391]: Loss 0.056075822561979294\n",
      "[Epoch 17] Training Batch [291/391]: Loss 0.08141712844371796\n",
      "[Epoch 17] Training Batch [292/391]: Loss 0.13348469138145447\n",
      "[Epoch 17] Training Batch [293/391]: Loss 0.14012879133224487\n",
      "[Epoch 17] Training Batch [294/391]: Loss 0.07364866882562637\n",
      "[Epoch 17] Training Batch [295/391]: Loss 0.07465942203998566\n",
      "[Epoch 17] Training Batch [296/391]: Loss 0.06829562783241272\n",
      "[Epoch 17] Training Batch [297/391]: Loss 0.08392560482025146\n",
      "[Epoch 17] Training Batch [298/391]: Loss 0.12881991267204285\n",
      "[Epoch 17] Training Batch [299/391]: Loss 0.0676509365439415\n",
      "[Epoch 17] Training Batch [300/391]: Loss 0.08644017577171326\n",
      "[Epoch 17] Training Batch [301/391]: Loss 0.11888395994901657\n",
      "[Epoch 17] Training Batch [302/391]: Loss 0.07387066632509232\n",
      "[Epoch 17] Training Batch [303/391]: Loss 0.07071094959974289\n",
      "[Epoch 17] Training Batch [304/391]: Loss 0.08078421652317047\n",
      "[Epoch 17] Training Batch [305/391]: Loss 0.11367278546094894\n",
      "[Epoch 17] Training Batch [306/391]: Loss 0.0879676342010498\n",
      "[Epoch 17] Training Batch [307/391]: Loss 0.20817387104034424\n",
      "[Epoch 17] Training Batch [308/391]: Loss 0.06957220286130905\n",
      "[Epoch 17] Training Batch [309/391]: Loss 0.10590440779924393\n",
      "[Epoch 17] Training Batch [310/391]: Loss 0.07717614620923996\n",
      "[Epoch 17] Training Batch [311/391]: Loss 0.0980934426188469\n",
      "[Epoch 17] Training Batch [312/391]: Loss 0.10242205113172531\n",
      "[Epoch 17] Training Batch [313/391]: Loss 0.03951321542263031\n",
      "[Epoch 17] Training Batch [314/391]: Loss 0.047190964221954346\n",
      "[Epoch 17] Training Batch [315/391]: Loss 0.09073188900947571\n",
      "[Epoch 17] Training Batch [316/391]: Loss 0.06812392920255661\n",
      "[Epoch 17] Training Batch [317/391]: Loss 0.08127135783433914\n",
      "[Epoch 17] Training Batch [318/391]: Loss 0.0546790175139904\n",
      "[Epoch 17] Training Batch [319/391]: Loss 0.09922593832015991\n",
      "[Epoch 17] Training Batch [320/391]: Loss 0.10621423274278641\n",
      "[Epoch 17] Training Batch [321/391]: Loss 0.0573698952794075\n",
      "[Epoch 17] Training Batch [322/391]: Loss 0.06469237059354782\n",
      "[Epoch 17] Training Batch [323/391]: Loss 0.07161322236061096\n",
      "[Epoch 17] Training Batch [324/391]: Loss 0.08463183045387268\n",
      "[Epoch 17] Training Batch [325/391]: Loss 0.13412658870220184\n",
      "[Epoch 17] Training Batch [326/391]: Loss 0.09103791415691376\n",
      "[Epoch 17] Training Batch [327/391]: Loss 0.07419838011264801\n",
      "[Epoch 17] Training Batch [328/391]: Loss 0.09798457473516464\n",
      "[Epoch 17] Training Batch [329/391]: Loss 0.13642795383930206\n",
      "[Epoch 17] Training Batch [330/391]: Loss 0.09047535061836243\n",
      "[Epoch 17] Training Batch [331/391]: Loss 0.048859965056180954\n",
      "[Epoch 17] Training Batch [332/391]: Loss 0.07243667542934418\n",
      "[Epoch 17] Training Batch [333/391]: Loss 0.08934268355369568\n",
      "[Epoch 17] Training Batch [334/391]: Loss 0.11680275201797485\n",
      "[Epoch 17] Training Batch [335/391]: Loss 0.060178183019161224\n",
      "[Epoch 17] Training Batch [336/391]: Loss 0.07923261821269989\n",
      "[Epoch 17] Training Batch [337/391]: Loss 0.02573614940047264\n",
      "[Epoch 17] Training Batch [338/391]: Loss 0.03888145834207535\n",
      "[Epoch 17] Training Batch [339/391]: Loss 0.16893266141414642\n",
      "[Epoch 17] Training Batch [340/391]: Loss 0.127034530043602\n",
      "[Epoch 17] Training Batch [341/391]: Loss 0.05128360539674759\n",
      "[Epoch 17] Training Batch [342/391]: Loss 0.1744786649942398\n",
      "[Epoch 17] Training Batch [343/391]: Loss 0.04045731574296951\n",
      "[Epoch 17] Training Batch [344/391]: Loss 0.10555166006088257\n",
      "[Epoch 17] Training Batch [345/391]: Loss 0.031903743743896484\n",
      "[Epoch 17] Training Batch [346/391]: Loss 0.1582392305135727\n",
      "[Epoch 17] Training Batch [347/391]: Loss 0.0705011785030365\n",
      "[Epoch 17] Training Batch [348/391]: Loss 0.12573665380477905\n",
      "[Epoch 17] Training Batch [349/391]: Loss 0.10045041143894196\n",
      "[Epoch 17] Training Batch [350/391]: Loss 0.12317855656147003\n",
      "[Epoch 17] Training Batch [351/391]: Loss 0.08331020176410675\n",
      "[Epoch 17] Training Batch [352/391]: Loss 0.12203645706176758\n",
      "[Epoch 17] Training Batch [353/391]: Loss 0.10882434993982315\n",
      "[Epoch 17] Training Batch [354/391]: Loss 0.05531127750873566\n",
      "[Epoch 17] Training Batch [355/391]: Loss 0.05157496780157089\n",
      "[Epoch 17] Training Batch [356/391]: Loss 0.08107957988977432\n",
      "[Epoch 17] Training Batch [357/391]: Loss 0.13003885746002197\n",
      "[Epoch 17] Training Batch [358/391]: Loss 0.09078055620193481\n",
      "[Epoch 17] Training Batch [359/391]: Loss 0.06800480931997299\n",
      "[Epoch 17] Training Batch [360/391]: Loss 0.14048008620738983\n",
      "[Epoch 17] Training Batch [361/391]: Loss 0.14016351103782654\n",
      "[Epoch 17] Training Batch [362/391]: Loss 0.09359430521726608\n",
      "[Epoch 17] Training Batch [363/391]: Loss 0.036352239549160004\n",
      "[Epoch 17] Training Batch [364/391]: Loss 0.0836184173822403\n",
      "[Epoch 17] Training Batch [365/391]: Loss 0.04592728242278099\n",
      "[Epoch 17] Training Batch [366/391]: Loss 0.07551764696836472\n",
      "[Epoch 17] Training Batch [367/391]: Loss 0.09611411392688751\n",
      "[Epoch 17] Training Batch [368/391]: Loss 0.04747694358229637\n",
      "[Epoch 17] Training Batch [369/391]: Loss 0.052228592336177826\n",
      "[Epoch 17] Training Batch [370/391]: Loss 0.08272179216146469\n",
      "[Epoch 17] Training Batch [371/391]: Loss 0.14510740339756012\n",
      "[Epoch 17] Training Batch [372/391]: Loss 0.1279570311307907\n",
      "[Epoch 17] Training Batch [373/391]: Loss 0.11828994005918503\n",
      "[Epoch 17] Training Batch [374/391]: Loss 0.0332285575568676\n",
      "[Epoch 17] Training Batch [375/391]: Loss 0.06644265353679657\n",
      "[Epoch 17] Training Batch [376/391]: Loss 0.16881398856639862\n",
      "[Epoch 17] Training Batch [377/391]: Loss 0.06288734823465347\n",
      "[Epoch 17] Training Batch [378/391]: Loss 0.07845243811607361\n",
      "[Epoch 17] Training Batch [379/391]: Loss 0.084673672914505\n",
      "[Epoch 17] Training Batch [380/391]: Loss 0.10988208651542664\n",
      "[Epoch 17] Training Batch [381/391]: Loss 0.05336970463395119\n",
      "[Epoch 17] Training Batch [382/391]: Loss 0.07811863720417023\n",
      "[Epoch 17] Training Batch [383/391]: Loss 0.17025108635425568\n",
      "[Epoch 17] Training Batch [384/391]: Loss 0.0786900445818901\n",
      "[Epoch 17] Training Batch [385/391]: Loss 0.1544027030467987\n",
      "[Epoch 17] Training Batch [386/391]: Loss 0.04507165402173996\n",
      "[Epoch 17] Training Batch [387/391]: Loss 0.04730451852083206\n",
      "[Epoch 17] Training Batch [388/391]: Loss 0.053511667996644974\n",
      "[Epoch 17] Training Batch [389/391]: Loss 0.06518284976482391\n",
      "[Epoch 17] Training Batch [390/391]: Loss 0.09373399615287781\n",
      "[Epoch 17] Training Batch [391/391]: Loss 0.067346952855587\n",
      "Epoch 17 - Train Loss: 0.0771\n",
      "*********  Epoch 18/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Training Batch [1/391]: Loss 0.0753987580537796\n",
      "[Epoch 18] Training Batch [2/391]: Loss 0.06262655556201935\n",
      "[Epoch 18] Training Batch [3/391]: Loss 0.08046569675207138\n",
      "[Epoch 18] Training Batch [4/391]: Loss 0.04117380455136299\n",
      "[Epoch 18] Training Batch [5/391]: Loss 0.06655854731798172\n",
      "[Epoch 18] Training Batch [6/391]: Loss 0.10602268576622009\n",
      "[Epoch 18] Training Batch [7/391]: Loss 0.12579119205474854\n",
      "[Epoch 18] Training Batch [8/391]: Loss 0.09246480464935303\n",
      "[Epoch 18] Training Batch [9/391]: Loss 0.0534328930079937\n",
      "[Epoch 18] Training Batch [10/391]: Loss 0.12067053467035294\n",
      "[Epoch 18] Training Batch [11/391]: Loss 0.05964356288313866\n",
      "[Epoch 18] Training Batch [12/391]: Loss 0.024073151871562004\n",
      "[Epoch 18] Training Batch [13/391]: Loss 0.05317484959959984\n",
      "[Epoch 18] Training Batch [14/391]: Loss 0.03882042318582535\n",
      "[Epoch 18] Training Batch [15/391]: Loss 0.08861877769231796\n",
      "[Epoch 18] Training Batch [16/391]: Loss 0.04169779643416405\n",
      "[Epoch 18] Training Batch [17/391]: Loss 0.10068248957395554\n",
      "[Epoch 18] Training Batch [18/391]: Loss 0.038866668939590454\n",
      "[Epoch 18] Training Batch [19/391]: Loss 0.027141684666275978\n",
      "[Epoch 18] Training Batch [20/391]: Loss 0.031922709196805954\n",
      "[Epoch 18] Training Batch [21/391]: Loss 0.12697909772396088\n",
      "[Epoch 18] Training Batch [22/391]: Loss 0.08284447342157364\n",
      "[Epoch 18] Training Batch [23/391]: Loss 0.03787948563694954\n",
      "[Epoch 18] Training Batch [24/391]: Loss 0.025018487125635147\n",
      "[Epoch 18] Training Batch [25/391]: Loss 0.055064857006073\n",
      "[Epoch 18] Training Batch [26/391]: Loss 0.03838234394788742\n",
      "[Epoch 18] Training Batch [27/391]: Loss 0.02229825034737587\n",
      "[Epoch 18] Training Batch [28/391]: Loss 0.04857752099633217\n",
      "[Epoch 18] Training Batch [29/391]: Loss 0.03358515352010727\n",
      "[Epoch 18] Training Batch [30/391]: Loss 0.06839074194431305\n",
      "[Epoch 18] Training Batch [31/391]: Loss 0.043406903743743896\n",
      "[Epoch 18] Training Batch [32/391]: Loss 0.028587251901626587\n",
      "[Epoch 18] Training Batch [33/391]: Loss 0.07507066428661346\n",
      "[Epoch 18] Training Batch [34/391]: Loss 0.03515636548399925\n",
      "[Epoch 18] Training Batch [35/391]: Loss 0.08639616519212723\n",
      "[Epoch 18] Training Batch [36/391]: Loss 0.03015843592584133\n",
      "[Epoch 18] Training Batch [37/391]: Loss 0.0168338343501091\n",
      "[Epoch 18] Training Batch [38/391]: Loss 0.014821750111877918\n",
      "[Epoch 18] Training Batch [39/391]: Loss 0.02817610651254654\n",
      "[Epoch 18] Training Batch [40/391]: Loss 0.07240848988294601\n",
      "[Epoch 18] Training Batch [41/391]: Loss 0.058673642575740814\n",
      "[Epoch 18] Training Batch [42/391]: Loss 0.05513311177492142\n",
      "[Epoch 18] Training Batch [43/391]: Loss 0.051865674555301666\n",
      "[Epoch 18] Training Batch [44/391]: Loss 0.03631603717803955\n",
      "[Epoch 18] Training Batch [45/391]: Loss 0.0512697771191597\n",
      "[Epoch 18] Training Batch [46/391]: Loss 0.05110899731516838\n",
      "[Epoch 18] Training Batch [47/391]: Loss 0.022524846717715263\n",
      "[Epoch 18] Training Batch [48/391]: Loss 0.05534284934401512\n",
      "[Epoch 18] Training Batch [49/391]: Loss 0.01713486574590206\n",
      "[Epoch 18] Training Batch [50/391]: Loss 0.028217488899827003\n",
      "[Epoch 18] Training Batch [51/391]: Loss 0.01020369865000248\n",
      "[Epoch 18] Training Batch [52/391]: Loss 0.053275786340236664\n",
      "[Epoch 18] Training Batch [53/391]: Loss 0.017602762207388878\n",
      "[Epoch 18] Training Batch [54/391]: Loss 0.05521590635180473\n",
      "[Epoch 18] Training Batch [55/391]: Loss 0.026953062042593956\n",
      "[Epoch 18] Training Batch [56/391]: Loss 0.0397692546248436\n",
      "[Epoch 18] Training Batch [57/391]: Loss 0.05698187276721001\n",
      "[Epoch 18] Training Batch [58/391]: Loss 0.041458796709775925\n",
      "[Epoch 18] Training Batch [59/391]: Loss 0.07687892764806747\n",
      "[Epoch 18] Training Batch [60/391]: Loss 0.08511670678853989\n",
      "[Epoch 18] Training Batch [61/391]: Loss 0.052191928029060364\n",
      "[Epoch 18] Training Batch [62/391]: Loss 0.05096184462308884\n",
      "[Epoch 18] Training Batch [63/391]: Loss 0.027295684441924095\n",
      "[Epoch 18] Training Batch [64/391]: Loss 0.01974824257194996\n",
      "[Epoch 18] Training Batch [65/391]: Loss 0.054733145982027054\n",
      "[Epoch 18] Training Batch [66/391]: Loss 0.028500590473413467\n",
      "[Epoch 18] Training Batch [67/391]: Loss 0.018983760848641396\n",
      "[Epoch 18] Training Batch [68/391]: Loss 0.024115080013871193\n",
      "[Epoch 18] Training Batch [69/391]: Loss 0.034242913126945496\n",
      "[Epoch 18] Training Batch [70/391]: Loss 0.052683815360069275\n",
      "[Epoch 18] Training Batch [71/391]: Loss 0.05840873345732689\n",
      "[Epoch 18] Training Batch [72/391]: Loss 0.07453735172748566\n",
      "[Epoch 18] Training Batch [73/391]: Loss 0.04677926376461983\n",
      "[Epoch 18] Training Batch [74/391]: Loss 0.03361409530043602\n",
      "[Epoch 18] Training Batch [75/391]: Loss 0.03068569302558899\n",
      "[Epoch 18] Training Batch [76/391]: Loss 0.06002649664878845\n",
      "[Epoch 18] Training Batch [77/391]: Loss 0.02332082949578762\n",
      "[Epoch 18] Training Batch [78/391]: Loss 0.06944235414266586\n",
      "[Epoch 18] Training Batch [79/391]: Loss 0.06989580392837524\n",
      "[Epoch 18] Training Batch [80/391]: Loss 0.11078131943941116\n",
      "[Epoch 18] Training Batch [81/391]: Loss 0.01178554818034172\n",
      "[Epoch 18] Training Batch [82/391]: Loss 0.012984367087483406\n",
      "[Epoch 18] Training Batch [83/391]: Loss 0.013186613097786903\n",
      "[Epoch 18] Training Batch [84/391]: Loss 0.016436245292425156\n",
      "[Epoch 18] Training Batch [85/391]: Loss 0.016980377957224846\n",
      "[Epoch 18] Training Batch [86/391]: Loss 0.02839401550590992\n",
      "[Epoch 18] Training Batch [87/391]: Loss 0.028998447582125664\n",
      "[Epoch 18] Training Batch [88/391]: Loss 0.02465113066136837\n",
      "[Epoch 18] Training Batch [89/391]: Loss 0.05257650092244148\n",
      "[Epoch 18] Training Batch [90/391]: Loss 0.022539285942912102\n",
      "[Epoch 18] Training Batch [91/391]: Loss 0.03911898657679558\n",
      "[Epoch 18] Training Batch [92/391]: Loss 0.02713584341108799\n",
      "[Epoch 18] Training Batch [93/391]: Loss 0.025741800665855408\n",
      "[Epoch 18] Training Batch [94/391]: Loss 0.055753886699676514\n",
      "[Epoch 18] Training Batch [95/391]: Loss 0.05950072780251503\n",
      "[Epoch 18] Training Batch [96/391]: Loss 0.030515847727656364\n",
      "[Epoch 18] Training Batch [97/391]: Loss 0.014941997826099396\n",
      "[Epoch 18] Training Batch [98/391]: Loss 0.06412209570407867\n",
      "[Epoch 18] Training Batch [99/391]: Loss 0.022913360968232155\n",
      "[Epoch 18] Training Batch [100/391]: Loss 0.04974464699625969\n",
      "[Epoch 18] Training Batch [101/391]: Loss 0.03508453443646431\n",
      "[Epoch 18] Training Batch [102/391]: Loss 0.03176064044237137\n",
      "[Epoch 18] Training Batch [103/391]: Loss 0.10259392112493515\n",
      "[Epoch 18] Training Batch [104/391]: Loss 0.017076721414923668\n",
      "[Epoch 18] Training Batch [105/391]: Loss 0.01607627049088478\n",
      "[Epoch 18] Training Batch [106/391]: Loss 0.028761539608240128\n",
      "[Epoch 18] Training Batch [107/391]: Loss 0.03283330798149109\n",
      "[Epoch 18] Training Batch [108/391]: Loss 0.04680313169956207\n",
      "[Epoch 18] Training Batch [109/391]: Loss 0.09369928389787674\n",
      "[Epoch 18] Training Batch [110/391]: Loss 0.03148004412651062\n",
      "[Epoch 18] Training Batch [111/391]: Loss 0.02015550062060356\n",
      "[Epoch 18] Training Batch [112/391]: Loss 0.04383205622434616\n",
      "[Epoch 18] Training Batch [113/391]: Loss 0.019706400111317635\n",
      "[Epoch 18] Training Batch [114/391]: Loss 0.01864684745669365\n",
      "[Epoch 18] Training Batch [115/391]: Loss 0.037455469369888306\n",
      "[Epoch 18] Training Batch [116/391]: Loss 0.07410668581724167\n",
      "[Epoch 18] Training Batch [117/391]: Loss 0.08535537868738174\n",
      "[Epoch 18] Training Batch [118/391]: Loss 0.1259520947933197\n",
      "[Epoch 18] Training Batch [119/391]: Loss 0.031084589660167694\n",
      "[Epoch 18] Training Batch [120/391]: Loss 0.01643972657620907\n",
      "[Epoch 18] Training Batch [121/391]: Loss 0.0390353798866272\n",
      "[Epoch 18] Training Batch [122/391]: Loss 0.03244031220674515\n",
      "[Epoch 18] Training Batch [123/391]: Loss 0.01693757250905037\n",
      "[Epoch 18] Training Batch [124/391]: Loss 0.02597488835453987\n",
      "[Epoch 18] Training Batch [125/391]: Loss 0.031706616282463074\n",
      "[Epoch 18] Training Batch [126/391]: Loss 0.04373595118522644\n",
      "[Epoch 18] Training Batch [127/391]: Loss 0.05308523029088974\n",
      "[Epoch 18] Training Batch [128/391]: Loss 0.06536717712879181\n",
      "[Epoch 18] Training Batch [129/391]: Loss 0.03555290400981903\n",
      "[Epoch 18] Training Batch [130/391]: Loss 0.07349161803722382\n",
      "[Epoch 18] Training Batch [131/391]: Loss 0.06003379076719284\n",
      "[Epoch 18] Training Batch [132/391]: Loss 0.06030408293008804\n",
      "[Epoch 18] Training Batch [133/391]: Loss 0.05465835705399513\n",
      "[Epoch 18] Training Batch [134/391]: Loss 0.026302395388484\n",
      "[Epoch 18] Training Batch [135/391]: Loss 0.010541748255491257\n",
      "[Epoch 18] Training Batch [136/391]: Loss 0.05989694595336914\n",
      "[Epoch 18] Training Batch [137/391]: Loss 0.03174087032675743\n",
      "[Epoch 18] Training Batch [138/391]: Loss 0.022722961381077766\n",
      "[Epoch 18] Training Batch [139/391]: Loss 0.08358684182167053\n",
      "[Epoch 18] Training Batch [140/391]: Loss 0.056198664009571075\n",
      "[Epoch 18] Training Batch [141/391]: Loss 0.03573986142873764\n",
      "[Epoch 18] Training Batch [142/391]: Loss 0.05015932023525238\n",
      "[Epoch 18] Training Batch [143/391]: Loss 0.08052932471036911\n",
      "[Epoch 18] Training Batch [144/391]: Loss 0.07119377702474594\n",
      "[Epoch 18] Training Batch [145/391]: Loss 0.03708633407950401\n",
      "[Epoch 18] Training Batch [146/391]: Loss 0.0333143025636673\n",
      "[Epoch 18] Training Batch [147/391]: Loss 0.02878878265619278\n",
      "[Epoch 18] Training Batch [148/391]: Loss 0.027101455256342888\n",
      "[Epoch 18] Training Batch [149/391]: Loss 0.026916345581412315\n",
      "[Epoch 18] Training Batch [150/391]: Loss 0.06666477024555206\n",
      "[Epoch 18] Training Batch [151/391]: Loss 0.01728370040655136\n",
      "[Epoch 18] Training Batch [152/391]: Loss 0.057808324694633484\n",
      "[Epoch 18] Training Batch [153/391]: Loss 0.06842498481273651\n",
      "[Epoch 18] Training Batch [154/391]: Loss 0.021261675283312798\n",
      "[Epoch 18] Training Batch [155/391]: Loss 0.052081335335969925\n",
      "[Epoch 18] Training Batch [156/391]: Loss 0.019986702129244804\n",
      "[Epoch 18] Training Batch [157/391]: Loss 0.02628922276198864\n",
      "[Epoch 18] Training Batch [158/391]: Loss 0.04695511609315872\n",
      "[Epoch 18] Training Batch [159/391]: Loss 0.05713599920272827\n",
      "[Epoch 18] Training Batch [160/391]: Loss 0.07419223338365555\n",
      "[Epoch 18] Training Batch [161/391]: Loss 0.037424154579639435\n",
      "[Epoch 18] Training Batch [162/391]: Loss 0.031059786677360535\n",
      "[Epoch 18] Training Batch [163/391]: Loss 0.009245328605175018\n",
      "[Epoch 18] Training Batch [164/391]: Loss 0.028823455795645714\n",
      "[Epoch 18] Training Batch [165/391]: Loss 0.05435052141547203\n",
      "[Epoch 18] Training Batch [166/391]: Loss 0.057397808879613876\n",
      "[Epoch 18] Training Batch [167/391]: Loss 0.04554397612810135\n",
      "[Epoch 18] Training Batch [168/391]: Loss 0.015573238022625446\n",
      "[Epoch 18] Training Batch [169/391]: Loss 0.04833724722266197\n",
      "[Epoch 18] Training Batch [170/391]: Loss 0.03138513118028641\n",
      "[Epoch 18] Training Batch [171/391]: Loss 0.055961232632398605\n",
      "[Epoch 18] Training Batch [172/391]: Loss 0.06185827776789665\n",
      "[Epoch 18] Training Batch [173/391]: Loss 0.03420709818601608\n",
      "[Epoch 18] Training Batch [174/391]: Loss 0.08161157369613647\n",
      "[Epoch 18] Training Batch [175/391]: Loss 0.04133870080113411\n",
      "[Epoch 18] Training Batch [176/391]: Loss 0.0393451489508152\n",
      "[Epoch 18] Training Batch [177/391]: Loss 0.04672277346253395\n",
      "[Epoch 18] Training Batch [178/391]: Loss 0.10279380530118942\n",
      "[Epoch 18] Training Batch [179/391]: Loss 0.035340145230293274\n",
      "[Epoch 18] Training Batch [180/391]: Loss 0.03122727945446968\n",
      "[Epoch 18] Training Batch [181/391]: Loss 0.05362752452492714\n",
      "[Epoch 18] Training Batch [182/391]: Loss 0.11412549018859863\n",
      "[Epoch 18] Training Batch [183/391]: Loss 0.011920682154595852\n",
      "[Epoch 18] Training Batch [184/391]: Loss 0.06956890225410461\n",
      "[Epoch 18] Training Batch [185/391]: Loss 0.12562961876392365\n",
      "[Epoch 18] Training Batch [186/391]: Loss 0.05079907923936844\n",
      "[Epoch 18] Training Batch [187/391]: Loss 0.028430704027414322\n",
      "[Epoch 18] Training Batch [188/391]: Loss 0.0437876395881176\n",
      "[Epoch 18] Training Batch [189/391]: Loss 0.03382759913802147\n",
      "[Epoch 18] Training Batch [190/391]: Loss 0.07382985204458237\n",
      "[Epoch 18] Training Batch [191/391]: Loss 0.032989487051963806\n",
      "[Epoch 18] Training Batch [192/391]: Loss 0.08054439723491669\n",
      "[Epoch 18] Training Batch [193/391]: Loss 0.04873070493340492\n",
      "[Epoch 18] Training Batch [194/391]: Loss 0.04746412858366966\n",
      "[Epoch 18] Training Batch [195/391]: Loss 0.03874250873923302\n",
      "[Epoch 18] Training Batch [196/391]: Loss 0.06540368497371674\n",
      "[Epoch 18] Training Batch [197/391]: Loss 0.07634422183036804\n",
      "[Epoch 18] Training Batch [198/391]: Loss 0.05267719179391861\n",
      "[Epoch 18] Training Batch [199/391]: Loss 0.03204312175512314\n",
      "[Epoch 18] Training Batch [200/391]: Loss 0.07199574261903763\n",
      "[Epoch 18] Training Batch [201/391]: Loss 0.060142237693071365\n",
      "[Epoch 18] Training Batch [202/391]: Loss 0.05607488006353378\n",
      "[Epoch 18] Training Batch [203/391]: Loss 0.06578719615936279\n",
      "[Epoch 18] Training Batch [204/391]: Loss 0.052748993039131165\n",
      "[Epoch 18] Training Batch [205/391]: Loss 0.03452352434396744\n",
      "[Epoch 18] Training Batch [206/391]: Loss 0.05542173236608505\n",
      "[Epoch 18] Training Batch [207/391]: Loss 0.05029067397117615\n",
      "[Epoch 18] Training Batch [208/391]: Loss 0.07130219787359238\n",
      "[Epoch 18] Training Batch [209/391]: Loss 0.030383357778191566\n",
      "[Epoch 18] Training Batch [210/391]: Loss 0.06592515110969543\n",
      "[Epoch 18] Training Batch [211/391]: Loss 0.07372230291366577\n",
      "[Epoch 18] Training Batch [212/391]: Loss 0.07857535779476166\n",
      "[Epoch 18] Training Batch [213/391]: Loss 0.09447360038757324\n",
      "[Epoch 18] Training Batch [214/391]: Loss 0.04403172805905342\n",
      "[Epoch 18] Training Batch [215/391]: Loss 0.061048198491334915\n",
      "[Epoch 18] Training Batch [216/391]: Loss 0.07648103684186935\n",
      "[Epoch 18] Training Batch [217/391]: Loss 0.06102283298969269\n",
      "[Epoch 18] Training Batch [218/391]: Loss 0.06693277508020401\n",
      "[Epoch 18] Training Batch [219/391]: Loss 0.02337842993438244\n",
      "[Epoch 18] Training Batch [220/391]: Loss 0.04677361249923706\n",
      "[Epoch 18] Training Batch [221/391]: Loss 0.0705135390162468\n",
      "[Epoch 18] Training Batch [222/391]: Loss 0.04862203821539879\n",
      "[Epoch 18] Training Batch [223/391]: Loss 0.05563631281256676\n",
      "[Epoch 18] Training Batch [224/391]: Loss 0.05209705978631973\n",
      "[Epoch 18] Training Batch [225/391]: Loss 0.10009713470935822\n",
      "[Epoch 18] Training Batch [226/391]: Loss 0.026022059842944145\n",
      "[Epoch 18] Training Batch [227/391]: Loss 0.03013676404953003\n",
      "[Epoch 18] Training Batch [228/391]: Loss 0.06624181568622589\n",
      "[Epoch 18] Training Batch [229/391]: Loss 0.03431852161884308\n",
      "[Epoch 18] Training Batch [230/391]: Loss 0.12526915967464447\n",
      "[Epoch 18] Training Batch [231/391]: Loss 0.08866079151630402\n",
      "[Epoch 18] Training Batch [232/391]: Loss 0.07709060609340668\n",
      "[Epoch 18] Training Batch [233/391]: Loss 0.03208710625767708\n",
      "[Epoch 18] Training Batch [234/391]: Loss 0.04657586291432381\n",
      "[Epoch 18] Training Batch [235/391]: Loss 0.042509883642196655\n",
      "[Epoch 18] Training Batch [236/391]: Loss 0.080773264169693\n",
      "[Epoch 18] Training Batch [237/391]: Loss 0.09149745851755142\n",
      "[Epoch 18] Training Batch [238/391]: Loss 0.03254808485507965\n",
      "[Epoch 18] Training Batch [239/391]: Loss 0.03034519962966442\n",
      "[Epoch 18] Training Batch [240/391]: Loss 0.07790463417768478\n",
      "[Epoch 18] Training Batch [241/391]: Loss 0.08895447850227356\n",
      "[Epoch 18] Training Batch [242/391]: Loss 0.11102036386728287\n",
      "[Epoch 18] Training Batch [243/391]: Loss 0.06468003243207932\n",
      "[Epoch 18] Training Batch [244/391]: Loss 0.014411477372050285\n",
      "[Epoch 18] Training Batch [245/391]: Loss 0.05217587947845459\n",
      "[Epoch 18] Training Batch [246/391]: Loss 0.08258865773677826\n",
      "[Epoch 18] Training Batch [247/391]: Loss 0.12431136518716812\n",
      "[Epoch 18] Training Batch [248/391]: Loss 0.09274531155824661\n",
      "[Epoch 18] Training Batch [249/391]: Loss 0.11798641085624695\n",
      "[Epoch 18] Training Batch [250/391]: Loss 0.0849456787109375\n",
      "[Epoch 18] Training Batch [251/391]: Loss 0.04831092432141304\n",
      "[Epoch 18] Training Batch [252/391]: Loss 0.11392088979482651\n",
      "[Epoch 18] Training Batch [253/391]: Loss 0.16219298541545868\n",
      "[Epoch 18] Training Batch [254/391]: Loss 0.024203309789299965\n",
      "[Epoch 18] Training Batch [255/391]: Loss 0.10632579773664474\n",
      "[Epoch 18] Training Batch [256/391]: Loss 0.06566231697797775\n",
      "[Epoch 18] Training Batch [257/391]: Loss 0.05333251506090164\n",
      "[Epoch 18] Training Batch [258/391]: Loss 0.06885090470314026\n",
      "[Epoch 18] Training Batch [259/391]: Loss 0.06846756488084793\n",
      "[Epoch 18] Training Batch [260/391]: Loss 0.10791420936584473\n",
      "[Epoch 18] Training Batch [261/391]: Loss 0.0909111425280571\n",
      "[Epoch 18] Training Batch [262/391]: Loss 0.0667075589299202\n",
      "[Epoch 18] Training Batch [263/391]: Loss 0.10462944209575653\n",
      "[Epoch 18] Training Batch [264/391]: Loss 0.044847194105386734\n",
      "[Epoch 18] Training Batch [265/391]: Loss 0.0645872950553894\n",
      "[Epoch 18] Training Batch [266/391]: Loss 0.0491047129034996\n",
      "[Epoch 18] Training Batch [267/391]: Loss 0.05686444789171219\n",
      "[Epoch 18] Training Batch [268/391]: Loss 0.07583434879779816\n",
      "[Epoch 18] Training Batch [269/391]: Loss 0.018964311107993126\n",
      "[Epoch 18] Training Batch [270/391]: Loss 0.0478036031126976\n",
      "[Epoch 18] Training Batch [271/391]: Loss 0.07504375278949738\n",
      "[Epoch 18] Training Batch [272/391]: Loss 0.05999679118394852\n",
      "[Epoch 18] Training Batch [273/391]: Loss 0.14432856440544128\n",
      "[Epoch 18] Training Batch [274/391]: Loss 0.06411101669073105\n",
      "[Epoch 18] Training Batch [275/391]: Loss 0.07083992660045624\n",
      "[Epoch 18] Training Batch [276/391]: Loss 0.048939675092697144\n",
      "[Epoch 18] Training Batch [277/391]: Loss 0.057900190353393555\n",
      "[Epoch 18] Training Batch [278/391]: Loss 0.050681497901678085\n",
      "[Epoch 18] Training Batch [279/391]: Loss 0.10889418423175812\n",
      "[Epoch 18] Training Batch [280/391]: Loss 0.04856457933783531\n",
      "[Epoch 18] Training Batch [281/391]: Loss 0.06272776424884796\n",
      "[Epoch 18] Training Batch [282/391]: Loss 0.052077293395996094\n",
      "[Epoch 18] Training Batch [283/391]: Loss 0.07435081899166107\n",
      "[Epoch 18] Training Batch [284/391]: Loss 0.06562858074903488\n",
      "[Epoch 18] Training Batch [285/391]: Loss 0.06257672607898712\n",
      "[Epoch 18] Training Batch [286/391]: Loss 0.06467284262180328\n",
      "[Epoch 18] Training Batch [287/391]: Loss 0.11610756069421768\n",
      "[Epoch 18] Training Batch [288/391]: Loss 0.09782908111810684\n",
      "[Epoch 18] Training Batch [289/391]: Loss 0.09889166802167892\n",
      "[Epoch 18] Training Batch [290/391]: Loss 0.01629767008125782\n",
      "[Epoch 18] Training Batch [291/391]: Loss 0.06964834779500961\n",
      "[Epoch 18] Training Batch [292/391]: Loss 0.032405491918325424\n",
      "[Epoch 18] Training Batch [293/391]: Loss 0.04380881413817406\n",
      "[Epoch 18] Training Batch [294/391]: Loss 0.07841696590185165\n",
      "[Epoch 18] Training Batch [295/391]: Loss 0.04201023653149605\n",
      "[Epoch 18] Training Batch [296/391]: Loss 0.10580163449048996\n",
      "[Epoch 18] Training Batch [297/391]: Loss 0.07375738024711609\n",
      "[Epoch 18] Training Batch [298/391]: Loss 0.0396810844540596\n",
      "[Epoch 18] Training Batch [299/391]: Loss 0.0780077874660492\n",
      "[Epoch 18] Training Batch [300/391]: Loss 0.07893788814544678\n",
      "[Epoch 18] Training Batch [301/391]: Loss 0.06344220787286758\n",
      "[Epoch 18] Training Batch [302/391]: Loss 0.02986006624996662\n",
      "[Epoch 18] Training Batch [303/391]: Loss 0.16195787489414215\n",
      "[Epoch 18] Training Batch [304/391]: Loss 0.02862590178847313\n",
      "[Epoch 18] Training Batch [305/391]: Loss 0.07239988446235657\n",
      "[Epoch 18] Training Batch [306/391]: Loss 0.0542394183576107\n",
      "[Epoch 18] Training Batch [307/391]: Loss 0.08122207969427109\n",
      "[Epoch 18] Training Batch [308/391]: Loss 0.05109414830803871\n",
      "[Epoch 18] Training Batch [309/391]: Loss 0.04583915323019028\n",
      "[Epoch 18] Training Batch [310/391]: Loss 0.05630156770348549\n",
      "[Epoch 18] Training Batch [311/391]: Loss 0.04902416095137596\n",
      "[Epoch 18] Training Batch [312/391]: Loss 0.025509174913167953\n",
      "[Epoch 18] Training Batch [313/391]: Loss 0.032303955405950546\n",
      "[Epoch 18] Training Batch [314/391]: Loss 0.06110348179936409\n",
      "[Epoch 18] Training Batch [315/391]: Loss 0.03165155276656151\n",
      "[Epoch 18] Training Batch [316/391]: Loss 0.11591081321239471\n",
      "[Epoch 18] Training Batch [317/391]: Loss 0.03514530137181282\n",
      "[Epoch 18] Training Batch [318/391]: Loss 0.03259832784533501\n",
      "[Epoch 18] Training Batch [319/391]: Loss 0.07635671645402908\n",
      "[Epoch 18] Training Batch [320/391]: Loss 0.107216477394104\n",
      "[Epoch 18] Training Batch [321/391]: Loss 0.06407579779624939\n",
      "[Epoch 18] Training Batch [322/391]: Loss 0.1112666130065918\n",
      "[Epoch 18] Training Batch [323/391]: Loss 0.0362834632396698\n",
      "[Epoch 18] Training Batch [324/391]: Loss 0.06297076493501663\n",
      "[Epoch 18] Training Batch [325/391]: Loss 0.1365744173526764\n",
      "[Epoch 18] Training Batch [326/391]: Loss 0.04072651267051697\n",
      "[Epoch 18] Training Batch [327/391]: Loss 0.06958169490098953\n",
      "[Epoch 18] Training Batch [328/391]: Loss 0.062256090342998505\n",
      "[Epoch 18] Training Batch [329/391]: Loss 0.14216455817222595\n",
      "[Epoch 18] Training Batch [330/391]: Loss 0.041733723133802414\n",
      "[Epoch 18] Training Batch [331/391]: Loss 0.0728299617767334\n",
      "[Epoch 18] Training Batch [332/391]: Loss 0.10158203542232513\n",
      "[Epoch 18] Training Batch [333/391]: Loss 0.10184601694345474\n",
      "[Epoch 18] Training Batch [334/391]: Loss 0.03733622655272484\n",
      "[Epoch 18] Training Batch [335/391]: Loss 0.06108414754271507\n",
      "[Epoch 18] Training Batch [336/391]: Loss 0.07637494057416916\n",
      "[Epoch 18] Training Batch [337/391]: Loss 0.1027420312166214\n",
      "[Epoch 18] Training Batch [338/391]: Loss 0.06498603522777557\n",
      "[Epoch 18] Training Batch [339/391]: Loss 0.07023341208696365\n",
      "[Epoch 18] Training Batch [340/391]: Loss 0.13031339645385742\n",
      "[Epoch 18] Training Batch [341/391]: Loss 0.06597436219453812\n",
      "[Epoch 18] Training Batch [342/391]: Loss 0.09201504290103912\n",
      "[Epoch 18] Training Batch [343/391]: Loss 0.04530170559883118\n",
      "[Epoch 18] Training Batch [344/391]: Loss 0.15233463048934937\n",
      "[Epoch 18] Training Batch [345/391]: Loss 0.0598771795630455\n",
      "[Epoch 18] Training Batch [346/391]: Loss 0.10277721285820007\n",
      "[Epoch 18] Training Batch [347/391]: Loss 0.06163375824689865\n",
      "[Epoch 18] Training Batch [348/391]: Loss 0.05348720774054527\n",
      "[Epoch 18] Training Batch [349/391]: Loss 0.04421421140432358\n",
      "[Epoch 18] Training Batch [350/391]: Loss 0.12019119411706924\n",
      "[Epoch 18] Training Batch [351/391]: Loss 0.0771227553486824\n",
      "[Epoch 18] Training Batch [352/391]: Loss 0.06356478482484818\n",
      "[Epoch 18] Training Batch [353/391]: Loss 0.12419779598712921\n",
      "[Epoch 18] Training Batch [354/391]: Loss 0.0880906879901886\n",
      "[Epoch 18] Training Batch [355/391]: Loss 0.06874784827232361\n",
      "[Epoch 18] Training Batch [356/391]: Loss 0.04120917618274689\n",
      "[Epoch 18] Training Batch [357/391]: Loss 0.04120570421218872\n",
      "[Epoch 18] Training Batch [358/391]: Loss 0.0769704133272171\n",
      "[Epoch 18] Training Batch [359/391]: Loss 0.03622958064079285\n",
      "[Epoch 18] Training Batch [360/391]: Loss 0.058926571160554886\n",
      "[Epoch 18] Training Batch [361/391]: Loss 0.10816124826669693\n",
      "[Epoch 18] Training Batch [362/391]: Loss 0.037278756499290466\n",
      "[Epoch 18] Training Batch [363/391]: Loss 0.06628593802452087\n",
      "[Epoch 18] Training Batch [364/391]: Loss 0.14394621551036835\n",
      "[Epoch 18] Training Batch [365/391]: Loss 0.030922139063477516\n",
      "[Epoch 18] Training Batch [366/391]: Loss 0.035448599606752396\n",
      "[Epoch 18] Training Batch [367/391]: Loss 0.048754215240478516\n",
      "[Epoch 18] Training Batch [368/391]: Loss 0.04057798907160759\n",
      "[Epoch 18] Training Batch [369/391]: Loss 0.04798600822687149\n",
      "[Epoch 18] Training Batch [370/391]: Loss 0.04934213310480118\n",
      "[Epoch 18] Training Batch [371/391]: Loss 0.11929818242788315\n",
      "[Epoch 18] Training Batch [372/391]: Loss 0.0910884216427803\n",
      "[Epoch 18] Training Batch [373/391]: Loss 0.06515120714902878\n",
      "[Epoch 18] Training Batch [374/391]: Loss 0.06318730115890503\n",
      "[Epoch 18] Training Batch [375/391]: Loss 0.14617832005023956\n",
      "[Epoch 18] Training Batch [376/391]: Loss 0.04738922044634819\n",
      "[Epoch 18] Training Batch [377/391]: Loss 0.050297223031520844\n",
      "[Epoch 18] Training Batch [378/391]: Loss 0.10120309889316559\n",
      "[Epoch 18] Training Batch [379/391]: Loss 0.13981671631336212\n",
      "[Epoch 18] Training Batch [380/391]: Loss 0.0789991021156311\n",
      "[Epoch 18] Training Batch [381/391]: Loss 0.04762456566095352\n",
      "[Epoch 18] Training Batch [382/391]: Loss 0.04957398772239685\n",
      "[Epoch 18] Training Batch [383/391]: Loss 0.05732613801956177\n",
      "[Epoch 18] Training Batch [384/391]: Loss 0.04945483058691025\n",
      "[Epoch 18] Training Batch [385/391]: Loss 0.06175345182418823\n",
      "[Epoch 18] Training Batch [386/391]: Loss 0.0677938312292099\n",
      "[Epoch 18] Training Batch [387/391]: Loss 0.04114782437682152\n",
      "[Epoch 18] Training Batch [388/391]: Loss 0.09325910359621048\n",
      "[Epoch 18] Training Batch [389/391]: Loss 0.16911683976650238\n",
      "[Epoch 18] Training Batch [390/391]: Loss 0.06569186598062515\n",
      "[Epoch 18] Training Batch [391/391]: Loss 0.048590898513793945\n",
      "Epoch 18 - Train Loss: 0.0582\n",
      "*********  Epoch 19/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Training Batch [1/391]: Loss 0.012246513739228249\n",
      "[Epoch 19] Training Batch [2/391]: Loss 0.02566559426486492\n",
      "[Epoch 19] Training Batch [3/391]: Loss 0.06745947897434235\n",
      "[Epoch 19] Training Batch [4/391]: Loss 0.07634414732456207\n",
      "[Epoch 19] Training Batch [5/391]: Loss 0.0825151577591896\n",
      "[Epoch 19] Training Batch [6/391]: Loss 0.02697940729558468\n",
      "[Epoch 19] Training Batch [7/391]: Loss 0.024334611371159554\n",
      "[Epoch 19] Training Batch [8/391]: Loss 0.02217920497059822\n",
      "[Epoch 19] Training Batch [9/391]: Loss 0.02381805330514908\n",
      "[Epoch 19] Training Batch [10/391]: Loss 0.06471636146306992\n",
      "[Epoch 19] Training Batch [11/391]: Loss 0.05952691286802292\n",
      "[Epoch 19] Training Batch [12/391]: Loss 0.06808566302061081\n",
      "[Epoch 19] Training Batch [13/391]: Loss 0.016424821689724922\n",
      "[Epoch 19] Training Batch [14/391]: Loss 0.06376682221889496\n",
      "[Epoch 19] Training Batch [15/391]: Loss 0.1685643196105957\n",
      "[Epoch 19] Training Batch [16/391]: Loss 0.016079965978860855\n",
      "[Epoch 19] Training Batch [17/391]: Loss 0.022917434573173523\n",
      "[Epoch 19] Training Batch [18/391]: Loss 0.06293712556362152\n",
      "[Epoch 19] Training Batch [19/391]: Loss 0.04493222013115883\n",
      "[Epoch 19] Training Batch [20/391]: Loss 0.046195805072784424\n",
      "[Epoch 19] Training Batch [21/391]: Loss 0.030019016936421394\n",
      "[Epoch 19] Training Batch [22/391]: Loss 0.06025588884949684\n",
      "[Epoch 19] Training Batch [23/391]: Loss 0.02893889881670475\n",
      "[Epoch 19] Training Batch [24/391]: Loss 0.025721516460180283\n",
      "[Epoch 19] Training Batch [25/391]: Loss 0.05980060622096062\n",
      "[Epoch 19] Training Batch [26/391]: Loss 0.05896405503153801\n",
      "[Epoch 19] Training Batch [27/391]: Loss 0.0210304856300354\n",
      "[Epoch 19] Training Batch [28/391]: Loss 0.015728548169136047\n",
      "[Epoch 19] Training Batch [29/391]: Loss 0.03200726583600044\n",
      "[Epoch 19] Training Batch [30/391]: Loss 0.06520166248083115\n",
      "[Epoch 19] Training Batch [31/391]: Loss 0.03160592541098595\n",
      "[Epoch 19] Training Batch [32/391]: Loss 0.09771741181612015\n",
      "[Epoch 19] Training Batch [33/391]: Loss 0.06360401213169098\n",
      "[Epoch 19] Training Batch [34/391]: Loss 0.028226321563124657\n",
      "[Epoch 19] Training Batch [35/391]: Loss 0.0442022867500782\n",
      "[Epoch 19] Training Batch [36/391]: Loss 0.04036343842744827\n",
      "[Epoch 19] Training Batch [37/391]: Loss 0.033574096858501434\n",
      "[Epoch 19] Training Batch [38/391]: Loss 0.04134972393512726\n",
      "[Epoch 19] Training Batch [39/391]: Loss 0.04447991028428078\n",
      "[Epoch 19] Training Batch [40/391]: Loss 0.0871281623840332\n",
      "[Epoch 19] Training Batch [41/391]: Loss 0.030894191935658455\n",
      "[Epoch 19] Training Batch [42/391]: Loss 0.0473378449678421\n",
      "[Epoch 19] Training Batch [43/391]: Loss 0.0328059196472168\n",
      "[Epoch 19] Training Batch [44/391]: Loss 0.053076766431331635\n",
      "[Epoch 19] Training Batch [45/391]: Loss 0.01995030790567398\n",
      "[Epoch 19] Training Batch [46/391]: Loss 0.05320539325475693\n",
      "[Epoch 19] Training Batch [47/391]: Loss 0.020765408873558044\n",
      "[Epoch 19] Training Batch [48/391]: Loss 0.047664813697338104\n",
      "[Epoch 19] Training Batch [49/391]: Loss 0.025730425491929054\n",
      "[Epoch 19] Training Batch [50/391]: Loss 0.033404961228370667\n",
      "[Epoch 19] Training Batch [51/391]: Loss 0.04618311673402786\n",
      "[Epoch 19] Training Batch [52/391]: Loss 0.0797070637345314\n",
      "[Epoch 19] Training Batch [53/391]: Loss 0.03348182886838913\n",
      "[Epoch 19] Training Batch [54/391]: Loss 0.02114168554544449\n",
      "[Epoch 19] Training Batch [55/391]: Loss 0.04706979915499687\n",
      "[Epoch 19] Training Batch [56/391]: Loss 0.061605021357536316\n",
      "[Epoch 19] Training Batch [57/391]: Loss 0.012693729251623154\n",
      "[Epoch 19] Training Batch [58/391]: Loss 0.018582141026854515\n",
      "[Epoch 19] Training Batch [59/391]: Loss 0.10379235446453094\n",
      "[Epoch 19] Training Batch [60/391]: Loss 0.019424619153141975\n",
      "[Epoch 19] Training Batch [61/391]: Loss 0.030473832041025162\n",
      "[Epoch 19] Training Batch [62/391]: Loss 0.03492886945605278\n",
      "[Epoch 19] Training Batch [63/391]: Loss 0.06921117007732391\n",
      "[Epoch 19] Training Batch [64/391]: Loss 0.06509225070476532\n",
      "[Epoch 19] Training Batch [65/391]: Loss 0.02726215124130249\n",
      "[Epoch 19] Training Batch [66/391]: Loss 0.09863726049661636\n",
      "[Epoch 19] Training Batch [67/391]: Loss 0.03169192001223564\n",
      "[Epoch 19] Training Batch [68/391]: Loss 0.02579006366431713\n",
      "[Epoch 19] Training Batch [69/391]: Loss 0.06745334714651108\n",
      "[Epoch 19] Training Batch [70/391]: Loss 0.03152725100517273\n",
      "[Epoch 19] Training Batch [71/391]: Loss 0.050274692475795746\n",
      "[Epoch 19] Training Batch [72/391]: Loss 0.018264921382069588\n",
      "[Epoch 19] Training Batch [73/391]: Loss 0.011584063060581684\n",
      "[Epoch 19] Training Batch [74/391]: Loss 0.03557770326733589\n",
      "[Epoch 19] Training Batch [75/391]: Loss 0.04775003343820572\n",
      "[Epoch 19] Training Batch [76/391]: Loss 0.04888703674077988\n",
      "[Epoch 19] Training Batch [77/391]: Loss 0.061257798224687576\n",
      "[Epoch 19] Training Batch [78/391]: Loss 0.03001372143626213\n",
      "[Epoch 19] Training Batch [79/391]: Loss 0.04071415588259697\n",
      "[Epoch 19] Training Batch [80/391]: Loss 0.05254649370908737\n",
      "[Epoch 19] Training Batch [81/391]: Loss 0.03899916633963585\n",
      "[Epoch 19] Training Batch [82/391]: Loss 0.009432165883481503\n",
      "[Epoch 19] Training Batch [83/391]: Loss 0.04128555953502655\n",
      "[Epoch 19] Training Batch [84/391]: Loss 0.0286193136125803\n",
      "[Epoch 19] Training Batch [85/391]: Loss 0.04800792783498764\n",
      "[Epoch 19] Training Batch [86/391]: Loss 0.1387433111667633\n",
      "[Epoch 19] Training Batch [87/391]: Loss 0.043302394449710846\n",
      "[Epoch 19] Training Batch [88/391]: Loss 0.04091852530837059\n",
      "[Epoch 19] Training Batch [89/391]: Loss 0.04313071072101593\n",
      "[Epoch 19] Training Batch [90/391]: Loss 0.027047723531723022\n",
      "[Epoch 19] Training Batch [91/391]: Loss 0.03254038095474243\n",
      "[Epoch 19] Training Batch [92/391]: Loss 0.03508523479104042\n",
      "[Epoch 19] Training Batch [93/391]: Loss 0.049235232174396515\n",
      "[Epoch 19] Training Batch [94/391]: Loss 0.051244694739580154\n",
      "[Epoch 19] Training Batch [95/391]: Loss 0.06714183837175369\n",
      "[Epoch 19] Training Batch [96/391]: Loss 0.06729783117771149\n",
      "[Epoch 19] Training Batch [97/391]: Loss 0.030219193547964096\n",
      "[Epoch 19] Training Batch [98/391]: Loss 0.030085457488894463\n",
      "[Epoch 19] Training Batch [99/391]: Loss 0.04182109236717224\n",
      "[Epoch 19] Training Batch [100/391]: Loss 0.031022220849990845\n",
      "[Epoch 19] Training Batch [101/391]: Loss 0.06408655643463135\n",
      "[Epoch 19] Training Batch [102/391]: Loss 0.0314340703189373\n",
      "[Epoch 19] Training Batch [103/391]: Loss 0.061902888119220734\n",
      "[Epoch 19] Training Batch [104/391]: Loss 0.04113065078854561\n",
      "[Epoch 19] Training Batch [105/391]: Loss 0.04852840304374695\n",
      "[Epoch 19] Training Batch [106/391]: Loss 0.0285358689725399\n",
      "[Epoch 19] Training Batch [107/391]: Loss 0.05836137756705284\n",
      "[Epoch 19] Training Batch [108/391]: Loss 0.030129410326480865\n",
      "[Epoch 19] Training Batch [109/391]: Loss 0.028024334460496902\n",
      "[Epoch 19] Training Batch [110/391]: Loss 0.04360182583332062\n",
      "[Epoch 19] Training Batch [111/391]: Loss 0.04256301000714302\n",
      "[Epoch 19] Training Batch [112/391]: Loss 0.020023398101329803\n",
      "[Epoch 19] Training Batch [113/391]: Loss 0.01162165030837059\n",
      "[Epoch 19] Training Batch [114/391]: Loss 0.048226915299892426\n",
      "[Epoch 19] Training Batch [115/391]: Loss 0.059676676988601685\n",
      "[Epoch 19] Training Batch [116/391]: Loss 0.034143660217523575\n",
      "[Epoch 19] Training Batch [117/391]: Loss 0.09422707557678223\n",
      "[Epoch 19] Training Batch [118/391]: Loss 0.06377746909856796\n",
      "[Epoch 19] Training Batch [119/391]: Loss 0.08352082967758179\n",
      "[Epoch 19] Training Batch [120/391]: Loss 0.02523072063922882\n",
      "[Epoch 19] Training Batch [121/391]: Loss 0.05709167197346687\n",
      "[Epoch 19] Training Batch [122/391]: Loss 0.0617327019572258\n",
      "[Epoch 19] Training Batch [123/391]: Loss 0.03559140861034393\n",
      "[Epoch 19] Training Batch [124/391]: Loss 0.01668521575629711\n",
      "[Epoch 19] Training Batch [125/391]: Loss 0.06073061749339104\n",
      "[Epoch 19] Training Batch [126/391]: Loss 0.028751175850629807\n",
      "[Epoch 19] Training Batch [127/391]: Loss 0.07837479561567307\n",
      "[Epoch 19] Training Batch [128/391]: Loss 0.04958845674991608\n",
      "[Epoch 19] Training Batch [129/391]: Loss 0.07467490434646606\n",
      "[Epoch 19] Training Batch [130/391]: Loss 0.01811683364212513\n",
      "[Epoch 19] Training Batch [131/391]: Loss 0.0235897246748209\n",
      "[Epoch 19] Training Batch [132/391]: Loss 0.08926314115524292\n",
      "[Epoch 19] Training Batch [133/391]: Loss 0.05559634417295456\n",
      "[Epoch 19] Training Batch [134/391]: Loss 0.029635824263095856\n",
      "[Epoch 19] Training Batch [135/391]: Loss 0.04572861269116402\n",
      "[Epoch 19] Training Batch [136/391]: Loss 0.03122345358133316\n",
      "[Epoch 19] Training Batch [137/391]: Loss 0.05522792041301727\n",
      "[Epoch 19] Training Batch [138/391]: Loss 0.018436647951602936\n",
      "[Epoch 19] Training Batch [139/391]: Loss 0.09411949664354324\n",
      "[Epoch 19] Training Batch [140/391]: Loss 0.04077905789017677\n",
      "[Epoch 19] Training Batch [141/391]: Loss 0.03353765979409218\n",
      "[Epoch 19] Training Batch [142/391]: Loss 0.012715466320514679\n",
      "[Epoch 19] Training Batch [143/391]: Loss 0.05228295177221298\n",
      "[Epoch 19] Training Batch [144/391]: Loss 0.0373564139008522\n",
      "[Epoch 19] Training Batch [145/391]: Loss 0.029476268216967583\n",
      "[Epoch 19] Training Batch [146/391]: Loss 0.05486510321497917\n",
      "[Epoch 19] Training Batch [147/391]: Loss 0.03096351958811283\n",
      "[Epoch 19] Training Batch [148/391]: Loss 0.03739282861351967\n",
      "[Epoch 19] Training Batch [149/391]: Loss 0.02047380805015564\n",
      "[Epoch 19] Training Batch [150/391]: Loss 0.062250372022390366\n",
      "[Epoch 19] Training Batch [151/391]: Loss 0.031874969601631165\n",
      "[Epoch 19] Training Batch [152/391]: Loss 0.022697605192661285\n",
      "[Epoch 19] Training Batch [153/391]: Loss 0.05086880549788475\n",
      "[Epoch 19] Training Batch [154/391]: Loss 0.03409827873110771\n",
      "[Epoch 19] Training Batch [155/391]: Loss 0.0213346928358078\n",
      "[Epoch 19] Training Batch [156/391]: Loss 0.03932979702949524\n",
      "[Epoch 19] Training Batch [157/391]: Loss 0.04089971259236336\n",
      "[Epoch 19] Training Batch [158/391]: Loss 0.03507433831691742\n",
      "[Epoch 19] Training Batch [159/391]: Loss 0.011671656742691994\n",
      "[Epoch 19] Training Batch [160/391]: Loss 0.019690928980708122\n",
      "[Epoch 19] Training Batch [161/391]: Loss 0.057531584054231644\n",
      "[Epoch 19] Training Batch [162/391]: Loss 0.035070184618234634\n",
      "[Epoch 19] Training Batch [163/391]: Loss 0.02140994556248188\n",
      "[Epoch 19] Training Batch [164/391]: Loss 0.030155979096889496\n",
      "[Epoch 19] Training Batch [165/391]: Loss 0.0254701916128397\n",
      "[Epoch 19] Training Batch [166/391]: Loss 0.031753454357385635\n",
      "[Epoch 19] Training Batch [167/391]: Loss 0.07731489837169647\n",
      "[Epoch 19] Training Batch [168/391]: Loss 0.018651209771633148\n",
      "[Epoch 19] Training Batch [169/391]: Loss 0.009733892977237701\n",
      "[Epoch 19] Training Batch [170/391]: Loss 0.02399919368326664\n",
      "[Epoch 19] Training Batch [171/391]: Loss 0.02655804343521595\n",
      "[Epoch 19] Training Batch [172/391]: Loss 0.02087736874818802\n",
      "[Epoch 19] Training Batch [173/391]: Loss 0.04191223159432411\n",
      "[Epoch 19] Training Batch [174/391]: Loss 0.06317762285470963\n",
      "[Epoch 19] Training Batch [175/391]: Loss 0.07481077313423157\n",
      "[Epoch 19] Training Batch [176/391]: Loss 0.02810683473944664\n",
      "[Epoch 19] Training Batch [177/391]: Loss 0.024584494531154633\n",
      "[Epoch 19] Training Batch [178/391]: Loss 0.024443812668323517\n",
      "[Epoch 19] Training Batch [179/391]: Loss 0.05562931299209595\n",
      "[Epoch 19] Training Batch [180/391]: Loss 0.05098467692732811\n",
      "[Epoch 19] Training Batch [181/391]: Loss 0.033180881291627884\n",
      "[Epoch 19] Training Batch [182/391]: Loss 0.03302526846528053\n",
      "[Epoch 19] Training Batch [183/391]: Loss 0.029574938118457794\n",
      "[Epoch 19] Training Batch [184/391]: Loss 0.01569392718374729\n",
      "[Epoch 19] Training Batch [185/391]: Loss 0.07264307141304016\n",
      "[Epoch 19] Training Batch [186/391]: Loss 0.040076736360788345\n",
      "[Epoch 19] Training Batch [187/391]: Loss 0.021860959008336067\n",
      "[Epoch 19] Training Batch [188/391]: Loss 0.030688205733895302\n",
      "[Epoch 19] Training Batch [189/391]: Loss 0.012636609375476837\n",
      "[Epoch 19] Training Batch [190/391]: Loss 0.032282277941703796\n",
      "[Epoch 19] Training Batch [191/391]: Loss 0.07434652000665665\n",
      "[Epoch 19] Training Batch [192/391]: Loss 0.06763973087072372\n",
      "[Epoch 19] Training Batch [193/391]: Loss 0.05331632122397423\n",
      "[Epoch 19] Training Batch [194/391]: Loss 0.039755892008543015\n",
      "[Epoch 19] Training Batch [195/391]: Loss 0.048027656972408295\n",
      "[Epoch 19] Training Batch [196/391]: Loss 0.1334085762500763\n",
      "[Epoch 19] Training Batch [197/391]: Loss 0.07806932926177979\n",
      "[Epoch 19] Training Batch [198/391]: Loss 0.0500306598842144\n",
      "[Epoch 19] Training Batch [199/391]: Loss 0.02285599708557129\n",
      "[Epoch 19] Training Batch [200/391]: Loss 0.04219583421945572\n",
      "[Epoch 19] Training Batch [201/391]: Loss 0.05056008696556091\n",
      "[Epoch 19] Training Batch [202/391]: Loss 0.06022670492529869\n",
      "[Epoch 19] Training Batch [203/391]: Loss 0.03286274895071983\n",
      "[Epoch 19] Training Batch [204/391]: Loss 0.16721558570861816\n",
      "[Epoch 19] Training Batch [205/391]: Loss 0.06081760674715042\n",
      "[Epoch 19] Training Batch [206/391]: Loss 0.04272480309009552\n",
      "[Epoch 19] Training Batch [207/391]: Loss 0.03889033943414688\n",
      "[Epoch 19] Training Batch [208/391]: Loss 0.07810952514410019\n",
      "[Epoch 19] Training Batch [209/391]: Loss 0.0354960560798645\n",
      "[Epoch 19] Training Batch [210/391]: Loss 0.07795797288417816\n",
      "[Epoch 19] Training Batch [211/391]: Loss 0.02177211083471775\n",
      "[Epoch 19] Training Batch [212/391]: Loss 0.044830016791820526\n",
      "[Epoch 19] Training Batch [213/391]: Loss 0.08618438988924026\n",
      "[Epoch 19] Training Batch [214/391]: Loss 0.06201878935098648\n",
      "[Epoch 19] Training Batch [215/391]: Loss 0.014523954130709171\n",
      "[Epoch 19] Training Batch [216/391]: Loss 0.07120265811681747\n",
      "[Epoch 19] Training Batch [217/391]: Loss 0.0522761233150959\n",
      "[Epoch 19] Training Batch [218/391]: Loss 0.06540749967098236\n",
      "[Epoch 19] Training Batch [219/391]: Loss 0.04416307434439659\n",
      "[Epoch 19] Training Batch [220/391]: Loss 0.08365370333194733\n",
      "[Epoch 19] Training Batch [221/391]: Loss 0.030645282939076424\n",
      "[Epoch 19] Training Batch [222/391]: Loss 0.033306900411844254\n",
      "[Epoch 19] Training Batch [223/391]: Loss 0.05096852779388428\n",
      "[Epoch 19] Training Batch [224/391]: Loss 0.07691670954227448\n",
      "[Epoch 19] Training Batch [225/391]: Loss 0.07873599231243134\n",
      "[Epoch 19] Training Batch [226/391]: Loss 0.10600781440734863\n",
      "[Epoch 19] Training Batch [227/391]: Loss 0.05443960800766945\n",
      "[Epoch 19] Training Batch [228/391]: Loss 0.06424033641815186\n",
      "[Epoch 19] Training Batch [229/391]: Loss 0.033798787742853165\n",
      "[Epoch 19] Training Batch [230/391]: Loss 0.03770357742905617\n",
      "[Epoch 19] Training Batch [231/391]: Loss 0.04804619029164314\n",
      "[Epoch 19] Training Batch [232/391]: Loss 0.0493781715631485\n",
      "[Epoch 19] Training Batch [233/391]: Loss 0.034656066447496414\n",
      "[Epoch 19] Training Batch [234/391]: Loss 0.03183834254741669\n",
      "[Epoch 19] Training Batch [235/391]: Loss 0.04318453371524811\n",
      "[Epoch 19] Training Batch [236/391]: Loss 0.05171967297792435\n",
      "[Epoch 19] Training Batch [237/391]: Loss 0.05705894157290459\n",
      "[Epoch 19] Training Batch [238/391]: Loss 0.06705799698829651\n",
      "[Epoch 19] Training Batch [239/391]: Loss 0.025574343279004097\n",
      "[Epoch 19] Training Batch [240/391]: Loss 0.05016548931598663\n",
      "[Epoch 19] Training Batch [241/391]: Loss 0.01644880883395672\n",
      "[Epoch 19] Training Batch [242/391]: Loss 0.030280740931630135\n",
      "[Epoch 19] Training Batch [243/391]: Loss 0.042172834277153015\n",
      "[Epoch 19] Training Batch [244/391]: Loss 0.09713361412286758\n",
      "[Epoch 19] Training Batch [245/391]: Loss 0.07488951832056046\n",
      "[Epoch 19] Training Batch [246/391]: Loss 0.050233207643032074\n",
      "[Epoch 19] Training Batch [247/391]: Loss 0.04585108533501625\n",
      "[Epoch 19] Training Batch [248/391]: Loss 0.013708625920116901\n",
      "[Epoch 19] Training Batch [249/391]: Loss 0.07314233481884003\n",
      "[Epoch 19] Training Batch [250/391]: Loss 0.05202930048108101\n",
      "[Epoch 19] Training Batch [251/391]: Loss 0.05941886827349663\n",
      "[Epoch 19] Training Batch [252/391]: Loss 0.03843240439891815\n",
      "[Epoch 19] Training Batch [253/391]: Loss 0.042705558240413666\n",
      "[Epoch 19] Training Batch [254/391]: Loss 0.07360003143548965\n",
      "[Epoch 19] Training Batch [255/391]: Loss 0.013113431632518768\n",
      "[Epoch 19] Training Batch [256/391]: Loss 0.04574939236044884\n",
      "[Epoch 19] Training Batch [257/391]: Loss 0.1419178694486618\n",
      "[Epoch 19] Training Batch [258/391]: Loss 0.06209341436624527\n",
      "[Epoch 19] Training Batch [259/391]: Loss 0.026498237624764442\n",
      "[Epoch 19] Training Batch [260/391]: Loss 0.0630076453089714\n",
      "[Epoch 19] Training Batch [261/391]: Loss 0.07125695049762726\n",
      "[Epoch 19] Training Batch [262/391]: Loss 0.014831572771072388\n",
      "[Epoch 19] Training Batch [263/391]: Loss 0.06434106081724167\n",
      "[Epoch 19] Training Batch [264/391]: Loss 0.07303894311189651\n",
      "[Epoch 19] Training Batch [265/391]: Loss 0.039756037294864655\n",
      "[Epoch 19] Training Batch [266/391]: Loss 0.03719023987650871\n",
      "[Epoch 19] Training Batch [267/391]: Loss 0.04927990213036537\n",
      "[Epoch 19] Training Batch [268/391]: Loss 0.06550639122724533\n",
      "[Epoch 19] Training Batch [269/391]: Loss 0.051484834402799606\n",
      "[Epoch 19] Training Batch [270/391]: Loss 0.05683111399412155\n",
      "[Epoch 19] Training Batch [271/391]: Loss 0.029607653617858887\n",
      "[Epoch 19] Training Batch [272/391]: Loss 0.05714624002575874\n",
      "[Epoch 19] Training Batch [273/391]: Loss 0.03307768702507019\n",
      "[Epoch 19] Training Batch [274/391]: Loss 0.04247519373893738\n",
      "[Epoch 19] Training Batch [275/391]: Loss 0.014284542761743069\n",
      "[Epoch 19] Training Batch [276/391]: Loss 0.05103190243244171\n",
      "[Epoch 19] Training Batch [277/391]: Loss 0.106080561876297\n",
      "[Epoch 19] Training Batch [278/391]: Loss 0.047858431935310364\n",
      "[Epoch 19] Training Batch [279/391]: Loss 0.06653092801570892\n",
      "[Epoch 19] Training Batch [280/391]: Loss 0.019341515377163887\n",
      "[Epoch 19] Training Batch [281/391]: Loss 0.058547839522361755\n",
      "[Epoch 19] Training Batch [282/391]: Loss 0.024018317461013794\n",
      "[Epoch 19] Training Batch [283/391]: Loss 0.029678957536816597\n",
      "[Epoch 19] Training Batch [284/391]: Loss 0.04644746705889702\n",
      "[Epoch 19] Training Batch [285/391]: Loss 0.047905728220939636\n",
      "[Epoch 19] Training Batch [286/391]: Loss 0.056818023324012756\n",
      "[Epoch 19] Training Batch [287/391]: Loss 0.026841338723897934\n",
      "[Epoch 19] Training Batch [288/391]: Loss 0.05302830785512924\n",
      "[Epoch 19] Training Batch [289/391]: Loss 0.060550395399332047\n",
      "[Epoch 19] Training Batch [290/391]: Loss 0.06262228637933731\n",
      "[Epoch 19] Training Batch [291/391]: Loss 0.02407970279455185\n",
      "[Epoch 19] Training Batch [292/391]: Loss 0.020882748067378998\n",
      "[Epoch 19] Training Batch [293/391]: Loss 0.03195629268884659\n",
      "[Epoch 19] Training Batch [294/391]: Loss 0.08439599722623825\n",
      "[Epoch 19] Training Batch [295/391]: Loss 0.020599739626049995\n",
      "[Epoch 19] Training Batch [296/391]: Loss 0.039642173796892166\n",
      "[Epoch 19] Training Batch [297/391]: Loss 0.04848171025514603\n",
      "[Epoch 19] Training Batch [298/391]: Loss 0.038076262921094894\n",
      "[Epoch 19] Training Batch [299/391]: Loss 0.07368767261505127\n",
      "[Epoch 19] Training Batch [300/391]: Loss 0.08020307123661041\n",
      "[Epoch 19] Training Batch [301/391]: Loss 0.15627984702587128\n",
      "[Epoch 19] Training Batch [302/391]: Loss 0.07889903336763382\n",
      "[Epoch 19] Training Batch [303/391]: Loss 0.04033593088388443\n",
      "[Epoch 19] Training Batch [304/391]: Loss 0.03324004262685776\n",
      "[Epoch 19] Training Batch [305/391]: Loss 0.04398256540298462\n",
      "[Epoch 19] Training Batch [306/391]: Loss 0.014644896611571312\n",
      "[Epoch 19] Training Batch [307/391]: Loss 0.06936796009540558\n",
      "[Epoch 19] Training Batch [308/391]: Loss 0.0777209922671318\n",
      "[Epoch 19] Training Batch [309/391]: Loss 0.07458595931529999\n",
      "[Epoch 19] Training Batch [310/391]: Loss 0.04264937341213226\n",
      "[Epoch 19] Training Batch [311/391]: Loss 0.02809625118970871\n",
      "[Epoch 19] Training Batch [312/391]: Loss 0.09387386590242386\n",
      "[Epoch 19] Training Batch [313/391]: Loss 0.05225856974720955\n",
      "[Epoch 19] Training Batch [314/391]: Loss 0.060313038527965546\n",
      "[Epoch 19] Training Batch [315/391]: Loss 0.04160111024975777\n",
      "[Epoch 19] Training Batch [316/391]: Loss 0.030614115297794342\n",
      "[Epoch 19] Training Batch [317/391]: Loss 0.05359264463186264\n",
      "[Epoch 19] Training Batch [318/391]: Loss 0.03563099354505539\n",
      "[Epoch 19] Training Batch [319/391]: Loss 0.10030505061149597\n",
      "[Epoch 19] Training Batch [320/391]: Loss 0.08701878786087036\n",
      "[Epoch 19] Training Batch [321/391]: Loss 0.03338974714279175\n",
      "[Epoch 19] Training Batch [322/391]: Loss 0.05227871611714363\n",
      "[Epoch 19] Training Batch [323/391]: Loss 0.044026635587215424\n",
      "[Epoch 19] Training Batch [324/391]: Loss 0.037800341844558716\n",
      "[Epoch 19] Training Batch [325/391]: Loss 0.05526115745306015\n",
      "[Epoch 19] Training Batch [326/391]: Loss 0.05174516141414642\n",
      "[Epoch 19] Training Batch [327/391]: Loss 0.06743305176496506\n",
      "[Epoch 19] Training Batch [328/391]: Loss 0.026153825223445892\n",
      "[Epoch 19] Training Batch [329/391]: Loss 0.06227259337902069\n",
      "[Epoch 19] Training Batch [330/391]: Loss 0.03131914138793945\n",
      "[Epoch 19] Training Batch [331/391]: Loss 0.06810817867517471\n",
      "[Epoch 19] Training Batch [332/391]: Loss 0.08679287135601044\n",
      "[Epoch 19] Training Batch [333/391]: Loss 0.07274974882602692\n",
      "[Epoch 19] Training Batch [334/391]: Loss 0.039443302899599075\n",
      "[Epoch 19] Training Batch [335/391]: Loss 0.06897009164094925\n",
      "[Epoch 19] Training Batch [336/391]: Loss 0.07227098941802979\n",
      "[Epoch 19] Training Batch [337/391]: Loss 0.04578893631696701\n",
      "[Epoch 19] Training Batch [338/391]: Loss 0.04314831271767616\n",
      "[Epoch 19] Training Batch [339/391]: Loss 0.02129008248448372\n",
      "[Epoch 19] Training Batch [340/391]: Loss 0.026713555678725243\n",
      "[Epoch 19] Training Batch [341/391]: Loss 0.03244083747267723\n",
      "[Epoch 19] Training Batch [342/391]: Loss 0.04349934682250023\n",
      "[Epoch 19] Training Batch [343/391]: Loss 0.04870349541306496\n",
      "[Epoch 19] Training Batch [344/391]: Loss 0.032585687935352325\n",
      "[Epoch 19] Training Batch [345/391]: Loss 0.12178293615579605\n",
      "[Epoch 19] Training Batch [346/391]: Loss 0.030396874994039536\n",
      "[Epoch 19] Training Batch [347/391]: Loss 0.051097385585308075\n",
      "[Epoch 19] Training Batch [348/391]: Loss 0.03798937425017357\n",
      "[Epoch 19] Training Batch [349/391]: Loss 0.08540475368499756\n",
      "[Epoch 19] Training Batch [350/391]: Loss 0.0887986347079277\n",
      "[Epoch 19] Training Batch [351/391]: Loss 0.03356558457016945\n",
      "[Epoch 19] Training Batch [352/391]: Loss 0.02368216961622238\n",
      "[Epoch 19] Training Batch [353/391]: Loss 0.04250210151076317\n",
      "[Epoch 19] Training Batch [354/391]: Loss 0.05677197128534317\n",
      "[Epoch 19] Training Batch [355/391]: Loss 0.0848928689956665\n",
      "[Epoch 19] Training Batch [356/391]: Loss 0.054459765553474426\n",
      "[Epoch 19] Training Batch [357/391]: Loss 0.09009752422571182\n",
      "[Epoch 19] Training Batch [358/391]: Loss 0.032502640038728714\n",
      "[Epoch 19] Training Batch [359/391]: Loss 0.019730033352971077\n",
      "[Epoch 19] Training Batch [360/391]: Loss 0.021918706595897675\n",
      "[Epoch 19] Training Batch [361/391]: Loss 0.05976666882634163\n",
      "[Epoch 19] Training Batch [362/391]: Loss 0.05306236445903778\n",
      "[Epoch 19] Training Batch [363/391]: Loss 0.03810742869973183\n",
      "[Epoch 19] Training Batch [364/391]: Loss 0.08504274487495422\n",
      "[Epoch 19] Training Batch [365/391]: Loss 0.029234804213047028\n",
      "[Epoch 19] Training Batch [366/391]: Loss 0.057630062103271484\n",
      "[Epoch 19] Training Batch [367/391]: Loss 0.05455407500267029\n",
      "[Epoch 19] Training Batch [368/391]: Loss 0.03717135637998581\n",
      "[Epoch 19] Training Batch [369/391]: Loss 0.028754867613315582\n",
      "[Epoch 19] Training Batch [370/391]: Loss 0.039062269032001495\n",
      "[Epoch 19] Training Batch [371/391]: Loss 0.05327336862683296\n",
      "[Epoch 19] Training Batch [372/391]: Loss 0.0495259091258049\n",
      "[Epoch 19] Training Batch [373/391]: Loss 0.02713039517402649\n",
      "[Epoch 19] Training Batch [374/391]: Loss 0.06636062264442444\n",
      "[Epoch 19] Training Batch [375/391]: Loss 0.08250001817941666\n",
      "[Epoch 19] Training Batch [376/391]: Loss 0.038737524300813675\n",
      "[Epoch 19] Training Batch [377/391]: Loss 0.06584058701992035\n",
      "[Epoch 19] Training Batch [378/391]: Loss 0.1078304573893547\n",
      "[Epoch 19] Training Batch [379/391]: Loss 0.08364123851060867\n",
      "[Epoch 19] Training Batch [380/391]: Loss 0.07888191193342209\n",
      "[Epoch 19] Training Batch [381/391]: Loss 0.10248425602912903\n",
      "[Epoch 19] Training Batch [382/391]: Loss 0.047162242233753204\n",
      "[Epoch 19] Training Batch [383/391]: Loss 0.05452108010649681\n",
      "[Epoch 19] Training Batch [384/391]: Loss 0.13560403883457184\n",
      "[Epoch 19] Training Batch [385/391]: Loss 0.015262196771800518\n",
      "[Epoch 19] Training Batch [386/391]: Loss 0.06560126692056656\n",
      "[Epoch 19] Training Batch [387/391]: Loss 0.03918018564581871\n",
      "[Epoch 19] Training Batch [388/391]: Loss 0.04238703101873398\n",
      "[Epoch 19] Training Batch [389/391]: Loss 0.026755455881357193\n",
      "[Epoch 19] Training Batch [390/391]: Loss 0.10368804633617401\n",
      "[Epoch 19] Training Batch [391/391]: Loss 0.0259627066552639\n",
      "Epoch 19 - Train Loss: 0.0485\n",
      "*********  Epoch 20/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Training Batch [1/391]: Loss 0.04059644788503647\n",
      "[Epoch 20] Training Batch [2/391]: Loss 0.05476895347237587\n",
      "[Epoch 20] Training Batch [3/391]: Loss 0.057318270206451416\n",
      "[Epoch 20] Training Batch [4/391]: Loss 0.044653672724962234\n",
      "[Epoch 20] Training Batch [5/391]: Loss 0.053368862718343735\n",
      "[Epoch 20] Training Batch [6/391]: Loss 0.04361814633011818\n",
      "[Epoch 20] Training Batch [7/391]: Loss 0.014313331805169582\n",
      "[Epoch 20] Training Batch [8/391]: Loss 0.053871747106313705\n",
      "[Epoch 20] Training Batch [9/391]: Loss 0.06674593687057495\n",
      "[Epoch 20] Training Batch [10/391]: Loss 0.029755303636193275\n",
      "[Epoch 20] Training Batch [11/391]: Loss 0.03417900204658508\n",
      "[Epoch 20] Training Batch [12/391]: Loss 0.018899017944931984\n",
      "[Epoch 20] Training Batch [13/391]: Loss 0.1010754182934761\n",
      "[Epoch 20] Training Batch [14/391]: Loss 0.016501449048519135\n",
      "[Epoch 20] Training Batch [15/391]: Loss 0.04755678400397301\n",
      "[Epoch 20] Training Batch [16/391]: Loss 0.008112398907542229\n",
      "[Epoch 20] Training Batch [17/391]: Loss 0.023281143978238106\n",
      "[Epoch 20] Training Batch [18/391]: Loss 0.02860405296087265\n",
      "[Epoch 20] Training Batch [19/391]: Loss 0.014187496155500412\n",
      "[Epoch 20] Training Batch [20/391]: Loss 0.09572678059339523\n",
      "[Epoch 20] Training Batch [21/391]: Loss 0.04019523411989212\n",
      "[Epoch 20] Training Batch [22/391]: Loss 0.0662679672241211\n",
      "[Epoch 20] Training Batch [23/391]: Loss 0.023258816450834274\n",
      "[Epoch 20] Training Batch [24/391]: Loss 0.01354435458779335\n",
      "[Epoch 20] Training Batch [25/391]: Loss 0.045989781618118286\n",
      "[Epoch 20] Training Batch [26/391]: Loss 0.06567972898483276\n",
      "[Epoch 20] Training Batch [27/391]: Loss 0.03978947177529335\n",
      "[Epoch 20] Training Batch [28/391]: Loss 0.026169583201408386\n",
      "[Epoch 20] Training Batch [29/391]: Loss 0.021924344822764397\n",
      "[Epoch 20] Training Batch [30/391]: Loss 0.02715119905769825\n",
      "[Epoch 20] Training Batch [31/391]: Loss 0.02107282541692257\n",
      "[Epoch 20] Training Batch [32/391]: Loss 0.06955387443304062\n",
      "[Epoch 20] Training Batch [33/391]: Loss 0.02060786820948124\n",
      "[Epoch 20] Training Batch [34/391]: Loss 0.039713114500045776\n",
      "[Epoch 20] Training Batch [35/391]: Loss 0.024405594915151596\n",
      "[Epoch 20] Training Batch [36/391]: Loss 0.019661400467157364\n",
      "[Epoch 20] Training Batch [37/391]: Loss 0.01603071019053459\n",
      "[Epoch 20] Training Batch [38/391]: Loss 0.010413825511932373\n",
      "[Epoch 20] Training Batch [39/391]: Loss 0.014076449908316135\n",
      "[Epoch 20] Training Batch [40/391]: Loss 0.053246352821588516\n",
      "[Epoch 20] Training Batch [41/391]: Loss 0.04619935527443886\n",
      "[Epoch 20] Training Batch [42/391]: Loss 0.04591338708996773\n",
      "[Epoch 20] Training Batch [43/391]: Loss 0.04868919029831886\n",
      "[Epoch 20] Training Batch [44/391]: Loss 0.11825797706842422\n",
      "[Epoch 20] Training Batch [45/391]: Loss 0.06713004410266876\n",
      "[Epoch 20] Training Batch [46/391]: Loss 0.13799749314785004\n",
      "[Epoch 20] Training Batch [47/391]: Loss 0.0345158576965332\n",
      "[Epoch 20] Training Batch [48/391]: Loss 0.02576799876987934\n",
      "[Epoch 20] Training Batch [49/391]: Loss 0.064347043633461\n",
      "[Epoch 20] Training Batch [50/391]: Loss 0.06788624078035355\n",
      "[Epoch 20] Training Batch [51/391]: Loss 0.02206427976489067\n",
      "[Epoch 20] Training Batch [52/391]: Loss 0.24121958017349243\n",
      "[Epoch 20] Training Batch [53/391]: Loss 0.015226433984935284\n",
      "[Epoch 20] Training Batch [54/391]: Loss 0.03265571594238281\n",
      "[Epoch 20] Training Batch [55/391]: Loss 0.02100788801908493\n",
      "[Epoch 20] Training Batch [56/391]: Loss 0.06875451654195786\n",
      "[Epoch 20] Training Batch [57/391]: Loss 0.09304854273796082\n",
      "[Epoch 20] Training Batch [58/391]: Loss 0.05669616162776947\n",
      "[Epoch 20] Training Batch [59/391]: Loss 0.03505278378725052\n",
      "[Epoch 20] Training Batch [60/391]: Loss 0.0684066116809845\n",
      "[Epoch 20] Training Batch [61/391]: Loss 0.021170837804675102\n",
      "[Epoch 20] Training Batch [62/391]: Loss 0.008382903411984444\n",
      "[Epoch 20] Training Batch [63/391]: Loss 0.10336341708898544\n",
      "[Epoch 20] Training Batch [64/391]: Loss 0.03196997568011284\n",
      "[Epoch 20] Training Batch [65/391]: Loss 0.0565352700650692\n",
      "[Epoch 20] Training Batch [66/391]: Loss 0.04803682118654251\n",
      "[Epoch 20] Training Batch [67/391]: Loss 0.03271343559026718\n",
      "[Epoch 20] Training Batch [68/391]: Loss 0.0593951977789402\n",
      "[Epoch 20] Training Batch [69/391]: Loss 0.05267273262143135\n",
      "[Epoch 20] Training Batch [70/391]: Loss 0.014066326431930065\n",
      "[Epoch 20] Training Batch [71/391]: Loss 0.049483321607112885\n",
      "[Epoch 20] Training Batch [72/391]: Loss 0.05983599275350571\n",
      "[Epoch 20] Training Batch [73/391]: Loss 0.03969700634479523\n",
      "[Epoch 20] Training Batch [74/391]: Loss 0.03450894355773926\n",
      "[Epoch 20] Training Batch [75/391]: Loss 0.0062736705876886845\n",
      "[Epoch 20] Training Batch [76/391]: Loss 0.02167350798845291\n",
      "[Epoch 20] Training Batch [77/391]: Loss 0.037678975611925125\n",
      "[Epoch 20] Training Batch [78/391]: Loss 0.030051423236727715\n",
      "[Epoch 20] Training Batch [79/391]: Loss 0.03462359681725502\n",
      "[Epoch 20] Training Batch [80/391]: Loss 0.09541413187980652\n",
      "[Epoch 20] Training Batch [81/391]: Loss 0.03470626473426819\n",
      "[Epoch 20] Training Batch [82/391]: Loss 0.027879582718014717\n",
      "[Epoch 20] Training Batch [83/391]: Loss 0.038135137408971786\n",
      "[Epoch 20] Training Batch [84/391]: Loss 0.04537137597799301\n",
      "[Epoch 20] Training Batch [85/391]: Loss 0.01720939576625824\n",
      "[Epoch 20] Training Batch [86/391]: Loss 0.026003332808613777\n",
      "[Epoch 20] Training Batch [87/391]: Loss 0.026476919651031494\n",
      "[Epoch 20] Training Batch [88/391]: Loss 0.031578607857227325\n",
      "[Epoch 20] Training Batch [89/391]: Loss 0.0693235769867897\n",
      "[Epoch 20] Training Batch [90/391]: Loss 0.05156189948320389\n",
      "[Epoch 20] Training Batch [91/391]: Loss 0.10969351977109909\n",
      "[Epoch 20] Training Batch [92/391]: Loss 0.03983630612492561\n",
      "[Epoch 20] Training Batch [93/391]: Loss 0.09204774349927902\n",
      "[Epoch 20] Training Batch [94/391]: Loss 0.017472056671977043\n",
      "[Epoch 20] Training Batch [95/391]: Loss 0.05645987018942833\n",
      "[Epoch 20] Training Batch [96/391]: Loss 0.00996372103691101\n",
      "[Epoch 20] Training Batch [97/391]: Loss 0.02778065763413906\n",
      "[Epoch 20] Training Batch [98/391]: Loss 0.08611340075731277\n",
      "[Epoch 20] Training Batch [99/391]: Loss 0.08402170240879059\n",
      "[Epoch 20] Training Batch [100/391]: Loss 0.06493981927633286\n",
      "[Epoch 20] Training Batch [101/391]: Loss 0.03557923808693886\n",
      "[Epoch 20] Training Batch [102/391]: Loss 0.031999096274375916\n",
      "[Epoch 20] Training Batch [103/391]: Loss 0.043490879237651825\n",
      "[Epoch 20] Training Batch [104/391]: Loss 0.029506511986255646\n",
      "[Epoch 20] Training Batch [105/391]: Loss 0.06639585644006729\n",
      "[Epoch 20] Training Batch [106/391]: Loss 0.04236924648284912\n",
      "[Epoch 20] Training Batch [107/391]: Loss 0.07715615630149841\n",
      "[Epoch 20] Training Batch [108/391]: Loss 0.04445147141814232\n",
      "[Epoch 20] Training Batch [109/391]: Loss 0.03031650185585022\n",
      "[Epoch 20] Training Batch [110/391]: Loss 0.12805722653865814\n",
      "[Epoch 20] Training Batch [111/391]: Loss 0.07550964504480362\n",
      "[Epoch 20] Training Batch [112/391]: Loss 0.07906763255596161\n",
      "[Epoch 20] Training Batch [113/391]: Loss 0.0605647936463356\n",
      "[Epoch 20] Training Batch [114/391]: Loss 0.036910269409418106\n",
      "[Epoch 20] Training Batch [115/391]: Loss 0.07018400728702545\n",
      "[Epoch 20] Training Batch [116/391]: Loss 0.04020536690950394\n",
      "[Epoch 20] Training Batch [117/391]: Loss 0.0594400092959404\n",
      "[Epoch 20] Training Batch [118/391]: Loss 0.04314516857266426\n",
      "[Epoch 20] Training Batch [119/391]: Loss 0.0629412829875946\n",
      "[Epoch 20] Training Batch [120/391]: Loss 0.0224594846367836\n",
      "[Epoch 20] Training Batch [121/391]: Loss 0.04120468348264694\n",
      "[Epoch 20] Training Batch [122/391]: Loss 0.06403158605098724\n",
      "[Epoch 20] Training Batch [123/391]: Loss 0.08125153183937073\n",
      "[Epoch 20] Training Batch [124/391]: Loss 0.03251459077000618\n",
      "[Epoch 20] Training Batch [125/391]: Loss 0.02567274682223797\n",
      "[Epoch 20] Training Batch [126/391]: Loss 0.015835454687476158\n",
      "[Epoch 20] Training Batch [127/391]: Loss 0.044949911534786224\n",
      "[Epoch 20] Training Batch [128/391]: Loss 0.05003802478313446\n",
      "[Epoch 20] Training Batch [129/391]: Loss 0.02215544879436493\n",
      "[Epoch 20] Training Batch [130/391]: Loss 0.024699434638023376\n",
      "[Epoch 20] Training Batch [131/391]: Loss 0.03502589091658592\n",
      "[Epoch 20] Training Batch [132/391]: Loss 0.035621222108602524\n",
      "[Epoch 20] Training Batch [133/391]: Loss 0.05311482399702072\n",
      "[Epoch 20] Training Batch [134/391]: Loss 0.06035982817411423\n",
      "[Epoch 20] Training Batch [135/391]: Loss 0.03399054333567619\n",
      "[Epoch 20] Training Batch [136/391]: Loss 0.023337332531809807\n",
      "[Epoch 20] Training Batch [137/391]: Loss 0.06169251725077629\n",
      "[Epoch 20] Training Batch [138/391]: Loss 0.01727197878062725\n",
      "[Epoch 20] Training Batch [139/391]: Loss 0.03905468061566353\n",
      "[Epoch 20] Training Batch [140/391]: Loss 0.05144916847348213\n",
      "[Epoch 20] Training Batch [141/391]: Loss 0.05844839662313461\n",
      "[Epoch 20] Training Batch [142/391]: Loss 0.027763647958636284\n",
      "[Epoch 20] Training Batch [143/391]: Loss 0.05502163991332054\n",
      "[Epoch 20] Training Batch [144/391]: Loss 0.034527648240327835\n",
      "[Epoch 20] Training Batch [145/391]: Loss 0.06817787885665894\n",
      "[Epoch 20] Training Batch [146/391]: Loss 0.04955511540174484\n",
      "[Epoch 20] Training Batch [147/391]: Loss 0.05121198296546936\n",
      "[Epoch 20] Training Batch [148/391]: Loss 0.018869804218411446\n",
      "[Epoch 20] Training Batch [149/391]: Loss 0.02359495870769024\n",
      "[Epoch 20] Training Batch [150/391]: Loss 0.01491905003786087\n",
      "[Epoch 20] Training Batch [151/391]: Loss 0.02698352187871933\n",
      "[Epoch 20] Training Batch [152/391]: Loss 0.03446111083030701\n",
      "[Epoch 20] Training Batch [153/391]: Loss 0.02981642819941044\n",
      "[Epoch 20] Training Batch [154/391]: Loss 0.013899211771786213\n",
      "[Epoch 20] Training Batch [155/391]: Loss 0.015293887816369534\n",
      "[Epoch 20] Training Batch [156/391]: Loss 0.09098135679960251\n",
      "[Epoch 20] Training Batch [157/391]: Loss 0.054348863661289215\n",
      "[Epoch 20] Training Batch [158/391]: Loss 0.05270763859152794\n",
      "[Epoch 20] Training Batch [159/391]: Loss 0.021829891949892044\n",
      "[Epoch 20] Training Batch [160/391]: Loss 0.052553940564394\n",
      "[Epoch 20] Training Batch [161/391]: Loss 0.06775373965501785\n",
      "[Epoch 20] Training Batch [162/391]: Loss 0.03277865797281265\n",
      "[Epoch 20] Training Batch [163/391]: Loss 0.033433809876441956\n",
      "[Epoch 20] Training Batch [164/391]: Loss 0.0625564306974411\n",
      "[Epoch 20] Training Batch [165/391]: Loss 0.05552380532026291\n",
      "[Epoch 20] Training Batch [166/391]: Loss 0.017707010731101036\n",
      "[Epoch 20] Training Batch [167/391]: Loss 0.07418811321258545\n",
      "[Epoch 20] Training Batch [168/391]: Loss 0.024981359019875526\n",
      "[Epoch 20] Training Batch [169/391]: Loss 0.012795427814126015\n",
      "[Epoch 20] Training Batch [170/391]: Loss 0.05122990906238556\n",
      "[Epoch 20] Training Batch [171/391]: Loss 0.0651547908782959\n",
      "[Epoch 20] Training Batch [172/391]: Loss 0.012168885208666325\n",
      "[Epoch 20] Training Batch [173/391]: Loss 0.056152310222387314\n",
      "[Epoch 20] Training Batch [174/391]: Loss 0.027795013040304184\n",
      "[Epoch 20] Training Batch [175/391]: Loss 0.01556788757443428\n",
      "[Epoch 20] Training Batch [176/391]: Loss 0.010353618301451206\n",
      "[Epoch 20] Training Batch [177/391]: Loss 0.013234886340796947\n",
      "[Epoch 20] Training Batch [178/391]: Loss 0.046870261430740356\n",
      "[Epoch 20] Training Batch [179/391]: Loss 0.031498588621616364\n",
      "[Epoch 20] Training Batch [180/391]: Loss 0.04078170284628868\n",
      "[Epoch 20] Training Batch [181/391]: Loss 0.03793381527066231\n",
      "[Epoch 20] Training Batch [182/391]: Loss 0.047190822660923004\n",
      "[Epoch 20] Training Batch [183/391]: Loss 0.10920584201812744\n",
      "[Epoch 20] Training Batch [184/391]: Loss 0.012251531705260277\n",
      "[Epoch 20] Training Batch [185/391]: Loss 0.040677689015865326\n",
      "[Epoch 20] Training Batch [186/391]: Loss 0.03174130991101265\n",
      "[Epoch 20] Training Batch [187/391]: Loss 0.020130790770053864\n",
      "[Epoch 20] Training Batch [188/391]: Loss 0.02022429369390011\n",
      "[Epoch 20] Training Batch [189/391]: Loss 0.0655694305896759\n",
      "[Epoch 20] Training Batch [190/391]: Loss 0.08209196478128433\n",
      "[Epoch 20] Training Batch [191/391]: Loss 0.034499991685152054\n",
      "[Epoch 20] Training Batch [192/391]: Loss 0.07411430031061172\n",
      "[Epoch 20] Training Batch [193/391]: Loss 0.029426557943224907\n",
      "[Epoch 20] Training Batch [194/391]: Loss 0.0159282386302948\n",
      "[Epoch 20] Training Batch [195/391]: Loss 0.022453416138887405\n",
      "[Epoch 20] Training Batch [196/391]: Loss 0.03834933787584305\n",
      "[Epoch 20] Training Batch [197/391]: Loss 0.03397723287343979\n",
      "[Epoch 20] Training Batch [198/391]: Loss 0.013048630207777023\n",
      "[Epoch 20] Training Batch [199/391]: Loss 0.017927682027220726\n",
      "[Epoch 20] Training Batch [200/391]: Loss 0.023622021079063416\n",
      "[Epoch 20] Training Batch [201/391]: Loss 0.013519383035600185\n",
      "[Epoch 20] Training Batch [202/391]: Loss 0.013433819636702538\n",
      "[Epoch 20] Training Batch [203/391]: Loss 0.029459336772561073\n",
      "[Epoch 20] Training Batch [204/391]: Loss 0.06783120334148407\n",
      "[Epoch 20] Training Batch [205/391]: Loss 0.06689423322677612\n",
      "[Epoch 20] Training Batch [206/391]: Loss 0.022367404773831367\n",
      "[Epoch 20] Training Batch [207/391]: Loss 0.04376183822751045\n",
      "[Epoch 20] Training Batch [208/391]: Loss 0.016755156219005585\n",
      "[Epoch 20] Training Batch [209/391]: Loss 0.043684400618076324\n",
      "[Epoch 20] Training Batch [210/391]: Loss 0.010297615081071854\n",
      "[Epoch 20] Training Batch [211/391]: Loss 0.023840410634875298\n",
      "[Epoch 20] Training Batch [212/391]: Loss 0.018419526517391205\n",
      "[Epoch 20] Training Batch [213/391]: Loss 0.049220360815525055\n",
      "[Epoch 20] Training Batch [214/391]: Loss 0.06114708632230759\n",
      "[Epoch 20] Training Batch [215/391]: Loss 0.012319435365498066\n",
      "[Epoch 20] Training Batch [216/391]: Loss 0.035855166614055634\n",
      "[Epoch 20] Training Batch [217/391]: Loss 0.013504879549145699\n",
      "[Epoch 20] Training Batch [218/391]: Loss 0.02138746716082096\n",
      "[Epoch 20] Training Batch [219/391]: Loss 0.01346212811768055\n",
      "[Epoch 20] Training Batch [220/391]: Loss 0.06877484172582626\n",
      "[Epoch 20] Training Batch [221/391]: Loss 0.0552007257938385\n",
      "[Epoch 20] Training Batch [222/391]: Loss 0.04579286649823189\n",
      "[Epoch 20] Training Batch [223/391]: Loss 0.08623071759939194\n",
      "[Epoch 20] Training Batch [224/391]: Loss 0.05786042660474777\n",
      "[Epoch 20] Training Batch [225/391]: Loss 0.04070289060473442\n",
      "[Epoch 20] Training Batch [226/391]: Loss 0.04121895506978035\n",
      "[Epoch 20] Training Batch [227/391]: Loss 0.05767256021499634\n",
      "[Epoch 20] Training Batch [228/391]: Loss 0.029209952801465988\n",
      "[Epoch 20] Training Batch [229/391]: Loss 0.05028451606631279\n",
      "[Epoch 20] Training Batch [230/391]: Loss 0.05663909390568733\n",
      "[Epoch 20] Training Batch [231/391]: Loss 0.0346672385931015\n",
      "[Epoch 20] Training Batch [232/391]: Loss 0.12041013687849045\n",
      "[Epoch 20] Training Batch [233/391]: Loss 0.07095389068126678\n",
      "[Epoch 20] Training Batch [234/391]: Loss 0.03512093424797058\n",
      "[Epoch 20] Training Batch [235/391]: Loss 0.09627756476402283\n",
      "[Epoch 20] Training Batch [236/391]: Loss 0.013517487794160843\n",
      "[Epoch 20] Training Batch [237/391]: Loss 0.030850166454911232\n",
      "[Epoch 20] Training Batch [238/391]: Loss 0.018562983721494675\n",
      "[Epoch 20] Training Batch [239/391]: Loss 0.02472996711730957\n",
      "[Epoch 20] Training Batch [240/391]: Loss 0.07646243274211884\n",
      "[Epoch 20] Training Batch [241/391]: Loss 0.02049873024225235\n",
      "[Epoch 20] Training Batch [242/391]: Loss 0.027439920231699944\n",
      "[Epoch 20] Training Batch [243/391]: Loss 0.017462637275457382\n",
      "[Epoch 20] Training Batch [244/391]: Loss 0.055885691195726395\n",
      "[Epoch 20] Training Batch [245/391]: Loss 0.02543444186449051\n",
      "[Epoch 20] Training Batch [246/391]: Loss 0.009925962425768375\n",
      "[Epoch 20] Training Batch [247/391]: Loss 0.04658041149377823\n",
      "[Epoch 20] Training Batch [248/391]: Loss 0.039193663746118546\n",
      "[Epoch 20] Training Batch [249/391]: Loss 0.0535125732421875\n",
      "[Epoch 20] Training Batch [250/391]: Loss 0.0246098805218935\n",
      "[Epoch 20] Training Batch [251/391]: Loss 0.031022261828184128\n",
      "[Epoch 20] Training Batch [252/391]: Loss 0.01792626455426216\n",
      "[Epoch 20] Training Batch [253/391]: Loss 0.09793800115585327\n",
      "[Epoch 20] Training Batch [254/391]: Loss 0.009220854379236698\n",
      "[Epoch 20] Training Batch [255/391]: Loss 0.03396812826395035\n",
      "[Epoch 20] Training Batch [256/391]: Loss 0.022491486743092537\n",
      "[Epoch 20] Training Batch [257/391]: Loss 0.06961352378129959\n",
      "[Epoch 20] Training Batch [258/391]: Loss 0.051376499235630035\n",
      "[Epoch 20] Training Batch [259/391]: Loss 0.06949996203184128\n",
      "[Epoch 20] Training Batch [260/391]: Loss 0.07135512679815292\n",
      "[Epoch 20] Training Batch [261/391]: Loss 0.02003251016139984\n",
      "[Epoch 20] Training Batch [262/391]: Loss 0.049677688628435135\n",
      "[Epoch 20] Training Batch [263/391]: Loss 0.06472118943929672\n",
      "[Epoch 20] Training Batch [264/391]: Loss 0.0366477370262146\n",
      "[Epoch 20] Training Batch [265/391]: Loss 0.014901500195264816\n",
      "[Epoch 20] Training Batch [266/391]: Loss 0.016610197722911835\n",
      "[Epoch 20] Training Batch [267/391]: Loss 0.10320932418107986\n",
      "[Epoch 20] Training Batch [268/391]: Loss 0.026316244155168533\n",
      "[Epoch 20] Training Batch [269/391]: Loss 0.03500908240675926\n",
      "[Epoch 20] Training Batch [270/391]: Loss 0.03957776352763176\n",
      "[Epoch 20] Training Batch [271/391]: Loss 0.021821169182658195\n",
      "[Epoch 20] Training Batch [272/391]: Loss 0.08998952060937881\n",
      "[Epoch 20] Training Batch [273/391]: Loss 0.012976719997823238\n",
      "[Epoch 20] Training Batch [274/391]: Loss 0.02290022000670433\n",
      "[Epoch 20] Training Batch [275/391]: Loss 0.04056960344314575\n",
      "[Epoch 20] Training Batch [276/391]: Loss 0.03413385525345802\n",
      "[Epoch 20] Training Batch [277/391]: Loss 0.04465887323021889\n",
      "[Epoch 20] Training Batch [278/391]: Loss 0.014987542293965816\n",
      "[Epoch 20] Training Batch [279/391]: Loss 0.029629459604620934\n",
      "[Epoch 20] Training Batch [280/391]: Loss 0.05066248029470444\n",
      "[Epoch 20] Training Batch [281/391]: Loss 0.10793204605579376\n",
      "[Epoch 20] Training Batch [282/391]: Loss 0.049200404435396194\n",
      "[Epoch 20] Training Batch [283/391]: Loss 0.040150225162506104\n",
      "[Epoch 20] Training Batch [284/391]: Loss 0.03550044447183609\n",
      "[Epoch 20] Training Batch [285/391]: Loss 0.03849700838327408\n",
      "[Epoch 20] Training Batch [286/391]: Loss 0.02292865701019764\n",
      "[Epoch 20] Training Batch [287/391]: Loss 0.025611434131860733\n",
      "[Epoch 20] Training Batch [288/391]: Loss 0.04012516513466835\n",
      "[Epoch 20] Training Batch [289/391]: Loss 0.08343686908483505\n",
      "[Epoch 20] Training Batch [290/391]: Loss 0.09701225906610489\n",
      "[Epoch 20] Training Batch [291/391]: Loss 0.05778590217232704\n",
      "[Epoch 20] Training Batch [292/391]: Loss 0.06497352570295334\n",
      "[Epoch 20] Training Batch [293/391]: Loss 0.026061339303851128\n",
      "[Epoch 20] Training Batch [294/391]: Loss 0.08026419579982758\n",
      "[Epoch 20] Training Batch [295/391]: Loss 0.028996100649237633\n",
      "[Epoch 20] Training Batch [296/391]: Loss 0.030776623636484146\n",
      "[Epoch 20] Training Batch [297/391]: Loss 0.03761426731944084\n",
      "[Epoch 20] Training Batch [298/391]: Loss 0.018709007650613785\n",
      "[Epoch 20] Training Batch [299/391]: Loss 0.07014479488134384\n",
      "[Epoch 20] Training Batch [300/391]: Loss 0.04508509859442711\n",
      "[Epoch 20] Training Batch [301/391]: Loss 0.049949683248996735\n",
      "[Epoch 20] Training Batch [302/391]: Loss 0.04103177785873413\n",
      "[Epoch 20] Training Batch [303/391]: Loss 0.030519865453243256\n",
      "[Epoch 20] Training Batch [304/391]: Loss 0.045982349663972855\n",
      "[Epoch 20] Training Batch [305/391]: Loss 0.05140555277466774\n",
      "[Epoch 20] Training Batch [306/391]: Loss 0.07566703110933304\n",
      "[Epoch 20] Training Batch [307/391]: Loss 0.040675390511751175\n",
      "[Epoch 20] Training Batch [308/391]: Loss 0.019367124885320663\n",
      "[Epoch 20] Training Batch [309/391]: Loss 0.015875451266765594\n",
      "[Epoch 20] Training Batch [310/391]: Loss 0.08833246678113937\n",
      "[Epoch 20] Training Batch [311/391]: Loss 0.048921480774879456\n",
      "[Epoch 20] Training Batch [312/391]: Loss 0.02355583943426609\n",
      "[Epoch 20] Training Batch [313/391]: Loss 0.07833985984325409\n",
      "[Epoch 20] Training Batch [314/391]: Loss 0.029968788847327232\n",
      "[Epoch 20] Training Batch [315/391]: Loss 0.03178729489445686\n",
      "[Epoch 20] Training Batch [316/391]: Loss 0.022410107776522636\n",
      "[Epoch 20] Training Batch [317/391]: Loss 0.062382329255342484\n",
      "[Epoch 20] Training Batch [318/391]: Loss 0.014752540737390518\n",
      "[Epoch 20] Training Batch [319/391]: Loss 0.05233798176050186\n",
      "[Epoch 20] Training Batch [320/391]: Loss 0.06101159751415253\n",
      "[Epoch 20] Training Batch [321/391]: Loss 0.03100414387881756\n",
      "[Epoch 20] Training Batch [322/391]: Loss 0.0259256511926651\n",
      "[Epoch 20] Training Batch [323/391]: Loss 0.037544120103120804\n",
      "[Epoch 20] Training Batch [324/391]: Loss 0.035061050206422806\n",
      "[Epoch 20] Training Batch [325/391]: Loss 0.1224069818854332\n",
      "[Epoch 20] Training Batch [326/391]: Loss 0.06989309936761856\n",
      "[Epoch 20] Training Batch [327/391]: Loss 0.0522998571395874\n",
      "[Epoch 20] Training Batch [328/391]: Loss 0.025516049936413765\n",
      "[Epoch 20] Training Batch [329/391]: Loss 0.04673008620738983\n",
      "[Epoch 20] Training Batch [330/391]: Loss 0.028610961511731148\n",
      "[Epoch 20] Training Batch [331/391]: Loss 0.0649142637848854\n",
      "[Epoch 20] Training Batch [332/391]: Loss 0.06000855565071106\n",
      "[Epoch 20] Training Batch [333/391]: Loss 0.044353801757097244\n",
      "[Epoch 20] Training Batch [334/391]: Loss 0.08120887726545334\n",
      "[Epoch 20] Training Batch [335/391]: Loss 0.02837839350104332\n",
      "[Epoch 20] Training Batch [336/391]: Loss 0.04631315916776657\n",
      "[Epoch 20] Training Batch [337/391]: Loss 0.02861158363521099\n",
      "[Epoch 20] Training Batch [338/391]: Loss 0.05722780525684357\n",
      "[Epoch 20] Training Batch [339/391]: Loss 0.08746103197336197\n",
      "[Epoch 20] Training Batch [340/391]: Loss 0.10188385844230652\n",
      "[Epoch 20] Training Batch [341/391]: Loss 0.013521803542971611\n",
      "[Epoch 20] Training Batch [342/391]: Loss 0.06374834477901459\n",
      "[Epoch 20] Training Batch [343/391]: Loss 0.03741806745529175\n",
      "[Epoch 20] Training Batch [344/391]: Loss 0.04131224751472473\n",
      "[Epoch 20] Training Batch [345/391]: Loss 0.058917198330163956\n",
      "[Epoch 20] Training Batch [346/391]: Loss 0.05977166071534157\n",
      "[Epoch 20] Training Batch [347/391]: Loss 0.06865130364894867\n",
      "[Epoch 20] Training Batch [348/391]: Loss 0.034528493881225586\n",
      "[Epoch 20] Training Batch [349/391]: Loss 0.06687682121992111\n",
      "[Epoch 20] Training Batch [350/391]: Loss 0.027838753536343575\n",
      "[Epoch 20] Training Batch [351/391]: Loss 0.019565166905522346\n",
      "[Epoch 20] Training Batch [352/391]: Loss 0.03171418607234955\n",
      "[Epoch 20] Training Batch [353/391]: Loss 0.05044837296009064\n",
      "[Epoch 20] Training Batch [354/391]: Loss 0.038127925246953964\n",
      "[Epoch 20] Training Batch [355/391]: Loss 0.012202885001897812\n",
      "[Epoch 20] Training Batch [356/391]: Loss 0.027855029329657555\n",
      "[Epoch 20] Training Batch [357/391]: Loss 0.059850167483091354\n",
      "[Epoch 20] Training Batch [358/391]: Loss 0.05050942301750183\n",
      "[Epoch 20] Training Batch [359/391]: Loss 0.1203276664018631\n",
      "[Epoch 20] Training Batch [360/391]: Loss 0.03018892928957939\n",
      "[Epoch 20] Training Batch [361/391]: Loss 0.04437361657619476\n",
      "[Epoch 20] Training Batch [362/391]: Loss 0.01855587214231491\n",
      "[Epoch 20] Training Batch [363/391]: Loss 0.03237590566277504\n",
      "[Epoch 20] Training Batch [364/391]: Loss 0.045808132737874985\n",
      "[Epoch 20] Training Batch [365/391]: Loss 0.01982523687183857\n",
      "[Epoch 20] Training Batch [366/391]: Loss 0.08545089513063431\n",
      "[Epoch 20] Training Batch [367/391]: Loss 0.06229972466826439\n",
      "[Epoch 20] Training Batch [368/391]: Loss 0.06930220127105713\n",
      "[Epoch 20] Training Batch [369/391]: Loss 0.029772350564599037\n",
      "[Epoch 20] Training Batch [370/391]: Loss 0.05623976141214371\n",
      "[Epoch 20] Training Batch [371/391]: Loss 0.056324563920497894\n",
      "[Epoch 20] Training Batch [372/391]: Loss 0.031050454825162888\n",
      "[Epoch 20] Training Batch [373/391]: Loss 0.13824337720870972\n",
      "[Epoch 20] Training Batch [374/391]: Loss 0.06943951547145844\n",
      "[Epoch 20] Training Batch [375/391]: Loss 0.02160283923149109\n",
      "[Epoch 20] Training Batch [376/391]: Loss 0.01200165320187807\n",
      "[Epoch 20] Training Batch [377/391]: Loss 0.055306483060121536\n",
      "[Epoch 20] Training Batch [378/391]: Loss 0.029359810054302216\n",
      "[Epoch 20] Training Batch [379/391]: Loss 0.12156437337398529\n",
      "[Epoch 20] Training Batch [380/391]: Loss 0.05009008198976517\n",
      "[Epoch 20] Training Batch [381/391]: Loss 0.052312735468149185\n",
      "[Epoch 20] Training Batch [382/391]: Loss 0.051190007477998734\n",
      "[Epoch 20] Training Batch [383/391]: Loss 0.025661379098892212\n",
      "[Epoch 20] Training Batch [384/391]: Loss 0.03168891370296478\n",
      "[Epoch 20] Training Batch [385/391]: Loss 0.024593554437160492\n",
      "[Epoch 20] Training Batch [386/391]: Loss 0.04969149827957153\n",
      "[Epoch 20] Training Batch [387/391]: Loss 0.0885351300239563\n",
      "[Epoch 20] Training Batch [388/391]: Loss 0.05274850130081177\n",
      "[Epoch 20] Training Batch [389/391]: Loss 0.05401266738772392\n",
      "[Epoch 20] Training Batch [390/391]: Loss 0.060005370527505875\n",
      "[Epoch 20] Training Batch [391/391]: Loss 0.01671687327325344\n",
      "Epoch 20 - Train Loss: 0.0448\n",
      "*********  Epoch 21/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Training Batch [1/391]: Loss 0.054184023290872574\n",
      "[Epoch 21] Training Batch [2/391]: Loss 0.10907404869794846\n",
      "[Epoch 21] Training Batch [3/391]: Loss 0.03470655530691147\n",
      "[Epoch 21] Training Batch [4/391]: Loss 0.018230168148875237\n",
      "[Epoch 21] Training Batch [5/391]: Loss 0.09154335409402847\n",
      "[Epoch 21] Training Batch [6/391]: Loss 0.0375954769551754\n",
      "[Epoch 21] Training Batch [7/391]: Loss 0.012044049799442291\n",
      "[Epoch 21] Training Batch [8/391]: Loss 0.021473702043294907\n",
      "[Epoch 21] Training Batch [9/391]: Loss 0.027712445706129074\n",
      "[Epoch 21] Training Batch [10/391]: Loss 0.05815498158335686\n",
      "[Epoch 21] Training Batch [11/391]: Loss 0.10321967303752899\n",
      "[Epoch 21] Training Batch [12/391]: Loss 0.04052148386836052\n",
      "[Epoch 21] Training Batch [13/391]: Loss 0.08643976598978043\n",
      "[Epoch 21] Training Batch [14/391]: Loss 0.06299031525850296\n",
      "[Epoch 21] Training Batch [15/391]: Loss 0.04264148697257042\n",
      "[Epoch 21] Training Batch [16/391]: Loss 0.02061256766319275\n",
      "[Epoch 21] Training Batch [17/391]: Loss 0.08044774830341339\n",
      "[Epoch 21] Training Batch [18/391]: Loss 0.008740808814764023\n",
      "[Epoch 21] Training Batch [19/391]: Loss 0.029748547822237015\n",
      "[Epoch 21] Training Batch [20/391]: Loss 0.011022559367120266\n",
      "[Epoch 21] Training Batch [21/391]: Loss 0.04810405522584915\n",
      "[Epoch 21] Training Batch [22/391]: Loss 0.025264721363782883\n",
      "[Epoch 21] Training Batch [23/391]: Loss 0.03616444766521454\n",
      "[Epoch 21] Training Batch [24/391]: Loss 0.027343619614839554\n",
      "[Epoch 21] Training Batch [25/391]: Loss 0.01725885644555092\n",
      "[Epoch 21] Training Batch [26/391]: Loss 0.01935960352420807\n",
      "[Epoch 21] Training Batch [27/391]: Loss 0.04757458716630936\n",
      "[Epoch 21] Training Batch [28/391]: Loss 0.03604069724678993\n",
      "[Epoch 21] Training Batch [29/391]: Loss 0.03263547271490097\n",
      "[Epoch 21] Training Batch [30/391]: Loss 0.05059048533439636\n",
      "[Epoch 21] Training Batch [31/391]: Loss 0.029836157336831093\n",
      "[Epoch 21] Training Batch [32/391]: Loss 0.038638949394226074\n",
      "[Epoch 21] Training Batch [33/391]: Loss 0.04218460991978645\n",
      "[Epoch 21] Training Batch [34/391]: Loss 0.023776262998580933\n",
      "[Epoch 21] Training Batch [35/391]: Loss 0.030627429485321045\n",
      "[Epoch 21] Training Batch [36/391]: Loss 0.022928578779101372\n",
      "[Epoch 21] Training Batch [37/391]: Loss 0.02380012534558773\n",
      "[Epoch 21] Training Batch [38/391]: Loss 0.003027004888281226\n",
      "[Epoch 21] Training Batch [39/391]: Loss 0.0136574637144804\n",
      "[Epoch 21] Training Batch [40/391]: Loss 0.023035364225506783\n",
      "[Epoch 21] Training Batch [41/391]: Loss 0.030517498031258583\n",
      "[Epoch 21] Training Batch [42/391]: Loss 0.036046333611011505\n",
      "[Epoch 21] Training Batch [43/391]: Loss 0.020765960216522217\n",
      "[Epoch 21] Training Batch [44/391]: Loss 0.0108987120911479\n",
      "[Epoch 21] Training Batch [45/391]: Loss 0.019971083849668503\n",
      "[Epoch 21] Training Batch [46/391]: Loss 0.03192745894193649\n",
      "[Epoch 21] Training Batch [47/391]: Loss 0.020671436563134193\n",
      "[Epoch 21] Training Batch [48/391]: Loss 0.018196746706962585\n",
      "[Epoch 21] Training Batch [49/391]: Loss 0.04336266592144966\n",
      "[Epoch 21] Training Batch [50/391]: Loss 0.009597038850188255\n",
      "[Epoch 21] Training Batch [51/391]: Loss 0.02742166444659233\n",
      "[Epoch 21] Training Batch [52/391]: Loss 0.050415776669979095\n",
      "[Epoch 21] Training Batch [53/391]: Loss 0.012920118868350983\n",
      "[Epoch 21] Training Batch [54/391]: Loss 0.016154123470187187\n",
      "[Epoch 21] Training Batch [55/391]: Loss 0.014715122058987617\n",
      "[Epoch 21] Training Batch [56/391]: Loss 0.012482293881475925\n",
      "[Epoch 21] Training Batch [57/391]: Loss 0.029871953651309013\n",
      "[Epoch 21] Training Batch [58/391]: Loss 0.009338834322988987\n",
      "[Epoch 21] Training Batch [59/391]: Loss 0.03465961664915085\n",
      "[Epoch 21] Training Batch [60/391]: Loss 0.021967526525259018\n",
      "[Epoch 21] Training Batch [61/391]: Loss 0.031383074820041656\n",
      "[Epoch 21] Training Batch [62/391]: Loss 0.019779568538069725\n",
      "[Epoch 21] Training Batch [63/391]: Loss 0.001718087587505579\n",
      "[Epoch 21] Training Batch [64/391]: Loss 0.011093445122241974\n",
      "[Epoch 21] Training Batch [65/391]: Loss 0.028371354565024376\n",
      "[Epoch 21] Training Batch [66/391]: Loss 0.0070742042735219\n",
      "[Epoch 21] Training Batch [67/391]: Loss 0.038824863731861115\n",
      "[Epoch 21] Training Batch [68/391]: Loss 0.021833233535289764\n",
      "[Epoch 21] Training Batch [69/391]: Loss 0.015469225123524666\n",
      "[Epoch 21] Training Batch [70/391]: Loss 0.045556679368019104\n",
      "[Epoch 21] Training Batch [71/391]: Loss 0.01420091837644577\n",
      "[Epoch 21] Training Batch [72/391]: Loss 0.02032567746937275\n",
      "[Epoch 21] Training Batch [73/391]: Loss 0.02945786528289318\n",
      "[Epoch 21] Training Batch [74/391]: Loss 0.020407723262906075\n",
      "[Epoch 21] Training Batch [75/391]: Loss 0.021339312195777893\n",
      "[Epoch 21] Training Batch [76/391]: Loss 0.029805300757288933\n",
      "[Epoch 21] Training Batch [77/391]: Loss 0.04392920061945915\n",
      "[Epoch 21] Training Batch [78/391]: Loss 0.05151286721229553\n",
      "[Epoch 21] Training Batch [79/391]: Loss 0.05930090695619583\n",
      "[Epoch 21] Training Batch [80/391]: Loss 0.02653983235359192\n",
      "[Epoch 21] Training Batch [81/391]: Loss 0.03166673704981804\n",
      "[Epoch 21] Training Batch [82/391]: Loss 0.03725547716021538\n",
      "[Epoch 21] Training Batch [83/391]: Loss 0.022816359996795654\n",
      "[Epoch 21] Training Batch [84/391]: Loss 0.03575391322374344\n",
      "[Epoch 21] Training Batch [85/391]: Loss 0.019134337082505226\n",
      "[Epoch 21] Training Batch [86/391]: Loss 0.011550527065992355\n",
      "[Epoch 21] Training Batch [87/391]: Loss 0.04693612828850746\n",
      "[Epoch 21] Training Batch [88/391]: Loss 0.024303950369358063\n",
      "[Epoch 21] Training Batch [89/391]: Loss 0.020861906930804253\n",
      "[Epoch 21] Training Batch [90/391]: Loss 0.050490282475948334\n",
      "[Epoch 21] Training Batch [91/391]: Loss 0.011868957430124283\n",
      "[Epoch 21] Training Batch [92/391]: Loss 0.01978904753923416\n",
      "[Epoch 21] Training Batch [93/391]: Loss 0.009273303672671318\n",
      "[Epoch 21] Training Batch [94/391]: Loss 0.02255113422870636\n",
      "[Epoch 21] Training Batch [95/391]: Loss 0.006948074325919151\n",
      "[Epoch 21] Training Batch [96/391]: Loss 0.02651563659310341\n",
      "[Epoch 21] Training Batch [97/391]: Loss 0.020022576674818993\n",
      "[Epoch 21] Training Batch [98/391]: Loss 0.015849053859710693\n",
      "[Epoch 21] Training Batch [99/391]: Loss 0.012823591940104961\n",
      "[Epoch 21] Training Batch [100/391]: Loss 0.013309411704540253\n",
      "[Epoch 21] Training Batch [101/391]: Loss 0.06451570987701416\n",
      "[Epoch 21] Training Batch [102/391]: Loss 0.04863181710243225\n",
      "[Epoch 21] Training Batch [103/391]: Loss 0.03560790792107582\n",
      "[Epoch 21] Training Batch [104/391]: Loss 0.006507389713078737\n",
      "[Epoch 21] Training Batch [105/391]: Loss 0.005056291818618774\n",
      "[Epoch 21] Training Batch [106/391]: Loss 0.02452390268445015\n",
      "[Epoch 21] Training Batch [107/391]: Loss 0.03815007209777832\n",
      "[Epoch 21] Training Batch [108/391]: Loss 0.01632189378142357\n",
      "[Epoch 21] Training Batch [109/391]: Loss 0.03857956826686859\n",
      "[Epoch 21] Training Batch [110/391]: Loss 0.06806864589452744\n",
      "[Epoch 21] Training Batch [111/391]: Loss 0.030138282105326653\n",
      "[Epoch 21] Training Batch [112/391]: Loss 0.023015206679701805\n",
      "[Epoch 21] Training Batch [113/391]: Loss 0.023287029936909676\n",
      "[Epoch 21] Training Batch [114/391]: Loss 0.022290103137493134\n",
      "[Epoch 21] Training Batch [115/391]: Loss 0.03520672395825386\n",
      "[Epoch 21] Training Batch [116/391]: Loss 0.031893450766801834\n",
      "[Epoch 21] Training Batch [117/391]: Loss 0.036857541650533676\n",
      "[Epoch 21] Training Batch [118/391]: Loss 0.03775062784552574\n",
      "[Epoch 21] Training Batch [119/391]: Loss 0.04037122800946236\n",
      "[Epoch 21] Training Batch [120/391]: Loss 0.04926983267068863\n",
      "[Epoch 21] Training Batch [121/391]: Loss 0.004064413718879223\n",
      "[Epoch 21] Training Batch [122/391]: Loss 0.03395876660943031\n",
      "[Epoch 21] Training Batch [123/391]: Loss 0.037413135170936584\n",
      "[Epoch 21] Training Batch [124/391]: Loss 0.013975651003420353\n",
      "[Epoch 21] Training Batch [125/391]: Loss 0.0412115678191185\n",
      "[Epoch 21] Training Batch [126/391]: Loss 0.023340333253145218\n",
      "[Epoch 21] Training Batch [127/391]: Loss 0.04291030392050743\n",
      "[Epoch 21] Training Batch [128/391]: Loss 0.027351686730980873\n",
      "[Epoch 21] Training Batch [129/391]: Loss 0.03785250335931778\n",
      "[Epoch 21] Training Batch [130/391]: Loss 0.03979189693927765\n",
      "[Epoch 21] Training Batch [131/391]: Loss 0.0355682373046875\n",
      "[Epoch 21] Training Batch [132/391]: Loss 0.01310696266591549\n",
      "[Epoch 21] Training Batch [133/391]: Loss 0.035456325858831406\n",
      "[Epoch 21] Training Batch [134/391]: Loss 0.005942017305642366\n",
      "[Epoch 21] Training Batch [135/391]: Loss 0.02941444143652916\n",
      "[Epoch 21] Training Batch [136/391]: Loss 0.016857190057635307\n",
      "[Epoch 21] Training Batch [137/391]: Loss 0.015086115337908268\n",
      "[Epoch 21] Training Batch [138/391]: Loss 0.03163544088602066\n",
      "[Epoch 21] Training Batch [139/391]: Loss 0.039874639362096786\n",
      "[Epoch 21] Training Batch [140/391]: Loss 0.006701803300529718\n",
      "[Epoch 21] Training Batch [141/391]: Loss 0.06865835189819336\n",
      "[Epoch 21] Training Batch [142/391]: Loss 0.05992330238223076\n",
      "[Epoch 21] Training Batch [143/391]: Loss 0.008379383012652397\n",
      "[Epoch 21] Training Batch [144/391]: Loss 0.02448694035410881\n",
      "[Epoch 21] Training Batch [145/391]: Loss 0.050309423357248306\n",
      "[Epoch 21] Training Batch [146/391]: Loss 0.019153166562318802\n",
      "[Epoch 21] Training Batch [147/391]: Loss 0.04835370555520058\n",
      "[Epoch 21] Training Batch [148/391]: Loss 0.06351266801357269\n",
      "[Epoch 21] Training Batch [149/391]: Loss 0.042501214891672134\n",
      "[Epoch 21] Training Batch [150/391]: Loss 0.05880185216665268\n",
      "[Epoch 21] Training Batch [151/391]: Loss 0.011651464737951756\n",
      "[Epoch 21] Training Batch [152/391]: Loss 0.05588037520647049\n",
      "[Epoch 21] Training Batch [153/391]: Loss 0.031534191220998764\n",
      "[Epoch 21] Training Batch [154/391]: Loss 0.023114552721381187\n",
      "[Epoch 21] Training Batch [155/391]: Loss 0.030487757176160812\n",
      "[Epoch 21] Training Batch [156/391]: Loss 0.0463934987783432\n",
      "[Epoch 21] Training Batch [157/391]: Loss 0.057120200246572495\n",
      "[Epoch 21] Training Batch [158/391]: Loss 0.04374292492866516\n",
      "[Epoch 21] Training Batch [159/391]: Loss 0.01737377792596817\n",
      "[Epoch 21] Training Batch [160/391]: Loss 0.01927921362221241\n",
      "[Epoch 21] Training Batch [161/391]: Loss 0.035659730434417725\n",
      "[Epoch 21] Training Batch [162/391]: Loss 0.011021005921065807\n",
      "[Epoch 21] Training Batch [163/391]: Loss 0.04124151170253754\n",
      "[Epoch 21] Training Batch [164/391]: Loss 0.050665367394685745\n",
      "[Epoch 21] Training Batch [165/391]: Loss 0.03311572223901749\n",
      "[Epoch 21] Training Batch [166/391]: Loss 0.02322240173816681\n",
      "[Epoch 21] Training Batch [167/391]: Loss 0.017404567450284958\n",
      "[Epoch 21] Training Batch [168/391]: Loss 0.039355624467134476\n",
      "[Epoch 21] Training Batch [169/391]: Loss 0.0186051856726408\n",
      "[Epoch 21] Training Batch [170/391]: Loss 0.028925202786922455\n",
      "[Epoch 21] Training Batch [171/391]: Loss 0.008311647921800613\n",
      "[Epoch 21] Training Batch [172/391]: Loss 0.028178229928016663\n",
      "[Epoch 21] Training Batch [173/391]: Loss 0.019603285938501358\n",
      "[Epoch 21] Training Batch [174/391]: Loss 0.04520892724394798\n",
      "[Epoch 21] Training Batch [175/391]: Loss 0.018496064469218254\n",
      "[Epoch 21] Training Batch [176/391]: Loss 0.051205459982156754\n",
      "[Epoch 21] Training Batch [177/391]: Loss 0.04356246069073677\n",
      "[Epoch 21] Training Batch [178/391]: Loss 0.013213076628744602\n",
      "[Epoch 21] Training Batch [179/391]: Loss 0.012923060916364193\n",
      "[Epoch 21] Training Batch [180/391]: Loss 0.012192091904580593\n",
      "[Epoch 21] Training Batch [181/391]: Loss 0.0338577926158905\n",
      "[Epoch 21] Training Batch [182/391]: Loss 0.028057076036930084\n",
      "[Epoch 21] Training Batch [183/391]: Loss 0.10374518483877182\n",
      "[Epoch 21] Training Batch [184/391]: Loss 0.02110237628221512\n",
      "[Epoch 21] Training Batch [185/391]: Loss 0.0644107237458229\n",
      "[Epoch 21] Training Batch [186/391]: Loss 0.014345036819577217\n",
      "[Epoch 21] Training Batch [187/391]: Loss 0.0225849486887455\n",
      "[Epoch 21] Training Batch [188/391]: Loss 0.08248633146286011\n",
      "[Epoch 21] Training Batch [189/391]: Loss 0.0173646230250597\n",
      "[Epoch 21] Training Batch [190/391]: Loss 0.023176899179816246\n",
      "[Epoch 21] Training Batch [191/391]: Loss 0.0371742844581604\n",
      "[Epoch 21] Training Batch [192/391]: Loss 0.025779707357287407\n",
      "[Epoch 21] Training Batch [193/391]: Loss 0.01830550841987133\n",
      "[Epoch 21] Training Batch [194/391]: Loss 0.03183446824550629\n",
      "[Epoch 21] Training Batch [195/391]: Loss 0.09168576449155807\n",
      "[Epoch 21] Training Batch [196/391]: Loss 0.06867555528879166\n",
      "[Epoch 21] Training Batch [197/391]: Loss 0.014958382584154606\n",
      "[Epoch 21] Training Batch [198/391]: Loss 0.03677412122488022\n",
      "[Epoch 21] Training Batch [199/391]: Loss 0.041061144322156906\n",
      "[Epoch 21] Training Batch [200/391]: Loss 0.03687773272395134\n",
      "[Epoch 21] Training Batch [201/391]: Loss 0.009661619551479816\n",
      "[Epoch 21] Training Batch [202/391]: Loss 0.04465295374393463\n",
      "[Epoch 21] Training Batch [203/391]: Loss 0.06299339234828949\n",
      "[Epoch 21] Training Batch [204/391]: Loss 0.019936753436923027\n",
      "[Epoch 21] Training Batch [205/391]: Loss 0.05026857554912567\n",
      "[Epoch 21] Training Batch [206/391]: Loss 0.06069101020693779\n",
      "[Epoch 21] Training Batch [207/391]: Loss 0.04648774117231369\n",
      "[Epoch 21] Training Batch [208/391]: Loss 0.018109500408172607\n",
      "[Epoch 21] Training Batch [209/391]: Loss 0.032377369701862335\n",
      "[Epoch 21] Training Batch [210/391]: Loss 0.017735429108142853\n",
      "[Epoch 21] Training Batch [211/391]: Loss 0.03220990300178528\n",
      "[Epoch 21] Training Batch [212/391]: Loss 0.05891922488808632\n",
      "[Epoch 21] Training Batch [213/391]: Loss 0.024459082633256912\n",
      "[Epoch 21] Training Batch [214/391]: Loss 0.028664806857705116\n",
      "[Epoch 21] Training Batch [215/391]: Loss 0.04775970056653023\n",
      "[Epoch 21] Training Batch [216/391]: Loss 0.03684540465474129\n",
      "[Epoch 21] Training Batch [217/391]: Loss 0.017717983573675156\n",
      "[Epoch 21] Training Batch [218/391]: Loss 0.043872490525245667\n",
      "[Epoch 21] Training Batch [219/391]: Loss 0.01775493659079075\n",
      "[Epoch 21] Training Batch [220/391]: Loss 0.1095602735877037\n",
      "[Epoch 21] Training Batch [221/391]: Loss 0.08067262172698975\n",
      "[Epoch 21] Training Batch [222/391]: Loss 0.04970970004796982\n",
      "[Epoch 21] Training Batch [223/391]: Loss 0.051155753433704376\n",
      "[Epoch 21] Training Batch [224/391]: Loss 0.04123362898826599\n",
      "[Epoch 21] Training Batch [225/391]: Loss 0.052911944687366486\n",
      "[Epoch 21] Training Batch [226/391]: Loss 0.09366609156131744\n",
      "[Epoch 21] Training Batch [227/391]: Loss 0.0945025235414505\n",
      "[Epoch 21] Training Batch [228/391]: Loss 0.035963620990514755\n",
      "[Epoch 21] Training Batch [229/391]: Loss 0.032474957406520844\n",
      "[Epoch 21] Training Batch [230/391]: Loss 0.05209650471806526\n",
      "[Epoch 21] Training Batch [231/391]: Loss 0.023458916693925858\n",
      "[Epoch 21] Training Batch [232/391]: Loss 0.030611922964453697\n",
      "[Epoch 21] Training Batch [233/391]: Loss 0.020503772422671318\n",
      "[Epoch 21] Training Batch [234/391]: Loss 0.028627682477235794\n",
      "[Epoch 21] Training Batch [235/391]: Loss 0.030138127505779266\n",
      "[Epoch 21] Training Batch [236/391]: Loss 0.051448240876197815\n",
      "[Epoch 21] Training Batch [237/391]: Loss 0.04387548565864563\n",
      "[Epoch 21] Training Batch [238/391]: Loss 0.0783534049987793\n",
      "[Epoch 21] Training Batch [239/391]: Loss 0.05540675297379494\n",
      "[Epoch 21] Training Batch [240/391]: Loss 0.05789538472890854\n",
      "[Epoch 21] Training Batch [241/391]: Loss 0.03480754792690277\n",
      "[Epoch 21] Training Batch [242/391]: Loss 0.07370272278785706\n",
      "[Epoch 21] Training Batch [243/391]: Loss 0.09155058115720749\n",
      "[Epoch 21] Training Batch [244/391]: Loss 0.0362098328769207\n",
      "[Epoch 21] Training Batch [245/391]: Loss 0.017749948427081108\n",
      "[Epoch 21] Training Batch [246/391]: Loss 0.07920343428850174\n",
      "[Epoch 21] Training Batch [247/391]: Loss 0.04030998796224594\n",
      "[Epoch 21] Training Batch [248/391]: Loss 0.12352287024259567\n",
      "[Epoch 21] Training Batch [249/391]: Loss 0.016385721042752266\n",
      "[Epoch 21] Training Batch [250/391]: Loss 0.05370311066508293\n",
      "[Epoch 21] Training Batch [251/391]: Loss 0.027470417320728302\n",
      "[Epoch 21] Training Batch [252/391]: Loss 0.006070086732506752\n",
      "[Epoch 21] Training Batch [253/391]: Loss 0.020650004968047142\n",
      "[Epoch 21] Training Batch [254/391]: Loss 0.03562120720744133\n",
      "[Epoch 21] Training Batch [255/391]: Loss 0.03138238191604614\n",
      "[Epoch 21] Training Batch [256/391]: Loss 0.024238796904683113\n",
      "[Epoch 21] Training Batch [257/391]: Loss 0.021352022886276245\n",
      "[Epoch 21] Training Batch [258/391]: Loss 0.050734490156173706\n",
      "[Epoch 21] Training Batch [259/391]: Loss 0.01942479982972145\n",
      "[Epoch 21] Training Batch [260/391]: Loss 0.09940633177757263\n",
      "[Epoch 21] Training Batch [261/391]: Loss 0.08400541543960571\n",
      "[Epoch 21] Training Batch [262/391]: Loss 0.07554232329130173\n",
      "[Epoch 21] Training Batch [263/391]: Loss 0.02019360661506653\n",
      "[Epoch 21] Training Batch [264/391]: Loss 0.060823794454336166\n",
      "[Epoch 21] Training Batch [265/391]: Loss 0.03489643707871437\n",
      "[Epoch 21] Training Batch [266/391]: Loss 0.07493073493242264\n",
      "[Epoch 21] Training Batch [267/391]: Loss 0.04639941453933716\n",
      "[Epoch 21] Training Batch [268/391]: Loss 0.0403066910803318\n",
      "[Epoch 21] Training Batch [269/391]: Loss 0.02575368620455265\n",
      "[Epoch 21] Training Batch [270/391]: Loss 0.009625769220292568\n",
      "[Epoch 21] Training Batch [271/391]: Loss 0.03676767647266388\n",
      "[Epoch 21] Training Batch [272/391]: Loss 0.06031034141778946\n",
      "[Epoch 21] Training Batch [273/391]: Loss 0.06858029216527939\n",
      "[Epoch 21] Training Batch [274/391]: Loss 0.031620871275663376\n",
      "[Epoch 21] Training Batch [275/391]: Loss 0.008115248754620552\n",
      "[Epoch 21] Training Batch [276/391]: Loss 0.026397544890642166\n",
      "[Epoch 21] Training Batch [277/391]: Loss 0.03725479170680046\n",
      "[Epoch 21] Training Batch [278/391]: Loss 0.03444482758641243\n",
      "[Epoch 21] Training Batch [279/391]: Loss 0.02873075008392334\n",
      "[Epoch 21] Training Batch [280/391]: Loss 0.0366060845553875\n",
      "[Epoch 21] Training Batch [281/391]: Loss 0.05017387866973877\n",
      "[Epoch 21] Training Batch [282/391]: Loss 0.06900797039270401\n",
      "[Epoch 21] Training Batch [283/391]: Loss 0.014755336567759514\n",
      "[Epoch 21] Training Batch [284/391]: Loss 0.02781306579709053\n",
      "[Epoch 21] Training Batch [285/391]: Loss 0.03712144494056702\n",
      "[Epoch 21] Training Batch [286/391]: Loss 0.027650291100144386\n",
      "[Epoch 21] Training Batch [287/391]: Loss 0.07016180455684662\n",
      "[Epoch 21] Training Batch [288/391]: Loss 0.016401441767811775\n",
      "[Epoch 21] Training Batch [289/391]: Loss 0.03411772474646568\n",
      "[Epoch 21] Training Batch [290/391]: Loss 0.05173053219914436\n",
      "[Epoch 21] Training Batch [291/391]: Loss 0.014800039120018482\n",
      "[Epoch 21] Training Batch [292/391]: Loss 0.03535141050815582\n",
      "[Epoch 21] Training Batch [293/391]: Loss 0.03589177131652832\n",
      "[Epoch 21] Training Batch [294/391]: Loss 0.044190458953380585\n",
      "[Epoch 21] Training Batch [295/391]: Loss 0.0496198870241642\n",
      "[Epoch 21] Training Batch [296/391]: Loss 0.009578097611665726\n",
      "[Epoch 21] Training Batch [297/391]: Loss 0.04606529697775841\n",
      "[Epoch 21] Training Batch [298/391]: Loss 0.03176096826791763\n",
      "[Epoch 21] Training Batch [299/391]: Loss 0.04046563431620598\n",
      "[Epoch 21] Training Batch [300/391]: Loss 0.03766948729753494\n",
      "[Epoch 21] Training Batch [301/391]: Loss 0.04694134742021561\n",
      "[Epoch 21] Training Batch [302/391]: Loss 0.01687145046889782\n",
      "[Epoch 21] Training Batch [303/391]: Loss 0.05960807204246521\n",
      "[Epoch 21] Training Batch [304/391]: Loss 0.07145118713378906\n",
      "[Epoch 21] Training Batch [305/391]: Loss 0.018496336415410042\n",
      "[Epoch 21] Training Batch [306/391]: Loss 0.032851532101631165\n",
      "[Epoch 21] Training Batch [307/391]: Loss 0.09887198358774185\n",
      "[Epoch 21] Training Batch [308/391]: Loss 0.06927371025085449\n",
      "[Epoch 21] Training Batch [309/391]: Loss 0.031021378934383392\n",
      "[Epoch 21] Training Batch [310/391]: Loss 0.026623932644724846\n",
      "[Epoch 21] Training Batch [311/391]: Loss 0.010733252391219139\n",
      "[Epoch 21] Training Batch [312/391]: Loss 0.0051090349443256855\n",
      "[Epoch 21] Training Batch [313/391]: Loss 0.022726843133568764\n",
      "[Epoch 21] Training Batch [314/391]: Loss 0.10904781520366669\n",
      "[Epoch 21] Training Batch [315/391]: Loss 0.03159962594509125\n",
      "[Epoch 21] Training Batch [316/391]: Loss 0.11443157494068146\n",
      "[Epoch 21] Training Batch [317/391]: Loss 0.01871105283498764\n",
      "[Epoch 21] Training Batch [318/391]: Loss 0.06393581628799438\n",
      "[Epoch 21] Training Batch [319/391]: Loss 0.02749219350516796\n",
      "[Epoch 21] Training Batch [320/391]: Loss 0.025548921898007393\n",
      "[Epoch 21] Training Batch [321/391]: Loss 0.10319802165031433\n",
      "[Epoch 21] Training Batch [322/391]: Loss 0.15220144391059875\n",
      "[Epoch 21] Training Batch [323/391]: Loss 0.10820741951465607\n",
      "[Epoch 21] Training Batch [324/391]: Loss 0.05897216126322746\n",
      "[Epoch 21] Training Batch [325/391]: Loss 0.07004522532224655\n",
      "[Epoch 21] Training Batch [326/391]: Loss 0.07537216693162918\n",
      "[Epoch 21] Training Batch [327/391]: Loss 0.02983017824590206\n",
      "[Epoch 21] Training Batch [328/391]: Loss 0.0463448204100132\n",
      "[Epoch 21] Training Batch [329/391]: Loss 0.03164788335561752\n",
      "[Epoch 21] Training Batch [330/391]: Loss 0.04459158703684807\n",
      "[Epoch 21] Training Batch [331/391]: Loss 0.0904776006937027\n",
      "[Epoch 21] Training Batch [332/391]: Loss 0.03329448029398918\n",
      "[Epoch 21] Training Batch [333/391]: Loss 0.030622221529483795\n",
      "[Epoch 21] Training Batch [334/391]: Loss 0.04205339401960373\n",
      "[Epoch 21] Training Batch [335/391]: Loss 0.07169891893863678\n",
      "[Epoch 21] Training Batch [336/391]: Loss 0.028055185452103615\n",
      "[Epoch 21] Training Batch [337/391]: Loss 0.0825343132019043\n",
      "[Epoch 21] Training Batch [338/391]: Loss 0.045424919575452805\n",
      "[Epoch 21] Training Batch [339/391]: Loss 0.08825404196977615\n",
      "[Epoch 21] Training Batch [340/391]: Loss 0.04638238623738289\n",
      "[Epoch 21] Training Batch [341/391]: Loss 0.10943026095628738\n",
      "[Epoch 21] Training Batch [342/391]: Loss 0.025775114074349403\n",
      "[Epoch 21] Training Batch [343/391]: Loss 0.021117456257343292\n",
      "[Epoch 21] Training Batch [344/391]: Loss 0.06610345095396042\n",
      "[Epoch 21] Training Batch [345/391]: Loss 0.05231207609176636\n",
      "[Epoch 21] Training Batch [346/391]: Loss 0.05205138772726059\n",
      "[Epoch 21] Training Batch [347/391]: Loss 0.059243619441986084\n",
      "[Epoch 21] Training Batch [348/391]: Loss 0.03492988273501396\n",
      "[Epoch 21] Training Batch [349/391]: Loss 0.02119763381779194\n",
      "[Epoch 21] Training Batch [350/391]: Loss 0.04003191366791725\n",
      "[Epoch 21] Training Batch [351/391]: Loss 0.032429538667201996\n",
      "[Epoch 21] Training Batch [352/391]: Loss 0.04824989661574364\n",
      "[Epoch 21] Training Batch [353/391]: Loss 0.03837607055902481\n",
      "[Epoch 21] Training Batch [354/391]: Loss 0.023744042962789536\n",
      "[Epoch 21] Training Batch [355/391]: Loss 0.02274584211409092\n",
      "[Epoch 21] Training Batch [356/391]: Loss 0.06758186221122742\n",
      "[Epoch 21] Training Batch [357/391]: Loss 0.03511665761470795\n",
      "[Epoch 21] Training Batch [358/391]: Loss 0.025047820061445236\n",
      "[Epoch 21] Training Batch [359/391]: Loss 0.0689670667052269\n",
      "[Epoch 21] Training Batch [360/391]: Loss 0.01721980608999729\n",
      "[Epoch 21] Training Batch [361/391]: Loss 0.04047881066799164\n",
      "[Epoch 21] Training Batch [362/391]: Loss 0.07132076472043991\n",
      "[Epoch 21] Training Batch [363/391]: Loss 0.05344191566109657\n",
      "[Epoch 21] Training Batch [364/391]: Loss 0.06144222244620323\n",
      "[Epoch 21] Training Batch [365/391]: Loss 0.06860284507274628\n",
      "[Epoch 21] Training Batch [366/391]: Loss 0.06144434213638306\n",
      "[Epoch 21] Training Batch [367/391]: Loss 0.02330923266708851\n",
      "[Epoch 21] Training Batch [368/391]: Loss 0.05855054035782814\n",
      "[Epoch 21] Training Batch [369/391]: Loss 0.045846227556467056\n",
      "[Epoch 21] Training Batch [370/391]: Loss 0.026644160971045494\n",
      "[Epoch 21] Training Batch [371/391]: Loss 0.035197094082832336\n",
      "[Epoch 21] Training Batch [372/391]: Loss 0.030524123460054398\n",
      "[Epoch 21] Training Batch [373/391]: Loss 0.008167751133441925\n",
      "[Epoch 21] Training Batch [374/391]: Loss 0.0661218985915184\n",
      "[Epoch 21] Training Batch [375/391]: Loss 0.10229452699422836\n",
      "[Epoch 21] Training Batch [376/391]: Loss 0.03040335886180401\n",
      "[Epoch 21] Training Batch [377/391]: Loss 0.07923067361116409\n",
      "[Epoch 21] Training Batch [378/391]: Loss 0.021736498922109604\n",
      "[Epoch 21] Training Batch [379/391]: Loss 0.061093635857105255\n",
      "[Epoch 21] Training Batch [380/391]: Loss 0.04465389624238014\n",
      "[Epoch 21] Training Batch [381/391]: Loss 0.02172519639134407\n",
      "[Epoch 21] Training Batch [382/391]: Loss 0.08168920874595642\n",
      "[Epoch 21] Training Batch [383/391]: Loss 0.10534505546092987\n",
      "[Epoch 21] Training Batch [384/391]: Loss 0.07185119390487671\n",
      "[Epoch 21] Training Batch [385/391]: Loss 0.07410223037004471\n",
      "[Epoch 21] Training Batch [386/391]: Loss 0.049659568816423416\n",
      "[Epoch 21] Training Batch [387/391]: Loss 0.04564427211880684\n",
      "[Epoch 21] Training Batch [388/391]: Loss 0.11284299194812775\n",
      "[Epoch 21] Training Batch [389/391]: Loss 0.02263559028506279\n",
      "[Epoch 21] Training Batch [390/391]: Loss 0.049655813723802567\n",
      "[Epoch 21] Training Batch [391/391]: Loss 0.06258850544691086\n",
      "Epoch 21 - Train Loss: 0.0393\n",
      "*********  Epoch 22/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Training Batch [1/391]: Loss 0.01368353608995676\n",
      "[Epoch 22] Training Batch [2/391]: Loss 0.015685103833675385\n",
      "[Epoch 22] Training Batch [3/391]: Loss 0.007712800987064838\n",
      "[Epoch 22] Training Batch [4/391]: Loss 0.048216186463832855\n",
      "[Epoch 22] Training Batch [5/391]: Loss 0.02750631608068943\n",
      "[Epoch 22] Training Batch [6/391]: Loss 0.04015589877963066\n",
      "[Epoch 22] Training Batch [7/391]: Loss 0.030030444264411926\n",
      "[Epoch 22] Training Batch [8/391]: Loss 0.04147251322865486\n",
      "[Epoch 22] Training Batch [9/391]: Loss 0.052536942064762115\n",
      "[Epoch 22] Training Batch [10/391]: Loss 0.021090012043714523\n",
      "[Epoch 22] Training Batch [11/391]: Loss 0.040313031524419785\n",
      "[Epoch 22] Training Batch [12/391]: Loss 0.05843886360526085\n",
      "[Epoch 22] Training Batch [13/391]: Loss 0.028100695461034775\n",
      "[Epoch 22] Training Batch [14/391]: Loss 0.022758252918720245\n",
      "[Epoch 22] Training Batch [15/391]: Loss 0.010099349543452263\n",
      "[Epoch 22] Training Batch [16/391]: Loss 0.034872494637966156\n",
      "[Epoch 22] Training Batch [17/391]: Loss 0.035180795937776566\n",
      "[Epoch 22] Training Batch [18/391]: Loss 0.03501123562455177\n",
      "[Epoch 22] Training Batch [19/391]: Loss 0.018097590655088425\n",
      "[Epoch 22] Training Batch [20/391]: Loss 0.013937429524958134\n",
      "[Epoch 22] Training Batch [21/391]: Loss 0.020402517169713974\n",
      "[Epoch 22] Training Batch [22/391]: Loss 0.013788082636892796\n",
      "[Epoch 22] Training Batch [23/391]: Loss 0.016190145164728165\n",
      "[Epoch 22] Training Batch [24/391]: Loss 0.04789486527442932\n",
      "[Epoch 22] Training Batch [25/391]: Loss 0.029689231887459755\n",
      "[Epoch 22] Training Batch [26/391]: Loss 0.01625019498169422\n",
      "[Epoch 22] Training Batch [27/391]: Loss 0.03532853350043297\n",
      "[Epoch 22] Training Batch [28/391]: Loss 0.06965253502130508\n",
      "[Epoch 22] Training Batch [29/391]: Loss 0.00939470250159502\n",
      "[Epoch 22] Training Batch [30/391]: Loss 0.020544758066534996\n",
      "[Epoch 22] Training Batch [31/391]: Loss 0.008086930960416794\n",
      "[Epoch 22] Training Batch [32/391]: Loss 0.02691160887479782\n",
      "[Epoch 22] Training Batch [33/391]: Loss 0.02328822761774063\n",
      "[Epoch 22] Training Batch [34/391]: Loss 0.014593031257390976\n",
      "[Epoch 22] Training Batch [35/391]: Loss 0.013072035275399685\n",
      "[Epoch 22] Training Batch [36/391]: Loss 0.016978032886981964\n",
      "[Epoch 22] Training Batch [37/391]: Loss 0.024357939139008522\n",
      "[Epoch 22] Training Batch [38/391]: Loss 0.00874454528093338\n",
      "[Epoch 22] Training Batch [39/391]: Loss 0.06387422233819962\n",
      "[Epoch 22] Training Batch [40/391]: Loss 0.012060212902724743\n",
      "[Epoch 22] Training Batch [41/391]: Loss 0.015540231950581074\n",
      "[Epoch 22] Training Batch [42/391]: Loss 0.01597139798104763\n",
      "[Epoch 22] Training Batch [43/391]: Loss 0.048959601670503616\n",
      "[Epoch 22] Training Batch [44/391]: Loss 0.015311593189835548\n",
      "[Epoch 22] Training Batch [45/391]: Loss 0.008513730019330978\n",
      "[Epoch 22] Training Batch [46/391]: Loss 0.022570932283997536\n",
      "[Epoch 22] Training Batch [47/391]: Loss 0.060770146548748016\n",
      "[Epoch 22] Training Batch [48/391]: Loss 0.02428617514669895\n",
      "[Epoch 22] Training Batch [49/391]: Loss 0.11888633668422699\n",
      "[Epoch 22] Training Batch [50/391]: Loss 0.02267639711499214\n",
      "[Epoch 22] Training Batch [51/391]: Loss 0.029563212767243385\n",
      "[Epoch 22] Training Batch [52/391]: Loss 0.08360889554023743\n",
      "[Epoch 22] Training Batch [53/391]: Loss 0.01762009598314762\n",
      "[Epoch 22] Training Batch [54/391]: Loss 0.030876316130161285\n",
      "[Epoch 22] Training Batch [55/391]: Loss 0.020565113052725792\n",
      "[Epoch 22] Training Batch [56/391]: Loss 0.0319070890545845\n",
      "[Epoch 22] Training Batch [57/391]: Loss 0.015796832740306854\n",
      "[Epoch 22] Training Batch [58/391]: Loss 0.008099381811916828\n",
      "[Epoch 22] Training Batch [59/391]: Loss 0.019289907068014145\n",
      "[Epoch 22] Training Batch [60/391]: Loss 0.037612445652484894\n",
      "[Epoch 22] Training Batch [61/391]: Loss 0.010620019398629665\n",
      "[Epoch 22] Training Batch [62/391]: Loss 0.017996706068515778\n",
      "[Epoch 22] Training Batch [63/391]: Loss 0.08204779028892517\n",
      "[Epoch 22] Training Batch [64/391]: Loss 0.015591843985021114\n",
      "[Epoch 22] Training Batch [65/391]: Loss 0.007787277922034264\n",
      "[Epoch 22] Training Batch [66/391]: Loss 0.04299987480044365\n",
      "[Epoch 22] Training Batch [67/391]: Loss 0.02649310790002346\n",
      "[Epoch 22] Training Batch [68/391]: Loss 0.03568904101848602\n",
      "[Epoch 22] Training Batch [69/391]: Loss 0.03403642028570175\n",
      "[Epoch 22] Training Batch [70/391]: Loss 0.01634131744503975\n",
      "[Epoch 22] Training Batch [71/391]: Loss 0.013007132336497307\n",
      "[Epoch 22] Training Batch [72/391]: Loss 0.029363704845309258\n",
      "[Epoch 22] Training Batch [73/391]: Loss 0.06695631891489029\n",
      "[Epoch 22] Training Batch [74/391]: Loss 0.04344993829727173\n",
      "[Epoch 22] Training Batch [75/391]: Loss 0.007479771506041288\n",
      "[Epoch 22] Training Batch [76/391]: Loss 0.05121631920337677\n",
      "[Epoch 22] Training Batch [77/391]: Loss 0.007404434960335493\n",
      "[Epoch 22] Training Batch [78/391]: Loss 0.007614913396537304\n",
      "[Epoch 22] Training Batch [79/391]: Loss 0.061396513134241104\n",
      "[Epoch 22] Training Batch [80/391]: Loss 0.030112603679299355\n",
      "[Epoch 22] Training Batch [81/391]: Loss 0.027930745854973793\n",
      "[Epoch 22] Training Batch [82/391]: Loss 0.022686850279569626\n",
      "[Epoch 22] Training Batch [83/391]: Loss 0.007773365825414658\n",
      "[Epoch 22] Training Batch [84/391]: Loss 0.020867545157670975\n",
      "[Epoch 22] Training Batch [85/391]: Loss 0.020110338926315308\n",
      "[Epoch 22] Training Batch [86/391]: Loss 0.012082393281161785\n",
      "[Epoch 22] Training Batch [87/391]: Loss 0.029814062640070915\n",
      "[Epoch 22] Training Batch [88/391]: Loss 0.005129885394126177\n",
      "[Epoch 22] Training Batch [89/391]: Loss 0.006732326466590166\n",
      "[Epoch 22] Training Batch [90/391]: Loss 0.01153440959751606\n",
      "[Epoch 22] Training Batch [91/391]: Loss 0.02718263678252697\n",
      "[Epoch 22] Training Batch [92/391]: Loss 0.04271416366100311\n",
      "[Epoch 22] Training Batch [93/391]: Loss 0.017003001645207405\n",
      "[Epoch 22] Training Batch [94/391]: Loss 0.009245107881724834\n",
      "[Epoch 22] Training Batch [95/391]: Loss 0.023010386154055595\n",
      "[Epoch 22] Training Batch [96/391]: Loss 0.032737962901592255\n",
      "[Epoch 22] Training Batch [97/391]: Loss 0.01192046981304884\n",
      "[Epoch 22] Training Batch [98/391]: Loss 0.006940204184502363\n",
      "[Epoch 22] Training Batch [99/391]: Loss 0.002386569045484066\n",
      "[Epoch 22] Training Batch [100/391]: Loss 0.0401821993291378\n",
      "[Epoch 22] Training Batch [101/391]: Loss 0.015375575050711632\n",
      "[Epoch 22] Training Batch [102/391]: Loss 0.014984099194407463\n",
      "[Epoch 22] Training Batch [103/391]: Loss 0.0069908141158521175\n",
      "[Epoch 22] Training Batch [104/391]: Loss 0.007278668228536844\n",
      "[Epoch 22] Training Batch [105/391]: Loss 0.05000028386712074\n",
      "[Epoch 22] Training Batch [106/391]: Loss 0.00938398577272892\n",
      "[Epoch 22] Training Batch [107/391]: Loss 0.03658895567059517\n",
      "[Epoch 22] Training Batch [108/391]: Loss 0.03353060409426689\n",
      "[Epoch 22] Training Batch [109/391]: Loss 0.02052014134824276\n",
      "[Epoch 22] Training Batch [110/391]: Loss 0.020433245226740837\n",
      "[Epoch 22] Training Batch [111/391]: Loss 0.035907186567783356\n",
      "[Epoch 22] Training Batch [112/391]: Loss 0.011334809474647045\n",
      "[Epoch 22] Training Batch [113/391]: Loss 0.01367299072444439\n",
      "[Epoch 22] Training Batch [114/391]: Loss 0.004831290803849697\n",
      "[Epoch 22] Training Batch [115/391]: Loss 0.023333342745900154\n",
      "[Epoch 22] Training Batch [116/391]: Loss 0.025411324575543404\n",
      "[Epoch 22] Training Batch [117/391]: Loss 0.009403940290212631\n",
      "[Epoch 22] Training Batch [118/391]: Loss 0.014456418342888355\n",
      "[Epoch 22] Training Batch [119/391]: Loss 0.02614714577794075\n",
      "[Epoch 22] Training Batch [120/391]: Loss 0.030364619567990303\n",
      "[Epoch 22] Training Batch [121/391]: Loss 0.017940877005457878\n",
      "[Epoch 22] Training Batch [122/391]: Loss 0.024086667224764824\n",
      "[Epoch 22] Training Batch [123/391]: Loss 0.011835098266601562\n",
      "[Epoch 22] Training Batch [124/391]: Loss 0.008557379245758057\n",
      "[Epoch 22] Training Batch [125/391]: Loss 0.014869598671793938\n",
      "[Epoch 22] Training Batch [126/391]: Loss 0.03343642130494118\n",
      "[Epoch 22] Training Batch [127/391]: Loss 0.03976524993777275\n",
      "[Epoch 22] Training Batch [128/391]: Loss 0.008207523263990879\n",
      "[Epoch 22] Training Batch [129/391]: Loss 0.006997500080615282\n",
      "[Epoch 22] Training Batch [130/391]: Loss 0.0077775102108716965\n",
      "[Epoch 22] Training Batch [131/391]: Loss 0.002726139733567834\n",
      "[Epoch 22] Training Batch [132/391]: Loss 0.009603388607501984\n",
      "[Epoch 22] Training Batch [133/391]: Loss 0.023142782971262932\n",
      "[Epoch 22] Training Batch [134/391]: Loss 0.0051010181196033955\n",
      "[Epoch 22] Training Batch [135/391]: Loss 0.10351631045341492\n",
      "[Epoch 22] Training Batch [136/391]: Loss 0.04369645193219185\n",
      "[Epoch 22] Training Batch [137/391]: Loss 0.00553523562848568\n",
      "[Epoch 22] Training Batch [138/391]: Loss 0.009717847220599651\n",
      "[Epoch 22] Training Batch [139/391]: Loss 0.05696775019168854\n",
      "[Epoch 22] Training Batch [140/391]: Loss 0.04456067457795143\n",
      "[Epoch 22] Training Batch [141/391]: Loss 0.05379777029156685\n",
      "[Epoch 22] Training Batch [142/391]: Loss 0.04861454665660858\n",
      "[Epoch 22] Training Batch [143/391]: Loss 0.02308211661875248\n",
      "[Epoch 22] Training Batch [144/391]: Loss 0.02118341252207756\n",
      "[Epoch 22] Training Batch [145/391]: Loss 0.02795046754181385\n",
      "[Epoch 22] Training Batch [146/391]: Loss 0.025698944926261902\n",
      "[Epoch 22] Training Batch [147/391]: Loss 0.0036589549854397774\n",
      "[Epoch 22] Training Batch [148/391]: Loss 0.028044339269399643\n",
      "[Epoch 22] Training Batch [149/391]: Loss 0.02507922239601612\n",
      "[Epoch 22] Training Batch [150/391]: Loss 0.0250688623636961\n",
      "[Epoch 22] Training Batch [151/391]: Loss 0.013143211603164673\n",
      "[Epoch 22] Training Batch [152/391]: Loss 0.02691633813083172\n",
      "[Epoch 22] Training Batch [153/391]: Loss 0.020786145702004433\n",
      "[Epoch 22] Training Batch [154/391]: Loss 0.028002940118312836\n",
      "[Epoch 22] Training Batch [155/391]: Loss 0.03668862581253052\n",
      "[Epoch 22] Training Batch [156/391]: Loss 0.026088491082191467\n",
      "[Epoch 22] Training Batch [157/391]: Loss 0.015257400460541248\n",
      "[Epoch 22] Training Batch [158/391]: Loss 0.030681459233164787\n",
      "[Epoch 22] Training Batch [159/391]: Loss 0.006379470229148865\n",
      "[Epoch 22] Training Batch [160/391]: Loss 0.03966439887881279\n",
      "[Epoch 22] Training Batch [161/391]: Loss 0.011415445245802402\n",
      "[Epoch 22] Training Batch [162/391]: Loss 0.015647221356630325\n",
      "[Epoch 22] Training Batch [163/391]: Loss 0.06284357607364655\n",
      "[Epoch 22] Training Batch [164/391]: Loss 0.021723594516515732\n",
      "[Epoch 22] Training Batch [165/391]: Loss 0.02288801409304142\n",
      "[Epoch 22] Training Batch [166/391]: Loss 0.017624584957957268\n",
      "[Epoch 22] Training Batch [167/391]: Loss 0.04409303143620491\n",
      "[Epoch 22] Training Batch [168/391]: Loss 0.011399093084037304\n",
      "[Epoch 22] Training Batch [169/391]: Loss 0.004500103183090687\n",
      "[Epoch 22] Training Batch [170/391]: Loss 0.0056279622949659824\n",
      "[Epoch 22] Training Batch [171/391]: Loss 0.02579253911972046\n",
      "[Epoch 22] Training Batch [172/391]: Loss 0.014925375580787659\n",
      "[Epoch 22] Training Batch [173/391]: Loss 0.01348130777478218\n",
      "[Epoch 22] Training Batch [174/391]: Loss 0.07338543981313705\n",
      "[Epoch 22] Training Batch [175/391]: Loss 0.04580507427453995\n",
      "[Epoch 22] Training Batch [176/391]: Loss 0.016690118238329887\n",
      "[Epoch 22] Training Batch [177/391]: Loss 0.03960432484745979\n",
      "[Epoch 22] Training Batch [178/391]: Loss 0.04132864624261856\n",
      "[Epoch 22] Training Batch [179/391]: Loss 0.0216609425842762\n",
      "[Epoch 22] Training Batch [180/391]: Loss 0.01566453091800213\n",
      "[Epoch 22] Training Batch [181/391]: Loss 0.010987356305122375\n",
      "[Epoch 22] Training Batch [182/391]: Loss 0.030510246753692627\n",
      "[Epoch 22] Training Batch [183/391]: Loss 0.0314149484038353\n",
      "[Epoch 22] Training Batch [184/391]: Loss 0.03512169048190117\n",
      "[Epoch 22] Training Batch [185/391]: Loss 0.014006983488798141\n",
      "[Epoch 22] Training Batch [186/391]: Loss 0.024752505123615265\n",
      "[Epoch 22] Training Batch [187/391]: Loss 0.026803234592080116\n",
      "[Epoch 22] Training Batch [188/391]: Loss 0.02122979424893856\n",
      "[Epoch 22] Training Batch [189/391]: Loss 0.01965349353849888\n",
      "[Epoch 22] Training Batch [190/391]: Loss 0.003237011842429638\n",
      "[Epoch 22] Training Batch [191/391]: Loss 0.053758617490530014\n",
      "[Epoch 22] Training Batch [192/391]: Loss 0.019337419420480728\n",
      "[Epoch 22] Training Batch [193/391]: Loss 0.02183418534696102\n",
      "[Epoch 22] Training Batch [194/391]: Loss 0.016847770661115646\n",
      "[Epoch 22] Training Batch [195/391]: Loss 0.044927917420864105\n",
      "[Epoch 22] Training Batch [196/391]: Loss 0.013166790828108788\n",
      "[Epoch 22] Training Batch [197/391]: Loss 0.03121611475944519\n",
      "[Epoch 22] Training Batch [198/391]: Loss 0.022097598761320114\n",
      "[Epoch 22] Training Batch [199/391]: Loss 0.040225014090538025\n",
      "[Epoch 22] Training Batch [200/391]: Loss 0.027524197474122047\n",
      "[Epoch 22] Training Batch [201/391]: Loss 0.015378229320049286\n",
      "[Epoch 22] Training Batch [202/391]: Loss 0.012960250489413738\n",
      "[Epoch 22] Training Batch [203/391]: Loss 0.00635102391242981\n",
      "[Epoch 22] Training Batch [204/391]: Loss 0.06709270179271698\n",
      "[Epoch 22] Training Batch [205/391]: Loss 0.004888862371444702\n",
      "[Epoch 22] Training Batch [206/391]: Loss 0.01887253113090992\n",
      "[Epoch 22] Training Batch [207/391]: Loss 0.01666994020342827\n",
      "[Epoch 22] Training Batch [208/391]: Loss 0.00418176269158721\n",
      "[Epoch 22] Training Batch [209/391]: Loss 0.01995706744492054\n",
      "[Epoch 22] Training Batch [210/391]: Loss 0.007874016650021076\n",
      "[Epoch 22] Training Batch [211/391]: Loss 0.009443051181733608\n",
      "[Epoch 22] Training Batch [212/391]: Loss 0.02305486425757408\n",
      "[Epoch 22] Training Batch [213/391]: Loss 0.020485229790210724\n",
      "[Epoch 22] Training Batch [214/391]: Loss 0.03122863546013832\n",
      "[Epoch 22] Training Batch [215/391]: Loss 0.04747731611132622\n",
      "[Epoch 22] Training Batch [216/391]: Loss 0.01000383123755455\n",
      "[Epoch 22] Training Batch [217/391]: Loss 0.055684421211481094\n",
      "[Epoch 22] Training Batch [218/391]: Loss 0.03087330423295498\n",
      "[Epoch 22] Training Batch [219/391]: Loss 0.0313512459397316\n",
      "[Epoch 22] Training Batch [220/391]: Loss 0.03711371123790741\n",
      "[Epoch 22] Training Batch [221/391]: Loss 0.056249137967824936\n",
      "[Epoch 22] Training Batch [222/391]: Loss 0.056151922792196274\n",
      "[Epoch 22] Training Batch [223/391]: Loss 0.012922711670398712\n",
      "[Epoch 22] Training Batch [224/391]: Loss 0.03787339851260185\n",
      "[Epoch 22] Training Batch [225/391]: Loss 0.028976714238524437\n",
      "[Epoch 22] Training Batch [226/391]: Loss 0.006973679177463055\n",
      "[Epoch 22] Training Batch [227/391]: Loss 0.05508744716644287\n",
      "[Epoch 22] Training Batch [228/391]: Loss 0.02802608162164688\n",
      "[Epoch 22] Training Batch [229/391]: Loss 0.019024038687348366\n",
      "[Epoch 22] Training Batch [230/391]: Loss 0.015531597658991814\n",
      "[Epoch 22] Training Batch [231/391]: Loss 0.017002619802951813\n",
      "[Epoch 22] Training Batch [232/391]: Loss 0.03513118997216225\n",
      "[Epoch 22] Training Batch [233/391]: Loss 0.012090521864593029\n",
      "[Epoch 22] Training Batch [234/391]: Loss 0.032016243785619736\n",
      "[Epoch 22] Training Batch [235/391]: Loss 0.017247308045625687\n",
      "[Epoch 22] Training Batch [236/391]: Loss 0.027267513796687126\n",
      "[Epoch 22] Training Batch [237/391]: Loss 0.03890124335885048\n",
      "[Epoch 22] Training Batch [238/391]: Loss 0.046411190181970596\n",
      "[Epoch 22] Training Batch [239/391]: Loss 0.006433084607124329\n",
      "[Epoch 22] Training Batch [240/391]: Loss 0.005328051745891571\n",
      "[Epoch 22] Training Batch [241/391]: Loss 0.03458808362483978\n",
      "[Epoch 22] Training Batch [242/391]: Loss 0.04758976399898529\n",
      "[Epoch 22] Training Batch [243/391]: Loss 0.018128346651792526\n",
      "[Epoch 22] Training Batch [244/391]: Loss 0.01300249993801117\n",
      "[Epoch 22] Training Batch [245/391]: Loss 0.020006071776151657\n",
      "[Epoch 22] Training Batch [246/391]: Loss 0.045658767223358154\n",
      "[Epoch 22] Training Batch [247/391]: Loss 0.0668359026312828\n",
      "[Epoch 22] Training Batch [248/391]: Loss 0.014231267385184765\n",
      "[Epoch 22] Training Batch [249/391]: Loss 0.0333348847925663\n",
      "[Epoch 22] Training Batch [250/391]: Loss 0.02650020457804203\n",
      "[Epoch 22] Training Batch [251/391]: Loss 0.01887957565486431\n",
      "[Epoch 22] Training Batch [252/391]: Loss 0.002944638952612877\n",
      "[Epoch 22] Training Batch [253/391]: Loss 0.012801604345440865\n",
      "[Epoch 22] Training Batch [254/391]: Loss 0.02848450280725956\n",
      "[Epoch 22] Training Batch [255/391]: Loss 0.007581752259284258\n",
      "[Epoch 22] Training Batch [256/391]: Loss 0.06103283911943436\n",
      "[Epoch 22] Training Batch [257/391]: Loss 0.05229676142334938\n",
      "[Epoch 22] Training Batch [258/391]: Loss 0.04102180153131485\n",
      "[Epoch 22] Training Batch [259/391]: Loss 0.04139941930770874\n",
      "[Epoch 22] Training Batch [260/391]: Loss 0.050360456109046936\n",
      "[Epoch 22] Training Batch [261/391]: Loss 0.004073634278029203\n",
      "[Epoch 22] Training Batch [262/391]: Loss 0.02387077547609806\n",
      "[Epoch 22] Training Batch [263/391]: Loss 0.012669561430811882\n",
      "[Epoch 22] Training Batch [264/391]: Loss 0.018926383927464485\n",
      "[Epoch 22] Training Batch [265/391]: Loss 0.041394054889678955\n",
      "[Epoch 22] Training Batch [266/391]: Loss 0.030158406123518944\n",
      "[Epoch 22] Training Batch [267/391]: Loss 0.012308234348893166\n",
      "[Epoch 22] Training Batch [268/391]: Loss 0.06537794321775436\n",
      "[Epoch 22] Training Batch [269/391]: Loss 0.02036210149526596\n",
      "[Epoch 22] Training Batch [270/391]: Loss 0.010864208452403545\n",
      "[Epoch 22] Training Batch [271/391]: Loss 0.053724441677331924\n",
      "[Epoch 22] Training Batch [272/391]: Loss 0.07373034954071045\n",
      "[Epoch 22] Training Batch [273/391]: Loss 0.07068012654781342\n",
      "[Epoch 22] Training Batch [274/391]: Loss 0.043618809431791306\n",
      "[Epoch 22] Training Batch [275/391]: Loss 0.01623791828751564\n",
      "[Epoch 22] Training Batch [276/391]: Loss 0.049419380724430084\n",
      "[Epoch 22] Training Batch [277/391]: Loss 0.028349455446004868\n",
      "[Epoch 22] Training Batch [278/391]: Loss 0.02215883880853653\n",
      "[Epoch 22] Training Batch [279/391]: Loss 0.05017249286174774\n",
      "[Epoch 22] Training Batch [280/391]: Loss 0.02788403071463108\n",
      "[Epoch 22] Training Batch [281/391]: Loss 0.0408596508204937\n",
      "[Epoch 22] Training Batch [282/391]: Loss 0.035985998809337616\n",
      "[Epoch 22] Training Batch [283/391]: Loss 0.04748847708106041\n",
      "[Epoch 22] Training Batch [284/391]: Loss 0.01904790848493576\n",
      "[Epoch 22] Training Batch [285/391]: Loss 0.0386287122964859\n",
      "[Epoch 22] Training Batch [286/391]: Loss 0.027609756216406822\n",
      "[Epoch 22] Training Batch [287/391]: Loss 0.05652788281440735\n",
      "[Epoch 22] Training Batch [288/391]: Loss 0.05097002163529396\n",
      "[Epoch 22] Training Batch [289/391]: Loss 0.014928294345736504\n",
      "[Epoch 22] Training Batch [290/391]: Loss 0.027783473953604698\n",
      "[Epoch 22] Training Batch [291/391]: Loss 0.05501154437661171\n",
      "[Epoch 22] Training Batch [292/391]: Loss 0.08382486552000046\n",
      "[Epoch 22] Training Batch [293/391]: Loss 0.024152519181370735\n",
      "[Epoch 22] Training Batch [294/391]: Loss 0.020050978288054466\n",
      "[Epoch 22] Training Batch [295/391]: Loss 0.018840888515114784\n",
      "[Epoch 22] Training Batch [296/391]: Loss 0.013398483395576477\n",
      "[Epoch 22] Training Batch [297/391]: Loss 0.049698442220687866\n",
      "[Epoch 22] Training Batch [298/391]: Loss 0.05133640393614769\n",
      "[Epoch 22] Training Batch [299/391]: Loss 0.027771130204200745\n",
      "[Epoch 22] Training Batch [300/391]: Loss 0.06719038635492325\n",
      "[Epoch 22] Training Batch [301/391]: Loss 0.039158713072538376\n",
      "[Epoch 22] Training Batch [302/391]: Loss 0.021038347855210304\n",
      "[Epoch 22] Training Batch [303/391]: Loss 0.0189770869910717\n",
      "[Epoch 22] Training Batch [304/391]: Loss 0.06000456586480141\n",
      "[Epoch 22] Training Batch [305/391]: Loss 0.017038952559232712\n",
      "[Epoch 22] Training Batch [306/391]: Loss 0.055421508848667145\n",
      "[Epoch 22] Training Batch [307/391]: Loss 0.02902475744485855\n",
      "[Epoch 22] Training Batch [308/391]: Loss 0.017867334187030792\n",
      "[Epoch 22] Training Batch [309/391]: Loss 0.014657890424132347\n",
      "[Epoch 22] Training Batch [310/391]: Loss 0.03199731931090355\n",
      "[Epoch 22] Training Batch [311/391]: Loss 0.06674153357744217\n",
      "[Epoch 22] Training Batch [312/391]: Loss 0.052254073321819305\n",
      "[Epoch 22] Training Batch [313/391]: Loss 0.01304880902171135\n",
      "[Epoch 22] Training Batch [314/391]: Loss 0.046362802386283875\n",
      "[Epoch 22] Training Batch [315/391]: Loss 0.05076489597558975\n",
      "[Epoch 22] Training Batch [316/391]: Loss 0.026176277548074722\n",
      "[Epoch 22] Training Batch [317/391]: Loss 0.03271667659282684\n",
      "[Epoch 22] Training Batch [318/391]: Loss 0.05400192737579346\n",
      "[Epoch 22] Training Batch [319/391]: Loss 0.06719163805246353\n",
      "[Epoch 22] Training Batch [320/391]: Loss 0.0786484107375145\n",
      "[Epoch 22] Training Batch [321/391]: Loss 0.05599020794034004\n",
      "[Epoch 22] Training Batch [322/391]: Loss 0.04446379095315933\n",
      "[Epoch 22] Training Batch [323/391]: Loss 0.032062504440546036\n",
      "[Epoch 22] Training Batch [324/391]: Loss 0.03380667418241501\n",
      "[Epoch 22] Training Batch [325/391]: Loss 0.028934461995959282\n",
      "[Epoch 22] Training Batch [326/391]: Loss 0.021348562091588974\n",
      "[Epoch 22] Training Batch [327/391]: Loss 0.02792256511747837\n",
      "[Epoch 22] Training Batch [328/391]: Loss 0.039310622960329056\n",
      "[Epoch 22] Training Batch [329/391]: Loss 0.009114071726799011\n",
      "[Epoch 22] Training Batch [330/391]: Loss 0.05796345695853233\n",
      "[Epoch 22] Training Batch [331/391]: Loss 0.021135810762643814\n",
      "[Epoch 22] Training Batch [332/391]: Loss 0.01590455137193203\n",
      "[Epoch 22] Training Batch [333/391]: Loss 0.01283684279769659\n",
      "[Epoch 22] Training Batch [334/391]: Loss 0.060016319155693054\n",
      "[Epoch 22] Training Batch [335/391]: Loss 0.024049589410424232\n",
      "[Epoch 22] Training Batch [336/391]: Loss 0.05960490554571152\n",
      "[Epoch 22] Training Batch [337/391]: Loss 0.037012726068496704\n",
      "[Epoch 22] Training Batch [338/391]: Loss 0.037121742963790894\n",
      "[Epoch 22] Training Batch [339/391]: Loss 0.0439421609044075\n",
      "[Epoch 22] Training Batch [340/391]: Loss 0.02339077927172184\n",
      "[Epoch 22] Training Batch [341/391]: Loss 0.025094635784626007\n",
      "[Epoch 22] Training Batch [342/391]: Loss 0.08605968207120895\n",
      "[Epoch 22] Training Batch [343/391]: Loss 0.04741121083498001\n",
      "[Epoch 22] Training Batch [344/391]: Loss 0.009785974398255348\n",
      "[Epoch 22] Training Batch [345/391]: Loss 0.006489952560514212\n",
      "[Epoch 22] Training Batch [346/391]: Loss 0.0500628836452961\n",
      "[Epoch 22] Training Batch [347/391]: Loss 0.024925054982304573\n",
      "[Epoch 22] Training Batch [348/391]: Loss 0.047084562480449677\n",
      "[Epoch 22] Training Batch [349/391]: Loss 0.04751644656062126\n",
      "[Epoch 22] Training Batch [350/391]: Loss 0.01958586648106575\n",
      "[Epoch 22] Training Batch [351/391]: Loss 0.040108878165483475\n",
      "[Epoch 22] Training Batch [352/391]: Loss 0.04007226601243019\n",
      "[Epoch 22] Training Batch [353/391]: Loss 0.012418940663337708\n",
      "[Epoch 22] Training Batch [354/391]: Loss 0.028507012873888016\n",
      "[Epoch 22] Training Batch [355/391]: Loss 0.025212334468960762\n",
      "[Epoch 22] Training Batch [356/391]: Loss 0.047120459377765656\n",
      "[Epoch 22] Training Batch [357/391]: Loss 0.06053060293197632\n",
      "[Epoch 22] Training Batch [358/391]: Loss 0.023215165361762047\n",
      "[Epoch 22] Training Batch [359/391]: Loss 0.02371428906917572\n",
      "[Epoch 22] Training Batch [360/391]: Loss 0.026093702763319016\n",
      "[Epoch 22] Training Batch [361/391]: Loss 0.09655383229255676\n",
      "[Epoch 22] Training Batch [362/391]: Loss 0.03963494673371315\n",
      "[Epoch 22] Training Batch [363/391]: Loss 0.036173030734062195\n",
      "[Epoch 22] Training Batch [364/391]: Loss 0.038605522364377975\n",
      "[Epoch 22] Training Batch [365/391]: Loss 0.04551122710108757\n",
      "[Epoch 22] Training Batch [366/391]: Loss 0.04963978752493858\n",
      "[Epoch 22] Training Batch [367/391]: Loss 0.05666689947247505\n",
      "[Epoch 22] Training Batch [368/391]: Loss 0.013622799888253212\n",
      "[Epoch 22] Training Batch [369/391]: Loss 0.03532348573207855\n",
      "[Epoch 22] Training Batch [370/391]: Loss 0.013856575824320316\n",
      "[Epoch 22] Training Batch [371/391]: Loss 0.021029476076364517\n",
      "[Epoch 22] Training Batch [372/391]: Loss 0.01414536964148283\n",
      "[Epoch 22] Training Batch [373/391]: Loss 0.025955814868211746\n",
      "[Epoch 22] Training Batch [374/391]: Loss 0.032263174653053284\n",
      "[Epoch 22] Training Batch [375/391]: Loss 0.044892486184835434\n",
      "[Epoch 22] Training Batch [376/391]: Loss 0.020806116983294487\n",
      "[Epoch 22] Training Batch [377/391]: Loss 0.04198457673192024\n",
      "[Epoch 22] Training Batch [378/391]: Loss 0.0244692824780941\n",
      "[Epoch 22] Training Batch [379/391]: Loss 0.10267861932516098\n",
      "[Epoch 22] Training Batch [380/391]: Loss 0.06350797414779663\n",
      "[Epoch 22] Training Batch [381/391]: Loss 0.025964360684156418\n",
      "[Epoch 22] Training Batch [382/391]: Loss 0.022893890738487244\n",
      "[Epoch 22] Training Batch [383/391]: Loss 0.06266086548566818\n",
      "[Epoch 22] Training Batch [384/391]: Loss 0.03383062407374382\n",
      "[Epoch 22] Training Batch [385/391]: Loss 0.01986171491444111\n",
      "[Epoch 22] Training Batch [386/391]: Loss 0.02275022119283676\n",
      "[Epoch 22] Training Batch [387/391]: Loss 0.037500180304050446\n",
      "[Epoch 22] Training Batch [388/391]: Loss 0.02764810249209404\n",
      "[Epoch 22] Training Batch [389/391]: Loss 0.04049117490649223\n",
      "[Epoch 22] Training Batch [390/391]: Loss 0.04039157181978226\n",
      "[Epoch 22] Training Batch [391/391]: Loss 0.024541446939110756\n",
      "Epoch 22 - Train Loss: 0.0297\n",
      "*********  Epoch 23/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Training Batch [1/391]: Loss 0.007916305214166641\n",
      "[Epoch 23] Training Batch [2/391]: Loss 0.04580157995223999\n",
      "[Epoch 23] Training Batch [3/391]: Loss 0.03475324064493179\n",
      "[Epoch 23] Training Batch [4/391]: Loss 0.011642087250947952\n",
      "[Epoch 23] Training Batch [5/391]: Loss 0.019965585321187973\n",
      "[Epoch 23] Training Batch [6/391]: Loss 0.04312997683882713\n",
      "[Epoch 23] Training Batch [7/391]: Loss 0.02722429484128952\n",
      "[Epoch 23] Training Batch [8/391]: Loss 0.02919561043381691\n",
      "[Epoch 23] Training Batch [9/391]: Loss 0.009669966995716095\n",
      "[Epoch 23] Training Batch [10/391]: Loss 0.008437363430857658\n",
      "[Epoch 23] Training Batch [11/391]: Loss 0.057580508291721344\n",
      "[Epoch 23] Training Batch [12/391]: Loss 0.027574272826313972\n",
      "[Epoch 23] Training Batch [13/391]: Loss 0.03326089680194855\n",
      "[Epoch 23] Training Batch [14/391]: Loss 0.013493125326931477\n",
      "[Epoch 23] Training Batch [15/391]: Loss 0.015630921348929405\n",
      "[Epoch 23] Training Batch [16/391]: Loss 0.010311529971659184\n",
      "[Epoch 23] Training Batch [17/391]: Loss 0.03190438076853752\n",
      "[Epoch 23] Training Batch [18/391]: Loss 0.017762381583452225\n",
      "[Epoch 23] Training Batch [19/391]: Loss 0.006030229851603508\n",
      "[Epoch 23] Training Batch [20/391]: Loss 0.016267241910099983\n",
      "[Epoch 23] Training Batch [21/391]: Loss 0.01636780984699726\n",
      "[Epoch 23] Training Batch [22/391]: Loss 0.025585804134607315\n",
      "[Epoch 23] Training Batch [23/391]: Loss 0.03459056466817856\n",
      "[Epoch 23] Training Batch [24/391]: Loss 0.041977085173130035\n",
      "[Epoch 23] Training Batch [25/391]: Loss 0.04820773005485535\n",
      "[Epoch 23] Training Batch [26/391]: Loss 0.013673260807991028\n",
      "[Epoch 23] Training Batch [27/391]: Loss 0.040354568511247635\n",
      "[Epoch 23] Training Batch [28/391]: Loss 0.04750566929578781\n",
      "[Epoch 23] Training Batch [29/391]: Loss 0.03144057095050812\n",
      "[Epoch 23] Training Batch [30/391]: Loss 0.008616743609309196\n",
      "[Epoch 23] Training Batch [31/391]: Loss 0.08037630468606949\n",
      "[Epoch 23] Training Batch [32/391]: Loss 0.01646539568901062\n",
      "[Epoch 23] Training Batch [33/391]: Loss 0.0240614116191864\n",
      "[Epoch 23] Training Batch [34/391]: Loss 0.00786132924258709\n",
      "[Epoch 23] Training Batch [35/391]: Loss 0.005280403885990381\n",
      "[Epoch 23] Training Batch [36/391]: Loss 0.02246209979057312\n",
      "[Epoch 23] Training Batch [37/391]: Loss 0.04992121458053589\n",
      "[Epoch 23] Training Batch [38/391]: Loss 0.025645524263381958\n",
      "[Epoch 23] Training Batch [39/391]: Loss 0.03993695601820946\n",
      "[Epoch 23] Training Batch [40/391]: Loss 0.04932614415884018\n",
      "[Epoch 23] Training Batch [41/391]: Loss 0.0033178117591887712\n",
      "[Epoch 23] Training Batch [42/391]: Loss 0.051228996366262436\n",
      "[Epoch 23] Training Batch [43/391]: Loss 0.014095577411353588\n",
      "[Epoch 23] Training Batch [44/391]: Loss 0.08676799386739731\n",
      "[Epoch 23] Training Batch [45/391]: Loss 0.014529845677316189\n",
      "[Epoch 23] Training Batch [46/391]: Loss 0.02869868464767933\n",
      "[Epoch 23] Training Batch [47/391]: Loss 0.022056853398680687\n",
      "[Epoch 23] Training Batch [48/391]: Loss 0.020718097686767578\n",
      "[Epoch 23] Training Batch [49/391]: Loss 0.007394981104880571\n",
      "[Epoch 23] Training Batch [50/391]: Loss 0.03393208608031273\n",
      "[Epoch 23] Training Batch [51/391]: Loss 0.051403697580099106\n",
      "[Epoch 23] Training Batch [52/391]: Loss 0.06055522337555885\n",
      "[Epoch 23] Training Batch [53/391]: Loss 0.010481948964297771\n",
      "[Epoch 23] Training Batch [54/391]: Loss 0.022893311455845833\n",
      "[Epoch 23] Training Batch [55/391]: Loss 0.03538303077220917\n",
      "[Epoch 23] Training Batch [56/391]: Loss 0.009761465713381767\n",
      "[Epoch 23] Training Batch [57/391]: Loss 0.014845902100205421\n",
      "[Epoch 23] Training Batch [58/391]: Loss 0.01621406152844429\n",
      "[Epoch 23] Training Batch [59/391]: Loss 0.02004512958228588\n",
      "[Epoch 23] Training Batch [60/391]: Loss 0.033528801053762436\n",
      "[Epoch 23] Training Batch [61/391]: Loss 0.05062223970890045\n",
      "[Epoch 23] Training Batch [62/391]: Loss 0.03616822510957718\n",
      "[Epoch 23] Training Batch [63/391]: Loss 0.027193574234843254\n",
      "[Epoch 23] Training Batch [64/391]: Loss 0.05708424001932144\n",
      "[Epoch 23] Training Batch [65/391]: Loss 0.025776004418730736\n",
      "[Epoch 23] Training Batch [66/391]: Loss 0.016724400222301483\n",
      "[Epoch 23] Training Batch [67/391]: Loss 0.030460992828011513\n",
      "[Epoch 23] Training Batch [68/391]: Loss 0.032858822494745255\n",
      "[Epoch 23] Training Batch [69/391]: Loss 0.05951901152729988\n",
      "[Epoch 23] Training Batch [70/391]: Loss 0.031265292316675186\n",
      "[Epoch 23] Training Batch [71/391]: Loss 0.009545117616653442\n",
      "[Epoch 23] Training Batch [72/391]: Loss 0.053680870682001114\n",
      "[Epoch 23] Training Batch [73/391]: Loss 0.0030158101581037045\n",
      "[Epoch 23] Training Batch [74/391]: Loss 0.010416066274046898\n",
      "[Epoch 23] Training Batch [75/391]: Loss 0.018769394606351852\n",
      "[Epoch 23] Training Batch [76/391]: Loss 0.03719283267855644\n",
      "[Epoch 23] Training Batch [77/391]: Loss 0.05438656359910965\n",
      "[Epoch 23] Training Batch [78/391]: Loss 0.010719586163759232\n",
      "[Epoch 23] Training Batch [79/391]: Loss 0.07709428668022156\n",
      "[Epoch 23] Training Batch [80/391]: Loss 0.023067142814397812\n",
      "[Epoch 23] Training Batch [81/391]: Loss 0.032971110194921494\n",
      "[Epoch 23] Training Batch [82/391]: Loss 0.011803106404840946\n",
      "[Epoch 23] Training Batch [83/391]: Loss 0.011572765186429024\n",
      "[Epoch 23] Training Batch [84/391]: Loss 0.04276648163795471\n",
      "[Epoch 23] Training Batch [85/391]: Loss 0.02086118794977665\n",
      "[Epoch 23] Training Batch [86/391]: Loss 0.036541648209095\n",
      "[Epoch 23] Training Batch [87/391]: Loss 0.040560539811849594\n",
      "[Epoch 23] Training Batch [88/391]: Loss 0.02728903479874134\n",
      "[Epoch 23] Training Batch [89/391]: Loss 0.037952445447444916\n",
      "[Epoch 23] Training Batch [90/391]: Loss 0.02438489720225334\n",
      "[Epoch 23] Training Batch [91/391]: Loss 0.015282566659152508\n",
      "[Epoch 23] Training Batch [92/391]: Loss 0.01886945217847824\n",
      "[Epoch 23] Training Batch [93/391]: Loss 0.05568839609622955\n",
      "[Epoch 23] Training Batch [94/391]: Loss 0.014414714649319649\n",
      "[Epoch 23] Training Batch [95/391]: Loss 0.0516522154211998\n",
      "[Epoch 23] Training Batch [96/391]: Loss 0.031070180237293243\n",
      "[Epoch 23] Training Batch [97/391]: Loss 0.007516829762607813\n",
      "[Epoch 23] Training Batch [98/391]: Loss 0.03237033635377884\n",
      "[Epoch 23] Training Batch [99/391]: Loss 0.03887062147259712\n",
      "[Epoch 23] Training Batch [100/391]: Loss 0.011548025533556938\n",
      "[Epoch 23] Training Batch [101/391]: Loss 0.027532903477549553\n",
      "[Epoch 23] Training Batch [102/391]: Loss 0.0388137623667717\n",
      "[Epoch 23] Training Batch [103/391]: Loss 0.01851210743188858\n",
      "[Epoch 23] Training Batch [104/391]: Loss 0.023052986711263657\n",
      "[Epoch 23] Training Batch [105/391]: Loss 0.05999022349715233\n",
      "[Epoch 23] Training Batch [106/391]: Loss 0.007673514541238546\n",
      "[Epoch 23] Training Batch [107/391]: Loss 0.03667210042476654\n",
      "[Epoch 23] Training Batch [108/391]: Loss 0.01681193709373474\n",
      "[Epoch 23] Training Batch [109/391]: Loss 0.04882943630218506\n",
      "[Epoch 23] Training Batch [110/391]: Loss 0.01233664434403181\n",
      "[Epoch 23] Training Batch [111/391]: Loss 0.038378868252038956\n",
      "[Epoch 23] Training Batch [112/391]: Loss 0.006765173282474279\n",
      "[Epoch 23] Training Batch [113/391]: Loss 0.050568222999572754\n",
      "[Epoch 23] Training Batch [114/391]: Loss 0.02810438722372055\n",
      "[Epoch 23] Training Batch [115/391]: Loss 0.03395206853747368\n",
      "[Epoch 23] Training Batch [116/391]: Loss 0.011248541995882988\n",
      "[Epoch 23] Training Batch [117/391]: Loss 0.014067092910408974\n",
      "[Epoch 23] Training Batch [118/391]: Loss 0.07968534529209137\n",
      "[Epoch 23] Training Batch [119/391]: Loss 0.024274220690131187\n",
      "[Epoch 23] Training Batch [120/391]: Loss 0.02697627805173397\n",
      "[Epoch 23] Training Batch [121/391]: Loss 0.06609290838241577\n",
      "[Epoch 23] Training Batch [122/391]: Loss 0.037311676889657974\n",
      "[Epoch 23] Training Batch [123/391]: Loss 0.037172142416238785\n",
      "[Epoch 23] Training Batch [124/391]: Loss 0.016124669462442398\n",
      "[Epoch 23] Training Batch [125/391]: Loss 0.01751188561320305\n",
      "[Epoch 23] Training Batch [126/391]: Loss 0.06257318705320358\n",
      "[Epoch 23] Training Batch [127/391]: Loss 0.06475059688091278\n",
      "[Epoch 23] Training Batch [128/391]: Loss 0.03916893154382706\n",
      "[Epoch 23] Training Batch [129/391]: Loss 0.0454721674323082\n",
      "[Epoch 23] Training Batch [130/391]: Loss 0.02728911302983761\n",
      "[Epoch 23] Training Batch [131/391]: Loss 0.06711459904909134\n",
      "[Epoch 23] Training Batch [132/391]: Loss 0.008072013966739178\n",
      "[Epoch 23] Training Batch [133/391]: Loss 0.01729206182062626\n",
      "[Epoch 23] Training Batch [134/391]: Loss 0.006282269023358822\n",
      "[Epoch 23] Training Batch [135/391]: Loss 0.0685560554265976\n",
      "[Epoch 23] Training Batch [136/391]: Loss 0.07430269569158554\n",
      "[Epoch 23] Training Batch [137/391]: Loss 0.08237611502408981\n",
      "[Epoch 23] Training Batch [138/391]: Loss 0.044188521802425385\n",
      "[Epoch 23] Training Batch [139/391]: Loss 0.026005124673247337\n",
      "[Epoch 23] Training Batch [140/391]: Loss 0.012804673984646797\n",
      "[Epoch 23] Training Batch [141/391]: Loss 0.020314740017056465\n",
      "[Epoch 23] Training Batch [142/391]: Loss 0.03540855273604393\n",
      "[Epoch 23] Training Batch [143/391]: Loss 0.025524796918034554\n",
      "[Epoch 23] Training Batch [144/391]: Loss 0.044979341328144073\n",
      "[Epoch 23] Training Batch [145/391]: Loss 0.03281252458691597\n",
      "[Epoch 23] Training Batch [146/391]: Loss 0.03957073763012886\n",
      "[Epoch 23] Training Batch [147/391]: Loss 0.0021378067322075367\n",
      "[Epoch 23] Training Batch [148/391]: Loss 0.036179590970277786\n",
      "[Epoch 23] Training Batch [149/391]: Loss 0.037874214351177216\n",
      "[Epoch 23] Training Batch [150/391]: Loss 0.030023721978068352\n",
      "[Epoch 23] Training Batch [151/391]: Loss 0.013571631163358688\n",
      "[Epoch 23] Training Batch [152/391]: Loss 0.028820956125855446\n",
      "[Epoch 23] Training Batch [153/391]: Loss 0.018973205238580704\n",
      "[Epoch 23] Training Batch [154/391]: Loss 0.011592368595302105\n",
      "[Epoch 23] Training Batch [155/391]: Loss 0.0292708370834589\n",
      "[Epoch 23] Training Batch [156/391]: Loss 0.02945699915289879\n",
      "[Epoch 23] Training Batch [157/391]: Loss 0.008142337203025818\n",
      "[Epoch 23] Training Batch [158/391]: Loss 0.008015062659978867\n",
      "[Epoch 23] Training Batch [159/391]: Loss 0.00824550911784172\n",
      "[Epoch 23] Training Batch [160/391]: Loss 0.03881077840924263\n",
      "[Epoch 23] Training Batch [161/391]: Loss 0.038766197860240936\n",
      "[Epoch 23] Training Batch [162/391]: Loss 0.011272326111793518\n",
      "[Epoch 23] Training Batch [163/391]: Loss 0.0117037920281291\n",
      "[Epoch 23] Training Batch [164/391]: Loss 0.029829269275069237\n",
      "[Epoch 23] Training Batch [165/391]: Loss 0.059594787657260895\n",
      "[Epoch 23] Training Batch [166/391]: Loss 0.08412916213274002\n",
      "[Epoch 23] Training Batch [167/391]: Loss 0.025925852358341217\n",
      "[Epoch 23] Training Batch [168/391]: Loss 0.06420175731182098\n",
      "[Epoch 23] Training Batch [169/391]: Loss 0.015112776309251785\n",
      "[Epoch 23] Training Batch [170/391]: Loss 0.009930106811225414\n",
      "[Epoch 23] Training Batch [171/391]: Loss 0.03788139671087265\n",
      "[Epoch 23] Training Batch [172/391]: Loss 0.0060427566058933735\n",
      "[Epoch 23] Training Batch [173/391]: Loss 0.013076769188046455\n",
      "[Epoch 23] Training Batch [174/391]: Loss 0.04830101132392883\n",
      "[Epoch 23] Training Batch [175/391]: Loss 0.007383421063423157\n",
      "[Epoch 23] Training Batch [176/391]: Loss 0.010810171253979206\n",
      "[Epoch 23] Training Batch [177/391]: Loss 0.011799867264926434\n",
      "[Epoch 23] Training Batch [178/391]: Loss 0.004509110935032368\n",
      "[Epoch 23] Training Batch [179/391]: Loss 0.026424655690789223\n",
      "[Epoch 23] Training Batch [180/391]: Loss 0.027145516127347946\n",
      "[Epoch 23] Training Batch [181/391]: Loss 0.0226158257573843\n",
      "[Epoch 23] Training Batch [182/391]: Loss 0.038620900362730026\n",
      "[Epoch 23] Training Batch [183/391]: Loss 0.04877514764666557\n",
      "[Epoch 23] Training Batch [184/391]: Loss 0.012116190046072006\n",
      "[Epoch 23] Training Batch [185/391]: Loss 0.016704080626368523\n",
      "[Epoch 23] Training Batch [186/391]: Loss 0.01633540354669094\n",
      "[Epoch 23] Training Batch [187/391]: Loss 0.04811401665210724\n",
      "[Epoch 23] Training Batch [188/391]: Loss 0.025511400774121284\n",
      "[Epoch 23] Training Batch [189/391]: Loss 0.04723219946026802\n",
      "[Epoch 23] Training Batch [190/391]: Loss 0.00877551268786192\n",
      "[Epoch 23] Training Batch [191/391]: Loss 0.05653245747089386\n",
      "[Epoch 23] Training Batch [192/391]: Loss 0.012308348901569843\n",
      "[Epoch 23] Training Batch [193/391]: Loss 0.03086869604885578\n",
      "[Epoch 23] Training Batch [194/391]: Loss 0.022008292376995087\n",
      "[Epoch 23] Training Batch [195/391]: Loss 0.019162820652127266\n",
      "[Epoch 23] Training Batch [196/391]: Loss 0.01002195943146944\n",
      "[Epoch 23] Training Batch [197/391]: Loss 0.018548717722296715\n",
      "[Epoch 23] Training Batch [198/391]: Loss 0.007362954318523407\n",
      "[Epoch 23] Training Batch [199/391]: Loss 0.01111198216676712\n",
      "[Epoch 23] Training Batch [200/391]: Loss 0.059231389313936234\n",
      "[Epoch 23] Training Batch [201/391]: Loss 0.025081664323806763\n",
      "[Epoch 23] Training Batch [202/391]: Loss 0.008469943888485432\n",
      "[Epoch 23] Training Batch [203/391]: Loss 0.04372388496994972\n",
      "[Epoch 23] Training Batch [204/391]: Loss 0.008112401701509953\n",
      "[Epoch 23] Training Batch [205/391]: Loss 0.01862187497317791\n",
      "[Epoch 23] Training Batch [206/391]: Loss 0.003451472846791148\n",
      "[Epoch 23] Training Batch [207/391]: Loss 0.01252591423690319\n",
      "[Epoch 23] Training Batch [208/391]: Loss 0.009717789478600025\n",
      "[Epoch 23] Training Batch [209/391]: Loss 0.014352594502270222\n",
      "[Epoch 23] Training Batch [210/391]: Loss 0.026249412447214127\n",
      "[Epoch 23] Training Batch [211/391]: Loss 0.02388840541243553\n",
      "[Epoch 23] Training Batch [212/391]: Loss 0.05354154855012894\n",
      "[Epoch 23] Training Batch [213/391]: Loss 0.032555196434259415\n",
      "[Epoch 23] Training Batch [214/391]: Loss 0.03154042363166809\n",
      "[Epoch 23] Training Batch [215/391]: Loss 0.026221448555588722\n",
      "[Epoch 23] Training Batch [216/391]: Loss 0.05085757002234459\n",
      "[Epoch 23] Training Batch [217/391]: Loss 0.05327707529067993\n",
      "[Epoch 23] Training Batch [218/391]: Loss 0.007997683249413967\n",
      "[Epoch 23] Training Batch [219/391]: Loss 0.007217145059257746\n",
      "[Epoch 23] Training Batch [220/391]: Loss 0.012125580571591854\n",
      "[Epoch 23] Training Batch [221/391]: Loss 0.00988790299743414\n",
      "[Epoch 23] Training Batch [222/391]: Loss 0.02054809033870697\n",
      "[Epoch 23] Training Batch [223/391]: Loss 0.061188142746686935\n",
      "[Epoch 23] Training Batch [224/391]: Loss 0.00825836043804884\n",
      "[Epoch 23] Training Batch [225/391]: Loss 0.05155184864997864\n",
      "[Epoch 23] Training Batch [226/391]: Loss 0.008475081995129585\n",
      "[Epoch 23] Training Batch [227/391]: Loss 0.013537839986383915\n",
      "[Epoch 23] Training Batch [228/391]: Loss 0.021629802882671356\n",
      "[Epoch 23] Training Batch [229/391]: Loss 0.016057264059782028\n",
      "[Epoch 23] Training Batch [230/391]: Loss 0.021456139162182808\n",
      "[Epoch 23] Training Batch [231/391]: Loss 0.015935828909277916\n",
      "[Epoch 23] Training Batch [232/391]: Loss 0.0030553629621863365\n",
      "[Epoch 23] Training Batch [233/391]: Loss 0.030617639422416687\n",
      "[Epoch 23] Training Batch [234/391]: Loss 0.03875485062599182\n",
      "[Epoch 23] Training Batch [235/391]: Loss 0.047494109719991684\n",
      "[Epoch 23] Training Batch [236/391]: Loss 0.0032638825941830873\n",
      "[Epoch 23] Training Batch [237/391]: Loss 0.03661402314901352\n",
      "[Epoch 23] Training Batch [238/391]: Loss 0.098481684923172\n",
      "[Epoch 23] Training Batch [239/391]: Loss 0.006522401701658964\n",
      "[Epoch 23] Training Batch [240/391]: Loss 0.027586570009589195\n",
      "[Epoch 23] Training Batch [241/391]: Loss 0.0343288779258728\n",
      "[Epoch 23] Training Batch [242/391]: Loss 0.028519602492451668\n",
      "[Epoch 23] Training Batch [243/391]: Loss 0.00508276978507638\n",
      "[Epoch 23] Training Batch [244/391]: Loss 0.008766965009272099\n",
      "[Epoch 23] Training Batch [245/391]: Loss 0.025380056351423264\n",
      "[Epoch 23] Training Batch [246/391]: Loss 0.038024451583623886\n",
      "[Epoch 23] Training Batch [247/391]: Loss 0.034664660692214966\n",
      "[Epoch 23] Training Batch [248/391]: Loss 0.032682761549949646\n",
      "[Epoch 23] Training Batch [249/391]: Loss 0.01229938119649887\n",
      "[Epoch 23] Training Batch [250/391]: Loss 0.03047523833811283\n",
      "[Epoch 23] Training Batch [251/391]: Loss 0.03355630114674568\n",
      "[Epoch 23] Training Batch [252/391]: Loss 0.06316106766462326\n",
      "[Epoch 23] Training Batch [253/391]: Loss 0.044942885637283325\n",
      "[Epoch 23] Training Batch [254/391]: Loss 0.026389839127659798\n",
      "[Epoch 23] Training Batch [255/391]: Loss 0.026779470965266228\n",
      "[Epoch 23] Training Batch [256/391]: Loss 0.02235729619860649\n",
      "[Epoch 23] Training Batch [257/391]: Loss 0.029942486435174942\n",
      "[Epoch 23] Training Batch [258/391]: Loss 0.006707843393087387\n",
      "[Epoch 23] Training Batch [259/391]: Loss 0.05425650626420975\n",
      "[Epoch 23] Training Batch [260/391]: Loss 0.011885627172887325\n",
      "[Epoch 23] Training Batch [261/391]: Loss 0.010991393588483334\n",
      "[Epoch 23] Training Batch [262/391]: Loss 0.008081929758191109\n",
      "[Epoch 23] Training Batch [263/391]: Loss 0.01892979070544243\n",
      "[Epoch 23] Training Batch [264/391]: Loss 0.05742010846734047\n",
      "[Epoch 23] Training Batch [265/391]: Loss 0.019636252894997597\n",
      "[Epoch 23] Training Batch [266/391]: Loss 0.017769508063793182\n",
      "[Epoch 23] Training Batch [267/391]: Loss 0.022533150389790535\n",
      "[Epoch 23] Training Batch [268/391]: Loss 0.025451814755797386\n",
      "[Epoch 23] Training Batch [269/391]: Loss 0.09214583039283752\n",
      "[Epoch 23] Training Batch [270/391]: Loss 0.03662024438381195\n",
      "[Epoch 23] Training Batch [271/391]: Loss 0.08417371660470963\n",
      "[Epoch 23] Training Batch [272/391]: Loss 0.00915224477648735\n",
      "[Epoch 23] Training Batch [273/391]: Loss 0.016863463446497917\n",
      "[Epoch 23] Training Batch [274/391]: Loss 0.08844579756259918\n",
      "[Epoch 23] Training Batch [275/391]: Loss 0.04535175859928131\n",
      "[Epoch 23] Training Batch [276/391]: Loss 0.013493306003510952\n",
      "[Epoch 23] Training Batch [277/391]: Loss 0.03639170154929161\n",
      "[Epoch 23] Training Batch [278/391]: Loss 0.016600869596004486\n",
      "[Epoch 23] Training Batch [279/391]: Loss 0.038187433034181595\n",
      "[Epoch 23] Training Batch [280/391]: Loss 0.14068645238876343\n",
      "[Epoch 23] Training Batch [281/391]: Loss 0.10605160146951675\n",
      "[Epoch 23] Training Batch [282/391]: Loss 0.006674860138446093\n",
      "[Epoch 23] Training Batch [283/391]: Loss 0.04472017288208008\n",
      "[Epoch 23] Training Batch [284/391]: Loss 0.005750246345996857\n",
      "[Epoch 23] Training Batch [285/391]: Loss 0.01824350655078888\n",
      "[Epoch 23] Training Batch [286/391]: Loss 0.0087096206843853\n",
      "[Epoch 23] Training Batch [287/391]: Loss 0.09091795980930328\n",
      "[Epoch 23] Training Batch [288/391]: Loss 0.06505312770605087\n",
      "[Epoch 23] Training Batch [289/391]: Loss 0.10030652582645416\n",
      "[Epoch 23] Training Batch [290/391]: Loss 0.054768092930316925\n",
      "[Epoch 23] Training Batch [291/391]: Loss 0.033616676926612854\n",
      "[Epoch 23] Training Batch [292/391]: Loss 0.039437878876924515\n",
      "[Epoch 23] Training Batch [293/391]: Loss 0.046938154846429825\n",
      "[Epoch 23] Training Batch [294/391]: Loss 0.01637423038482666\n",
      "[Epoch 23] Training Batch [295/391]: Loss 0.013840739615261555\n",
      "[Epoch 23] Training Batch [296/391]: Loss 0.014449787326157093\n",
      "[Epoch 23] Training Batch [297/391]: Loss 0.0454317070543766\n",
      "[Epoch 23] Training Batch [298/391]: Loss 0.0749599039554596\n",
      "[Epoch 23] Training Batch [299/391]: Loss 0.034097637981176376\n",
      "[Epoch 23] Training Batch [300/391]: Loss 0.06452205032110214\n",
      "[Epoch 23] Training Batch [301/391]: Loss 0.06065257266163826\n",
      "[Epoch 23] Training Batch [302/391]: Loss 0.024628598242998123\n",
      "[Epoch 23] Training Batch [303/391]: Loss 0.011780024506151676\n",
      "[Epoch 23] Training Batch [304/391]: Loss 0.02785848267376423\n",
      "[Epoch 23] Training Batch [305/391]: Loss 0.04173693433403969\n",
      "[Epoch 23] Training Batch [306/391]: Loss 0.036234207451343536\n",
      "[Epoch 23] Training Batch [307/391]: Loss 0.04532395303249359\n",
      "[Epoch 23] Training Batch [308/391]: Loss 0.010585486888885498\n",
      "[Epoch 23] Training Batch [309/391]: Loss 0.026673123240470886\n",
      "[Epoch 23] Training Batch [310/391]: Loss 0.008296906016767025\n",
      "[Epoch 23] Training Batch [311/391]: Loss 0.05693613737821579\n",
      "[Epoch 23] Training Batch [312/391]: Loss 0.00958905927836895\n",
      "[Epoch 23] Training Batch [313/391]: Loss 0.021820073947310448\n",
      "[Epoch 23] Training Batch [314/391]: Loss 0.022923294454813004\n",
      "[Epoch 23] Training Batch [315/391]: Loss 0.06308446824550629\n",
      "[Epoch 23] Training Batch [316/391]: Loss 0.01713191159069538\n",
      "[Epoch 23] Training Batch [317/391]: Loss 0.014362595975399017\n",
      "[Epoch 23] Training Batch [318/391]: Loss 0.030966388061642647\n",
      "[Epoch 23] Training Batch [319/391]: Loss 0.01796148158609867\n",
      "[Epoch 23] Training Batch [320/391]: Loss 0.02533087506890297\n",
      "[Epoch 23] Training Batch [321/391]: Loss 0.04524495452642441\n",
      "[Epoch 23] Training Batch [322/391]: Loss 0.019254308193922043\n",
      "[Epoch 23] Training Batch [323/391]: Loss 0.025911612436175346\n",
      "[Epoch 23] Training Batch [324/391]: Loss 0.015418400056660175\n",
      "[Epoch 23] Training Batch [325/391]: Loss 0.02232312224805355\n",
      "[Epoch 23] Training Batch [326/391]: Loss 0.009920548647642136\n",
      "[Epoch 23] Training Batch [327/391]: Loss 0.06521143019199371\n",
      "[Epoch 23] Training Batch [328/391]: Loss 0.016930921003222466\n",
      "[Epoch 23] Training Batch [329/391]: Loss 0.051473818719387054\n",
      "[Epoch 23] Training Batch [330/391]: Loss 0.009738750755786896\n",
      "[Epoch 23] Training Batch [331/391]: Loss 0.06506791710853577\n",
      "[Epoch 23] Training Batch [332/391]: Loss 0.04020068049430847\n",
      "[Epoch 23] Training Batch [333/391]: Loss 0.019605685025453568\n",
      "[Epoch 23] Training Batch [334/391]: Loss 0.027625681832432747\n",
      "[Epoch 23] Training Batch [335/391]: Loss 0.021853502839803696\n",
      "[Epoch 23] Training Batch [336/391]: Loss 0.03008236177265644\n",
      "[Epoch 23] Training Batch [337/391]: Loss 0.03635011240839958\n",
      "[Epoch 23] Training Batch [338/391]: Loss 0.012859788723289967\n",
      "[Epoch 23] Training Batch [339/391]: Loss 0.037570416927337646\n",
      "[Epoch 23] Training Batch [340/391]: Loss 0.019845325499773026\n",
      "[Epoch 23] Training Batch [341/391]: Loss 0.03190712258219719\n",
      "[Epoch 23] Training Batch [342/391]: Loss 0.047417011111974716\n",
      "[Epoch 23] Training Batch [343/391]: Loss 0.014116588979959488\n",
      "[Epoch 23] Training Batch [344/391]: Loss 0.018456680700182915\n",
      "[Epoch 23] Training Batch [345/391]: Loss 0.014328142628073692\n",
      "[Epoch 23] Training Batch [346/391]: Loss 0.02136126533150673\n",
      "[Epoch 23] Training Batch [347/391]: Loss 0.014706066809594631\n",
      "[Epoch 23] Training Batch [348/391]: Loss 0.08191704005002975\n",
      "[Epoch 23] Training Batch [349/391]: Loss 0.02822798304259777\n",
      "[Epoch 23] Training Batch [350/391]: Loss 0.020988479256629944\n",
      "[Epoch 23] Training Batch [351/391]: Loss 0.010082283057272434\n",
      "[Epoch 23] Training Batch [352/391]: Loss 0.00995628722012043\n",
      "[Epoch 23] Training Batch [353/391]: Loss 0.006681712809950113\n",
      "[Epoch 23] Training Batch [354/391]: Loss 0.021798742935061455\n",
      "[Epoch 23] Training Batch [355/391]: Loss 0.010537555441260338\n",
      "[Epoch 23] Training Batch [356/391]: Loss 0.010808102786540985\n",
      "[Epoch 23] Training Batch [357/391]: Loss 0.014900371432304382\n",
      "[Epoch 23] Training Batch [358/391]: Loss 0.08679267764091492\n",
      "[Epoch 23] Training Batch [359/391]: Loss 0.052578166127204895\n",
      "[Epoch 23] Training Batch [360/391]: Loss 0.017123721539974213\n",
      "[Epoch 23] Training Batch [361/391]: Loss 0.022108133882284164\n",
      "[Epoch 23] Training Batch [362/391]: Loss 0.034113869071006775\n",
      "[Epoch 23] Training Batch [363/391]: Loss 0.021966509521007538\n",
      "[Epoch 23] Training Batch [364/391]: Loss 0.03274121135473251\n",
      "[Epoch 23] Training Batch [365/391]: Loss 0.02646910771727562\n",
      "[Epoch 23] Training Batch [366/391]: Loss 0.009121463634073734\n",
      "[Epoch 23] Training Batch [367/391]: Loss 0.017003066837787628\n",
      "[Epoch 23] Training Batch [368/391]: Loss 0.017739854753017426\n",
      "[Epoch 23] Training Batch [369/391]: Loss 0.033832162618637085\n",
      "[Epoch 23] Training Batch [370/391]: Loss 0.009245635941624641\n",
      "[Epoch 23] Training Batch [371/391]: Loss 0.0323668010532856\n",
      "[Epoch 23] Training Batch [372/391]: Loss 0.05020206421613693\n",
      "[Epoch 23] Training Batch [373/391]: Loss 0.012883252464234829\n",
      "[Epoch 23] Training Batch [374/391]: Loss 0.026207590475678444\n",
      "[Epoch 23] Training Batch [375/391]: Loss 0.02445445954799652\n",
      "[Epoch 23] Training Batch [376/391]: Loss 0.03360844403505325\n",
      "[Epoch 23] Training Batch [377/391]: Loss 0.0227578803896904\n",
      "[Epoch 23] Training Batch [378/391]: Loss 0.019486291334033012\n",
      "[Epoch 23] Training Batch [379/391]: Loss 0.012889143079519272\n",
      "[Epoch 23] Training Batch [380/391]: Loss 0.05557925999164581\n",
      "[Epoch 23] Training Batch [381/391]: Loss 0.009705116041004658\n",
      "[Epoch 23] Training Batch [382/391]: Loss 0.009408421814441681\n",
      "[Epoch 23] Training Batch [383/391]: Loss 0.07880068570375443\n",
      "[Epoch 23] Training Batch [384/391]: Loss 0.09088035672903061\n",
      "[Epoch 23] Training Batch [385/391]: Loss 0.007440485991537571\n",
      "[Epoch 23] Training Batch [386/391]: Loss 0.06389990448951721\n",
      "[Epoch 23] Training Batch [387/391]: Loss 0.04087012633681297\n",
      "[Epoch 23] Training Batch [388/391]: Loss 0.009183264337480068\n",
      "[Epoch 23] Training Batch [389/391]: Loss 0.03754613921046257\n",
      "[Epoch 23] Training Batch [390/391]: Loss 0.032627545297145844\n",
      "[Epoch 23] Training Batch [391/391]: Loss 0.0151633620262146\n",
      "Epoch 23 - Train Loss: 0.0301\n",
      "*********  Epoch 24/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Training Batch [1/391]: Loss 0.03565297648310661\n",
      "[Epoch 24] Training Batch [2/391]: Loss 0.042636558413505554\n",
      "[Epoch 24] Training Batch [3/391]: Loss 0.01240624487400055\n",
      "[Epoch 24] Training Batch [4/391]: Loss 0.01894160732626915\n",
      "[Epoch 24] Training Batch [5/391]: Loss 0.03364543616771698\n",
      "[Epoch 24] Training Batch [6/391]: Loss 0.008865433745086193\n",
      "[Epoch 24] Training Batch [7/391]: Loss 0.015715982764959335\n",
      "[Epoch 24] Training Batch [8/391]: Loss 0.010948363691568375\n",
      "[Epoch 24] Training Batch [9/391]: Loss 0.03664816543459892\n",
      "[Epoch 24] Training Batch [10/391]: Loss 0.034239280968904495\n",
      "[Epoch 24] Training Batch [11/391]: Loss 0.07932751625776291\n",
      "[Epoch 24] Training Batch [12/391]: Loss 0.014947442337870598\n",
      "[Epoch 24] Training Batch [13/391]: Loss 0.07111652940511703\n",
      "[Epoch 24] Training Batch [14/391]: Loss 0.0037631129380315542\n",
      "[Epoch 24] Training Batch [15/391]: Loss 0.06686590611934662\n",
      "[Epoch 24] Training Batch [16/391]: Loss 0.02489154040813446\n",
      "[Epoch 24] Training Batch [17/391]: Loss 0.01684042438864708\n",
      "[Epoch 24] Training Batch [18/391]: Loss 0.012615804560482502\n",
      "[Epoch 24] Training Batch [19/391]: Loss 0.0038937153294682503\n",
      "[Epoch 24] Training Batch [20/391]: Loss 0.03314923495054245\n",
      "[Epoch 24] Training Batch [21/391]: Loss 0.027056897059082985\n",
      "[Epoch 24] Training Batch [22/391]: Loss 0.05509540066123009\n",
      "[Epoch 24] Training Batch [23/391]: Loss 0.02021867036819458\n",
      "[Epoch 24] Training Batch [24/391]: Loss 0.007254568859934807\n",
      "[Epoch 24] Training Batch [25/391]: Loss 0.06429412215948105\n",
      "[Epoch 24] Training Batch [26/391]: Loss 0.024079877883195877\n",
      "[Epoch 24] Training Batch [27/391]: Loss 0.008421283215284348\n",
      "[Epoch 24] Training Batch [28/391]: Loss 0.04861288517713547\n",
      "[Epoch 24] Training Batch [29/391]: Loss 0.02334003336727619\n",
      "[Epoch 24] Training Batch [30/391]: Loss 0.034826792776584625\n",
      "[Epoch 24] Training Batch [31/391]: Loss 0.04800444468855858\n",
      "[Epoch 24] Training Batch [32/391]: Loss 0.024242326617240906\n",
      "[Epoch 24] Training Batch [33/391]: Loss 0.03600413724780083\n",
      "[Epoch 24] Training Batch [34/391]: Loss 0.007216327358037233\n",
      "[Epoch 24] Training Batch [35/391]: Loss 0.012724426575005054\n",
      "[Epoch 24] Training Batch [36/391]: Loss 0.027649594470858574\n",
      "[Epoch 24] Training Batch [37/391]: Loss 0.006829006131738424\n",
      "[Epoch 24] Training Batch [38/391]: Loss 0.014642375521361828\n",
      "[Epoch 24] Training Batch [39/391]: Loss 0.04377874732017517\n",
      "[Epoch 24] Training Batch [40/391]: Loss 0.013710645958781242\n",
      "[Epoch 24] Training Batch [41/391]: Loss 0.014099542051553726\n",
      "[Epoch 24] Training Batch [42/391]: Loss 0.011900391429662704\n",
      "[Epoch 24] Training Batch [43/391]: Loss 0.01656932570040226\n",
      "[Epoch 24] Training Batch [44/391]: Loss 0.01692304201424122\n",
      "[Epoch 24] Training Batch [45/391]: Loss 0.05193794518709183\n",
      "[Epoch 24] Training Batch [46/391]: Loss 0.004378017969429493\n",
      "[Epoch 24] Training Batch [47/391]: Loss 0.054916661232709885\n",
      "[Epoch 24] Training Batch [48/391]: Loss 0.014252830296754837\n",
      "[Epoch 24] Training Batch [49/391]: Loss 0.034206610172986984\n",
      "[Epoch 24] Training Batch [50/391]: Loss 0.03065882995724678\n",
      "[Epoch 24] Training Batch [51/391]: Loss 0.017310652881860733\n",
      "[Epoch 24] Training Batch [52/391]: Loss 0.04399925097823143\n",
      "[Epoch 24] Training Batch [53/391]: Loss 0.037413716316223145\n",
      "[Epoch 24] Training Batch [54/391]: Loss 0.016247408464550972\n",
      "[Epoch 24] Training Batch [55/391]: Loss 0.014197994023561478\n",
      "[Epoch 24] Training Batch [56/391]: Loss 0.008012775331735611\n",
      "[Epoch 24] Training Batch [57/391]: Loss 0.009215896017849445\n",
      "[Epoch 24] Training Batch [58/391]: Loss 0.11549414694309235\n",
      "[Epoch 24] Training Batch [59/391]: Loss 0.033916134387254715\n",
      "[Epoch 24] Training Batch [60/391]: Loss 0.010279620997607708\n",
      "[Epoch 24] Training Batch [61/391]: Loss 0.04323422163724899\n",
      "[Epoch 24] Training Batch [62/391]: Loss 0.018865397199988365\n",
      "[Epoch 24] Training Batch [63/391]: Loss 0.04606812074780464\n",
      "[Epoch 24] Training Batch [64/391]: Loss 0.028636714443564415\n",
      "[Epoch 24] Training Batch [65/391]: Loss 0.006387438625097275\n",
      "[Epoch 24] Training Batch [66/391]: Loss 0.059578780084848404\n",
      "[Epoch 24] Training Batch [67/391]: Loss 0.006590505596250296\n",
      "[Epoch 24] Training Batch [68/391]: Loss 0.017392070963978767\n",
      "[Epoch 24] Training Batch [69/391]: Loss 0.010059969499707222\n",
      "[Epoch 24] Training Batch [70/391]: Loss 0.056918974965810776\n",
      "[Epoch 24] Training Batch [71/391]: Loss 0.0029842632357031107\n",
      "[Epoch 24] Training Batch [72/391]: Loss 0.036171477288007736\n",
      "[Epoch 24] Training Batch [73/391]: Loss 0.04419645294547081\n",
      "[Epoch 24] Training Batch [74/391]: Loss 0.007094583474099636\n",
      "[Epoch 24] Training Batch [75/391]: Loss 0.017558056861162186\n",
      "[Epoch 24] Training Batch [76/391]: Loss 0.05515284463763237\n",
      "[Epoch 24] Training Batch [77/391]: Loss 0.0192329753190279\n",
      "[Epoch 24] Training Batch [78/391]: Loss 0.004770717583596706\n",
      "[Epoch 24] Training Batch [79/391]: Loss 0.019871242344379425\n",
      "[Epoch 24] Training Batch [80/391]: Loss 0.018513180315494537\n",
      "[Epoch 24] Training Batch [81/391]: Loss 0.018597718328237534\n",
      "[Epoch 24] Training Batch [82/391]: Loss 0.013064420782029629\n",
      "[Epoch 24] Training Batch [83/391]: Loss 0.009920173324644566\n",
      "[Epoch 24] Training Batch [84/391]: Loss 0.0205997284501791\n",
      "[Epoch 24] Training Batch [85/391]: Loss 0.019745294004678726\n",
      "[Epoch 24] Training Batch [86/391]: Loss 0.00632066372781992\n",
      "[Epoch 24] Training Batch [87/391]: Loss 0.025078481063246727\n",
      "[Epoch 24] Training Batch [88/391]: Loss 0.032194092869758606\n",
      "[Epoch 24] Training Batch [89/391]: Loss 0.013609289191663265\n",
      "[Epoch 24] Training Batch [90/391]: Loss 0.0739559680223465\n",
      "[Epoch 24] Training Batch [91/391]: Loss 0.023378072306513786\n",
      "[Epoch 24] Training Batch [92/391]: Loss 0.011685130186378956\n",
      "[Epoch 24] Training Batch [93/391]: Loss 0.019431645050644875\n",
      "[Epoch 24] Training Batch [94/391]: Loss 0.005594844464212656\n",
      "[Epoch 24] Training Batch [95/391]: Loss 0.0051641580648720264\n",
      "[Epoch 24] Training Batch [96/391]: Loss 0.01702044904232025\n",
      "[Epoch 24] Training Batch [97/391]: Loss 0.013535293750464916\n",
      "[Epoch 24] Training Batch [98/391]: Loss 0.022336259484291077\n",
      "[Epoch 24] Training Batch [99/391]: Loss 0.0393417589366436\n",
      "[Epoch 24] Training Batch [100/391]: Loss 0.026327792555093765\n",
      "[Epoch 24] Training Batch [101/391]: Loss 0.04737699031829834\n",
      "[Epoch 24] Training Batch [102/391]: Loss 0.0035847763065248728\n",
      "[Epoch 24] Training Batch [103/391]: Loss 0.011926824226975441\n",
      "[Epoch 24] Training Batch [104/391]: Loss 0.017262572422623634\n",
      "[Epoch 24] Training Batch [105/391]: Loss 0.017392218112945557\n",
      "[Epoch 24] Training Batch [106/391]: Loss 0.0463520847260952\n",
      "[Epoch 24] Training Batch [107/391]: Loss 0.009486466646194458\n",
      "[Epoch 24] Training Batch [108/391]: Loss 0.00532161770388484\n",
      "[Epoch 24] Training Batch [109/391]: Loss 0.011861679144203663\n",
      "[Epoch 24] Training Batch [110/391]: Loss 0.0038875462487339973\n",
      "[Epoch 24] Training Batch [111/391]: Loss 0.03812047466635704\n",
      "[Epoch 24] Training Batch [112/391]: Loss 0.02315572276711464\n",
      "[Epoch 24] Training Batch [113/391]: Loss 0.011041587218642235\n",
      "[Epoch 24] Training Batch [114/391]: Loss 0.0741237998008728\n",
      "[Epoch 24] Training Batch [115/391]: Loss 0.02704210951924324\n",
      "[Epoch 24] Training Batch [116/391]: Loss 0.0642227828502655\n",
      "[Epoch 24] Training Batch [117/391]: Loss 0.012671783566474915\n",
      "[Epoch 24] Training Batch [118/391]: Loss 0.00360574247315526\n",
      "[Epoch 24] Training Batch [119/391]: Loss 0.0074927918612957\n",
      "[Epoch 24] Training Batch [120/391]: Loss 0.004254640080034733\n",
      "[Epoch 24] Training Batch [121/391]: Loss 0.046376243233680725\n",
      "[Epoch 24] Training Batch [122/391]: Loss 0.01680988445878029\n",
      "[Epoch 24] Training Batch [123/391]: Loss 0.009653879329562187\n",
      "[Epoch 24] Training Batch [124/391]: Loss 0.04193922132253647\n",
      "[Epoch 24] Training Batch [125/391]: Loss 0.0060369404964149\n",
      "[Epoch 24] Training Batch [126/391]: Loss 0.034076008945703506\n",
      "[Epoch 24] Training Batch [127/391]: Loss 0.025987330824136734\n",
      "[Epoch 24] Training Batch [128/391]: Loss 0.03915783390402794\n",
      "[Epoch 24] Training Batch [129/391]: Loss 0.04726015776395798\n",
      "[Epoch 24] Training Batch [130/391]: Loss 0.01886872574687004\n",
      "[Epoch 24] Training Batch [131/391]: Loss 0.04832526296377182\n",
      "[Epoch 24] Training Batch [132/391]: Loss 0.05197034403681755\n",
      "[Epoch 24] Training Batch [133/391]: Loss 0.014690794050693512\n",
      "[Epoch 24] Training Batch [134/391]: Loss 0.03624488040804863\n",
      "[Epoch 24] Training Batch [135/391]: Loss 0.019250724464654922\n",
      "[Epoch 24] Training Batch [136/391]: Loss 0.024751795455813408\n",
      "[Epoch 24] Training Batch [137/391]: Loss 0.12490683794021606\n",
      "[Epoch 24] Training Batch [138/391]: Loss 0.00894584134221077\n",
      "[Epoch 24] Training Batch [139/391]: Loss 0.018656354397535324\n",
      "[Epoch 24] Training Batch [140/391]: Loss 0.020378271117806435\n",
      "[Epoch 24] Training Batch [141/391]: Loss 0.022475680336356163\n",
      "[Epoch 24] Training Batch [142/391]: Loss 0.022542325779795647\n",
      "[Epoch 24] Training Batch [143/391]: Loss 0.014272654429078102\n",
      "[Epoch 24] Training Batch [144/391]: Loss 0.0732000395655632\n",
      "[Epoch 24] Training Batch [145/391]: Loss 0.026502318680286407\n",
      "[Epoch 24] Training Batch [146/391]: Loss 0.01833830587565899\n",
      "[Epoch 24] Training Batch [147/391]: Loss 0.016484765335917473\n",
      "[Epoch 24] Training Batch [148/391]: Loss 0.032043807208538055\n",
      "[Epoch 24] Training Batch [149/391]: Loss 0.01816364750266075\n",
      "[Epoch 24] Training Batch [150/391]: Loss 0.027310777455568314\n",
      "[Epoch 24] Training Batch [151/391]: Loss 0.015361612662672997\n",
      "[Epoch 24] Training Batch [152/391]: Loss 0.06993230432271957\n",
      "[Epoch 24] Training Batch [153/391]: Loss 0.01659318245947361\n",
      "[Epoch 24] Training Batch [154/391]: Loss 0.028048716485500336\n",
      "[Epoch 24] Training Batch [155/391]: Loss 0.034341294318437576\n",
      "[Epoch 24] Training Batch [156/391]: Loss 0.009969662874937057\n",
      "[Epoch 24] Training Batch [157/391]: Loss 0.01284454483538866\n",
      "[Epoch 24] Training Batch [158/391]: Loss 0.015891551971435547\n",
      "[Epoch 24] Training Batch [159/391]: Loss 0.06633081287145615\n",
      "[Epoch 24] Training Batch [160/391]: Loss 0.07249599695205688\n",
      "[Epoch 24] Training Batch [161/391]: Loss 0.004171885550022125\n",
      "[Epoch 24] Training Batch [162/391]: Loss 0.033159732818603516\n",
      "[Epoch 24] Training Batch [163/391]: Loss 0.015504484996199608\n",
      "[Epoch 24] Training Batch [164/391]: Loss 0.033871766179800034\n",
      "[Epoch 24] Training Batch [165/391]: Loss 0.021127814427018166\n",
      "[Epoch 24] Training Batch [166/391]: Loss 0.010217243805527687\n",
      "[Epoch 24] Training Batch [167/391]: Loss 0.00687275780364871\n",
      "[Epoch 24] Training Batch [168/391]: Loss 0.03221040219068527\n",
      "[Epoch 24] Training Batch [169/391]: Loss 0.020399246364831924\n",
      "[Epoch 24] Training Batch [170/391]: Loss 0.03604953736066818\n",
      "[Epoch 24] Training Batch [171/391]: Loss 0.02312222495675087\n",
      "[Epoch 24] Training Batch [172/391]: Loss 0.015459229238331318\n",
      "[Epoch 24] Training Batch [173/391]: Loss 0.04107921943068504\n",
      "[Epoch 24] Training Batch [174/391]: Loss 0.0379837267100811\n",
      "[Epoch 24] Training Batch [175/391]: Loss 0.05479445308446884\n",
      "[Epoch 24] Training Batch [176/391]: Loss 0.00839249137789011\n",
      "[Epoch 24] Training Batch [177/391]: Loss 0.006765897385776043\n",
      "[Epoch 24] Training Batch [178/391]: Loss 0.025747179985046387\n",
      "[Epoch 24] Training Batch [179/391]: Loss 0.023101910948753357\n",
      "[Epoch 24] Training Batch [180/391]: Loss 0.056143756955862045\n",
      "[Epoch 24] Training Batch [181/391]: Loss 0.062063030898571014\n",
      "[Epoch 24] Training Batch [182/391]: Loss 0.016878491267561913\n",
      "[Epoch 24] Training Batch [183/391]: Loss 0.04473792016506195\n",
      "[Epoch 24] Training Batch [184/391]: Loss 0.00833123829215765\n",
      "[Epoch 24] Training Batch [185/391]: Loss 0.02540810965001583\n",
      "[Epoch 24] Training Batch [186/391]: Loss 0.02524431049823761\n",
      "[Epoch 24] Training Batch [187/391]: Loss 0.005296354182064533\n",
      "[Epoch 24] Training Batch [188/391]: Loss 0.029552290216088295\n",
      "[Epoch 24] Training Batch [189/391]: Loss 0.045059576630592346\n",
      "[Epoch 24] Training Batch [190/391]: Loss 0.03633367642760277\n",
      "[Epoch 24] Training Batch [191/391]: Loss 0.005961287766695023\n",
      "[Epoch 24] Training Batch [192/391]: Loss 0.010606505908071995\n",
      "[Epoch 24] Training Batch [193/391]: Loss 0.02748556062579155\n",
      "[Epoch 24] Training Batch [194/391]: Loss 0.035930488258600235\n",
      "[Epoch 24] Training Batch [195/391]: Loss 0.03174256533384323\n",
      "[Epoch 24] Training Batch [196/391]: Loss 0.0144636956974864\n",
      "[Epoch 24] Training Batch [197/391]: Loss 0.02902168408036232\n",
      "[Epoch 24] Training Batch [198/391]: Loss 0.020821740850806236\n",
      "[Epoch 24] Training Batch [199/391]: Loss 0.016585366800427437\n",
      "[Epoch 24] Training Batch [200/391]: Loss 0.04229374974966049\n",
      "[Epoch 24] Training Batch [201/391]: Loss 0.017667949199676514\n",
      "[Epoch 24] Training Batch [202/391]: Loss 0.07744889706373215\n",
      "[Epoch 24] Training Batch [203/391]: Loss 0.006349675823003054\n",
      "[Epoch 24] Training Batch [204/391]: Loss 0.01643083430826664\n",
      "[Epoch 24] Training Batch [205/391]: Loss 0.018343104049563408\n",
      "[Epoch 24] Training Batch [206/391]: Loss 0.0405777245759964\n",
      "[Epoch 24] Training Batch [207/391]: Loss 0.013290289789438248\n",
      "[Epoch 24] Training Batch [208/391]: Loss 0.011129640974104404\n",
      "[Epoch 24] Training Batch [209/391]: Loss 0.0122012784704566\n",
      "[Epoch 24] Training Batch [210/391]: Loss 0.01845294050872326\n",
      "[Epoch 24] Training Batch [211/391]: Loss 0.039765991270542145\n",
      "[Epoch 24] Training Batch [212/391]: Loss 0.011039970442652702\n",
      "[Epoch 24] Training Batch [213/391]: Loss 0.011063304729759693\n",
      "[Epoch 24] Training Batch [214/391]: Loss 0.027584709227085114\n",
      "[Epoch 24] Training Batch [215/391]: Loss 0.006000388879328966\n",
      "[Epoch 24] Training Batch [216/391]: Loss 0.014763430692255497\n",
      "[Epoch 24] Training Batch [217/391]: Loss 0.04247087985277176\n",
      "[Epoch 24] Training Batch [218/391]: Loss 0.006743164267390966\n",
      "[Epoch 24] Training Batch [219/391]: Loss 0.00689902575686574\n",
      "[Epoch 24] Training Batch [220/391]: Loss 0.09590046107769012\n",
      "[Epoch 24] Training Batch [221/391]: Loss 0.02930072322487831\n",
      "[Epoch 24] Training Batch [222/391]: Loss 0.0366063229739666\n",
      "[Epoch 24] Training Batch [223/391]: Loss 0.007919598370790482\n",
      "[Epoch 24] Training Batch [224/391]: Loss 0.006326508708298206\n",
      "[Epoch 24] Training Batch [225/391]: Loss 0.07009924203157425\n",
      "[Epoch 24] Training Batch [226/391]: Loss 0.03441737964749336\n",
      "[Epoch 24] Training Batch [227/391]: Loss 0.01808762550354004\n",
      "[Epoch 24] Training Batch [228/391]: Loss 0.0075171575881540775\n",
      "[Epoch 24] Training Batch [229/391]: Loss 0.009705341421067715\n",
      "[Epoch 24] Training Batch [230/391]: Loss 0.025134162977337837\n",
      "[Epoch 24] Training Batch [231/391]: Loss 0.028489818796515465\n",
      "[Epoch 24] Training Batch [232/391]: Loss 0.03659674525260925\n",
      "[Epoch 24] Training Batch [233/391]: Loss 0.0660373792052269\n",
      "[Epoch 24] Training Batch [234/391]: Loss 0.040880799293518066\n",
      "[Epoch 24] Training Batch [235/391]: Loss 0.015368780121207237\n",
      "[Epoch 24] Training Batch [236/391]: Loss 0.006582868751138449\n",
      "[Epoch 24] Training Batch [237/391]: Loss 0.027936743572354317\n",
      "[Epoch 24] Training Batch [238/391]: Loss 0.007483907975256443\n",
      "[Epoch 24] Training Batch [239/391]: Loss 0.011729706078767776\n",
      "[Epoch 24] Training Batch [240/391]: Loss 0.013694951310753822\n",
      "[Epoch 24] Training Batch [241/391]: Loss 0.03598581254482269\n",
      "[Epoch 24] Training Batch [242/391]: Loss 0.00770140253007412\n",
      "[Epoch 24] Training Batch [243/391]: Loss 0.054842881858348846\n",
      "[Epoch 24] Training Batch [244/391]: Loss 0.024026116356253624\n",
      "[Epoch 24] Training Batch [245/391]: Loss 0.015104694291949272\n",
      "[Epoch 24] Training Batch [246/391]: Loss 0.01525106094777584\n",
      "[Epoch 24] Training Batch [247/391]: Loss 0.01338932104408741\n",
      "[Epoch 24] Training Batch [248/391]: Loss 0.0375298373401165\n",
      "[Epoch 24] Training Batch [249/391]: Loss 0.06059044599533081\n",
      "[Epoch 24] Training Batch [250/391]: Loss 0.008463412523269653\n",
      "[Epoch 24] Training Batch [251/391]: Loss 0.029112620279192924\n",
      "[Epoch 24] Training Batch [252/391]: Loss 0.009138058871030807\n",
      "[Epoch 24] Training Batch [253/391]: Loss 0.013031813316047192\n",
      "[Epoch 24] Training Batch [254/391]: Loss 0.0324954129755497\n",
      "[Epoch 24] Training Batch [255/391]: Loss 0.013607854954898357\n",
      "[Epoch 24] Training Batch [256/391]: Loss 0.015481865033507347\n",
      "[Epoch 24] Training Batch [257/391]: Loss 0.010382977314293385\n",
      "[Epoch 24] Training Batch [258/391]: Loss 0.005738118197768927\n",
      "[Epoch 24] Training Batch [259/391]: Loss 0.0416671484708786\n",
      "[Epoch 24] Training Batch [260/391]: Loss 0.012163604609668255\n",
      "[Epoch 24] Training Batch [261/391]: Loss 0.01633492484688759\n",
      "[Epoch 24] Training Batch [262/391]: Loss 0.01519636157900095\n",
      "[Epoch 24] Training Batch [263/391]: Loss 0.011294739320874214\n",
      "[Epoch 24] Training Batch [264/391]: Loss 0.014553248882293701\n",
      "[Epoch 24] Training Batch [265/391]: Loss 0.015818089246749878\n",
      "[Epoch 24] Training Batch [266/391]: Loss 0.06787943840026855\n",
      "[Epoch 24] Training Batch [267/391]: Loss 0.016811024397611618\n",
      "[Epoch 24] Training Batch [268/391]: Loss 0.021997926756739616\n",
      "[Epoch 24] Training Batch [269/391]: Loss 0.014344768598675728\n",
      "[Epoch 24] Training Batch [270/391]: Loss 0.00791336689144373\n",
      "[Epoch 24] Training Batch [271/391]: Loss 0.02829730696976185\n",
      "[Epoch 24] Training Batch [272/391]: Loss 0.02119048498570919\n",
      "[Epoch 24] Training Batch [273/391]: Loss 0.01544430572539568\n",
      "[Epoch 24] Training Batch [274/391]: Loss 0.006222158204764128\n",
      "[Epoch 24] Training Batch [275/391]: Loss 0.013618833385407925\n",
      "[Epoch 24] Training Batch [276/391]: Loss 0.02497137151658535\n",
      "[Epoch 24] Training Batch [277/391]: Loss 0.008789604529738426\n",
      "[Epoch 24] Training Batch [278/391]: Loss 0.07388830929994583\n",
      "[Epoch 24] Training Batch [279/391]: Loss 0.03767933323979378\n",
      "[Epoch 24] Training Batch [280/391]: Loss 0.008103126659989357\n",
      "[Epoch 24] Training Batch [281/391]: Loss 0.015118776820600033\n",
      "[Epoch 24] Training Batch [282/391]: Loss 0.004928588401526213\n",
      "[Epoch 24] Training Batch [283/391]: Loss 0.03509289771318436\n",
      "[Epoch 24] Training Batch [284/391]: Loss 0.030153989791870117\n",
      "[Epoch 24] Training Batch [285/391]: Loss 0.011747793294489384\n",
      "[Epoch 24] Training Batch [286/391]: Loss 0.017541969195008278\n",
      "[Epoch 24] Training Batch [287/391]: Loss 0.03643022105097771\n",
      "[Epoch 24] Training Batch [288/391]: Loss 0.010672067292034626\n",
      "[Epoch 24] Training Batch [289/391]: Loss 0.047688744962215424\n",
      "[Epoch 24] Training Batch [290/391]: Loss 0.0638323649764061\n",
      "[Epoch 24] Training Batch [291/391]: Loss 0.015564575791358948\n",
      "[Epoch 24] Training Batch [292/391]: Loss 0.05316564068198204\n",
      "[Epoch 24] Training Batch [293/391]: Loss 0.06911075860261917\n",
      "[Epoch 24] Training Batch [294/391]: Loss 0.03341079503297806\n",
      "[Epoch 24] Training Batch [295/391]: Loss 0.012611769139766693\n",
      "[Epoch 24] Training Batch [296/391]: Loss 0.03676121309399605\n",
      "[Epoch 24] Training Batch [297/391]: Loss 0.029869485646486282\n",
      "[Epoch 24] Training Batch [298/391]: Loss 0.024213016033172607\n",
      "[Epoch 24] Training Batch [299/391]: Loss 0.01939537189900875\n",
      "[Epoch 24] Training Batch [300/391]: Loss 0.035368889570236206\n",
      "[Epoch 24] Training Batch [301/391]: Loss 0.05686986818909645\n",
      "[Epoch 24] Training Batch [302/391]: Loss 0.026746578514575958\n",
      "[Epoch 24] Training Batch [303/391]: Loss 0.016800139099359512\n",
      "[Epoch 24] Training Batch [304/391]: Loss 0.08554916083812714\n",
      "[Epoch 24] Training Batch [305/391]: Loss 0.04125865176320076\n",
      "[Epoch 24] Training Batch [306/391]: Loss 0.019907981157302856\n",
      "[Epoch 24] Training Batch [307/391]: Loss 0.023945501074194908\n",
      "[Epoch 24] Training Batch [308/391]: Loss 0.03187783062458038\n",
      "[Epoch 24] Training Batch [309/391]: Loss 0.014113999903202057\n",
      "[Epoch 24] Training Batch [310/391]: Loss 0.016939345747232437\n",
      "[Epoch 24] Training Batch [311/391]: Loss 0.022354530170559883\n",
      "[Epoch 24] Training Batch [312/391]: Loss 0.01941552571952343\n",
      "[Epoch 24] Training Batch [313/391]: Loss 0.02077312581241131\n",
      "[Epoch 24] Training Batch [314/391]: Loss 0.030966131016612053\n",
      "[Epoch 24] Training Batch [315/391]: Loss 0.02235540561378002\n",
      "[Epoch 24] Training Batch [316/391]: Loss 0.00796354841440916\n",
      "[Epoch 24] Training Batch [317/391]: Loss 0.06515281647443771\n",
      "[Epoch 24] Training Batch [318/391]: Loss 0.042847223579883575\n",
      "[Epoch 24] Training Batch [319/391]: Loss 0.056014858186244965\n",
      "[Epoch 24] Training Batch [320/391]: Loss 0.016018861904740334\n",
      "[Epoch 24] Training Batch [321/391]: Loss 0.01358991302549839\n",
      "[Epoch 24] Training Batch [322/391]: Loss 0.018044190481305122\n",
      "[Epoch 24] Training Batch [323/391]: Loss 0.009197866544127464\n",
      "[Epoch 24] Training Batch [324/391]: Loss 0.02235914207994938\n",
      "[Epoch 24] Training Batch [325/391]: Loss 0.029910065233707428\n",
      "[Epoch 24] Training Batch [326/391]: Loss 0.020242249593138695\n",
      "[Epoch 24] Training Batch [327/391]: Loss 0.014124915935099125\n",
      "[Epoch 24] Training Batch [328/391]: Loss 0.013087288476526737\n",
      "[Epoch 24] Training Batch [329/391]: Loss 0.011511161923408508\n",
      "[Epoch 24] Training Batch [330/391]: Loss 0.0200326107442379\n",
      "[Epoch 24] Training Batch [331/391]: Loss 0.01390750426799059\n",
      "[Epoch 24] Training Batch [332/391]: Loss 0.00819569081068039\n",
      "[Epoch 24] Training Batch [333/391]: Loss 0.0029918518848717213\n",
      "[Epoch 24] Training Batch [334/391]: Loss 0.005278915632516146\n",
      "[Epoch 24] Training Batch [335/391]: Loss 0.016794683411717415\n",
      "[Epoch 24] Training Batch [336/391]: Loss 0.012575055472552776\n",
      "[Epoch 24] Training Batch [337/391]: Loss 0.015392597764730453\n",
      "[Epoch 24] Training Batch [338/391]: Loss 0.0384506955742836\n",
      "[Epoch 24] Training Batch [339/391]: Loss 0.008213495835661888\n",
      "[Epoch 24] Training Batch [340/391]: Loss 0.018834900110960007\n",
      "[Epoch 24] Training Batch [341/391]: Loss 0.01713445968925953\n",
      "[Epoch 24] Training Batch [342/391]: Loss 0.02457447163760662\n",
      "[Epoch 24] Training Batch [343/391]: Loss 0.0060224952176213264\n",
      "[Epoch 24] Training Batch [344/391]: Loss 0.03413240984082222\n",
      "[Epoch 24] Training Batch [345/391]: Loss 0.015205573290586472\n",
      "[Epoch 24] Training Batch [346/391]: Loss 0.011474903672933578\n",
      "[Epoch 24] Training Batch [347/391]: Loss 0.01072300598025322\n",
      "[Epoch 24] Training Batch [348/391]: Loss 0.06378395855426788\n",
      "[Epoch 24] Training Batch [349/391]: Loss 0.03721228241920471\n",
      "[Epoch 24] Training Batch [350/391]: Loss 0.013729297555983067\n",
      "[Epoch 24] Training Batch [351/391]: Loss 0.02968328446149826\n",
      "[Epoch 24] Training Batch [352/391]: Loss 0.05037704110145569\n",
      "[Epoch 24] Training Batch [353/391]: Loss 0.013602725230157375\n",
      "[Epoch 24] Training Batch [354/391]: Loss 0.021205704659223557\n",
      "[Epoch 24] Training Batch [355/391]: Loss 0.007361845578998327\n",
      "[Epoch 24] Training Batch [356/391]: Loss 0.026790231466293335\n",
      "[Epoch 24] Training Batch [357/391]: Loss 0.0043420433066785336\n",
      "[Epoch 24] Training Batch [358/391]: Loss 0.12467266619205475\n",
      "[Epoch 24] Training Batch [359/391]: Loss 0.01629759557545185\n",
      "[Epoch 24] Training Batch [360/391]: Loss 0.00738158356398344\n",
      "[Epoch 24] Training Batch [361/391]: Loss 0.017980486154556274\n",
      "[Epoch 24] Training Batch [362/391]: Loss 0.043552882969379425\n",
      "[Epoch 24] Training Batch [363/391]: Loss 0.004228114150464535\n",
      "[Epoch 24] Training Batch [364/391]: Loss 0.05470193922519684\n",
      "[Epoch 24] Training Batch [365/391]: Loss 0.03427586331963539\n",
      "[Epoch 24] Training Batch [366/391]: Loss 0.023642372339963913\n",
      "[Epoch 24] Training Batch [367/391]: Loss 0.025545185431838036\n",
      "[Epoch 24] Training Batch [368/391]: Loss 0.02352420799434185\n",
      "[Epoch 24] Training Batch [369/391]: Loss 0.01863122172653675\n",
      "[Epoch 24] Training Batch [370/391]: Loss 0.030050067231059074\n",
      "[Epoch 24] Training Batch [371/391]: Loss 0.04006163403391838\n",
      "[Epoch 24] Training Batch [372/391]: Loss 0.013238472864031792\n",
      "[Epoch 24] Training Batch [373/391]: Loss 0.010648058727383614\n",
      "[Epoch 24] Training Batch [374/391]: Loss 0.008877982385456562\n",
      "[Epoch 24] Training Batch [375/391]: Loss 0.02108946442604065\n",
      "[Epoch 24] Training Batch [376/391]: Loss 0.07021649926900864\n",
      "[Epoch 24] Training Batch [377/391]: Loss 0.007078238762915134\n",
      "[Epoch 24] Training Batch [378/391]: Loss 0.03204342722892761\n",
      "[Epoch 24] Training Batch [379/391]: Loss 0.028306132182478905\n",
      "[Epoch 24] Training Batch [380/391]: Loss 0.014494171366095543\n",
      "[Epoch 24] Training Batch [381/391]: Loss 0.04596224054694176\n",
      "[Epoch 24] Training Batch [382/391]: Loss 0.09986283630132675\n",
      "[Epoch 24] Training Batch [383/391]: Loss 0.006248523015528917\n",
      "[Epoch 24] Training Batch [384/391]: Loss 0.012882567942142487\n",
      "[Epoch 24] Training Batch [385/391]: Loss 0.06341925263404846\n",
      "[Epoch 24] Training Batch [386/391]: Loss 0.024167824536561966\n",
      "[Epoch 24] Training Batch [387/391]: Loss 0.06838371604681015\n",
      "[Epoch 24] Training Batch [388/391]: Loss 0.02642420306801796\n",
      "[Epoch 24] Training Batch [389/391]: Loss 0.009794171899557114\n",
      "[Epoch 24] Training Batch [390/391]: Loss 0.012193193659186363\n",
      "[Epoch 24] Training Batch [391/391]: Loss 0.049850862473249435\n",
      "Epoch 24 - Train Loss: 0.0263\n",
      "*********  Epoch 25/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Training Batch [1/391]: Loss 0.030948400497436523\n",
      "[Epoch 25] Training Batch [2/391]: Loss 0.02694191224873066\n",
      "[Epoch 25] Training Batch [3/391]: Loss 0.04311038926243782\n",
      "[Epoch 25] Training Batch [4/391]: Loss 0.0302165225148201\n",
      "[Epoch 25] Training Batch [5/391]: Loss 0.035301294177770615\n",
      "[Epoch 25] Training Batch [6/391]: Loss 0.017243770882487297\n",
      "[Epoch 25] Training Batch [7/391]: Loss 0.04840196669101715\n",
      "[Epoch 25] Training Batch [8/391]: Loss 0.003249258501455188\n",
      "[Epoch 25] Training Batch [9/391]: Loss 0.021060649305582047\n",
      "[Epoch 25] Training Batch [10/391]: Loss 0.016933823004364967\n",
      "[Epoch 25] Training Batch [11/391]: Loss 0.022006459534168243\n",
      "[Epoch 25] Training Batch [12/391]: Loss 0.01124074961990118\n",
      "[Epoch 25] Training Batch [13/391]: Loss 0.013248216360807419\n",
      "[Epoch 25] Training Batch [14/391]: Loss 0.021284008398652077\n",
      "[Epoch 25] Training Batch [15/391]: Loss 0.03503275290131569\n",
      "[Epoch 25] Training Batch [16/391]: Loss 0.031727954745292664\n",
      "[Epoch 25] Training Batch [17/391]: Loss 0.01004382036626339\n",
      "[Epoch 25] Training Batch [18/391]: Loss 0.022051502019166946\n",
      "[Epoch 25] Training Batch [19/391]: Loss 0.00791695062071085\n",
      "[Epoch 25] Training Batch [20/391]: Loss 0.007523521780967712\n",
      "[Epoch 25] Training Batch [21/391]: Loss 0.021747998893260956\n",
      "[Epoch 25] Training Batch [22/391]: Loss 0.004586107097566128\n",
      "[Epoch 25] Training Batch [23/391]: Loss 0.04468417912721634\n",
      "[Epoch 25] Training Batch [24/391]: Loss 0.011358136311173439\n",
      "[Epoch 25] Training Batch [25/391]: Loss 0.014312731102108955\n",
      "[Epoch 25] Training Batch [26/391]: Loss 0.021747054532170296\n",
      "[Epoch 25] Training Batch [27/391]: Loss 0.008061864413321018\n",
      "[Epoch 25] Training Batch [28/391]: Loss 0.08639031648635864\n",
      "[Epoch 25] Training Batch [29/391]: Loss 0.06797780841588974\n",
      "[Epoch 25] Training Batch [30/391]: Loss 0.03633718192577362\n",
      "[Epoch 25] Training Batch [31/391]: Loss 0.013196080923080444\n",
      "[Epoch 25] Training Batch [32/391]: Loss 0.025577988475561142\n",
      "[Epoch 25] Training Batch [33/391]: Loss 0.013307280838489532\n",
      "[Epoch 25] Training Batch [34/391]: Loss 0.05243109166622162\n",
      "[Epoch 25] Training Batch [35/391]: Loss 0.061030708253383636\n",
      "[Epoch 25] Training Batch [36/391]: Loss 0.021004296839237213\n",
      "[Epoch 25] Training Batch [37/391]: Loss 0.014163690619170666\n",
      "[Epoch 25] Training Batch [38/391]: Loss 0.05753125250339508\n",
      "[Epoch 25] Training Batch [39/391]: Loss 0.03727440536022186\n",
      "[Epoch 25] Training Batch [40/391]: Loss 0.01652950793504715\n",
      "[Epoch 25] Training Batch [41/391]: Loss 0.027858085930347443\n",
      "[Epoch 25] Training Batch [42/391]: Loss 0.005410727579146624\n",
      "[Epoch 25] Training Batch [43/391]: Loss 0.01582937128841877\n",
      "[Epoch 25] Training Batch [44/391]: Loss 0.03520719334483147\n",
      "[Epoch 25] Training Batch [45/391]: Loss 0.03796021640300751\n",
      "[Epoch 25] Training Batch [46/391]: Loss 0.004316131118685007\n",
      "[Epoch 25] Training Batch [47/391]: Loss 0.03301633521914482\n",
      "[Epoch 25] Training Batch [48/391]: Loss 0.01004821714013815\n",
      "[Epoch 25] Training Batch [49/391]: Loss 0.018531832844018936\n",
      "[Epoch 25] Training Batch [50/391]: Loss 0.05235821753740311\n",
      "[Epoch 25] Training Batch [51/391]: Loss 0.018890120089054108\n",
      "[Epoch 25] Training Batch [52/391]: Loss 0.021148981526494026\n",
      "[Epoch 25] Training Batch [53/391]: Loss 0.008865533396601677\n",
      "[Epoch 25] Training Batch [54/391]: Loss 0.02245166152715683\n",
      "[Epoch 25] Training Batch [55/391]: Loss 0.02542898803949356\n",
      "[Epoch 25] Training Batch [56/391]: Loss 0.025964563712477684\n",
      "[Epoch 25] Training Batch [57/391]: Loss 0.0561387836933136\n",
      "[Epoch 25] Training Batch [58/391]: Loss 0.017403125762939453\n",
      "[Epoch 25] Training Batch [59/391]: Loss 0.025112690404057503\n",
      "[Epoch 25] Training Batch [60/391]: Loss 0.026128537952899933\n",
      "[Epoch 25] Training Batch [61/391]: Loss 0.011501938104629517\n",
      "[Epoch 25] Training Batch [62/391]: Loss 0.015140444971621037\n",
      "[Epoch 25] Training Batch [63/391]: Loss 0.021414343267679214\n",
      "[Epoch 25] Training Batch [64/391]: Loss 0.007919887080788612\n",
      "[Epoch 25] Training Batch [65/391]: Loss 0.02055182307958603\n",
      "[Epoch 25] Training Batch [66/391]: Loss 0.031632304191589355\n",
      "[Epoch 25] Training Batch [67/391]: Loss 0.016151221469044685\n",
      "[Epoch 25] Training Batch [68/391]: Loss 0.031146293506026268\n",
      "[Epoch 25] Training Batch [69/391]: Loss 0.021661099046468735\n",
      "[Epoch 25] Training Batch [70/391]: Loss 0.01585712656378746\n",
      "[Epoch 25] Training Batch [71/391]: Loss 0.013751471415162086\n",
      "[Epoch 25] Training Batch [72/391]: Loss 0.00423695333302021\n",
      "[Epoch 25] Training Batch [73/391]: Loss 0.03240630030632019\n",
      "[Epoch 25] Training Batch [74/391]: Loss 0.004171102307736874\n",
      "[Epoch 25] Training Batch [75/391]: Loss 0.010607290081679821\n",
      "[Epoch 25] Training Batch [76/391]: Loss 0.014858830720186234\n",
      "[Epoch 25] Training Batch [77/391]: Loss 0.006418759468942881\n",
      "[Epoch 25] Training Batch [78/391]: Loss 0.005458159372210503\n",
      "[Epoch 25] Training Batch [79/391]: Loss 0.030199119821190834\n",
      "[Epoch 25] Training Batch [80/391]: Loss 0.021818507462739944\n",
      "[Epoch 25] Training Batch [81/391]: Loss 0.013189498335123062\n",
      "[Epoch 25] Training Batch [82/391]: Loss 0.02106057107448578\n",
      "[Epoch 25] Training Batch [83/391]: Loss 0.017405681312084198\n",
      "[Epoch 25] Training Batch [84/391]: Loss 0.03198924660682678\n",
      "[Epoch 25] Training Batch [85/391]: Loss 0.007822878658771515\n",
      "[Epoch 25] Training Batch [86/391]: Loss 0.010244360193610191\n",
      "[Epoch 25] Training Batch [87/391]: Loss 0.033358361572027206\n",
      "[Epoch 25] Training Batch [88/391]: Loss 0.016131363809108734\n",
      "[Epoch 25] Training Batch [89/391]: Loss 0.013856219127774239\n",
      "[Epoch 25] Training Batch [90/391]: Loss 0.0548606738448143\n",
      "[Epoch 25] Training Batch [91/391]: Loss 0.04648125171661377\n",
      "[Epoch 25] Training Batch [92/391]: Loss 0.00666699418798089\n",
      "[Epoch 25] Training Batch [93/391]: Loss 0.009051056578755379\n",
      "[Epoch 25] Training Batch [94/391]: Loss 0.0531197115778923\n",
      "[Epoch 25] Training Batch [95/391]: Loss 0.022089406847953796\n",
      "[Epoch 25] Training Batch [96/391]: Loss 0.013628446497023106\n",
      "[Epoch 25] Training Batch [97/391]: Loss 0.005004413891583681\n",
      "[Epoch 25] Training Batch [98/391]: Loss 0.010441523045301437\n",
      "[Epoch 25] Training Batch [99/391]: Loss 0.006274172570556402\n",
      "[Epoch 25] Training Batch [100/391]: Loss 0.004898563027381897\n",
      "[Epoch 25] Training Batch [101/391]: Loss 0.0513220876455307\n",
      "[Epoch 25] Training Batch [102/391]: Loss 0.013818662613630295\n",
      "[Epoch 25] Training Batch [103/391]: Loss 0.02608233317732811\n",
      "[Epoch 25] Training Batch [104/391]: Loss 0.004252620041370392\n",
      "[Epoch 25] Training Batch [105/391]: Loss 0.006165402475744486\n",
      "[Epoch 25] Training Batch [106/391]: Loss 0.01616881974041462\n",
      "[Epoch 25] Training Batch [107/391]: Loss 0.0075110746547579765\n",
      "[Epoch 25] Training Batch [108/391]: Loss 0.02027546986937523\n",
      "[Epoch 25] Training Batch [109/391]: Loss 0.012485872022807598\n",
      "[Epoch 25] Training Batch [110/391]: Loss 0.010099759325385094\n",
      "[Epoch 25] Training Batch [111/391]: Loss 0.009867832995951176\n",
      "[Epoch 25] Training Batch [112/391]: Loss 0.026225296780467033\n",
      "[Epoch 25] Training Batch [113/391]: Loss 0.003074959386140108\n",
      "[Epoch 25] Training Batch [114/391]: Loss 0.007461776025593281\n",
      "[Epoch 25] Training Batch [115/391]: Loss 0.014276890084147453\n",
      "[Epoch 25] Training Batch [116/391]: Loss 0.0054071685299277306\n",
      "[Epoch 25] Training Batch [117/391]: Loss 0.010803339071571827\n",
      "[Epoch 25] Training Batch [118/391]: Loss 0.0064665768295526505\n",
      "[Epoch 25] Training Batch [119/391]: Loss 0.0131522286683321\n",
      "[Epoch 25] Training Batch [120/391]: Loss 0.012850349768996239\n",
      "[Epoch 25] Training Batch [121/391]: Loss 0.03603389859199524\n",
      "[Epoch 25] Training Batch [122/391]: Loss 0.00980441365391016\n",
      "[Epoch 25] Training Batch [123/391]: Loss 0.019658733159303665\n",
      "[Epoch 25] Training Batch [124/391]: Loss 0.0025955387391149998\n",
      "[Epoch 25] Training Batch [125/391]: Loss 0.0052071926183998585\n",
      "[Epoch 25] Training Batch [126/391]: Loss 0.05097884684801102\n",
      "[Epoch 25] Training Batch [127/391]: Loss 0.02687167190015316\n",
      "[Epoch 25] Training Batch [128/391]: Loss 0.017640357837080956\n",
      "[Epoch 25] Training Batch [129/391]: Loss 0.0477539487183094\n",
      "[Epoch 25] Training Batch [130/391]: Loss 0.006135348230600357\n",
      "[Epoch 25] Training Batch [131/391]: Loss 0.0026032989844679832\n",
      "[Epoch 25] Training Batch [132/391]: Loss 0.015176628716289997\n",
      "[Epoch 25] Training Batch [133/391]: Loss 0.008221933618187904\n",
      "[Epoch 25] Training Batch [134/391]: Loss 0.004227176308631897\n",
      "[Epoch 25] Training Batch [135/391]: Loss 0.04952787980437279\n",
      "[Epoch 25] Training Batch [136/391]: Loss 0.018249791115522385\n",
      "[Epoch 25] Training Batch [137/391]: Loss 0.007043716963380575\n",
      "[Epoch 25] Training Batch [138/391]: Loss 0.03872041776776314\n",
      "[Epoch 25] Training Batch [139/391]: Loss 0.013688025996088982\n",
      "[Epoch 25] Training Batch [140/391]: Loss 0.004238538909703493\n",
      "[Epoch 25] Training Batch [141/391]: Loss 0.012204433791339397\n",
      "[Epoch 25] Training Batch [142/391]: Loss 0.00180427182931453\n",
      "[Epoch 25] Training Batch [143/391]: Loss 0.0040368433110415936\n",
      "[Epoch 25] Training Batch [144/391]: Loss 0.02709585428237915\n",
      "[Epoch 25] Training Batch [145/391]: Loss 0.0026879936922341585\n",
      "[Epoch 25] Training Batch [146/391]: Loss 0.004614577628672123\n",
      "[Epoch 25] Training Batch [147/391]: Loss 0.003082600189372897\n",
      "[Epoch 25] Training Batch [148/391]: Loss 0.009581943042576313\n",
      "[Epoch 25] Training Batch [149/391]: Loss 0.006850061472505331\n",
      "[Epoch 25] Training Batch [150/391]: Loss 0.002851894125342369\n",
      "[Epoch 25] Training Batch [151/391]: Loss 0.013587651774287224\n",
      "[Epoch 25] Training Batch [152/391]: Loss 0.003199349856004119\n",
      "[Epoch 25] Training Batch [153/391]: Loss 0.012568322010338306\n",
      "[Epoch 25] Training Batch [154/391]: Loss 0.005459294188767672\n",
      "[Epoch 25] Training Batch [155/391]: Loss 0.012338606640696526\n",
      "[Epoch 25] Training Batch [156/391]: Loss 0.024813378229737282\n",
      "[Epoch 25] Training Batch [157/391]: Loss 0.004764216486364603\n",
      "[Epoch 25] Training Batch [158/391]: Loss 0.004019260872155428\n",
      "[Epoch 25] Training Batch [159/391]: Loss 0.01962384395301342\n",
      "[Epoch 25] Training Batch [160/391]: Loss 0.022637013345956802\n",
      "[Epoch 25] Training Batch [161/391]: Loss 0.02157207950949669\n",
      "[Epoch 25] Training Batch [162/391]: Loss 0.007317801006138325\n",
      "[Epoch 25] Training Batch [163/391]: Loss 0.005395423620939255\n",
      "[Epoch 25] Training Batch [164/391]: Loss 0.005095794331282377\n",
      "[Epoch 25] Training Batch [165/391]: Loss 0.010073710232973099\n",
      "[Epoch 25] Training Batch [166/391]: Loss 0.010559383779764175\n",
      "[Epoch 25] Training Batch [167/391]: Loss 0.005637407768517733\n",
      "[Epoch 25] Training Batch [168/391]: Loss 0.00555865652859211\n",
      "[Epoch 25] Training Batch [169/391]: Loss 0.012729580514132977\n",
      "[Epoch 25] Training Batch [170/391]: Loss 0.018450304865837097\n",
      "[Epoch 25] Training Batch [171/391]: Loss 0.008845645003020763\n",
      "[Epoch 25] Training Batch [172/391]: Loss 0.008544529788196087\n",
      "[Epoch 25] Training Batch [173/391]: Loss 0.006457130890339613\n",
      "[Epoch 25] Training Batch [174/391]: Loss 0.015790535137057304\n",
      "[Epoch 25] Training Batch [175/391]: Loss 0.0517272874712944\n",
      "[Epoch 25] Training Batch [176/391]: Loss 0.005871094297617674\n",
      "[Epoch 25] Training Batch [177/391]: Loss 0.0015583381755277514\n",
      "[Epoch 25] Training Batch [178/391]: Loss 0.01458720676600933\n",
      "[Epoch 25] Training Batch [179/391]: Loss 0.013981087133288383\n",
      "[Epoch 25] Training Batch [180/391]: Loss 0.01039610430598259\n",
      "[Epoch 25] Training Batch [181/391]: Loss 0.0018621041672304273\n",
      "[Epoch 25] Training Batch [182/391]: Loss 0.03208296000957489\n",
      "[Epoch 25] Training Batch [183/391]: Loss 0.019921714439988136\n",
      "[Epoch 25] Training Batch [184/391]: Loss 0.033507365733385086\n",
      "[Epoch 25] Training Batch [185/391]: Loss 0.058244846761226654\n",
      "[Epoch 25] Training Batch [186/391]: Loss 0.0021489975042641163\n",
      "[Epoch 25] Training Batch [187/391]: Loss 0.031284622848033905\n",
      "[Epoch 25] Training Batch [188/391]: Loss 0.010563422925770283\n",
      "[Epoch 25] Training Batch [189/391]: Loss 0.018909096717834473\n",
      "[Epoch 25] Training Batch [190/391]: Loss 0.005896889138966799\n",
      "[Epoch 25] Training Batch [191/391]: Loss 0.006341295316815376\n",
      "[Epoch 25] Training Batch [192/391]: Loss 0.018927600234746933\n",
      "[Epoch 25] Training Batch [193/391]: Loss 0.0040258863009512424\n",
      "[Epoch 25] Training Batch [194/391]: Loss 0.008776535280048847\n",
      "[Epoch 25] Training Batch [195/391]: Loss 0.0038118527736514807\n",
      "[Epoch 25] Training Batch [196/391]: Loss 0.011549771763384342\n",
      "[Epoch 25] Training Batch [197/391]: Loss 0.009421544149518013\n",
      "[Epoch 25] Training Batch [198/391]: Loss 0.03493373841047287\n",
      "[Epoch 25] Training Batch [199/391]: Loss 0.04039129987359047\n",
      "[Epoch 25] Training Batch [200/391]: Loss 0.06995735317468643\n",
      "[Epoch 25] Training Batch [201/391]: Loss 0.003729777177795768\n",
      "[Epoch 25] Training Batch [202/391]: Loss 0.005074309650808573\n",
      "[Epoch 25] Training Batch [203/391]: Loss 0.0040346235036849976\n",
      "[Epoch 25] Training Batch [204/391]: Loss 0.02442510984838009\n",
      "[Epoch 25] Training Batch [205/391]: Loss 0.001987407449632883\n",
      "[Epoch 25] Training Batch [206/391]: Loss 0.013622507452964783\n",
      "[Epoch 25] Training Batch [207/391]: Loss 0.0038853867445141077\n",
      "[Epoch 25] Training Batch [208/391]: Loss 0.006505938246846199\n",
      "[Epoch 25] Training Batch [209/391]: Loss 0.05235046148300171\n",
      "[Epoch 25] Training Batch [210/391]: Loss 0.011297550052404404\n",
      "[Epoch 25] Training Batch [211/391]: Loss 0.027966072782874107\n",
      "[Epoch 25] Training Batch [212/391]: Loss 0.0028011787217110395\n",
      "[Epoch 25] Training Batch [213/391]: Loss 0.008162379264831543\n",
      "[Epoch 25] Training Batch [214/391]: Loss 0.022609476000070572\n",
      "[Epoch 25] Training Batch [215/391]: Loss 0.009926613420248032\n",
      "[Epoch 25] Training Batch [216/391]: Loss 0.013041215017437935\n",
      "[Epoch 25] Training Batch [217/391]: Loss 0.004996683914214373\n",
      "[Epoch 25] Training Batch [218/391]: Loss 0.007656577043235302\n",
      "[Epoch 25] Training Batch [219/391]: Loss 0.009584596380591393\n",
      "[Epoch 25] Training Batch [220/391]: Loss 0.0037939068861305714\n",
      "[Epoch 25] Training Batch [221/391]: Loss 0.0016141274245455861\n",
      "[Epoch 25] Training Batch [222/391]: Loss 0.024333320558071136\n",
      "[Epoch 25] Training Batch [223/391]: Loss 0.005144360940903425\n",
      "[Epoch 25] Training Batch [224/391]: Loss 0.014118663966655731\n",
      "[Epoch 25] Training Batch [225/391]: Loss 0.012171619571745396\n",
      "[Epoch 25] Training Batch [226/391]: Loss 0.025776319205760956\n",
      "[Epoch 25] Training Batch [227/391]: Loss 0.005204564891755581\n",
      "[Epoch 25] Training Batch [228/391]: Loss 0.018175514414906502\n",
      "[Epoch 25] Training Batch [229/391]: Loss 0.00891895778477192\n",
      "[Epoch 25] Training Batch [230/391]: Loss 0.0032746666111052036\n",
      "[Epoch 25] Training Batch [231/391]: Loss 0.0208969097584486\n",
      "[Epoch 25] Training Batch [232/391]: Loss 0.01131847221404314\n",
      "[Epoch 25] Training Batch [233/391]: Loss 0.018225349485874176\n",
      "[Epoch 25] Training Batch [234/391]: Loss 0.010725150816142559\n",
      "[Epoch 25] Training Batch [235/391]: Loss 0.024676235392689705\n",
      "[Epoch 25] Training Batch [236/391]: Loss 0.00304573867470026\n",
      "[Epoch 25] Training Batch [237/391]: Loss 0.0035885199904441833\n",
      "[Epoch 25] Training Batch [238/391]: Loss 0.0027470560744404793\n",
      "[Epoch 25] Training Batch [239/391]: Loss 0.008041425608098507\n",
      "[Epoch 25] Training Batch [240/391]: Loss 0.010403276421129704\n",
      "[Epoch 25] Training Batch [241/391]: Loss 0.011756401509046555\n",
      "[Epoch 25] Training Batch [242/391]: Loss 0.0022988205309957266\n",
      "[Epoch 25] Training Batch [243/391]: Loss 0.005118838045746088\n",
      "[Epoch 25] Training Batch [244/391]: Loss 0.005011501256376505\n",
      "[Epoch 25] Training Batch [245/391]: Loss 0.009134773164987564\n",
      "[Epoch 25] Training Batch [246/391]: Loss 0.0012928409269079566\n",
      "[Epoch 25] Training Batch [247/391]: Loss 0.012759429402649403\n",
      "[Epoch 25] Training Batch [248/391]: Loss 0.007254361640661955\n",
      "[Epoch 25] Training Batch [249/391]: Loss 0.0011532121570780873\n",
      "[Epoch 25] Training Batch [250/391]: Loss 0.0019729845225811005\n",
      "[Epoch 25] Training Batch [251/391]: Loss 0.0321071483194828\n",
      "[Epoch 25] Training Batch [252/391]: Loss 0.01726599782705307\n",
      "[Epoch 25] Training Batch [253/391]: Loss 0.007539724465459585\n",
      "[Epoch 25] Training Batch [254/391]: Loss 0.018451077863574028\n",
      "[Epoch 25] Training Batch [255/391]: Loss 0.011771239340305328\n",
      "[Epoch 25] Training Batch [256/391]: Loss 0.017734918743371964\n",
      "[Epoch 25] Training Batch [257/391]: Loss 0.005079412832856178\n",
      "[Epoch 25] Training Batch [258/391]: Loss 0.024150658398866653\n",
      "[Epoch 25] Training Batch [259/391]: Loss 0.015855509787797928\n",
      "[Epoch 25] Training Batch [260/391]: Loss 0.007659561932086945\n",
      "[Epoch 25] Training Batch [261/391]: Loss 0.011534766294062138\n",
      "[Epoch 25] Training Batch [262/391]: Loss 0.009028183296322823\n",
      "[Epoch 25] Training Batch [263/391]: Loss 0.020014876499772072\n",
      "[Epoch 25] Training Batch [264/391]: Loss 0.0122825400903821\n",
      "[Epoch 25] Training Batch [265/391]: Loss 0.006026830989867449\n",
      "[Epoch 25] Training Batch [266/391]: Loss 0.008441572077572346\n",
      "[Epoch 25] Training Batch [267/391]: Loss 0.013611922040581703\n",
      "[Epoch 25] Training Batch [268/391]: Loss 0.10035355389118195\n",
      "[Epoch 25] Training Batch [269/391]: Loss 0.026764271780848503\n",
      "[Epoch 25] Training Batch [270/391]: Loss 0.0173596553504467\n",
      "[Epoch 25] Training Batch [271/391]: Loss 0.0034894212149083614\n",
      "[Epoch 25] Training Batch [272/391]: Loss 0.023308757692575455\n",
      "[Epoch 25] Training Batch [273/391]: Loss 0.01800858974456787\n",
      "[Epoch 25] Training Batch [274/391]: Loss 0.026977555826306343\n",
      "[Epoch 25] Training Batch [275/391]: Loss 0.009325487539172173\n",
      "[Epoch 25] Training Batch [276/391]: Loss 0.022763734683394432\n",
      "[Epoch 25] Training Batch [277/391]: Loss 0.005007460713386536\n",
      "[Epoch 25] Training Batch [278/391]: Loss 0.0030641122721135616\n",
      "[Epoch 25] Training Batch [279/391]: Loss 0.011049066670238972\n",
      "[Epoch 25] Training Batch [280/391]: Loss 0.06739476323127747\n",
      "[Epoch 25] Training Batch [281/391]: Loss 0.03101344220340252\n",
      "[Epoch 25] Training Batch [282/391]: Loss 0.017939269542694092\n",
      "[Epoch 25] Training Batch [283/391]: Loss 0.0251935962587595\n",
      "[Epoch 25] Training Batch [284/391]: Loss 0.009779884479939938\n",
      "[Epoch 25] Training Batch [285/391]: Loss 0.050218675285577774\n",
      "[Epoch 25] Training Batch [286/391]: Loss 0.04579559713602066\n",
      "[Epoch 25] Training Batch [287/391]: Loss 0.01229581143707037\n",
      "[Epoch 25] Training Batch [288/391]: Loss 0.02871151827275753\n",
      "[Epoch 25] Training Batch [289/391]: Loss 0.0283287912607193\n",
      "[Epoch 25] Training Batch [290/391]: Loss 0.04423930123448372\n",
      "[Epoch 25] Training Batch [291/391]: Loss 0.006192509084939957\n",
      "[Epoch 25] Training Batch [292/391]: Loss 0.025332577526569366\n",
      "[Epoch 25] Training Batch [293/391]: Loss 0.019541718065738678\n",
      "[Epoch 25] Training Batch [294/391]: Loss 0.0090137654915452\n",
      "[Epoch 25] Training Batch [295/391]: Loss 0.009462921880185604\n",
      "[Epoch 25] Training Batch [296/391]: Loss 0.02867690846323967\n",
      "[Epoch 25] Training Batch [297/391]: Loss 0.03366387262940407\n",
      "[Epoch 25] Training Batch [298/391]: Loss 0.08393211662769318\n",
      "[Epoch 25] Training Batch [299/391]: Loss 0.012331747449934483\n",
      "[Epoch 25] Training Batch [300/391]: Loss 0.013932065106928349\n",
      "[Epoch 25] Training Batch [301/391]: Loss 0.015134947374463081\n",
      "[Epoch 25] Training Batch [302/391]: Loss 0.014500296674668789\n",
      "[Epoch 25] Training Batch [303/391]: Loss 0.045380864292383194\n",
      "[Epoch 25] Training Batch [304/391]: Loss 0.01822807639837265\n",
      "[Epoch 25] Training Batch [305/391]: Loss 0.003531738417223096\n",
      "[Epoch 25] Training Batch [306/391]: Loss 0.0232969019562006\n",
      "[Epoch 25] Training Batch [307/391]: Loss 0.036934301257133484\n",
      "[Epoch 25] Training Batch [308/391]: Loss 0.014417894184589386\n",
      "[Epoch 25] Training Batch [309/391]: Loss 0.011944699101150036\n",
      "[Epoch 25] Training Batch [310/391]: Loss 0.007752898149192333\n",
      "[Epoch 25] Training Batch [311/391]: Loss 0.04579387232661247\n",
      "[Epoch 25] Training Batch [312/391]: Loss 0.0075850156135857105\n",
      "[Epoch 25] Training Batch [313/391]: Loss 0.022609421983361244\n",
      "[Epoch 25] Training Batch [314/391]: Loss 0.012545566074550152\n",
      "[Epoch 25] Training Batch [315/391]: Loss 0.01067437045276165\n",
      "[Epoch 25] Training Batch [316/391]: Loss 0.05282992124557495\n",
      "[Epoch 25] Training Batch [317/391]: Loss 0.0264179278165102\n",
      "[Epoch 25] Training Batch [318/391]: Loss 0.021710436791181564\n",
      "[Epoch 25] Training Batch [319/391]: Loss 0.03514479100704193\n",
      "[Epoch 25] Training Batch [320/391]: Loss 0.10913213342428207\n",
      "[Epoch 25] Training Batch [321/391]: Loss 0.026977214962244034\n",
      "[Epoch 25] Training Batch [322/391]: Loss 0.0018054418032988906\n",
      "[Epoch 25] Training Batch [323/391]: Loss 0.016130050644278526\n",
      "[Epoch 25] Training Batch [324/391]: Loss 0.01845749281346798\n",
      "[Epoch 25] Training Batch [325/391]: Loss 0.012414774857461452\n",
      "[Epoch 25] Training Batch [326/391]: Loss 0.024764936417341232\n",
      "[Epoch 25] Training Batch [327/391]: Loss 0.014717845246195793\n",
      "[Epoch 25] Training Batch [328/391]: Loss 0.04819357395172119\n",
      "[Epoch 25] Training Batch [329/391]: Loss 0.006091831251978874\n",
      "[Epoch 25] Training Batch [330/391]: Loss 0.01374760176986456\n",
      "[Epoch 25] Training Batch [331/391]: Loss 0.027153559029102325\n",
      "[Epoch 25] Training Batch [332/391]: Loss 0.015428684651851654\n",
      "[Epoch 25] Training Batch [333/391]: Loss 0.0038706178311258554\n",
      "[Epoch 25] Training Batch [334/391]: Loss 0.010644232854247093\n",
      "[Epoch 25] Training Batch [335/391]: Loss 0.028255391865968704\n",
      "[Epoch 25] Training Batch [336/391]: Loss 0.018433116376399994\n",
      "[Epoch 25] Training Batch [337/391]: Loss 0.0016208331799134612\n",
      "[Epoch 25] Training Batch [338/391]: Loss 0.024225786328315735\n",
      "[Epoch 25] Training Batch [339/391]: Loss 0.03986320272088051\n",
      "[Epoch 25] Training Batch [340/391]: Loss 0.02042381279170513\n",
      "[Epoch 25] Training Batch [341/391]: Loss 0.033672310411930084\n",
      "[Epoch 25] Training Batch [342/391]: Loss 0.023600159212946892\n",
      "[Epoch 25] Training Batch [343/391]: Loss 0.0073075443506240845\n",
      "[Epoch 25] Training Batch [344/391]: Loss 0.015901776030659676\n",
      "[Epoch 25] Training Batch [345/391]: Loss 0.022333776578307152\n",
      "[Epoch 25] Training Batch [346/391]: Loss 0.05338014289736748\n",
      "[Epoch 25] Training Batch [347/391]: Loss 0.024227334186434746\n",
      "[Epoch 25] Training Batch [348/391]: Loss 0.01072405744343996\n",
      "[Epoch 25] Training Batch [349/391]: Loss 0.020689288154244423\n",
      "[Epoch 25] Training Batch [350/391]: Loss 0.03368867561221123\n",
      "[Epoch 25] Training Batch [351/391]: Loss 0.03756721690297127\n",
      "[Epoch 25] Training Batch [352/391]: Loss 0.0458829440176487\n",
      "[Epoch 25] Training Batch [353/391]: Loss 0.014914876781404018\n",
      "[Epoch 25] Training Batch [354/391]: Loss 0.012523011304438114\n",
      "[Epoch 25] Training Batch [355/391]: Loss 0.03130084648728371\n",
      "[Epoch 25] Training Batch [356/391]: Loss 0.03530999645590782\n",
      "[Epoch 25] Training Batch [357/391]: Loss 0.024948902428150177\n",
      "[Epoch 25] Training Batch [358/391]: Loss 0.0221693255007267\n",
      "[Epoch 25] Training Batch [359/391]: Loss 0.07321324199438095\n",
      "[Epoch 25] Training Batch [360/391]: Loss 0.015899479389190674\n",
      "[Epoch 25] Training Batch [361/391]: Loss 0.005978782195597887\n",
      "[Epoch 25] Training Batch [362/391]: Loss 0.009976722300052643\n",
      "[Epoch 25] Training Batch [363/391]: Loss 0.028511464595794678\n",
      "[Epoch 25] Training Batch [364/391]: Loss 0.029501769691705704\n",
      "[Epoch 25] Training Batch [365/391]: Loss 0.025547703728079796\n",
      "[Epoch 25] Training Batch [366/391]: Loss 0.06481344997882843\n",
      "[Epoch 25] Training Batch [367/391]: Loss 0.0864734798669815\n",
      "[Epoch 25] Training Batch [368/391]: Loss 0.04268033802509308\n",
      "[Epoch 25] Training Batch [369/391]: Loss 0.01781579665839672\n",
      "[Epoch 25] Training Batch [370/391]: Loss 0.004677418619394302\n",
      "[Epoch 25] Training Batch [371/391]: Loss 0.056561507284641266\n",
      "[Epoch 25] Training Batch [372/391]: Loss 0.007069294340908527\n",
      "[Epoch 25] Training Batch [373/391]: Loss 0.016445616260170937\n",
      "[Epoch 25] Training Batch [374/391]: Loss 0.012881124392151833\n",
      "[Epoch 25] Training Batch [375/391]: Loss 0.03514798358082771\n",
      "[Epoch 25] Training Batch [376/391]: Loss 0.027729017660021782\n",
      "[Epoch 25] Training Batch [377/391]: Loss 0.03275279700756073\n",
      "[Epoch 25] Training Batch [378/391]: Loss 0.03947078436613083\n",
      "[Epoch 25] Training Batch [379/391]: Loss 0.03213806077837944\n",
      "[Epoch 25] Training Batch [380/391]: Loss 0.02116571180522442\n",
      "[Epoch 25] Training Batch [381/391]: Loss 0.08888725191354752\n",
      "[Epoch 25] Training Batch [382/391]: Loss 0.06487063318490982\n",
      "[Epoch 25] Training Batch [383/391]: Loss 0.01546416711062193\n",
      "[Epoch 25] Training Batch [384/391]: Loss 0.022796399891376495\n",
      "[Epoch 25] Training Batch [385/391]: Loss 0.004227647557854652\n",
      "[Epoch 25] Training Batch [386/391]: Loss 0.005014702677726746\n",
      "[Epoch 25] Training Batch [387/391]: Loss 0.005718932021409273\n",
      "[Epoch 25] Training Batch [388/391]: Loss 0.011506980285048485\n",
      "[Epoch 25] Training Batch [389/391]: Loss 0.01430108118802309\n",
      "[Epoch 25] Training Batch [390/391]: Loss 0.01762339472770691\n",
      "[Epoch 25] Training Batch [391/391]: Loss 0.03588495030999184\n",
      "Epoch 25 - Train Loss: 0.0200\n",
      "*********  Epoch 26/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Training Batch [1/391]: Loss 0.011598403565585613\n",
      "[Epoch 26] Training Batch [2/391]: Loss 0.026510411873459816\n",
      "[Epoch 26] Training Batch [3/391]: Loss 0.01504573505371809\n",
      "[Epoch 26] Training Batch [4/391]: Loss 0.009679010137915611\n",
      "[Epoch 26] Training Batch [5/391]: Loss 0.025487298145890236\n",
      "[Epoch 26] Training Batch [6/391]: Loss 0.004576801788061857\n",
      "[Epoch 26] Training Batch [7/391]: Loss 0.028284255415201187\n",
      "[Epoch 26] Training Batch [8/391]: Loss 0.038976140320301056\n",
      "[Epoch 26] Training Batch [9/391]: Loss 0.0043127210810780525\n",
      "[Epoch 26] Training Batch [10/391]: Loss 0.0034566805697977543\n",
      "[Epoch 26] Training Batch [11/391]: Loss 0.02435850352048874\n",
      "[Epoch 26] Training Batch [12/391]: Loss 0.016242627054452896\n",
      "[Epoch 26] Training Batch [13/391]: Loss 0.011749965138733387\n",
      "[Epoch 26] Training Batch [14/391]: Loss 0.012972850352525711\n",
      "[Epoch 26] Training Batch [15/391]: Loss 0.00567806838080287\n",
      "[Epoch 26] Training Batch [16/391]: Loss 0.012923196889460087\n",
      "[Epoch 26] Training Batch [17/391]: Loss 0.01135239191353321\n",
      "[Epoch 26] Training Batch [18/391]: Loss 0.02160985954105854\n",
      "[Epoch 26] Training Batch [19/391]: Loss 0.046945903450250626\n",
      "[Epoch 26] Training Batch [20/391]: Loss 0.028684286400675774\n",
      "[Epoch 26] Training Batch [21/391]: Loss 0.022810345515608788\n",
      "[Epoch 26] Training Batch [22/391]: Loss 0.026692291721701622\n",
      "[Epoch 26] Training Batch [23/391]: Loss 0.048863012343645096\n",
      "[Epoch 26] Training Batch [24/391]: Loss 0.0141758993268013\n",
      "[Epoch 26] Training Batch [25/391]: Loss 0.003738309955224395\n",
      "[Epoch 26] Training Batch [26/391]: Loss 0.009936092421412468\n",
      "[Epoch 26] Training Batch [27/391]: Loss 0.08132170140743256\n",
      "[Epoch 26] Training Batch [28/391]: Loss 0.004555317107588053\n",
      "[Epoch 26] Training Batch [29/391]: Loss 0.012730010785162449\n",
      "[Epoch 26] Training Batch [30/391]: Loss 0.0636189803481102\n",
      "[Epoch 26] Training Batch [31/391]: Loss 0.008715292438864708\n",
      "[Epoch 26] Training Batch [32/391]: Loss 0.019340813159942627\n",
      "[Epoch 26] Training Batch [33/391]: Loss 0.05622496083378792\n",
      "[Epoch 26] Training Batch [34/391]: Loss 0.005338116083294153\n",
      "[Epoch 26] Training Batch [35/391]: Loss 0.002087077358737588\n",
      "[Epoch 26] Training Batch [36/391]: Loss 0.010948223061859608\n",
      "[Epoch 26] Training Batch [37/391]: Loss 0.010728497058153152\n",
      "[Epoch 26] Training Batch [38/391]: Loss 0.014837058261036873\n",
      "[Epoch 26] Training Batch [39/391]: Loss 0.00691290432587266\n",
      "[Epoch 26] Training Batch [40/391]: Loss 0.10184302181005478\n",
      "[Epoch 26] Training Batch [41/391]: Loss 0.009925498627126217\n",
      "[Epoch 26] Training Batch [42/391]: Loss 0.02940891869366169\n",
      "[Epoch 26] Training Batch [43/391]: Loss 0.05941593274474144\n",
      "[Epoch 26] Training Batch [44/391]: Loss 0.008599873632192612\n",
      "[Epoch 26] Training Batch [45/391]: Loss 0.01104576513171196\n",
      "[Epoch 26] Training Batch [46/391]: Loss 0.009752199985086918\n",
      "[Epoch 26] Training Batch [47/391]: Loss 0.007213129661977291\n",
      "[Epoch 26] Training Batch [48/391]: Loss 0.012062914669513702\n",
      "[Epoch 26] Training Batch [49/391]: Loss 0.05859414115548134\n",
      "[Epoch 26] Training Batch [50/391]: Loss 0.028747348114848137\n",
      "[Epoch 26] Training Batch [51/391]: Loss 0.006906855385750532\n",
      "[Epoch 26] Training Batch [52/391]: Loss 0.061674490571022034\n",
      "[Epoch 26] Training Batch [53/391]: Loss 0.009670126251876354\n",
      "[Epoch 26] Training Batch [54/391]: Loss 0.0666174665093422\n",
      "[Epoch 26] Training Batch [55/391]: Loss 0.013965493999421597\n",
      "[Epoch 26] Training Batch [56/391]: Loss 0.008263800293207169\n",
      "[Epoch 26] Training Batch [57/391]: Loss 0.002842472866177559\n",
      "[Epoch 26] Training Batch [58/391]: Loss 0.007059100549668074\n",
      "[Epoch 26] Training Batch [59/391]: Loss 0.026376334950327873\n",
      "[Epoch 26] Training Batch [60/391]: Loss 0.03772764280438423\n",
      "[Epoch 26] Training Batch [61/391]: Loss 0.003532598027959466\n",
      "[Epoch 26] Training Batch [62/391]: Loss 0.018816987052559853\n",
      "[Epoch 26] Training Batch [63/391]: Loss 0.00462240818887949\n",
      "[Epoch 26] Training Batch [64/391]: Loss 0.040990766137838364\n",
      "[Epoch 26] Training Batch [65/391]: Loss 0.004646306857466698\n",
      "[Epoch 26] Training Batch [66/391]: Loss 0.008818842470645905\n",
      "[Epoch 26] Training Batch [67/391]: Loss 0.0018857523100450635\n",
      "[Epoch 26] Training Batch [68/391]: Loss 0.012323323637247086\n",
      "[Epoch 26] Training Batch [69/391]: Loss 0.006707611493766308\n",
      "[Epoch 26] Training Batch [70/391]: Loss 0.019480165094137192\n",
      "[Epoch 26] Training Batch [71/391]: Loss 0.021579217165708542\n",
      "[Epoch 26] Training Batch [72/391]: Loss 0.008998827077448368\n",
      "[Epoch 26] Training Batch [73/391]: Loss 0.028349120169878006\n",
      "[Epoch 26] Training Batch [74/391]: Loss 0.013453986495733261\n",
      "[Epoch 26] Training Batch [75/391]: Loss 0.007095709443092346\n",
      "[Epoch 26] Training Batch [76/391]: Loss 0.01928965002298355\n",
      "[Epoch 26] Training Batch [77/391]: Loss 0.036430228501558304\n",
      "[Epoch 26] Training Batch [78/391]: Loss 0.016566190868616104\n",
      "[Epoch 26] Training Batch [79/391]: Loss 0.008445854298770428\n",
      "[Epoch 26] Training Batch [80/391]: Loss 0.02853669598698616\n",
      "[Epoch 26] Training Batch [81/391]: Loss 0.018172262236475945\n",
      "[Epoch 26] Training Batch [82/391]: Loss 0.011823145672678947\n",
      "[Epoch 26] Training Batch [83/391]: Loss 0.011559979058802128\n",
      "[Epoch 26] Training Batch [84/391]: Loss 0.023268001154065132\n",
      "[Epoch 26] Training Batch [85/391]: Loss 0.002823063638061285\n",
      "[Epoch 26] Training Batch [86/391]: Loss 0.0018688570708036423\n",
      "[Epoch 26] Training Batch [87/391]: Loss 0.011459410190582275\n",
      "[Epoch 26] Training Batch [88/391]: Loss 0.008337113074958324\n",
      "[Epoch 26] Training Batch [89/391]: Loss 0.012148940935730934\n",
      "[Epoch 26] Training Batch [90/391]: Loss 0.014435366727411747\n",
      "[Epoch 26] Training Batch [91/391]: Loss 0.005476939491927624\n",
      "[Epoch 26] Training Batch [92/391]: Loss 0.012154966592788696\n",
      "[Epoch 26] Training Batch [93/391]: Loss 0.0200569499284029\n",
      "[Epoch 26] Training Batch [94/391]: Loss 0.015297050587832928\n",
      "[Epoch 26] Training Batch [95/391]: Loss 0.056206587702035904\n",
      "[Epoch 26] Training Batch [96/391]: Loss 0.005695127882063389\n",
      "[Epoch 26] Training Batch [97/391]: Loss 0.011885281652212143\n",
      "[Epoch 26] Training Batch [98/391]: Loss 0.010970615781843662\n",
      "[Epoch 26] Training Batch [99/391]: Loss 0.015315746888518333\n",
      "[Epoch 26] Training Batch [100/391]: Loss 0.0022468934766948223\n",
      "[Epoch 26] Training Batch [101/391]: Loss 0.0024023207370191813\n",
      "[Epoch 26] Training Batch [102/391]: Loss 0.010970350354909897\n",
      "[Epoch 26] Training Batch [103/391]: Loss 0.014350458979606628\n",
      "[Epoch 26] Training Batch [104/391]: Loss 0.007165688555687666\n",
      "[Epoch 26] Training Batch [105/391]: Loss 0.00661711348220706\n",
      "[Epoch 26] Training Batch [106/391]: Loss 0.010173667222261429\n",
      "[Epoch 26] Training Batch [107/391]: Loss 0.006363203749060631\n",
      "[Epoch 26] Training Batch [108/391]: Loss 0.00669292826205492\n",
      "[Epoch 26] Training Batch [109/391]: Loss 0.011236844584345818\n",
      "[Epoch 26] Training Batch [110/391]: Loss 0.007986760698258877\n",
      "[Epoch 26] Training Batch [111/391]: Loss 0.021870791912078857\n",
      "[Epoch 26] Training Batch [112/391]: Loss 0.017303049564361572\n",
      "[Epoch 26] Training Batch [113/391]: Loss 0.009316117502748966\n",
      "[Epoch 26] Training Batch [114/391]: Loss 0.004436114337295294\n",
      "[Epoch 26] Training Batch [115/391]: Loss 0.006258006207644939\n",
      "[Epoch 26] Training Batch [116/391]: Loss 0.01678933948278427\n",
      "[Epoch 26] Training Batch [117/391]: Loss 0.024356693029403687\n",
      "[Epoch 26] Training Batch [118/391]: Loss 0.0034475582651793957\n",
      "[Epoch 26] Training Batch [119/391]: Loss 0.015434244647622108\n",
      "[Epoch 26] Training Batch [120/391]: Loss 0.012498291209340096\n",
      "[Epoch 26] Training Batch [121/391]: Loss 0.04055134952068329\n",
      "[Epoch 26] Training Batch [122/391]: Loss 0.05036512017250061\n",
      "[Epoch 26] Training Batch [123/391]: Loss 0.011681215837597847\n",
      "[Epoch 26] Training Batch [124/391]: Loss 0.012258422560989857\n",
      "[Epoch 26] Training Batch [125/391]: Loss 0.035581596195697784\n",
      "[Epoch 26] Training Batch [126/391]: Loss 0.025555133819580078\n",
      "[Epoch 26] Training Batch [127/391]: Loss 0.002685670042410493\n",
      "[Epoch 26] Training Batch [128/391]: Loss 0.0021921303123235703\n",
      "[Epoch 26] Training Batch [129/391]: Loss 0.001853970461525023\n",
      "[Epoch 26] Training Batch [130/391]: Loss 0.02038591168820858\n",
      "[Epoch 26] Training Batch [131/391]: Loss 0.07495832443237305\n",
      "[Epoch 26] Training Batch [132/391]: Loss 0.0025822478346526623\n",
      "[Epoch 26] Training Batch [133/391]: Loss 0.00915247481316328\n",
      "[Epoch 26] Training Batch [134/391]: Loss 0.023197539150714874\n",
      "[Epoch 26] Training Batch [135/391]: Loss 0.003293320769444108\n",
      "[Epoch 26] Training Batch [136/391]: Loss 0.0041574593633413315\n",
      "[Epoch 26] Training Batch [137/391]: Loss 0.00422783475369215\n",
      "[Epoch 26] Training Batch [138/391]: Loss 0.0066727688536047935\n",
      "[Epoch 26] Training Batch [139/391]: Loss 0.01922772452235222\n",
      "[Epoch 26] Training Batch [140/391]: Loss 0.009112225845456123\n",
      "[Epoch 26] Training Batch [141/391]: Loss 0.014660467393696308\n",
      "[Epoch 26] Training Batch [142/391]: Loss 0.03023943491280079\n",
      "[Epoch 26] Training Batch [143/391]: Loss 0.007008480839431286\n",
      "[Epoch 26] Training Batch [144/391]: Loss 0.014139669947326183\n",
      "[Epoch 26] Training Batch [145/391]: Loss 0.0031568873673677444\n",
      "[Epoch 26] Training Batch [146/391]: Loss 0.010253815911710262\n",
      "[Epoch 26] Training Batch [147/391]: Loss 0.0040885573253035545\n",
      "[Epoch 26] Training Batch [148/391]: Loss 0.0081049595028162\n",
      "[Epoch 26] Training Batch [149/391]: Loss 0.005181100219488144\n",
      "[Epoch 26] Training Batch [150/391]: Loss 0.013204522430896759\n",
      "[Epoch 26] Training Batch [151/391]: Loss 0.033286746591329575\n",
      "[Epoch 26] Training Batch [152/391]: Loss 0.00639131898060441\n",
      "[Epoch 26] Training Batch [153/391]: Loss 0.02842169813811779\n",
      "[Epoch 26] Training Batch [154/391]: Loss 0.007795514538884163\n",
      "[Epoch 26] Training Batch [155/391]: Loss 0.011442303657531738\n",
      "[Epoch 26] Training Batch [156/391]: Loss 0.009579676203429699\n",
      "[Epoch 26] Training Batch [157/391]: Loss 0.002767005702480674\n",
      "[Epoch 26] Training Batch [158/391]: Loss 0.00459520285949111\n",
      "[Epoch 26] Training Batch [159/391]: Loss 0.00239055254496634\n",
      "[Epoch 26] Training Batch [160/391]: Loss 0.012936829589307308\n",
      "[Epoch 26] Training Batch [161/391]: Loss 0.0018644265364855528\n",
      "[Epoch 26] Training Batch [162/391]: Loss 0.0160309299826622\n",
      "[Epoch 26] Training Batch [163/391]: Loss 0.004991164430975914\n",
      "[Epoch 26] Training Batch [164/391]: Loss 0.010095766745507717\n",
      "[Epoch 26] Training Batch [165/391]: Loss 0.004947863984853029\n",
      "[Epoch 26] Training Batch [166/391]: Loss 0.005583357531577349\n",
      "[Epoch 26] Training Batch [167/391]: Loss 0.007582967169582844\n",
      "[Epoch 26] Training Batch [168/391]: Loss 0.0054371473379433155\n",
      "[Epoch 26] Training Batch [169/391]: Loss 0.0034687723964452744\n",
      "[Epoch 26] Training Batch [170/391]: Loss 0.017073720693588257\n",
      "[Epoch 26] Training Batch [171/391]: Loss 0.0018936754204332829\n",
      "[Epoch 26] Training Batch [172/391]: Loss 0.004295107908546925\n",
      "[Epoch 26] Training Batch [173/391]: Loss 0.006814456079155207\n",
      "[Epoch 26] Training Batch [174/391]: Loss 0.008341304957866669\n",
      "[Epoch 26] Training Batch [175/391]: Loss 0.0011135212844237685\n",
      "[Epoch 26] Training Batch [176/391]: Loss 0.00303861778229475\n",
      "[Epoch 26] Training Batch [177/391]: Loss 0.006018795073032379\n",
      "[Epoch 26] Training Batch [178/391]: Loss 0.011392679996788502\n",
      "[Epoch 26] Training Batch [179/391]: Loss 0.028515765443444252\n",
      "[Epoch 26] Training Batch [180/391]: Loss 0.0003530248359311372\n",
      "[Epoch 26] Training Batch [181/391]: Loss 0.004619885701686144\n",
      "[Epoch 26] Training Batch [182/391]: Loss 0.005663673393428326\n",
      "[Epoch 26] Training Batch [183/391]: Loss 0.00235846103169024\n",
      "[Epoch 26] Training Batch [184/391]: Loss 0.03289753198623657\n",
      "[Epoch 26] Training Batch [185/391]: Loss 0.005284248385578394\n",
      "[Epoch 26] Training Batch [186/391]: Loss 0.0028219951782375574\n",
      "[Epoch 26] Training Batch [187/391]: Loss 0.025613732635974884\n",
      "[Epoch 26] Training Batch [188/391]: Loss 0.006497916765511036\n",
      "[Epoch 26] Training Batch [189/391]: Loss 0.005179429426789284\n",
      "[Epoch 26] Training Batch [190/391]: Loss 0.008374694734811783\n",
      "[Epoch 26] Training Batch [191/391]: Loss 0.0022736063692718744\n",
      "[Epoch 26] Training Batch [192/391]: Loss 0.004845115356147289\n",
      "[Epoch 26] Training Batch [193/391]: Loss 0.02490944415330887\n",
      "[Epoch 26] Training Batch [194/391]: Loss 0.008081727661192417\n",
      "[Epoch 26] Training Batch [195/391]: Loss 0.013824749737977982\n",
      "[Epoch 26] Training Batch [196/391]: Loss 0.0035165450535714626\n",
      "[Epoch 26] Training Batch [197/391]: Loss 0.01088311430066824\n",
      "[Epoch 26] Training Batch [198/391]: Loss 0.002596181118860841\n",
      "[Epoch 26] Training Batch [199/391]: Loss 0.0014860434457659721\n",
      "[Epoch 26] Training Batch [200/391]: Loss 0.02104741707444191\n",
      "[Epoch 26] Training Batch [201/391]: Loss 0.01538971159607172\n",
      "[Epoch 26] Training Batch [202/391]: Loss 0.0177958644926548\n",
      "[Epoch 26] Training Batch [203/391]: Loss 0.0036532538942992687\n",
      "[Epoch 26] Training Batch [204/391]: Loss 0.005433585960417986\n",
      "[Epoch 26] Training Batch [205/391]: Loss 0.002080303616821766\n",
      "[Epoch 26] Training Batch [206/391]: Loss 0.005352606996893883\n",
      "[Epoch 26] Training Batch [207/391]: Loss 0.003772529773414135\n",
      "[Epoch 26] Training Batch [208/391]: Loss 0.007506976369768381\n",
      "[Epoch 26] Training Batch [209/391]: Loss 0.0023529240861535072\n",
      "[Epoch 26] Training Batch [210/391]: Loss 0.005156130529940128\n",
      "[Epoch 26] Training Batch [211/391]: Loss 0.024318315088748932\n",
      "[Epoch 26] Training Batch [212/391]: Loss 0.007411114405840635\n",
      "[Epoch 26] Training Batch [213/391]: Loss 0.0035801688209176064\n",
      "[Epoch 26] Training Batch [214/391]: Loss 0.04069792479276657\n",
      "[Epoch 26] Training Batch [215/391]: Loss 0.0021582748740911484\n",
      "[Epoch 26] Training Batch [216/391]: Loss 0.02439018338918686\n",
      "[Epoch 26] Training Batch [217/391]: Loss 0.012942146509885788\n",
      "[Epoch 26] Training Batch [218/391]: Loss 0.0055481670424342155\n",
      "[Epoch 26] Training Batch [219/391]: Loss 0.01670018956065178\n",
      "[Epoch 26] Training Batch [220/391]: Loss 0.008346212096512318\n",
      "[Epoch 26] Training Batch [221/391]: Loss 0.0017030405579134822\n",
      "[Epoch 26] Training Batch [222/391]: Loss 0.02586442045867443\n",
      "[Epoch 26] Training Batch [223/391]: Loss 0.0024221453350037336\n",
      "[Epoch 26] Training Batch [224/391]: Loss 0.0361669585108757\n",
      "[Epoch 26] Training Batch [225/391]: Loss 0.001522897626273334\n",
      "[Epoch 26] Training Batch [226/391]: Loss 0.014801976270973682\n",
      "[Epoch 26] Training Batch [227/391]: Loss 0.0074768816120922565\n",
      "[Epoch 26] Training Batch [228/391]: Loss 0.0022902435157448053\n",
      "[Epoch 26] Training Batch [229/391]: Loss 0.004537372384220362\n",
      "[Epoch 26] Training Batch [230/391]: Loss 0.018746236339211464\n",
      "[Epoch 26] Training Batch [231/391]: Loss 0.001960963709279895\n",
      "[Epoch 26] Training Batch [232/391]: Loss 0.0185319185256958\n",
      "[Epoch 26] Training Batch [233/391]: Loss 0.015000807121396065\n",
      "[Epoch 26] Training Batch [234/391]: Loss 0.004294532351195812\n",
      "[Epoch 26] Training Batch [235/391]: Loss 0.00966486893594265\n",
      "[Epoch 26] Training Batch [236/391]: Loss 0.0012784198625013232\n",
      "[Epoch 26] Training Batch [237/391]: Loss 0.024567602202296257\n",
      "[Epoch 26] Training Batch [238/391]: Loss 0.003817161777988076\n",
      "[Epoch 26] Training Batch [239/391]: Loss 0.004783858545124531\n",
      "[Epoch 26] Training Batch [240/391]: Loss 0.006665411870926619\n",
      "[Epoch 26] Training Batch [241/391]: Loss 0.003939672838896513\n",
      "[Epoch 26] Training Batch [242/391]: Loss 0.02903021313250065\n",
      "[Epoch 26] Training Batch [243/391]: Loss 0.011896805837750435\n",
      "[Epoch 26] Training Batch [244/391]: Loss 0.03327617421746254\n",
      "[Epoch 26] Training Batch [245/391]: Loss 0.04588494449853897\n",
      "[Epoch 26] Training Batch [246/391]: Loss 0.005619492381811142\n",
      "[Epoch 26] Training Batch [247/391]: Loss 0.0010110675357282162\n",
      "[Epoch 26] Training Batch [248/391]: Loss 0.009479374624788761\n",
      "[Epoch 26] Training Batch [249/391]: Loss 0.0025918097235262394\n",
      "[Epoch 26] Training Batch [250/391]: Loss 0.037624649703502655\n",
      "[Epoch 26] Training Batch [251/391]: Loss 0.010749634355306625\n",
      "[Epoch 26] Training Batch [252/391]: Loss 0.0039885989390313625\n",
      "[Epoch 26] Training Batch [253/391]: Loss 0.004186210688203573\n",
      "[Epoch 26] Training Batch [254/391]: Loss 0.007071797735989094\n",
      "[Epoch 26] Training Batch [255/391]: Loss 0.0034875194542109966\n",
      "[Epoch 26] Training Batch [256/391]: Loss 0.005114275496453047\n",
      "[Epoch 26] Training Batch [257/391]: Loss 0.015690229833126068\n",
      "[Epoch 26] Training Batch [258/391]: Loss 0.0066192857921123505\n",
      "[Epoch 26] Training Batch [259/391]: Loss 0.004477182403206825\n",
      "[Epoch 26] Training Batch [260/391]: Loss 0.006611954420804977\n",
      "[Epoch 26] Training Batch [261/391]: Loss 0.00986560620367527\n",
      "[Epoch 26] Training Batch [262/391]: Loss 0.013159951195120811\n",
      "[Epoch 26] Training Batch [263/391]: Loss 0.010852456092834473\n",
      "[Epoch 26] Training Batch [264/391]: Loss 0.00656475592404604\n",
      "[Epoch 26] Training Batch [265/391]: Loss 0.0016558946808800101\n",
      "[Epoch 26] Training Batch [266/391]: Loss 0.015067175962030888\n",
      "[Epoch 26] Training Batch [267/391]: Loss 0.01760368049144745\n",
      "[Epoch 26] Training Batch [268/391]: Loss 0.0015277445781975985\n",
      "[Epoch 26] Training Batch [269/391]: Loss 0.005537280812859535\n",
      "[Epoch 26] Training Batch [270/391]: Loss 0.002869594842195511\n",
      "[Epoch 26] Training Batch [271/391]: Loss 0.009904918260872364\n",
      "[Epoch 26] Training Batch [272/391]: Loss 0.007508706301450729\n",
      "[Epoch 26] Training Batch [273/391]: Loss 0.007828976958990097\n",
      "[Epoch 26] Training Batch [274/391]: Loss 0.014167899265885353\n",
      "[Epoch 26] Training Batch [275/391]: Loss 0.0017819409258663654\n",
      "[Epoch 26] Training Batch [276/391]: Loss 0.021485239267349243\n",
      "[Epoch 26] Training Batch [277/391]: Loss 0.002902985317632556\n",
      "[Epoch 26] Training Batch [278/391]: Loss 0.0047977389767766\n",
      "[Epoch 26] Training Batch [279/391]: Loss 0.009416581131517887\n",
      "[Epoch 26] Training Batch [280/391]: Loss 0.005106091964989901\n",
      "[Epoch 26] Training Batch [281/391]: Loss 0.0030670331325381994\n",
      "[Epoch 26] Training Batch [282/391]: Loss 0.001478424295783043\n",
      "[Epoch 26] Training Batch [283/391]: Loss 0.013029935769736767\n",
      "[Epoch 26] Training Batch [284/391]: Loss 0.041240401566028595\n",
      "[Epoch 26] Training Batch [285/391]: Loss 0.005321770906448364\n",
      "[Epoch 26] Training Batch [286/391]: Loss 0.006464230362325907\n",
      "[Epoch 26] Training Batch [287/391]: Loss 0.024714073166251183\n",
      "[Epoch 26] Training Batch [288/391]: Loss 0.001402051653712988\n",
      "[Epoch 26] Training Batch [289/391]: Loss 0.0038692918606102467\n",
      "[Epoch 26] Training Batch [290/391]: Loss 0.008830852806568146\n",
      "[Epoch 26] Training Batch [291/391]: Loss 0.008069528266787529\n",
      "[Epoch 26] Training Batch [292/391]: Loss 0.0027356238570064306\n",
      "[Epoch 26] Training Batch [293/391]: Loss 0.0032677738927304745\n",
      "[Epoch 26] Training Batch [294/391]: Loss 0.047885358333587646\n",
      "[Epoch 26] Training Batch [295/391]: Loss 0.006808481644839048\n",
      "[Epoch 26] Training Batch [296/391]: Loss 0.05803142488002777\n",
      "[Epoch 26] Training Batch [297/391]: Loss 0.009286666288971901\n",
      "[Epoch 26] Training Batch [298/391]: Loss 0.005311377812176943\n",
      "[Epoch 26] Training Batch [299/391]: Loss 0.003918956965208054\n",
      "[Epoch 26] Training Batch [300/391]: Loss 0.001484307344071567\n",
      "[Epoch 26] Training Batch [301/391]: Loss 0.0064261904917657375\n",
      "[Epoch 26] Training Batch [302/391]: Loss 0.030535267665982246\n",
      "[Epoch 26] Training Batch [303/391]: Loss 0.009828169830143452\n",
      "[Epoch 26] Training Batch [304/391]: Loss 0.014831222593784332\n",
      "[Epoch 26] Training Batch [305/391]: Loss 0.005822110455483198\n",
      "[Epoch 26] Training Batch [306/391]: Loss 0.005541646387428045\n",
      "[Epoch 26] Training Batch [307/391]: Loss 0.009271800518035889\n",
      "[Epoch 26] Training Batch [308/391]: Loss 0.025801783427596092\n",
      "[Epoch 26] Training Batch [309/391]: Loss 0.013699718751013279\n",
      "[Epoch 26] Training Batch [310/391]: Loss 0.014922498725354671\n",
      "[Epoch 26] Training Batch [311/391]: Loss 0.0019372666720300913\n",
      "[Epoch 26] Training Batch [312/391]: Loss 0.0007498867926187813\n",
      "[Epoch 26] Training Batch [313/391]: Loss 0.0007792370743118227\n",
      "[Epoch 26] Training Batch [314/391]: Loss 0.0014946089359000325\n",
      "[Epoch 26] Training Batch [315/391]: Loss 0.03270601108670235\n",
      "[Epoch 26] Training Batch [316/391]: Loss 0.006048363167792559\n",
      "[Epoch 26] Training Batch [317/391]: Loss 0.0009541732142679393\n",
      "[Epoch 26] Training Batch [318/391]: Loss 0.0034651928581297398\n",
      "[Epoch 26] Training Batch [319/391]: Loss 0.003249702975153923\n",
      "[Epoch 26] Training Batch [320/391]: Loss 0.0025704207364469767\n",
      "[Epoch 26] Training Batch [321/391]: Loss 0.018053419888019562\n",
      "[Epoch 26] Training Batch [322/391]: Loss 0.030378924682736397\n",
      "[Epoch 26] Training Batch [323/391]: Loss 0.02410755679011345\n",
      "[Epoch 26] Training Batch [324/391]: Loss 0.011485663242638111\n",
      "[Epoch 26] Training Batch [325/391]: Loss 0.007219454273581505\n",
      "[Epoch 26] Training Batch [326/391]: Loss 0.0173405222594738\n",
      "[Epoch 26] Training Batch [327/391]: Loss 0.009880579076707363\n",
      "[Epoch 26] Training Batch [328/391]: Loss 0.0034132590517401695\n",
      "[Epoch 26] Training Batch [329/391]: Loss 0.00425459910184145\n",
      "[Epoch 26] Training Batch [330/391]: Loss 0.01313219778239727\n",
      "[Epoch 26] Training Batch [331/391]: Loss 0.006463547702878714\n",
      "[Epoch 26] Training Batch [332/391]: Loss 0.0011107079917564988\n",
      "[Epoch 26] Training Batch [333/391]: Loss 0.002051117131486535\n",
      "[Epoch 26] Training Batch [334/391]: Loss 0.0011773447040468454\n",
      "[Epoch 26] Training Batch [335/391]: Loss 0.004429171793162823\n",
      "[Epoch 26] Training Batch [336/391]: Loss 0.019654350355267525\n",
      "[Epoch 26] Training Batch [337/391]: Loss 0.004804721102118492\n",
      "[Epoch 26] Training Batch [338/391]: Loss 0.019539479166269302\n",
      "[Epoch 26] Training Batch [339/391]: Loss 0.02527720108628273\n",
      "[Epoch 26] Training Batch [340/391]: Loss 0.033809300512075424\n",
      "[Epoch 26] Training Batch [341/391]: Loss 0.008546920493245125\n",
      "[Epoch 26] Training Batch [342/391]: Loss 0.008095267228782177\n",
      "[Epoch 26] Training Batch [343/391]: Loss 0.0020081978291273117\n",
      "[Epoch 26] Training Batch [344/391]: Loss 0.008421657606959343\n",
      "[Epoch 26] Training Batch [345/391]: Loss 0.01872195117175579\n",
      "[Epoch 26] Training Batch [346/391]: Loss 0.007875582203269005\n",
      "[Epoch 26] Training Batch [347/391]: Loss 0.004068422131240368\n",
      "[Epoch 26] Training Batch [348/391]: Loss 0.00876520574092865\n",
      "[Epoch 26] Training Batch [349/391]: Loss 0.00873475056141615\n",
      "[Epoch 26] Training Batch [350/391]: Loss 0.005354405380785465\n",
      "[Epoch 26] Training Batch [351/391]: Loss 0.03477460518479347\n",
      "[Epoch 26] Training Batch [352/391]: Loss 0.0016441679326817393\n",
      "[Epoch 26] Training Batch [353/391]: Loss 0.0015990152023732662\n",
      "[Epoch 26] Training Batch [354/391]: Loss 0.01680346578359604\n",
      "[Epoch 26] Training Batch [355/391]: Loss 0.006588498130440712\n",
      "[Epoch 26] Training Batch [356/391]: Loss 0.005167047493159771\n",
      "[Epoch 26] Training Batch [357/391]: Loss 0.0032085212878882885\n",
      "[Epoch 26] Training Batch [358/391]: Loss 0.00991919357329607\n",
      "[Epoch 26] Training Batch [359/391]: Loss 0.004157337360084057\n",
      "[Epoch 26] Training Batch [360/391]: Loss 0.009680062532424927\n",
      "[Epoch 26] Training Batch [361/391]: Loss 0.021196847781538963\n",
      "[Epoch 26] Training Batch [362/391]: Loss 0.0052129486575722694\n",
      "[Epoch 26] Training Batch [363/391]: Loss 0.005534385330975056\n",
      "[Epoch 26] Training Batch [364/391]: Loss 0.004003874491900206\n",
      "[Epoch 26] Training Batch [365/391]: Loss 0.0032619761768728495\n",
      "[Epoch 26] Training Batch [366/391]: Loss 0.019187822937965393\n",
      "[Epoch 26] Training Batch [367/391]: Loss 0.0060828994028270245\n",
      "[Epoch 26] Training Batch [368/391]: Loss 0.012085970491170883\n",
      "[Epoch 26] Training Batch [369/391]: Loss 0.029130909591913223\n",
      "[Epoch 26] Training Batch [370/391]: Loss 0.005946738179773092\n",
      "[Epoch 26] Training Batch [371/391]: Loss 0.0035790754482150078\n",
      "[Epoch 26] Training Batch [372/391]: Loss 0.0024989137891680002\n",
      "[Epoch 26] Training Batch [373/391]: Loss 0.004082123748958111\n",
      "[Epoch 26] Training Batch [374/391]: Loss 0.007096839137375355\n",
      "[Epoch 26] Training Batch [375/391]: Loss 0.0016757830744609237\n",
      "[Epoch 26] Training Batch [376/391]: Loss 0.0046483916230499744\n",
      "[Epoch 26] Training Batch [377/391]: Loss 0.006666812114417553\n",
      "[Epoch 26] Training Batch [378/391]: Loss 0.003997169900685549\n",
      "[Epoch 26] Training Batch [379/391]: Loss 0.010737331584095955\n",
      "[Epoch 26] Training Batch [380/391]: Loss 0.0042977286502718925\n",
      "[Epoch 26] Training Batch [381/391]: Loss 0.002100118901580572\n",
      "[Epoch 26] Training Batch [382/391]: Loss 0.011624428443610668\n",
      "[Epoch 26] Training Batch [383/391]: Loss 0.015091005712747574\n",
      "[Epoch 26] Training Batch [384/391]: Loss 0.004960875492542982\n",
      "[Epoch 26] Training Batch [385/391]: Loss 0.0006597519968636334\n",
      "[Epoch 26] Training Batch [386/391]: Loss 0.019044607877731323\n",
      "[Epoch 26] Training Batch [387/391]: Loss 0.007568124681711197\n",
      "[Epoch 26] Training Batch [388/391]: Loss 0.006662706844508648\n",
      "[Epoch 26] Training Batch [389/391]: Loss 0.010705624707043171\n",
      "[Epoch 26] Training Batch [390/391]: Loss 0.004723711870610714\n",
      "[Epoch 26] Training Batch [391/391]: Loss 0.003354037646204233\n",
      "Epoch 26 - Train Loss: 0.0129\n",
      "*********  Epoch 27/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Training Batch [1/391]: Loss 0.003978149499744177\n",
      "[Epoch 27] Training Batch [2/391]: Loss 0.0024844908621162176\n",
      "[Epoch 27] Training Batch [3/391]: Loss 0.001852724002674222\n",
      "[Epoch 27] Training Batch [4/391]: Loss 0.00181351020000875\n",
      "[Epoch 27] Training Batch [5/391]: Loss 0.0012302768882364035\n",
      "[Epoch 27] Training Batch [6/391]: Loss 0.0012442403240129352\n",
      "[Epoch 27] Training Batch [7/391]: Loss 0.0016559370560571551\n",
      "[Epoch 27] Training Batch [8/391]: Loss 0.005308771040290594\n",
      "[Epoch 27] Training Batch [9/391]: Loss 0.013104347512125969\n",
      "[Epoch 27] Training Batch [10/391]: Loss 0.001881855889223516\n",
      "[Epoch 27] Training Batch [11/391]: Loss 0.0037226637359708548\n",
      "[Epoch 27] Training Batch [12/391]: Loss 0.003938120324164629\n",
      "[Epoch 27] Training Batch [13/391]: Loss 0.003913951106369495\n",
      "[Epoch 27] Training Batch [14/391]: Loss 0.002616900485008955\n",
      "[Epoch 27] Training Batch [15/391]: Loss 0.002252088161185384\n",
      "[Epoch 27] Training Batch [16/391]: Loss 0.0067483801394701\n",
      "[Epoch 27] Training Batch [17/391]: Loss 0.009868273511528969\n",
      "[Epoch 27] Training Batch [18/391]: Loss 0.004187928978353739\n",
      "[Epoch 27] Training Batch [19/391]: Loss 0.01050876546651125\n",
      "[Epoch 27] Training Batch [20/391]: Loss 0.0007904497906565666\n",
      "[Epoch 27] Training Batch [21/391]: Loss 0.006337687373161316\n",
      "[Epoch 27] Training Batch [22/391]: Loss 0.01782252825796604\n",
      "[Epoch 27] Training Batch [23/391]: Loss 0.0027805704157799482\n",
      "[Epoch 27] Training Batch [24/391]: Loss 0.0040895938873291016\n",
      "[Epoch 27] Training Batch [25/391]: Loss 0.0041569131426513195\n",
      "[Epoch 27] Training Batch [26/391]: Loss 0.004317752551287413\n",
      "[Epoch 27] Training Batch [27/391]: Loss 0.0011625459883362055\n",
      "[Epoch 27] Training Batch [28/391]: Loss 0.00580293545499444\n",
      "[Epoch 27] Training Batch [29/391]: Loss 0.0013311217771843076\n",
      "[Epoch 27] Training Batch [30/391]: Loss 0.0011911268811672926\n",
      "[Epoch 27] Training Batch [31/391]: Loss 0.0010470490669831634\n",
      "[Epoch 27] Training Batch [32/391]: Loss 0.0011889685411006212\n",
      "[Epoch 27] Training Batch [33/391]: Loss 0.0008388070855289698\n",
      "[Epoch 27] Training Batch [34/391]: Loss 0.00142132758628577\n",
      "[Epoch 27] Training Batch [35/391]: Loss 0.005220698192715645\n",
      "[Epoch 27] Training Batch [36/391]: Loss 0.004629503004252911\n",
      "[Epoch 27] Training Batch [37/391]: Loss 0.0011360271601006389\n",
      "[Epoch 27] Training Batch [38/391]: Loss 0.001649536774493754\n",
      "[Epoch 27] Training Batch [39/391]: Loss 0.004007554613053799\n",
      "[Epoch 27] Training Batch [40/391]: Loss 0.0009385543526150286\n",
      "[Epoch 27] Training Batch [41/391]: Loss 0.0008479933603666723\n",
      "[Epoch 27] Training Batch [42/391]: Loss 0.0016494521405547857\n",
      "[Epoch 27] Training Batch [43/391]: Loss 0.004662193823605776\n",
      "[Epoch 27] Training Batch [44/391]: Loss 0.002052127383649349\n",
      "[Epoch 27] Training Batch [45/391]: Loss 0.002147500403225422\n",
      "[Epoch 27] Training Batch [46/391]: Loss 0.001781072816811502\n",
      "[Epoch 27] Training Batch [47/391]: Loss 0.0032457548659294844\n",
      "[Epoch 27] Training Batch [48/391]: Loss 0.01409937534481287\n",
      "[Epoch 27] Training Batch [49/391]: Loss 0.0004340794694144279\n",
      "[Epoch 27] Training Batch [50/391]: Loss 0.0033676724415272474\n",
      "[Epoch 27] Training Batch [51/391]: Loss 0.0010992900934070349\n",
      "[Epoch 27] Training Batch [52/391]: Loss 0.002804103307425976\n",
      "[Epoch 27] Training Batch [53/391]: Loss 0.001298372633755207\n",
      "[Epoch 27] Training Batch [54/391]: Loss 0.001552591915242374\n",
      "[Epoch 27] Training Batch [55/391]: Loss 0.002645090688019991\n",
      "[Epoch 27] Training Batch [56/391]: Loss 0.001601163181476295\n",
      "[Epoch 27] Training Batch [57/391]: Loss 0.0026845878455787897\n",
      "[Epoch 27] Training Batch [58/391]: Loss 0.0013149610022082925\n",
      "[Epoch 27] Training Batch [59/391]: Loss 0.0029449129942804575\n",
      "[Epoch 27] Training Batch [60/391]: Loss 0.004221354145556688\n",
      "[Epoch 27] Training Batch [61/391]: Loss 0.003463295754045248\n",
      "[Epoch 27] Training Batch [62/391]: Loss 0.005205419380217791\n",
      "[Epoch 27] Training Batch [63/391]: Loss 0.001110912417061627\n",
      "[Epoch 27] Training Batch [64/391]: Loss 0.005727223586291075\n",
      "[Epoch 27] Training Batch [65/391]: Loss 0.000644006475340575\n",
      "[Epoch 27] Training Batch [66/391]: Loss 0.0019180824747309089\n",
      "[Epoch 27] Training Batch [67/391]: Loss 0.005314659792929888\n",
      "[Epoch 27] Training Batch [68/391]: Loss 0.0003169092524331063\n",
      "[Epoch 27] Training Batch [69/391]: Loss 0.0008763263467699289\n",
      "[Epoch 27] Training Batch [70/391]: Loss 0.0022830034140497446\n",
      "[Epoch 27] Training Batch [71/391]: Loss 0.0008195537375286222\n",
      "[Epoch 27] Training Batch [72/391]: Loss 0.0005547254695557058\n",
      "[Epoch 27] Training Batch [73/391]: Loss 0.0006754047935828567\n",
      "[Epoch 27] Training Batch [74/391]: Loss 0.002093123272061348\n",
      "[Epoch 27] Training Batch [75/391]: Loss 0.00600490951910615\n",
      "[Epoch 27] Training Batch [76/391]: Loss 0.0022804331965744495\n",
      "[Epoch 27] Training Batch [77/391]: Loss 0.0008024323033168912\n",
      "[Epoch 27] Training Batch [78/391]: Loss 0.003611604915931821\n",
      "[Epoch 27] Training Batch [79/391]: Loss 0.004027588292956352\n",
      "[Epoch 27] Training Batch [80/391]: Loss 0.0018097537104040384\n",
      "[Epoch 27] Training Batch [81/391]: Loss 0.0019825249910354614\n",
      "[Epoch 27] Training Batch [82/391]: Loss 0.0013317191042006016\n",
      "[Epoch 27] Training Batch [83/391]: Loss 0.000680105178616941\n",
      "[Epoch 27] Training Batch [84/391]: Loss 0.002597327344119549\n",
      "[Epoch 27] Training Batch [85/391]: Loss 0.0008079485851339996\n",
      "[Epoch 27] Training Batch [86/391]: Loss 0.001984203467145562\n",
      "[Epoch 27] Training Batch [87/391]: Loss 0.0009304176201112568\n",
      "[Epoch 27] Training Batch [88/391]: Loss 0.0007526564295403659\n",
      "[Epoch 27] Training Batch [89/391]: Loss 0.0164061076939106\n",
      "[Epoch 27] Training Batch [90/391]: Loss 0.0011408980935811996\n",
      "[Epoch 27] Training Batch [91/391]: Loss 0.0008847499848343432\n",
      "[Epoch 27] Training Batch [92/391]: Loss 0.0006854796083644032\n",
      "[Epoch 27] Training Batch [93/391]: Loss 0.023508913815021515\n",
      "[Epoch 27] Training Batch [94/391]: Loss 0.0019228749442845583\n",
      "[Epoch 27] Training Batch [95/391]: Loss 0.0018473711097612977\n",
      "[Epoch 27] Training Batch [96/391]: Loss 0.011987494304776192\n",
      "[Epoch 27] Training Batch [97/391]: Loss 0.0019224120769649744\n",
      "[Epoch 27] Training Batch [98/391]: Loss 0.0005312582361511886\n",
      "[Epoch 27] Training Batch [99/391]: Loss 0.00467094499617815\n",
      "[Epoch 27] Training Batch [100/391]: Loss 0.0009418081026524305\n",
      "[Epoch 27] Training Batch [101/391]: Loss 0.0006886920891702175\n",
      "[Epoch 27] Training Batch [102/391]: Loss 0.011566528119146824\n",
      "[Epoch 27] Training Batch [103/391]: Loss 0.0030622133053839207\n",
      "[Epoch 27] Training Batch [104/391]: Loss 0.0022165137343108654\n",
      "[Epoch 27] Training Batch [105/391]: Loss 0.01706218719482422\n",
      "[Epoch 27] Training Batch [106/391]: Loss 0.0018359939567744732\n",
      "[Epoch 27] Training Batch [107/391]: Loss 0.015110500156879425\n",
      "[Epoch 27] Training Batch [108/391]: Loss 0.001364908879622817\n",
      "[Epoch 27] Training Batch [109/391]: Loss 0.0019046890083700418\n",
      "[Epoch 27] Training Batch [110/391]: Loss 0.0012100081657990813\n",
      "[Epoch 27] Training Batch [111/391]: Loss 0.0006780771655030549\n",
      "[Epoch 27] Training Batch [112/391]: Loss 0.0013859543250873685\n",
      "[Epoch 27] Training Batch [113/391]: Loss 0.004374232608824968\n",
      "[Epoch 27] Training Batch [114/391]: Loss 0.00511260237544775\n",
      "[Epoch 27] Training Batch [115/391]: Loss 0.0018949350342154503\n",
      "[Epoch 27] Training Batch [116/391]: Loss 0.0011096291709691286\n",
      "[Epoch 27] Training Batch [117/391]: Loss 0.0016955327009782195\n",
      "[Epoch 27] Training Batch [118/391]: Loss 0.005729554686695337\n",
      "[Epoch 27] Training Batch [119/391]: Loss 0.008302515372633934\n",
      "[Epoch 27] Training Batch [120/391]: Loss 0.000532840087544173\n",
      "[Epoch 27] Training Batch [121/391]: Loss 0.0011470782337710261\n",
      "[Epoch 27] Training Batch [122/391]: Loss 0.001409730059094727\n",
      "[Epoch 27] Training Batch [123/391]: Loss 0.0035148810129612684\n",
      "[Epoch 27] Training Batch [124/391]: Loss 0.0023293476551771164\n",
      "[Epoch 27] Training Batch [125/391]: Loss 0.001324537442997098\n",
      "[Epoch 27] Training Batch [126/391]: Loss 0.0010299093555659056\n",
      "[Epoch 27] Training Batch [127/391]: Loss 0.00042112701339647174\n",
      "[Epoch 27] Training Batch [128/391]: Loss 0.0034306449815630913\n",
      "[Epoch 27] Training Batch [129/391]: Loss 0.0004988384316675365\n",
      "[Epoch 27] Training Batch [130/391]: Loss 0.0013807332143187523\n",
      "[Epoch 27] Training Batch [131/391]: Loss 0.0009266997221857309\n",
      "[Epoch 27] Training Batch [132/391]: Loss 0.038104765117168427\n",
      "[Epoch 27] Training Batch [133/391]: Loss 0.0012132131960242987\n",
      "[Epoch 27] Training Batch [134/391]: Loss 0.0004984361003153026\n",
      "[Epoch 27] Training Batch [135/391]: Loss 0.0009159019682556391\n",
      "[Epoch 27] Training Batch [136/391]: Loss 0.0027746602427214384\n",
      "[Epoch 27] Training Batch [137/391]: Loss 0.0002945105661638081\n",
      "[Epoch 27] Training Batch [138/391]: Loss 0.002820618450641632\n",
      "[Epoch 27] Training Batch [139/391]: Loss 0.004498130641877651\n",
      "[Epoch 27] Training Batch [140/391]: Loss 0.0065523358061909676\n",
      "[Epoch 27] Training Batch [141/391]: Loss 0.008123241364955902\n",
      "[Epoch 27] Training Batch [142/391]: Loss 0.004696266259998083\n",
      "[Epoch 27] Training Batch [143/391]: Loss 0.0089681101962924\n",
      "[Epoch 27] Training Batch [144/391]: Loss 0.0044031934812664986\n",
      "[Epoch 27] Training Batch [145/391]: Loss 0.0014363331720232964\n",
      "[Epoch 27] Training Batch [146/391]: Loss 0.003559571923688054\n",
      "[Epoch 27] Training Batch [147/391]: Loss 0.0031650627497583628\n",
      "[Epoch 27] Training Batch [148/391]: Loss 0.004883105400949717\n",
      "[Epoch 27] Training Batch [149/391]: Loss 0.0027399398386478424\n",
      "[Epoch 27] Training Batch [150/391]: Loss 0.0010144886327907443\n",
      "[Epoch 27] Training Batch [151/391]: Loss 0.047927260398864746\n",
      "[Epoch 27] Training Batch [152/391]: Loss 0.0026479687076061964\n",
      "[Epoch 27] Training Batch [153/391]: Loss 0.0035282992757856846\n",
      "[Epoch 27] Training Batch [154/391]: Loss 0.001704902620986104\n",
      "[Epoch 27] Training Batch [155/391]: Loss 0.007445448078215122\n",
      "[Epoch 27] Training Batch [156/391]: Loss 0.0075063519179821014\n",
      "[Epoch 27] Training Batch [157/391]: Loss 0.0018091672100126743\n",
      "[Epoch 27] Training Batch [158/391]: Loss 0.0022725507151335478\n",
      "[Epoch 27] Training Batch [159/391]: Loss 0.019415998831391335\n",
      "[Epoch 27] Training Batch [160/391]: Loss 0.0015711915912106633\n",
      "[Epoch 27] Training Batch [161/391]: Loss 0.00975478533655405\n",
      "[Epoch 27] Training Batch [162/391]: Loss 0.014236509799957275\n",
      "[Epoch 27] Training Batch [163/391]: Loss 0.0067420764826238155\n",
      "[Epoch 27] Training Batch [164/391]: Loss 0.007282712031155825\n",
      "[Epoch 27] Training Batch [165/391]: Loss 0.0009094821289181709\n",
      "[Epoch 27] Training Batch [166/391]: Loss 0.004049741663038731\n",
      "[Epoch 27] Training Batch [167/391]: Loss 0.0012017081025987864\n",
      "[Epoch 27] Training Batch [168/391]: Loss 0.0008803617674857378\n",
      "[Epoch 27] Training Batch [169/391]: Loss 0.001057223416864872\n",
      "[Epoch 27] Training Batch [170/391]: Loss 0.005580165423452854\n",
      "[Epoch 27] Training Batch [171/391]: Loss 0.004803008399903774\n",
      "[Epoch 27] Training Batch [172/391]: Loss 0.011874882504343987\n",
      "[Epoch 27] Training Batch [173/391]: Loss 0.005487772636115551\n",
      "[Epoch 27] Training Batch [174/391]: Loss 0.011952422559261322\n",
      "[Epoch 27] Training Batch [175/391]: Loss 0.03454889357089996\n",
      "[Epoch 27] Training Batch [176/391]: Loss 0.005418566986918449\n",
      "[Epoch 27] Training Batch [177/391]: Loss 0.0005295178852975368\n",
      "[Epoch 27] Training Batch [178/391]: Loss 0.001344334683381021\n",
      "[Epoch 27] Training Batch [179/391]: Loss 0.0025758682750165462\n",
      "[Epoch 27] Training Batch [180/391]: Loss 0.008301121182739735\n",
      "[Epoch 27] Training Batch [181/391]: Loss 0.0033943792805075645\n",
      "[Epoch 27] Training Batch [182/391]: Loss 0.00264959828928113\n",
      "[Epoch 27] Training Batch [183/391]: Loss 0.007314641494303942\n",
      "[Epoch 27] Training Batch [184/391]: Loss 0.02205543965101242\n",
      "[Epoch 27] Training Batch [185/391]: Loss 0.008248805068433285\n",
      "[Epoch 27] Training Batch [186/391]: Loss 0.0012540635652840137\n",
      "[Epoch 27] Training Batch [187/391]: Loss 0.008729005232453346\n",
      "[Epoch 27] Training Batch [188/391]: Loss 0.002423129975795746\n",
      "[Epoch 27] Training Batch [189/391]: Loss 0.0010141645325347781\n",
      "[Epoch 27] Training Batch [190/391]: Loss 0.00630217557772994\n",
      "[Epoch 27] Training Batch [191/391]: Loss 0.000869586190674454\n",
      "[Epoch 27] Training Batch [192/391]: Loss 0.0011733709834516048\n",
      "[Epoch 27] Training Batch [193/391]: Loss 0.00926341861486435\n",
      "[Epoch 27] Training Batch [194/391]: Loss 0.0037154722958803177\n",
      "[Epoch 27] Training Batch [195/391]: Loss 0.0027472958900034428\n",
      "[Epoch 27] Training Batch [196/391]: Loss 0.004842295777052641\n",
      "[Epoch 27] Training Batch [197/391]: Loss 0.01218416914343834\n",
      "[Epoch 27] Training Batch [198/391]: Loss 0.001187506946735084\n",
      "[Epoch 27] Training Batch [199/391]: Loss 0.006006093695759773\n",
      "[Epoch 27] Training Batch [200/391]: Loss 0.014654994010925293\n",
      "[Epoch 27] Training Batch [201/391]: Loss 0.009004626423120499\n",
      "[Epoch 27] Training Batch [202/391]: Loss 0.0029883261304348707\n",
      "[Epoch 27] Training Batch [203/391]: Loss 0.003100048750638962\n",
      "[Epoch 27] Training Batch [204/391]: Loss 0.0017903281841427088\n",
      "[Epoch 27] Training Batch [205/391]: Loss 0.023353278636932373\n",
      "[Epoch 27] Training Batch [206/391]: Loss 0.02214483544230461\n",
      "[Epoch 27] Training Batch [207/391]: Loss 0.0017400422366335988\n",
      "[Epoch 27] Training Batch [208/391]: Loss 0.004473397508263588\n",
      "[Epoch 27] Training Batch [209/391]: Loss 0.010747203603386879\n",
      "[Epoch 27] Training Batch [210/391]: Loss 0.0049355910159647465\n",
      "[Epoch 27] Training Batch [211/391]: Loss 0.0014759717741981149\n",
      "[Epoch 27] Training Batch [212/391]: Loss 0.003796772798523307\n",
      "[Epoch 27] Training Batch [213/391]: Loss 0.00407786900177598\n",
      "[Epoch 27] Training Batch [214/391]: Loss 0.04938603565096855\n",
      "[Epoch 27] Training Batch [215/391]: Loss 0.009152772836387157\n",
      "[Epoch 27] Training Batch [216/391]: Loss 0.0006857870612293482\n",
      "[Epoch 27] Training Batch [217/391]: Loss 0.006425864994525909\n",
      "[Epoch 27] Training Batch [218/391]: Loss 0.002597071463242173\n",
      "[Epoch 27] Training Batch [219/391]: Loss 0.0034607420675456524\n",
      "[Epoch 27] Training Batch [220/391]: Loss 0.0003178679326083511\n",
      "[Epoch 27] Training Batch [221/391]: Loss 0.003234598319977522\n",
      "[Epoch 27] Training Batch [222/391]: Loss 0.0019614461343735456\n",
      "[Epoch 27] Training Batch [223/391]: Loss 0.01445841696113348\n",
      "[Epoch 27] Training Batch [224/391]: Loss 0.02019408345222473\n",
      "[Epoch 27] Training Batch [225/391]: Loss 0.0037464466877281666\n",
      "[Epoch 27] Training Batch [226/391]: Loss 0.0005995129467919469\n",
      "[Epoch 27] Training Batch [227/391]: Loss 0.040014199912548065\n",
      "[Epoch 27] Training Batch [228/391]: Loss 0.0015158330788835883\n",
      "[Epoch 27] Training Batch [229/391]: Loss 0.0018748483853414655\n",
      "[Epoch 27] Training Batch [230/391]: Loss 0.0034722343552857637\n",
      "[Epoch 27] Training Batch [231/391]: Loss 0.005747816059738398\n",
      "[Epoch 27] Training Batch [232/391]: Loss 0.007446518167853355\n",
      "[Epoch 27] Training Batch [233/391]: Loss 0.006108991336077452\n",
      "[Epoch 27] Training Batch [234/391]: Loss 0.0018171267583966255\n",
      "[Epoch 27] Training Batch [235/391]: Loss 0.0022449928801506758\n",
      "[Epoch 27] Training Batch [236/391]: Loss 0.00040396302938461304\n",
      "[Epoch 27] Training Batch [237/391]: Loss 0.025765905156731606\n",
      "[Epoch 27] Training Batch [238/391]: Loss 0.0015521650202572346\n",
      "[Epoch 27] Training Batch [239/391]: Loss 0.0009841457940638065\n",
      "[Epoch 27] Training Batch [240/391]: Loss 0.0021948402281850576\n",
      "[Epoch 27] Training Batch [241/391]: Loss 0.0014410722069442272\n",
      "[Epoch 27] Training Batch [242/391]: Loss 0.003856781404465437\n",
      "[Epoch 27] Training Batch [243/391]: Loss 0.00354550639167428\n",
      "[Epoch 27] Training Batch [244/391]: Loss 0.006850250996649265\n",
      "[Epoch 27] Training Batch [245/391]: Loss 0.0010030524572357535\n",
      "[Epoch 27] Training Batch [246/391]: Loss 0.03504999727010727\n",
      "[Epoch 27] Training Batch [247/391]: Loss 0.02084800973534584\n",
      "[Epoch 27] Training Batch [248/391]: Loss 0.0023422830272465944\n",
      "[Epoch 27] Training Batch [249/391]: Loss 0.002539657521992922\n",
      "[Epoch 27] Training Batch [250/391]: Loss 0.006627283059060574\n",
      "[Epoch 27] Training Batch [251/391]: Loss 0.0017374049639329314\n",
      "[Epoch 27] Training Batch [252/391]: Loss 0.002056482946500182\n",
      "[Epoch 27] Training Batch [253/391]: Loss 0.002091570757329464\n",
      "[Epoch 27] Training Batch [254/391]: Loss 0.0020661239977926016\n",
      "[Epoch 27] Training Batch [255/391]: Loss 0.004678253550082445\n",
      "[Epoch 27] Training Batch [256/391]: Loss 0.00400042999535799\n",
      "[Epoch 27] Training Batch [257/391]: Loss 0.013931171037256718\n",
      "[Epoch 27] Training Batch [258/391]: Loss 0.0006069500232115388\n",
      "[Epoch 27] Training Batch [259/391]: Loss 0.025269772857427597\n",
      "[Epoch 27] Training Batch [260/391]: Loss 0.014106342568993568\n",
      "[Epoch 27] Training Batch [261/391]: Loss 0.0012548459926620126\n",
      "[Epoch 27] Training Batch [262/391]: Loss 0.0010433195857331157\n",
      "[Epoch 27] Training Batch [263/391]: Loss 0.001866486156359315\n",
      "[Epoch 27] Training Batch [264/391]: Loss 0.03531694784760475\n",
      "[Epoch 27] Training Batch [265/391]: Loss 0.01566285826265812\n",
      "[Epoch 27] Training Batch [266/391]: Loss 0.001343829557299614\n",
      "[Epoch 27] Training Batch [267/391]: Loss 0.0025486433878540993\n",
      "[Epoch 27] Training Batch [268/391]: Loss 0.002025797264650464\n",
      "[Epoch 27] Training Batch [269/391]: Loss 0.0015085071790963411\n",
      "[Epoch 27] Training Batch [270/391]: Loss 0.02247687429189682\n",
      "[Epoch 27] Training Batch [271/391]: Loss 0.00163746508769691\n",
      "[Epoch 27] Training Batch [272/391]: Loss 0.003036412177607417\n",
      "[Epoch 27] Training Batch [273/391]: Loss 0.0016873939894139767\n",
      "[Epoch 27] Training Batch [274/391]: Loss 0.0008230885141529143\n",
      "[Epoch 27] Training Batch [275/391]: Loss 0.0023944424465298653\n",
      "[Epoch 27] Training Batch [276/391]: Loss 0.005342548247426748\n",
      "[Epoch 27] Training Batch [277/391]: Loss 0.011748245917260647\n",
      "[Epoch 27] Training Batch [278/391]: Loss 0.012326421216130257\n",
      "[Epoch 27] Training Batch [279/391]: Loss 0.007386783137917519\n",
      "[Epoch 27] Training Batch [280/391]: Loss 0.011278834193944931\n",
      "[Epoch 27] Training Batch [281/391]: Loss 0.005062666721642017\n",
      "[Epoch 27] Training Batch [282/391]: Loss 0.01882139965891838\n",
      "[Epoch 27] Training Batch [283/391]: Loss 0.0065739997662603855\n",
      "[Epoch 27] Training Batch [284/391]: Loss 0.00309730414301157\n",
      "[Epoch 27] Training Batch [285/391]: Loss 0.003023410215973854\n",
      "[Epoch 27] Training Batch [286/391]: Loss 0.0022412852849811316\n",
      "[Epoch 27] Training Batch [287/391]: Loss 0.007186939939856529\n",
      "[Epoch 27] Training Batch [288/391]: Loss 0.005334082990884781\n",
      "[Epoch 27] Training Batch [289/391]: Loss 0.0072795855812728405\n",
      "[Epoch 27] Training Batch [290/391]: Loss 0.014695558696985245\n",
      "[Epoch 27] Training Batch [291/391]: Loss 0.005167399998754263\n",
      "[Epoch 27] Training Batch [292/391]: Loss 0.003124980255961418\n",
      "[Epoch 27] Training Batch [293/391]: Loss 0.003277295734733343\n",
      "[Epoch 27] Training Batch [294/391]: Loss 0.001298514660447836\n",
      "[Epoch 27] Training Batch [295/391]: Loss 0.002812151564285159\n",
      "[Epoch 27] Training Batch [296/391]: Loss 0.003005944425240159\n",
      "[Epoch 27] Training Batch [297/391]: Loss 0.0005931302439421415\n",
      "[Epoch 27] Training Batch [298/391]: Loss 0.001289998646825552\n",
      "[Epoch 27] Training Batch [299/391]: Loss 0.006593433674424887\n",
      "[Epoch 27] Training Batch [300/391]: Loss 0.00433674780651927\n",
      "[Epoch 27] Training Batch [301/391]: Loss 0.0019591515883803368\n",
      "[Epoch 27] Training Batch [302/391]: Loss 0.0013935500755906105\n",
      "[Epoch 27] Training Batch [303/391]: Loss 0.010144149884581566\n",
      "[Epoch 27] Training Batch [304/391]: Loss 0.007085280027240515\n",
      "[Epoch 27] Training Batch [305/391]: Loss 0.0022868949454277754\n",
      "[Epoch 27] Training Batch [306/391]: Loss 0.0011529792100191116\n",
      "[Epoch 27] Training Batch [307/391]: Loss 0.0032116351649165154\n",
      "[Epoch 27] Training Batch [308/391]: Loss 0.0016220457619056106\n",
      "[Epoch 27] Training Batch [309/391]: Loss 0.0014716940931975842\n",
      "[Epoch 27] Training Batch [310/391]: Loss 0.003221384948119521\n",
      "[Epoch 27] Training Batch [311/391]: Loss 0.004062008578330278\n",
      "[Epoch 27] Training Batch [312/391]: Loss 0.0019407157087698579\n",
      "[Epoch 27] Training Batch [313/391]: Loss 0.033902350813150406\n",
      "[Epoch 27] Training Batch [314/391]: Loss 0.0027871248312294483\n",
      "[Epoch 27] Training Batch [315/391]: Loss 0.002843395108357072\n",
      "[Epoch 27] Training Batch [316/391]: Loss 0.0028534955345094204\n",
      "[Epoch 27] Training Batch [317/391]: Loss 0.010940768755972385\n",
      "[Epoch 27] Training Batch [318/391]: Loss 0.0023282174952328205\n",
      "[Epoch 27] Training Batch [319/391]: Loss 0.0023839385248720646\n",
      "[Epoch 27] Training Batch [320/391]: Loss 0.0028024264611303806\n",
      "[Epoch 27] Training Batch [321/391]: Loss 0.012538907118141651\n",
      "[Epoch 27] Training Batch [322/391]: Loss 0.0028318557888269424\n",
      "[Epoch 27] Training Batch [323/391]: Loss 0.010370983742177486\n",
      "[Epoch 27] Training Batch [324/391]: Loss 0.0008817756315693259\n",
      "[Epoch 27] Training Batch [325/391]: Loss 0.0008336205501109362\n",
      "[Epoch 27] Training Batch [326/391]: Loss 0.0015981571050360799\n",
      "[Epoch 27] Training Batch [327/391]: Loss 0.003580502001568675\n",
      "[Epoch 27] Training Batch [328/391]: Loss 0.01798512041568756\n",
      "[Epoch 27] Training Batch [329/391]: Loss 0.004546770825982094\n",
      "[Epoch 27] Training Batch [330/391]: Loss 0.0119961341843009\n",
      "[Epoch 27] Training Batch [331/391]: Loss 0.0012515581911429763\n",
      "[Epoch 27] Training Batch [332/391]: Loss 0.005224763881415129\n",
      "[Epoch 27] Training Batch [333/391]: Loss 0.047180838882923126\n",
      "[Epoch 27] Training Batch [334/391]: Loss 0.001637457637116313\n",
      "[Epoch 27] Training Batch [335/391]: Loss 0.0007265574531629682\n",
      "[Epoch 27] Training Batch [336/391]: Loss 0.011180541478097439\n",
      "[Epoch 27] Training Batch [337/391]: Loss 0.01637187972664833\n",
      "[Epoch 27] Training Batch [338/391]: Loss 0.0031035372521728277\n",
      "[Epoch 27] Training Batch [339/391]: Loss 0.004440799355506897\n",
      "[Epoch 27] Training Batch [340/391]: Loss 0.003967885859310627\n",
      "[Epoch 27] Training Batch [341/391]: Loss 0.012771902605891228\n",
      "[Epoch 27] Training Batch [342/391]: Loss 0.011911988258361816\n",
      "[Epoch 27] Training Batch [343/391]: Loss 0.00787860807031393\n",
      "[Epoch 27] Training Batch [344/391]: Loss 0.006629646755754948\n",
      "[Epoch 27] Training Batch [345/391]: Loss 0.002987224841490388\n",
      "[Epoch 27] Training Batch [346/391]: Loss 0.04547524079680443\n",
      "[Epoch 27] Training Batch [347/391]: Loss 0.004057788290083408\n",
      "[Epoch 27] Training Batch [348/391]: Loss 0.002817993750795722\n",
      "[Epoch 27] Training Batch [349/391]: Loss 0.0029528073500841856\n",
      "[Epoch 27] Training Batch [350/391]: Loss 0.01081913337111473\n",
      "[Epoch 27] Training Batch [351/391]: Loss 0.001212134025990963\n",
      "[Epoch 27] Training Batch [352/391]: Loss 0.004065673798322678\n",
      "[Epoch 27] Training Batch [353/391]: Loss 0.003821493126451969\n",
      "[Epoch 27] Training Batch [354/391]: Loss 0.016232358291745186\n",
      "[Epoch 27] Training Batch [355/391]: Loss 0.004199222661554813\n",
      "[Epoch 27] Training Batch [356/391]: Loss 0.02044125273823738\n",
      "[Epoch 27] Training Batch [357/391]: Loss 0.005918067414313555\n",
      "[Epoch 27] Training Batch [358/391]: Loss 0.001434389385394752\n",
      "[Epoch 27] Training Batch [359/391]: Loss 0.006803389638662338\n",
      "[Epoch 27] Training Batch [360/391]: Loss 0.001905749668367207\n",
      "[Epoch 27] Training Batch [361/391]: Loss 0.004067442379891872\n",
      "[Epoch 27] Training Batch [362/391]: Loss 0.008405004628002644\n",
      "[Epoch 27] Training Batch [363/391]: Loss 0.015578070655465126\n",
      "[Epoch 27] Training Batch [364/391]: Loss 0.008390692062675953\n",
      "[Epoch 27] Training Batch [365/391]: Loss 0.005569382570683956\n",
      "[Epoch 27] Training Batch [366/391]: Loss 0.004419887904077768\n",
      "[Epoch 27] Training Batch [367/391]: Loss 0.007198415696620941\n",
      "[Epoch 27] Training Batch [368/391]: Loss 0.005595923867076635\n",
      "[Epoch 27] Training Batch [369/391]: Loss 0.007678510155528784\n",
      "[Epoch 27] Training Batch [370/391]: Loss 0.0011993347434327006\n",
      "[Epoch 27] Training Batch [371/391]: Loss 0.002786556025967002\n",
      "[Epoch 27] Training Batch [372/391]: Loss 0.003872398054227233\n",
      "[Epoch 27] Training Batch [373/391]: Loss 0.016328217461705208\n",
      "[Epoch 27] Training Batch [374/391]: Loss 0.0018122958717867732\n",
      "[Epoch 27] Training Batch [375/391]: Loss 0.0048200227320194244\n",
      "[Epoch 27] Training Batch [376/391]: Loss 0.0024693042505532503\n",
      "[Epoch 27] Training Batch [377/391]: Loss 0.001152998534962535\n",
      "[Epoch 27] Training Batch [378/391]: Loss 0.007720773573964834\n",
      "[Epoch 27] Training Batch [379/391]: Loss 0.004186450038105249\n",
      "[Epoch 27] Training Batch [380/391]: Loss 0.013063889928162098\n",
      "[Epoch 27] Training Batch [381/391]: Loss 0.011524506844580173\n",
      "[Epoch 27] Training Batch [382/391]: Loss 0.0012761920224875212\n",
      "[Epoch 27] Training Batch [383/391]: Loss 0.009772461839020252\n",
      "[Epoch 27] Training Batch [384/391]: Loss 0.003988039214164019\n",
      "[Epoch 27] Training Batch [385/391]: Loss 0.0006453411187976599\n",
      "[Epoch 27] Training Batch [386/391]: Loss 0.01685027964413166\n",
      "[Epoch 27] Training Batch [387/391]: Loss 0.004439082928001881\n",
      "[Epoch 27] Training Batch [388/391]: Loss 0.005071151535958052\n",
      "[Epoch 27] Training Batch [389/391]: Loss 0.002146701095625758\n",
      "[Epoch 27] Training Batch [390/391]: Loss 0.0034331672359257936\n",
      "[Epoch 27] Training Batch [391/391]: Loss 0.010234485380351543\n",
      "Epoch 27 - Train Loss: 0.0057\n",
      "*********  Epoch 28/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Training Batch [1/391]: Loss 0.005906716454774141\n",
      "[Epoch 28] Training Batch [2/391]: Loss 0.0007120922673493624\n",
      "[Epoch 28] Training Batch [3/391]: Loss 0.0013377501163631678\n",
      "[Epoch 28] Training Batch [4/391]: Loss 0.0017294157296419144\n",
      "[Epoch 28] Training Batch [5/391]: Loss 0.0010135405464097857\n",
      "[Epoch 28] Training Batch [6/391]: Loss 0.003733233083039522\n",
      "[Epoch 28] Training Batch [7/391]: Loss 0.002571077086031437\n",
      "[Epoch 28] Training Batch [8/391]: Loss 0.02676190622150898\n",
      "[Epoch 28] Training Batch [9/391]: Loss 0.00047554681077599525\n",
      "[Epoch 28] Training Batch [10/391]: Loss 0.0030070601496845484\n",
      "[Epoch 28] Training Batch [11/391]: Loss 0.0029714240226894617\n",
      "[Epoch 28] Training Batch [12/391]: Loss 0.011622303165495396\n",
      "[Epoch 28] Training Batch [13/391]: Loss 0.06117276847362518\n",
      "[Epoch 28] Training Batch [14/391]: Loss 0.014141126535832882\n",
      "[Epoch 28] Training Batch [15/391]: Loss 0.005737047176808119\n",
      "[Epoch 28] Training Batch [16/391]: Loss 0.0032321636099368334\n",
      "[Epoch 28] Training Batch [17/391]: Loss 0.0011574544478207827\n",
      "[Epoch 28] Training Batch [18/391]: Loss 0.0021626106463372707\n",
      "[Epoch 28] Training Batch [19/391]: Loss 0.030155984684824944\n",
      "[Epoch 28] Training Batch [20/391]: Loss 0.0020537732634693384\n",
      "[Epoch 28] Training Batch [21/391]: Loss 0.057535938918590546\n",
      "[Epoch 28] Training Batch [22/391]: Loss 0.016130326315760612\n",
      "[Epoch 28] Training Batch [23/391]: Loss 0.009725428186357021\n",
      "[Epoch 28] Training Batch [24/391]: Loss 0.018983248621225357\n",
      "[Epoch 28] Training Batch [25/391]: Loss 0.043381765484809875\n",
      "[Epoch 28] Training Batch [26/391]: Loss 0.002815710846334696\n",
      "[Epoch 28] Training Batch [27/391]: Loss 0.04802815988659859\n",
      "[Epoch 28] Training Batch [28/391]: Loss 0.0021576369181275368\n",
      "[Epoch 28] Training Batch [29/391]: Loss 0.04477708414196968\n",
      "[Epoch 28] Training Batch [30/391]: Loss 0.004082777537405491\n",
      "[Epoch 28] Training Batch [31/391]: Loss 0.004340792540460825\n",
      "[Epoch 28] Training Batch [32/391]: Loss 0.004437913652509451\n",
      "[Epoch 28] Training Batch [33/391]: Loss 0.01606527902185917\n",
      "[Epoch 28] Training Batch [34/391]: Loss 0.0075042881071567535\n",
      "[Epoch 28] Training Batch [35/391]: Loss 0.003639382775872946\n",
      "[Epoch 28] Training Batch [36/391]: Loss 0.003927918151021004\n",
      "[Epoch 28] Training Batch [37/391]: Loss 0.0023400112986564636\n",
      "[Epoch 28] Training Batch [38/391]: Loss 0.007840967737138271\n",
      "[Epoch 28] Training Batch [39/391]: Loss 0.04757986590266228\n",
      "[Epoch 28] Training Batch [40/391]: Loss 0.007479039020836353\n",
      "[Epoch 28] Training Batch [41/391]: Loss 0.0047728316858410835\n",
      "[Epoch 28] Training Batch [42/391]: Loss 0.03546759486198425\n",
      "[Epoch 28] Training Batch [43/391]: Loss 0.0032860245555639267\n",
      "[Epoch 28] Training Batch [44/391]: Loss 0.0013538458151742816\n",
      "[Epoch 28] Training Batch [45/391]: Loss 0.001120480359531939\n",
      "[Epoch 28] Training Batch [46/391]: Loss 0.0013554845936596394\n",
      "[Epoch 28] Training Batch [47/391]: Loss 0.012072883546352386\n",
      "[Epoch 28] Training Batch [48/391]: Loss 0.015389513224363327\n",
      "[Epoch 28] Training Batch [49/391]: Loss 0.0018192545976489782\n",
      "[Epoch 28] Training Batch [50/391]: Loss 0.02871517837047577\n",
      "[Epoch 28] Training Batch [51/391]: Loss 0.004774455912411213\n",
      "[Epoch 28] Training Batch [52/391]: Loss 0.010217254981398582\n",
      "[Epoch 28] Training Batch [53/391]: Loss 0.0024100791197270155\n",
      "[Epoch 28] Training Batch [54/391]: Loss 0.008631167002022266\n",
      "[Epoch 28] Training Batch [55/391]: Loss 0.006726440973579884\n",
      "[Epoch 28] Training Batch [56/391]: Loss 0.001679004868492484\n",
      "[Epoch 28] Training Batch [57/391]: Loss 0.004446692299097776\n",
      "[Epoch 28] Training Batch [58/391]: Loss 0.0037644694093614817\n",
      "[Epoch 28] Training Batch [59/391]: Loss 0.006383836735039949\n",
      "[Epoch 28] Training Batch [60/391]: Loss 0.0009995410218834877\n",
      "[Epoch 28] Training Batch [61/391]: Loss 0.0009776597144082189\n",
      "[Epoch 28] Training Batch [62/391]: Loss 0.013793932273983955\n",
      "[Epoch 28] Training Batch [63/391]: Loss 0.0018659825436770916\n",
      "[Epoch 28] Training Batch [64/391]: Loss 0.026099292561411858\n",
      "[Epoch 28] Training Batch [65/391]: Loss 0.015321704559028149\n",
      "[Epoch 28] Training Batch [66/391]: Loss 0.0038519687950611115\n",
      "[Epoch 28] Training Batch [67/391]: Loss 0.0012216322356835008\n",
      "[Epoch 28] Training Batch [68/391]: Loss 0.0013058464974164963\n",
      "[Epoch 28] Training Batch [69/391]: Loss 0.026239510625600815\n",
      "[Epoch 28] Training Batch [70/391]: Loss 0.001820604084059596\n",
      "[Epoch 28] Training Batch [71/391]: Loss 0.005025219172239304\n",
      "[Epoch 28] Training Batch [72/391]: Loss 0.002275065053254366\n",
      "[Epoch 28] Training Batch [73/391]: Loss 0.0033550977241247892\n",
      "[Epoch 28] Training Batch [74/391]: Loss 0.003183054504916072\n",
      "[Epoch 28] Training Batch [75/391]: Loss 0.002040926367044449\n",
      "[Epoch 28] Training Batch [76/391]: Loss 0.00950353778898716\n",
      "[Epoch 28] Training Batch [77/391]: Loss 0.020397184416651726\n",
      "[Epoch 28] Training Batch [78/391]: Loss 0.00278320349752903\n",
      "[Epoch 28] Training Batch [79/391]: Loss 0.0019423561170697212\n",
      "[Epoch 28] Training Batch [80/391]: Loss 0.012216613627970219\n",
      "[Epoch 28] Training Batch [81/391]: Loss 0.0035575309302657843\n",
      "[Epoch 28] Training Batch [82/391]: Loss 0.002253300743177533\n",
      "[Epoch 28] Training Batch [83/391]: Loss 0.008125086314976215\n",
      "[Epoch 28] Training Batch [84/391]: Loss 0.005547248292714357\n",
      "[Epoch 28] Training Batch [85/391]: Loss 0.005501528736203909\n",
      "[Epoch 28] Training Batch [86/391]: Loss 0.002098471624776721\n",
      "[Epoch 28] Training Batch [87/391]: Loss 0.0025767777115106583\n",
      "[Epoch 28] Training Batch [88/391]: Loss 0.017033157870173454\n",
      "[Epoch 28] Training Batch [89/391]: Loss 0.008269228041172028\n",
      "[Epoch 28] Training Batch [90/391]: Loss 0.017803484573960304\n",
      "[Epoch 28] Training Batch [91/391]: Loss 0.005206741858273745\n",
      "[Epoch 28] Training Batch [92/391]: Loss 0.015349351800978184\n",
      "[Epoch 28] Training Batch [93/391]: Loss 0.0034420278389006853\n",
      "[Epoch 28] Training Batch [94/391]: Loss 0.002101498655974865\n",
      "[Epoch 28] Training Batch [95/391]: Loss 0.0007237251265905797\n",
      "[Epoch 28] Training Batch [96/391]: Loss 0.0029770564287900925\n",
      "[Epoch 28] Training Batch [97/391]: Loss 0.001522521604783833\n",
      "[Epoch 28] Training Batch [98/391]: Loss 0.0010576365748420358\n",
      "[Epoch 28] Training Batch [99/391]: Loss 0.006582245696336031\n",
      "[Epoch 28] Training Batch [100/391]: Loss 0.0018146762158721685\n",
      "[Epoch 28] Training Batch [101/391]: Loss 0.007653967477381229\n",
      "[Epoch 28] Training Batch [102/391]: Loss 0.0007887600222602487\n",
      "[Epoch 28] Training Batch [103/391]: Loss 0.006937972269952297\n",
      "[Epoch 28] Training Batch [104/391]: Loss 0.0029141060076653957\n",
      "[Epoch 28] Training Batch [105/391]: Loss 0.0052498709410429\n",
      "[Epoch 28] Training Batch [106/391]: Loss 0.0070632747374475\n",
      "[Epoch 28] Training Batch [107/391]: Loss 0.009453616105020046\n",
      "[Epoch 28] Training Batch [108/391]: Loss 0.0010279593989253044\n",
      "[Epoch 28] Training Batch [109/391]: Loss 0.0009045696933753788\n",
      "[Epoch 28] Training Batch [110/391]: Loss 0.0036511803045868874\n",
      "[Epoch 28] Training Batch [111/391]: Loss 0.014322818256914616\n",
      "[Epoch 28] Training Batch [112/391]: Loss 0.003568320069462061\n",
      "[Epoch 28] Training Batch [113/391]: Loss 0.015129873529076576\n",
      "[Epoch 28] Training Batch [114/391]: Loss 0.010222053155303001\n",
      "[Epoch 28] Training Batch [115/391]: Loss 0.0021119005978107452\n",
      "[Epoch 28] Training Batch [116/391]: Loss 0.0010854846332222223\n",
      "[Epoch 28] Training Batch [117/391]: Loss 0.003172011813148856\n",
      "[Epoch 28] Training Batch [118/391]: Loss 0.02822958491742611\n",
      "[Epoch 28] Training Batch [119/391]: Loss 0.0017645093612372875\n",
      "[Epoch 28] Training Batch [120/391]: Loss 0.010517067275941372\n",
      "[Epoch 28] Training Batch [121/391]: Loss 0.0051581962034106255\n",
      "[Epoch 28] Training Batch [122/391]: Loss 0.0036916069220751524\n",
      "[Epoch 28] Training Batch [123/391]: Loss 0.01601719669997692\n",
      "[Epoch 28] Training Batch [124/391]: Loss 0.008351189084351063\n",
      "[Epoch 28] Training Batch [125/391]: Loss 0.0043651992455124855\n",
      "[Epoch 28] Training Batch [126/391]: Loss 0.002686274703592062\n",
      "[Epoch 28] Training Batch [127/391]: Loss 0.001280777738429606\n",
      "[Epoch 28] Training Batch [128/391]: Loss 0.00039215758442878723\n",
      "[Epoch 28] Training Batch [129/391]: Loss 0.008252440951764584\n",
      "[Epoch 28] Training Batch [130/391]: Loss 0.0006153741851449013\n",
      "[Epoch 28] Training Batch [131/391]: Loss 0.007382294163107872\n",
      "[Epoch 28] Training Batch [132/391]: Loss 0.01340709999203682\n",
      "[Epoch 28] Training Batch [133/391]: Loss 0.0027012822683900595\n",
      "[Epoch 28] Training Batch [134/391]: Loss 0.0048764608800411224\n",
      "[Epoch 28] Training Batch [135/391]: Loss 0.008063849993050098\n",
      "[Epoch 28] Training Batch [136/391]: Loss 0.008699193596839905\n",
      "[Epoch 28] Training Batch [137/391]: Loss 0.0017389233689755201\n",
      "[Epoch 28] Training Batch [138/391]: Loss 0.0019371026428416371\n",
      "[Epoch 28] Training Batch [139/391]: Loss 0.0029421886429190636\n",
      "[Epoch 28] Training Batch [140/391]: Loss 0.005738310981541872\n",
      "[Epoch 28] Training Batch [141/391]: Loss 0.004622870124876499\n",
      "[Epoch 28] Training Batch [142/391]: Loss 0.021367663517594337\n",
      "[Epoch 28] Training Batch [143/391]: Loss 0.0007412172271870077\n",
      "[Epoch 28] Training Batch [144/391]: Loss 0.00486825592815876\n",
      "[Epoch 28] Training Batch [145/391]: Loss 0.0037955516017973423\n",
      "[Epoch 28] Training Batch [146/391]: Loss 0.007637151516973972\n",
      "[Epoch 28] Training Batch [147/391]: Loss 0.02968217246234417\n",
      "[Epoch 28] Training Batch [148/391]: Loss 0.002122331876307726\n",
      "[Epoch 28] Training Batch [149/391]: Loss 0.002070266753435135\n",
      "[Epoch 28] Training Batch [150/391]: Loss 0.01057477854192257\n",
      "[Epoch 28] Training Batch [151/391]: Loss 0.012257140129804611\n",
      "[Epoch 28] Training Batch [152/391]: Loss 0.0025784545578062534\n",
      "[Epoch 28] Training Batch [153/391]: Loss 0.00521116703748703\n",
      "[Epoch 28] Training Batch [154/391]: Loss 0.0018194864969700575\n",
      "[Epoch 28] Training Batch [155/391]: Loss 0.002184610115364194\n",
      "[Epoch 28] Training Batch [156/391]: Loss 0.006339845713227987\n",
      "[Epoch 28] Training Batch [157/391]: Loss 0.017326518893241882\n",
      "[Epoch 28] Training Batch [158/391]: Loss 0.0018184451619163156\n",
      "[Epoch 28] Training Batch [159/391]: Loss 0.0040870108641684055\n",
      "[Epoch 28] Training Batch [160/391]: Loss 0.0027279823552817106\n",
      "[Epoch 28] Training Batch [161/391]: Loss 0.012754195369780064\n",
      "[Epoch 28] Training Batch [162/391]: Loss 0.006937900558114052\n",
      "[Epoch 28] Training Batch [163/391]: Loss 0.0029612919315695763\n",
      "[Epoch 28] Training Batch [164/391]: Loss 0.0007490336429327726\n",
      "[Epoch 28] Training Batch [165/391]: Loss 0.00199069338850677\n",
      "[Epoch 28] Training Batch [166/391]: Loss 0.003902286756783724\n",
      "[Epoch 28] Training Batch [167/391]: Loss 0.021739620715379715\n",
      "[Epoch 28] Training Batch [168/391]: Loss 0.0013461913913488388\n",
      "[Epoch 28] Training Batch [169/391]: Loss 0.006198358256369829\n",
      "[Epoch 28] Training Batch [170/391]: Loss 0.0011218523140996695\n",
      "[Epoch 28] Training Batch [171/391]: Loss 0.000701199343893677\n",
      "[Epoch 28] Training Batch [172/391]: Loss 0.006740055978298187\n",
      "[Epoch 28] Training Batch [173/391]: Loss 0.006756176706403494\n",
      "[Epoch 28] Training Batch [174/391]: Loss 0.001747815520502627\n",
      "[Epoch 28] Training Batch [175/391]: Loss 0.0010462584905326366\n",
      "[Epoch 28] Training Batch [176/391]: Loss 0.011676006019115448\n",
      "[Epoch 28] Training Batch [177/391]: Loss 0.0014797811163589358\n",
      "[Epoch 28] Training Batch [178/391]: Loss 0.005539687350392342\n",
      "[Epoch 28] Training Batch [179/391]: Loss 0.0025824608746916056\n",
      "[Epoch 28] Training Batch [180/391]: Loss 0.003067530458793044\n",
      "[Epoch 28] Training Batch [181/391]: Loss 0.0016246977029368281\n",
      "[Epoch 28] Training Batch [182/391]: Loss 0.00234350492246449\n",
      "[Epoch 28] Training Batch [183/391]: Loss 0.005590952932834625\n",
      "[Epoch 28] Training Batch [184/391]: Loss 0.00214874604716897\n",
      "[Epoch 28] Training Batch [185/391]: Loss 0.006693761795759201\n",
      "[Epoch 28] Training Batch [186/391]: Loss 0.008305519819259644\n",
      "[Epoch 28] Training Batch [187/391]: Loss 0.003979346249252558\n",
      "[Epoch 28] Training Batch [188/391]: Loss 0.006740597542375326\n",
      "[Epoch 28] Training Batch [189/391]: Loss 0.005452610552310944\n",
      "[Epoch 28] Training Batch [190/391]: Loss 0.0011378146009519696\n",
      "[Epoch 28] Training Batch [191/391]: Loss 0.017154427245259285\n",
      "[Epoch 28] Training Batch [192/391]: Loss 0.0012540094321593642\n",
      "[Epoch 28] Training Batch [193/391]: Loss 0.018268493935465813\n",
      "[Epoch 28] Training Batch [194/391]: Loss 0.0003978477616328746\n",
      "[Epoch 28] Training Batch [195/391]: Loss 0.004911497700959444\n",
      "[Epoch 28] Training Batch [196/391]: Loss 0.0010445447405800223\n",
      "[Epoch 28] Training Batch [197/391]: Loss 0.0033739039208739996\n",
      "[Epoch 28] Training Batch [198/391]: Loss 0.017984725534915924\n",
      "[Epoch 28] Training Batch [199/391]: Loss 0.05550293251872063\n",
      "[Epoch 28] Training Batch [200/391]: Loss 0.006124951411038637\n",
      "[Epoch 28] Training Batch [201/391]: Loss 0.006162472069263458\n",
      "[Epoch 28] Training Batch [202/391]: Loss 0.0014630845980718732\n",
      "[Epoch 28] Training Batch [203/391]: Loss 0.004309579264372587\n",
      "[Epoch 28] Training Batch [204/391]: Loss 0.004064139910042286\n",
      "[Epoch 28] Training Batch [205/391]: Loss 0.001413008663803339\n",
      "[Epoch 28] Training Batch [206/391]: Loss 0.002283091889694333\n",
      "[Epoch 28] Training Batch [207/391]: Loss 0.006143580190837383\n",
      "[Epoch 28] Training Batch [208/391]: Loss 0.005886798724532127\n",
      "[Epoch 28] Training Batch [209/391]: Loss 0.00971047580242157\n",
      "[Epoch 28] Training Batch [210/391]: Loss 0.011458930559456348\n",
      "[Epoch 28] Training Batch [211/391]: Loss 0.0007201102562248707\n",
      "[Epoch 28] Training Batch [212/391]: Loss 0.005270098336040974\n",
      "[Epoch 28] Training Batch [213/391]: Loss 0.0019313095835968852\n",
      "[Epoch 28] Training Batch [214/391]: Loss 0.0034077526070177555\n",
      "[Epoch 28] Training Batch [215/391]: Loss 0.025622546672821045\n",
      "[Epoch 28] Training Batch [216/391]: Loss 0.01186039112508297\n",
      "[Epoch 28] Training Batch [217/391]: Loss 0.00340845575556159\n",
      "[Epoch 28] Training Batch [218/391]: Loss 0.001281814300455153\n",
      "[Epoch 28] Training Batch [219/391]: Loss 0.0013418259331956506\n",
      "[Epoch 28] Training Batch [220/391]: Loss 0.0028262371197342873\n",
      "[Epoch 28] Training Batch [221/391]: Loss 0.0012111050309613347\n",
      "[Epoch 28] Training Batch [222/391]: Loss 0.0029972170013934374\n",
      "[Epoch 28] Training Batch [223/391]: Loss 0.0009829699993133545\n",
      "[Epoch 28] Training Batch [224/391]: Loss 0.003312874585390091\n",
      "[Epoch 28] Training Batch [225/391]: Loss 0.05708380788564682\n",
      "[Epoch 28] Training Batch [226/391]: Loss 0.0071757519617676735\n",
      "[Epoch 28] Training Batch [227/391]: Loss 0.003762372536584735\n",
      "[Epoch 28] Training Batch [228/391]: Loss 0.02274041809141636\n",
      "[Epoch 28] Training Batch [229/391]: Loss 0.007774680852890015\n",
      "[Epoch 28] Training Batch [230/391]: Loss 0.023144202306866646\n",
      "[Epoch 28] Training Batch [231/391]: Loss 0.0012670003343373537\n",
      "[Epoch 28] Training Batch [232/391]: Loss 0.008191357366740704\n",
      "[Epoch 28] Training Batch [233/391]: Loss 0.0010495221940800548\n",
      "[Epoch 28] Training Batch [234/391]: Loss 0.0341939851641655\n",
      "[Epoch 28] Training Batch [235/391]: Loss 0.01724499650299549\n",
      "[Epoch 28] Training Batch [236/391]: Loss 0.013430306687951088\n",
      "[Epoch 28] Training Batch [237/391]: Loss 0.02525121159851551\n",
      "[Epoch 28] Training Batch [238/391]: Loss 0.03516041114926338\n",
      "[Epoch 28] Training Batch [239/391]: Loss 0.012055237777531147\n",
      "[Epoch 28] Training Batch [240/391]: Loss 0.008014173246920109\n",
      "[Epoch 28] Training Batch [241/391]: Loss 0.0009838832775130868\n",
      "[Epoch 28] Training Batch [242/391]: Loss 0.03511008992791176\n",
      "[Epoch 28] Training Batch [243/391]: Loss 0.0034429742954671383\n",
      "[Epoch 28] Training Batch [244/391]: Loss 0.0031923134811222553\n",
      "[Epoch 28] Training Batch [245/391]: Loss 0.06273053586483002\n",
      "[Epoch 28] Training Batch [246/391]: Loss 0.03044884093105793\n",
      "[Epoch 28] Training Batch [247/391]: Loss 0.006369238253682852\n",
      "[Epoch 28] Training Batch [248/391]: Loss 0.01713024079799652\n",
      "[Epoch 28] Training Batch [249/391]: Loss 0.008964031003415585\n",
      "[Epoch 28] Training Batch [250/391]: Loss 0.004744237754493952\n",
      "[Epoch 28] Training Batch [251/391]: Loss 0.004565199371427298\n",
      "[Epoch 28] Training Batch [252/391]: Loss 0.0076022702269256115\n",
      "[Epoch 28] Training Batch [253/391]: Loss 0.016245586797595024\n",
      "[Epoch 28] Training Batch [254/391]: Loss 0.010192311368882656\n",
      "[Epoch 28] Training Batch [255/391]: Loss 0.015955191105604172\n",
      "[Epoch 28] Training Batch [256/391]: Loss 0.009891149587929249\n",
      "[Epoch 28] Training Batch [257/391]: Loss 0.025956805795431137\n",
      "[Epoch 28] Training Batch [258/391]: Loss 0.017430460080504417\n",
      "[Epoch 28] Training Batch [259/391]: Loss 0.03159232437610626\n",
      "[Epoch 28] Training Batch [260/391]: Loss 0.007139136549085379\n",
      "[Epoch 28] Training Batch [261/391]: Loss 0.002866256982088089\n",
      "[Epoch 28] Training Batch [262/391]: Loss 0.0209036935120821\n",
      "[Epoch 28] Training Batch [263/391]: Loss 0.037109896540641785\n",
      "[Epoch 28] Training Batch [264/391]: Loss 0.031140029430389404\n",
      "[Epoch 28] Training Batch [265/391]: Loss 0.05454365536570549\n",
      "[Epoch 28] Training Batch [266/391]: Loss 0.09637502580881119\n",
      "[Epoch 28] Training Batch [267/391]: Loss 0.011660479940474033\n",
      "[Epoch 28] Training Batch [268/391]: Loss 0.009831004776060581\n",
      "[Epoch 28] Training Batch [269/391]: Loss 0.005304255522787571\n",
      "[Epoch 28] Training Batch [270/391]: Loss 0.012111935764551163\n",
      "[Epoch 28] Training Batch [271/391]: Loss 0.12276744097471237\n",
      "[Epoch 28] Training Batch [272/391]: Loss 0.03316396102309227\n",
      "[Epoch 28] Training Batch [273/391]: Loss 0.018798667937517166\n",
      "[Epoch 28] Training Batch [274/391]: Loss 0.03009418211877346\n",
      "[Epoch 28] Training Batch [275/391]: Loss 0.016761329025030136\n",
      "[Epoch 28] Training Batch [276/391]: Loss 0.010468045249581337\n",
      "[Epoch 28] Training Batch [277/391]: Loss 0.057564303278923035\n",
      "[Epoch 28] Training Batch [278/391]: Loss 0.0529901385307312\n",
      "[Epoch 28] Training Batch [279/391]: Loss 0.05296174809336662\n",
      "[Epoch 28] Training Batch [280/391]: Loss 0.011009745299816132\n",
      "[Epoch 28] Training Batch [281/391]: Loss 0.013091104105114937\n",
      "[Epoch 28] Training Batch [282/391]: Loss 0.035632550716400146\n",
      "[Epoch 28] Training Batch [283/391]: Loss 0.07928702235221863\n",
      "[Epoch 28] Training Batch [284/391]: Loss 0.01800522767007351\n",
      "[Epoch 28] Training Batch [285/391]: Loss 0.044661995023489\n",
      "[Epoch 28] Training Batch [286/391]: Loss 0.0051803733222186565\n",
      "[Epoch 28] Training Batch [287/391]: Loss 0.008179094642400742\n",
      "[Epoch 28] Training Batch [288/391]: Loss 0.009946147911250591\n",
      "[Epoch 28] Training Batch [289/391]: Loss 0.04708852618932724\n",
      "[Epoch 28] Training Batch [290/391]: Loss 0.004062699619680643\n",
      "[Epoch 28] Training Batch [291/391]: Loss 0.004188788589090109\n",
      "[Epoch 28] Training Batch [292/391]: Loss 0.025901740416884422\n",
      "[Epoch 28] Training Batch [293/391]: Loss 0.009025895968079567\n",
      "[Epoch 28] Training Batch [294/391]: Loss 0.0505109578371048\n",
      "[Epoch 28] Training Batch [295/391]: Loss 0.01142472866922617\n",
      "[Epoch 28] Training Batch [296/391]: Loss 0.07031187415122986\n",
      "[Epoch 28] Training Batch [297/391]: Loss 0.024145428091287613\n",
      "[Epoch 28] Training Batch [298/391]: Loss 0.02139969915151596\n",
      "[Epoch 28] Training Batch [299/391]: Loss 0.08865918964147568\n",
      "[Epoch 28] Training Batch [300/391]: Loss 0.08307240903377533\n",
      "[Epoch 28] Training Batch [301/391]: Loss 0.02138916775584221\n",
      "[Epoch 28] Training Batch [302/391]: Loss 0.007205459754914045\n",
      "[Epoch 28] Training Batch [303/391]: Loss 0.022417938336730003\n",
      "[Epoch 28] Training Batch [304/391]: Loss 0.041854485869407654\n",
      "[Epoch 28] Training Batch [305/391]: Loss 0.04065854847431183\n",
      "[Epoch 28] Training Batch [306/391]: Loss 0.020298611372709274\n",
      "[Epoch 28] Training Batch [307/391]: Loss 0.010567932389676571\n",
      "[Epoch 28] Training Batch [308/391]: Loss 0.0033825947903096676\n",
      "[Epoch 28] Training Batch [309/391]: Loss 0.026864055544137955\n",
      "[Epoch 28] Training Batch [310/391]: Loss 0.005277574528008699\n",
      "[Epoch 28] Training Batch [311/391]: Loss 0.02639431320130825\n",
      "[Epoch 28] Training Batch [312/391]: Loss 0.014398202300071716\n",
      "[Epoch 28] Training Batch [313/391]: Loss 0.0061805774457752705\n",
      "[Epoch 28] Training Batch [314/391]: Loss 0.025242146104574203\n",
      "[Epoch 28] Training Batch [315/391]: Loss 0.050936080515384674\n",
      "[Epoch 28] Training Batch [316/391]: Loss 0.011265764944255352\n",
      "[Epoch 28] Training Batch [317/391]: Loss 0.023523425683379173\n",
      "[Epoch 28] Training Batch [318/391]: Loss 0.04161052405834198\n",
      "[Epoch 28] Training Batch [319/391]: Loss 0.008213629014790058\n",
      "[Epoch 28] Training Batch [320/391]: Loss 0.022914234548807144\n",
      "[Epoch 28] Training Batch [321/391]: Loss 0.050398945808410645\n",
      "[Epoch 28] Training Batch [322/391]: Loss 0.05770357325673103\n",
      "[Epoch 28] Training Batch [323/391]: Loss 0.005670228041708469\n",
      "[Epoch 28] Training Batch [324/391]: Loss 0.0210027564316988\n",
      "[Epoch 28] Training Batch [325/391]: Loss 0.0019302802393212914\n",
      "[Epoch 28] Training Batch [326/391]: Loss 0.020584115758538246\n",
      "[Epoch 28] Training Batch [327/391]: Loss 0.021477648988366127\n",
      "[Epoch 28] Training Batch [328/391]: Loss 0.061274100095033646\n",
      "[Epoch 28] Training Batch [329/391]: Loss 0.01747065968811512\n",
      "[Epoch 28] Training Batch [330/391]: Loss 0.06182187795639038\n",
      "[Epoch 28] Training Batch [331/391]: Loss 0.00783492811024189\n",
      "[Epoch 28] Training Batch [332/391]: Loss 0.031248312443494797\n",
      "[Epoch 28] Training Batch [333/391]: Loss 0.05944381654262543\n",
      "[Epoch 28] Training Batch [334/391]: Loss 0.03207913413643837\n",
      "[Epoch 28] Training Batch [335/391]: Loss 0.02487040124833584\n",
      "[Epoch 28] Training Batch [336/391]: Loss 0.020274139940738678\n",
      "[Epoch 28] Training Batch [337/391]: Loss 0.02896496094763279\n",
      "[Epoch 28] Training Batch [338/391]: Loss 0.002339104423299432\n",
      "[Epoch 28] Training Batch [339/391]: Loss 0.0084585165604949\n",
      "[Epoch 28] Training Batch [340/391]: Loss 0.01843256875872612\n",
      "[Epoch 28] Training Batch [341/391]: Loss 0.011418025940656662\n",
      "[Epoch 28] Training Batch [342/391]: Loss 0.043022528290748596\n",
      "[Epoch 28] Training Batch [343/391]: Loss 0.03918056935071945\n",
      "[Epoch 28] Training Batch [344/391]: Loss 0.04285352677106857\n",
      "[Epoch 28] Training Batch [345/391]: Loss 0.012093323282897472\n",
      "[Epoch 28] Training Batch [346/391]: Loss 0.02115303836762905\n",
      "[Epoch 28] Training Batch [347/391]: Loss 0.022981861606240273\n",
      "[Epoch 28] Training Batch [348/391]: Loss 0.020645810291171074\n",
      "[Epoch 28] Training Batch [349/391]: Loss 0.01935349404811859\n",
      "[Epoch 28] Training Batch [350/391]: Loss 0.06993798166513443\n",
      "[Epoch 28] Training Batch [351/391]: Loss 0.049398187547922134\n",
      "[Epoch 28] Training Batch [352/391]: Loss 0.0321546271443367\n",
      "[Epoch 28] Training Batch [353/391]: Loss 0.039819177240133286\n",
      "[Epoch 28] Training Batch [354/391]: Loss 0.05835307389497757\n",
      "[Epoch 28] Training Batch [355/391]: Loss 0.024298321455717087\n",
      "[Epoch 28] Training Batch [356/391]: Loss 0.08046440780162811\n",
      "[Epoch 28] Training Batch [357/391]: Loss 0.022265594452619553\n",
      "[Epoch 28] Training Batch [358/391]: Loss 0.010202934965491295\n",
      "[Epoch 28] Training Batch [359/391]: Loss 0.06354338675737381\n",
      "[Epoch 28] Training Batch [360/391]: Loss 0.024578265845775604\n",
      "[Epoch 28] Training Batch [361/391]: Loss 0.01073238905519247\n",
      "[Epoch 28] Training Batch [362/391]: Loss 0.02258620597422123\n",
      "[Epoch 28] Training Batch [363/391]: Loss 0.011340465396642685\n",
      "[Epoch 28] Training Batch [364/391]: Loss 0.010539042763411999\n",
      "[Epoch 28] Training Batch [365/391]: Loss 0.020416172221302986\n",
      "[Epoch 28] Training Batch [366/391]: Loss 0.03233535960316658\n",
      "[Epoch 28] Training Batch [367/391]: Loss 0.012486995197832584\n",
      "[Epoch 28] Training Batch [368/391]: Loss 0.05884083732962608\n",
      "[Epoch 28] Training Batch [369/391]: Loss 0.046460364013910294\n",
      "[Epoch 28] Training Batch [370/391]: Loss 0.009905500337481499\n",
      "[Epoch 28] Training Batch [371/391]: Loss 0.07950429618358612\n",
      "[Epoch 28] Training Batch [372/391]: Loss 0.020288744941353798\n",
      "[Epoch 28] Training Batch [373/391]: Loss 0.043227970600128174\n",
      "[Epoch 28] Training Batch [374/391]: Loss 0.003045301651582122\n",
      "[Epoch 28] Training Batch [375/391]: Loss 0.053738467395305634\n",
      "[Epoch 28] Training Batch [376/391]: Loss 0.0071896882727742195\n",
      "[Epoch 28] Training Batch [377/391]: Loss 0.028930004686117172\n",
      "[Epoch 28] Training Batch [378/391]: Loss 0.038365937769412994\n",
      "[Epoch 28] Training Batch [379/391]: Loss 0.013536506332457066\n",
      "[Epoch 28] Training Batch [380/391]: Loss 0.054940059781074524\n",
      "[Epoch 28] Training Batch [381/391]: Loss 0.04777804762125015\n",
      "[Epoch 28] Training Batch [382/391]: Loss 0.011788098141551018\n",
      "[Epoch 28] Training Batch [383/391]: Loss 0.02314380183815956\n",
      "[Epoch 28] Training Batch [384/391]: Loss 0.03758344054222107\n",
      "[Epoch 28] Training Batch [385/391]: Loss 0.07716566324234009\n",
      "[Epoch 28] Training Batch [386/391]: Loss 0.02657066099345684\n",
      "[Epoch 28] Training Batch [387/391]: Loss 0.06316333264112473\n",
      "[Epoch 28] Training Batch [388/391]: Loss 0.009878167882561684\n",
      "[Epoch 28] Training Batch [389/391]: Loss 0.015424756333231926\n",
      "[Epoch 28] Training Batch [390/391]: Loss 0.06740957498550415\n",
      "[Epoch 28] Training Batch [391/391]: Loss 0.03049718774855137\n",
      "Epoch 28 - Train Loss: 0.0163\n",
      "*********  Epoch 29/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Training Batch [1/391]: Loss 0.02399054728448391\n",
      "[Epoch 29] Training Batch [2/391]: Loss 0.014909142628312111\n",
      "[Epoch 29] Training Batch [3/391]: Loss 0.030027301982045174\n",
      "[Epoch 29] Training Batch [4/391]: Loss 0.09344661980867386\n",
      "[Epoch 29] Training Batch [5/391]: Loss 0.03628331050276756\n",
      "[Epoch 29] Training Batch [6/391]: Loss 0.04650586098432541\n",
      "[Epoch 29] Training Batch [7/391]: Loss 0.0125865014269948\n",
      "[Epoch 29] Training Batch [8/391]: Loss 0.016663406044244766\n",
      "[Epoch 29] Training Batch [9/391]: Loss 0.08694049715995789\n",
      "[Epoch 29] Training Batch [10/391]: Loss 0.016764048486948013\n",
      "[Epoch 29] Training Batch [11/391]: Loss 0.01802727021276951\n",
      "[Epoch 29] Training Batch [12/391]: Loss 0.015603727661073208\n",
      "[Epoch 29] Training Batch [13/391]: Loss 0.014570779167115688\n",
      "[Epoch 29] Training Batch [14/391]: Loss 0.0468079149723053\n",
      "[Epoch 29] Training Batch [15/391]: Loss 0.0614173524081707\n",
      "[Epoch 29] Training Batch [16/391]: Loss 0.04293494671583176\n",
      "[Epoch 29] Training Batch [17/391]: Loss 0.05084279552102089\n",
      "[Epoch 29] Training Batch [18/391]: Loss 0.014807423576712608\n",
      "[Epoch 29] Training Batch [19/391]: Loss 0.012002960778772831\n",
      "[Epoch 29] Training Batch [20/391]: Loss 0.0179601963609457\n",
      "[Epoch 29] Training Batch [21/391]: Loss 0.0504344217479229\n",
      "[Epoch 29] Training Batch [22/391]: Loss 0.02031710371375084\n",
      "[Epoch 29] Training Batch [23/391]: Loss 0.03349592536687851\n",
      "[Epoch 29] Training Batch [24/391]: Loss 0.009151429869234562\n",
      "[Epoch 29] Training Batch [25/391]: Loss 0.01621917076408863\n",
      "[Epoch 29] Training Batch [26/391]: Loss 0.028671061620116234\n",
      "[Epoch 29] Training Batch [27/391]: Loss 0.049472205340862274\n",
      "[Epoch 29] Training Batch [28/391]: Loss 0.01351719256490469\n",
      "[Epoch 29] Training Batch [29/391]: Loss 0.009506437927484512\n",
      "[Epoch 29] Training Batch [30/391]: Loss 0.036556426435709\n",
      "[Epoch 29] Training Batch [31/391]: Loss 0.015740789473056793\n",
      "[Epoch 29] Training Batch [32/391]: Loss 0.038906071335077286\n",
      "[Epoch 29] Training Batch [33/391]: Loss 0.03239721804857254\n",
      "[Epoch 29] Training Batch [34/391]: Loss 0.05816689133644104\n",
      "[Epoch 29] Training Batch [35/391]: Loss 0.00903819315135479\n",
      "[Epoch 29] Training Batch [36/391]: Loss 0.04765167459845543\n",
      "[Epoch 29] Training Batch [37/391]: Loss 0.00324255577288568\n",
      "[Epoch 29] Training Batch [38/391]: Loss 0.011128140613436699\n",
      "[Epoch 29] Training Batch [39/391]: Loss 0.010969695635139942\n",
      "[Epoch 29] Training Batch [40/391]: Loss 0.01004696637392044\n",
      "[Epoch 29] Training Batch [41/391]: Loss 0.04091627895832062\n",
      "[Epoch 29] Training Batch [42/391]: Loss 0.07783455401659012\n",
      "[Epoch 29] Training Batch [43/391]: Loss 0.04198341816663742\n",
      "[Epoch 29] Training Batch [44/391]: Loss 0.009167790412902832\n",
      "[Epoch 29] Training Batch [45/391]: Loss 0.0164197888225317\n",
      "[Epoch 29] Training Batch [46/391]: Loss 0.033882562071084976\n",
      "[Epoch 29] Training Batch [47/391]: Loss 0.03205886110663414\n",
      "[Epoch 29] Training Batch [48/391]: Loss 0.022042185068130493\n",
      "[Epoch 29] Training Batch [49/391]: Loss 0.013930938206613064\n",
      "[Epoch 29] Training Batch [50/391]: Loss 0.04185612499713898\n",
      "[Epoch 29] Training Batch [51/391]: Loss 0.008712464943528175\n",
      "[Epoch 29] Training Batch [52/391]: Loss 0.02521139569580555\n",
      "[Epoch 29] Training Batch [53/391]: Loss 0.04184573143720627\n",
      "[Epoch 29] Training Batch [54/391]: Loss 0.023844122886657715\n",
      "[Epoch 29] Training Batch [55/391]: Loss 0.004063813015818596\n",
      "[Epoch 29] Training Batch [56/391]: Loss 0.016626952216029167\n",
      "[Epoch 29] Training Batch [57/391]: Loss 0.012483634985983372\n",
      "[Epoch 29] Training Batch [58/391]: Loss 0.0031547597609460354\n",
      "[Epoch 29] Training Batch [59/391]: Loss 0.05854181945323944\n",
      "[Epoch 29] Training Batch [60/391]: Loss 0.03210143372416496\n",
      "[Epoch 29] Training Batch [61/391]: Loss 0.003001586301252246\n",
      "[Epoch 29] Training Batch [62/391]: Loss 0.012150782160460949\n",
      "[Epoch 29] Training Batch [63/391]: Loss 0.01108842808753252\n",
      "[Epoch 29] Training Batch [64/391]: Loss 0.004084719344973564\n",
      "[Epoch 29] Training Batch [65/391]: Loss 0.002444841666147113\n",
      "[Epoch 29] Training Batch [66/391]: Loss 0.0169791541993618\n",
      "[Epoch 29] Training Batch [67/391]: Loss 0.01628410629928112\n",
      "[Epoch 29] Training Batch [68/391]: Loss 0.010031360201537609\n",
      "[Epoch 29] Training Batch [69/391]: Loss 0.03079826571047306\n",
      "[Epoch 29] Training Batch [70/391]: Loss 0.013894637115299702\n",
      "[Epoch 29] Training Batch [71/391]: Loss 0.026742752641439438\n",
      "[Epoch 29] Training Batch [72/391]: Loss 0.026122523471713066\n",
      "[Epoch 29] Training Batch [73/391]: Loss 0.021756015717983246\n",
      "[Epoch 29] Training Batch [74/391]: Loss 0.006969913840293884\n",
      "[Epoch 29] Training Batch [75/391]: Loss 0.004777146503329277\n",
      "[Epoch 29] Training Batch [76/391]: Loss 0.002899316605180502\n",
      "[Epoch 29] Training Batch [77/391]: Loss 0.015356226824223995\n",
      "[Epoch 29] Training Batch [78/391]: Loss 0.006175770424306393\n",
      "[Epoch 29] Training Batch [79/391]: Loss 0.002010445110499859\n",
      "[Epoch 29] Training Batch [80/391]: Loss 0.007679580245167017\n",
      "[Epoch 29] Training Batch [81/391]: Loss 0.03640026971697807\n",
      "[Epoch 29] Training Batch [82/391]: Loss 0.01989150606095791\n",
      "[Epoch 29] Training Batch [83/391]: Loss 0.007732418365776539\n",
      "[Epoch 29] Training Batch [84/391]: Loss 0.0050848727114498615\n",
      "[Epoch 29] Training Batch [85/391]: Loss 0.06167377158999443\n",
      "[Epoch 29] Training Batch [86/391]: Loss 0.00809128675609827\n",
      "[Epoch 29] Training Batch [87/391]: Loss 0.00902217160910368\n",
      "[Epoch 29] Training Batch [88/391]: Loss 0.012505651451647282\n",
      "[Epoch 29] Training Batch [89/391]: Loss 0.012879247777163982\n",
      "[Epoch 29] Training Batch [90/391]: Loss 0.01625244878232479\n",
      "[Epoch 29] Training Batch [91/391]: Loss 0.0031596713233739138\n",
      "[Epoch 29] Training Batch [92/391]: Loss 0.018707413226366043\n",
      "[Epoch 29] Training Batch [93/391]: Loss 0.0021101501770317554\n",
      "[Epoch 29] Training Batch [94/391]: Loss 0.006872904486954212\n",
      "[Epoch 29] Training Batch [95/391]: Loss 0.004516266752034426\n",
      "[Epoch 29] Training Batch [96/391]: Loss 0.003556814044713974\n",
      "[Epoch 29] Training Batch [97/391]: Loss 0.014037433080375195\n",
      "[Epoch 29] Training Batch [98/391]: Loss 0.02806844562292099\n",
      "[Epoch 29] Training Batch [99/391]: Loss 0.03970886766910553\n",
      "[Epoch 29] Training Batch [100/391]: Loss 0.044131241738796234\n",
      "[Epoch 29] Training Batch [101/391]: Loss 0.012514021247625351\n",
      "[Epoch 29] Training Batch [102/391]: Loss 0.003634209046140313\n",
      "[Epoch 29] Training Batch [103/391]: Loss 0.041442565619945526\n",
      "[Epoch 29] Training Batch [104/391]: Loss 0.006786107085645199\n",
      "[Epoch 29] Training Batch [105/391]: Loss 0.007198477629572153\n",
      "[Epoch 29] Training Batch [106/391]: Loss 0.004360351711511612\n",
      "[Epoch 29] Training Batch [107/391]: Loss 0.0010810915846377611\n",
      "[Epoch 29] Training Batch [108/391]: Loss 0.0024322238750755787\n",
      "[Epoch 29] Training Batch [109/391]: Loss 0.013605402782559395\n",
      "[Epoch 29] Training Batch [110/391]: Loss 0.0012964546913281083\n",
      "[Epoch 29] Training Batch [111/391]: Loss 0.062280986458063126\n",
      "[Epoch 29] Training Batch [112/391]: Loss 0.023352429270744324\n",
      "[Epoch 29] Training Batch [113/391]: Loss 0.008216661401093006\n",
      "[Epoch 29] Training Batch [114/391]: Loss 0.013049185276031494\n",
      "[Epoch 29] Training Batch [115/391]: Loss 0.047495659440755844\n",
      "[Epoch 29] Training Batch [116/391]: Loss 0.010016870684921741\n",
      "[Epoch 29] Training Batch [117/391]: Loss 0.03412213921546936\n",
      "[Epoch 29] Training Batch [118/391]: Loss 0.010160495527088642\n",
      "[Epoch 29] Training Batch [119/391]: Loss 0.021333636716008186\n",
      "[Epoch 29] Training Batch [120/391]: Loss 0.0026205393951386213\n",
      "[Epoch 29] Training Batch [121/391]: Loss 0.036390919238328934\n",
      "[Epoch 29] Training Batch [122/391]: Loss 0.025553293526172638\n",
      "[Epoch 29] Training Batch [123/391]: Loss 0.007871572859585285\n",
      "[Epoch 29] Training Batch [124/391]: Loss 0.031045367941260338\n",
      "[Epoch 29] Training Batch [125/391]: Loss 0.01200095470994711\n",
      "[Epoch 29] Training Batch [126/391]: Loss 0.02896839566528797\n",
      "[Epoch 29] Training Batch [127/391]: Loss 0.020722821354866028\n",
      "[Epoch 29] Training Batch [128/391]: Loss 0.0022424147464334965\n",
      "[Epoch 29] Training Batch [129/391]: Loss 0.015915943309664726\n",
      "[Epoch 29] Training Batch [130/391]: Loss 0.00616093585267663\n",
      "[Epoch 29] Training Batch [131/391]: Loss 0.0172194205224514\n",
      "[Epoch 29] Training Batch [132/391]: Loss 0.002759465016424656\n",
      "[Epoch 29] Training Batch [133/391]: Loss 0.03508884087204933\n",
      "[Epoch 29] Training Batch [134/391]: Loss 0.004115050658583641\n",
      "[Epoch 29] Training Batch [135/391]: Loss 0.01591821387410164\n",
      "[Epoch 29] Training Batch [136/391]: Loss 0.018749374896287918\n",
      "[Epoch 29] Training Batch [137/391]: Loss 0.012725922279059887\n",
      "[Epoch 29] Training Batch [138/391]: Loss 0.004357248544692993\n",
      "[Epoch 29] Training Batch [139/391]: Loss 0.028491834178566933\n",
      "[Epoch 29] Training Batch [140/391]: Loss 0.016982950270175934\n",
      "[Epoch 29] Training Batch [141/391]: Loss 0.0110543891787529\n",
      "[Epoch 29] Training Batch [142/391]: Loss 0.0035532512702047825\n",
      "[Epoch 29] Training Batch [143/391]: Loss 0.013741469010710716\n",
      "[Epoch 29] Training Batch [144/391]: Loss 0.00413266709074378\n",
      "[Epoch 29] Training Batch [145/391]: Loss 0.007950372993946075\n",
      "[Epoch 29] Training Batch [146/391]: Loss 0.0074440534226596355\n",
      "[Epoch 29] Training Batch [147/391]: Loss 0.01524251326918602\n",
      "[Epoch 29] Training Batch [148/391]: Loss 0.0018215804593637586\n",
      "[Epoch 29] Training Batch [149/391]: Loss 0.004119068384170532\n",
      "[Epoch 29] Training Batch [150/391]: Loss 0.007501577027142048\n",
      "[Epoch 29] Training Batch [151/391]: Loss 0.02370464988052845\n",
      "[Epoch 29] Training Batch [152/391]: Loss 0.04646766930818558\n",
      "[Epoch 29] Training Batch [153/391]: Loss 0.010380116291344166\n",
      "[Epoch 29] Training Batch [154/391]: Loss 0.002921710256487131\n",
      "[Epoch 29] Training Batch [155/391]: Loss 0.028419986367225647\n",
      "[Epoch 29] Training Batch [156/391]: Loss 0.008483649231493473\n",
      "[Epoch 29] Training Batch [157/391]: Loss 0.012246290221810341\n",
      "[Epoch 29] Training Batch [158/391]: Loss 0.004085834603756666\n",
      "[Epoch 29] Training Batch [159/391]: Loss 0.013843036256730556\n",
      "[Epoch 29] Training Batch [160/391]: Loss 0.01630069687962532\n",
      "[Epoch 29] Training Batch [161/391]: Loss 0.005072739440947771\n",
      "[Epoch 29] Training Batch [162/391]: Loss 0.004487213678658009\n",
      "[Epoch 29] Training Batch [163/391]: Loss 0.010487178340554237\n",
      "[Epoch 29] Training Batch [164/391]: Loss 0.014200784265995026\n",
      "[Epoch 29] Training Batch [165/391]: Loss 0.006488105282187462\n",
      "[Epoch 29] Training Batch [166/391]: Loss 0.010263489559292793\n",
      "[Epoch 29] Training Batch [167/391]: Loss 0.03400056064128876\n",
      "[Epoch 29] Training Batch [168/391]: Loss 0.023552242666482925\n",
      "[Epoch 29] Training Batch [169/391]: Loss 0.04162382706999779\n",
      "[Epoch 29] Training Batch [170/391]: Loss 0.011714761145412922\n",
      "[Epoch 29] Training Batch [171/391]: Loss 0.007539805956184864\n",
      "[Epoch 29] Training Batch [172/391]: Loss 0.007697169203311205\n",
      "[Epoch 29] Training Batch [173/391]: Loss 0.013244262896478176\n",
      "[Epoch 29] Training Batch [174/391]: Loss 0.0012890803627669811\n",
      "[Epoch 29] Training Batch [175/391]: Loss 0.028821568936109543\n",
      "[Epoch 29] Training Batch [176/391]: Loss 0.008773337118327618\n",
      "[Epoch 29] Training Batch [177/391]: Loss 0.0129019133746624\n",
      "[Epoch 29] Training Batch [178/391]: Loss 0.003219064325094223\n",
      "[Epoch 29] Training Batch [179/391]: Loss 0.041383516043424606\n",
      "[Epoch 29] Training Batch [180/391]: Loss 0.01309919636696577\n",
      "[Epoch 29] Training Batch [181/391]: Loss 0.00222288747318089\n",
      "[Epoch 29] Training Batch [182/391]: Loss 0.007395709864795208\n",
      "[Epoch 29] Training Batch [183/391]: Loss 0.0023527026642113924\n",
      "[Epoch 29] Training Batch [184/391]: Loss 0.0024103564210236073\n",
      "[Epoch 29] Training Batch [185/391]: Loss 0.012226295657455921\n",
      "[Epoch 29] Training Batch [186/391]: Loss 0.024952180683612823\n",
      "[Epoch 29] Training Batch [187/391]: Loss 0.016509326174855232\n",
      "[Epoch 29] Training Batch [188/391]: Loss 0.013790607452392578\n",
      "[Epoch 29] Training Batch [189/391]: Loss 0.0028714232612401247\n",
      "[Epoch 29] Training Batch [190/391]: Loss 0.001983792521059513\n",
      "[Epoch 29] Training Batch [191/391]: Loss 0.0012561421608552337\n",
      "[Epoch 29] Training Batch [192/391]: Loss 0.004175595473498106\n",
      "[Epoch 29] Training Batch [193/391]: Loss 0.004008102230727673\n",
      "[Epoch 29] Training Batch [194/391]: Loss 0.050340332090854645\n",
      "[Epoch 29] Training Batch [195/391]: Loss 0.0015639003831893206\n",
      "[Epoch 29] Training Batch [196/391]: Loss 0.010360508225858212\n",
      "[Epoch 29] Training Batch [197/391]: Loss 0.011091114021837711\n",
      "[Epoch 29] Training Batch [198/391]: Loss 0.02097262814640999\n",
      "[Epoch 29] Training Batch [199/391]: Loss 0.022963950410485268\n",
      "[Epoch 29] Training Batch [200/391]: Loss 0.02521349862217903\n",
      "[Epoch 29] Training Batch [201/391]: Loss 0.025794992223381996\n",
      "[Epoch 29] Training Batch [202/391]: Loss 0.02205643616616726\n",
      "[Epoch 29] Training Batch [203/391]: Loss 0.019258800894021988\n",
      "[Epoch 29] Training Batch [204/391]: Loss 0.0029892264865338802\n",
      "[Epoch 29] Training Batch [205/391]: Loss 0.0019973572343587875\n",
      "[Epoch 29] Training Batch [206/391]: Loss 0.00846531055867672\n",
      "[Epoch 29] Training Batch [207/391]: Loss 0.015851739794015884\n",
      "[Epoch 29] Training Batch [208/391]: Loss 0.009902969002723694\n",
      "[Epoch 29] Training Batch [209/391]: Loss 0.0032661310397088528\n",
      "[Epoch 29] Training Batch [210/391]: Loss 0.004079884383827448\n",
      "[Epoch 29] Training Batch [211/391]: Loss 0.0015837361570447683\n",
      "[Epoch 29] Training Batch [212/391]: Loss 0.006970768794417381\n",
      "[Epoch 29] Training Batch [213/391]: Loss 0.014315081760287285\n",
      "[Epoch 29] Training Batch [214/391]: Loss 0.002768724923953414\n",
      "[Epoch 29] Training Batch [215/391]: Loss 0.004592550452798605\n",
      "[Epoch 29] Training Batch [216/391]: Loss 0.0050451671704649925\n",
      "[Epoch 29] Training Batch [217/391]: Loss 0.003387653036043048\n",
      "[Epoch 29] Training Batch [218/391]: Loss 0.005577956326305866\n",
      "[Epoch 29] Training Batch [219/391]: Loss 0.00918055884540081\n",
      "[Epoch 29] Training Batch [220/391]: Loss 0.0022697686217725277\n",
      "[Epoch 29] Training Batch [221/391]: Loss 0.028660409152507782\n",
      "[Epoch 29] Training Batch [222/391]: Loss 0.004791922401636839\n",
      "[Epoch 29] Training Batch [223/391]: Loss 0.005686467979103327\n",
      "[Epoch 29] Training Batch [224/391]: Loss 0.01078132912516594\n",
      "[Epoch 29] Training Batch [225/391]: Loss 0.038233090192079544\n",
      "[Epoch 29] Training Batch [226/391]: Loss 0.017372433096170425\n",
      "[Epoch 29] Training Batch [227/391]: Loss 0.006887270603328943\n",
      "[Epoch 29] Training Batch [228/391]: Loss 0.010138669982552528\n",
      "[Epoch 29] Training Batch [229/391]: Loss 0.001969562377780676\n",
      "[Epoch 29] Training Batch [230/391]: Loss 0.0075719114392995834\n",
      "[Epoch 29] Training Batch [231/391]: Loss 0.026209896430373192\n",
      "[Epoch 29] Training Batch [232/391]: Loss 0.01658046990633011\n",
      "[Epoch 29] Training Batch [233/391]: Loss 0.005141592118889093\n",
      "[Epoch 29] Training Batch [234/391]: Loss 0.0049204579554498196\n",
      "[Epoch 29] Training Batch [235/391]: Loss 0.019378699362277985\n",
      "[Epoch 29] Training Batch [236/391]: Loss 0.01112719252705574\n",
      "[Epoch 29] Training Batch [237/391]: Loss 0.004742248449474573\n",
      "[Epoch 29] Training Batch [238/391]: Loss 0.02807609736919403\n",
      "[Epoch 29] Training Batch [239/391]: Loss 0.0042928121984004974\n",
      "[Epoch 29] Training Batch [240/391]: Loss 0.004037939943373203\n",
      "[Epoch 29] Training Batch [241/391]: Loss 0.008516745641827583\n",
      "[Epoch 29] Training Batch [242/391]: Loss 0.007270715199410915\n",
      "[Epoch 29] Training Batch [243/391]: Loss 0.01125335693359375\n",
      "[Epoch 29] Training Batch [244/391]: Loss 0.004151424393057823\n",
      "[Epoch 29] Training Batch [245/391]: Loss 0.009622415527701378\n",
      "[Epoch 29] Training Batch [246/391]: Loss 0.012945782393217087\n",
      "[Epoch 29] Training Batch [247/391]: Loss 0.018423767760396004\n",
      "[Epoch 29] Training Batch [248/391]: Loss 0.002636710647493601\n",
      "[Epoch 29] Training Batch [249/391]: Loss 0.00806676596403122\n",
      "[Epoch 29] Training Batch [250/391]: Loss 0.013333451934158802\n",
      "[Epoch 29] Training Batch [251/391]: Loss 0.017913298681378365\n",
      "[Epoch 29] Training Batch [252/391]: Loss 0.020387064665555954\n",
      "[Epoch 29] Training Batch [253/391]: Loss 0.0075874002650380135\n",
      "[Epoch 29] Training Batch [254/391]: Loss 0.0017368989065289497\n",
      "[Epoch 29] Training Batch [255/391]: Loss 0.009618700481951237\n",
      "[Epoch 29] Training Batch [256/391]: Loss 0.009336975403130054\n",
      "[Epoch 29] Training Batch [257/391]: Loss 0.003933190368115902\n",
      "[Epoch 29] Training Batch [258/391]: Loss 0.025867342948913574\n",
      "[Epoch 29] Training Batch [259/391]: Loss 0.004570487421005964\n",
      "[Epoch 29] Training Batch [260/391]: Loss 0.010632568039000034\n",
      "[Epoch 29] Training Batch [261/391]: Loss 0.007866082713007927\n",
      "[Epoch 29] Training Batch [262/391]: Loss 0.007881592027842999\n",
      "[Epoch 29] Training Batch [263/391]: Loss 0.025442980229854584\n",
      "[Epoch 29] Training Batch [264/391]: Loss 0.01747189462184906\n",
      "[Epoch 29] Training Batch [265/391]: Loss 0.012746456079185009\n",
      "[Epoch 29] Training Batch [266/391]: Loss 0.02412177063524723\n",
      "[Epoch 29] Training Batch [267/391]: Loss 0.006992411334067583\n",
      "[Epoch 29] Training Batch [268/391]: Loss 0.004875424783676863\n",
      "[Epoch 29] Training Batch [269/391]: Loss 0.03240681812167168\n",
      "[Epoch 29] Training Batch [270/391]: Loss 0.010347278788685799\n",
      "[Epoch 29] Training Batch [271/391]: Loss 0.0011860462836921215\n",
      "[Epoch 29] Training Batch [272/391]: Loss 0.0026829333510249853\n",
      "[Epoch 29] Training Batch [273/391]: Loss 0.0036687003448605537\n",
      "[Epoch 29] Training Batch [274/391]: Loss 0.021803325042128563\n",
      "[Epoch 29] Training Batch [275/391]: Loss 0.011617593467235565\n",
      "[Epoch 29] Training Batch [276/391]: Loss 0.004359118640422821\n",
      "[Epoch 29] Training Batch [277/391]: Loss 0.01137837115675211\n",
      "[Epoch 29] Training Batch [278/391]: Loss 0.001698745065368712\n",
      "[Epoch 29] Training Batch [279/391]: Loss 0.005053090397268534\n",
      "[Epoch 29] Training Batch [280/391]: Loss 0.0119162043556571\n",
      "[Epoch 29] Training Batch [281/391]: Loss 0.002522573573514819\n",
      "[Epoch 29] Training Batch [282/391]: Loss 0.0588226281106472\n",
      "[Epoch 29] Training Batch [283/391]: Loss 0.0020686108618974686\n",
      "[Epoch 29] Training Batch [284/391]: Loss 0.005800450686365366\n",
      "[Epoch 29] Training Batch [285/391]: Loss 0.010296562686562538\n",
      "[Epoch 29] Training Batch [286/391]: Loss 0.002788254525512457\n",
      "[Epoch 29] Training Batch [287/391]: Loss 0.0006105975480750203\n",
      "[Epoch 29] Training Batch [288/391]: Loss 0.004684756509959698\n",
      "[Epoch 29] Training Batch [289/391]: Loss 0.008515837602317333\n",
      "[Epoch 29] Training Batch [290/391]: Loss 0.007049249019473791\n",
      "[Epoch 29] Training Batch [291/391]: Loss 0.0025248960591852665\n",
      "[Epoch 29] Training Batch [292/391]: Loss 0.01458398625254631\n",
      "[Epoch 29] Training Batch [293/391]: Loss 0.001240485580638051\n",
      "[Epoch 29] Training Batch [294/391]: Loss 0.004103420302271843\n",
      "[Epoch 29] Training Batch [295/391]: Loss 0.0162811242043972\n",
      "[Epoch 29] Training Batch [296/391]: Loss 0.00996461883187294\n",
      "[Epoch 29] Training Batch [297/391]: Loss 0.004404023755341768\n",
      "[Epoch 29] Training Batch [298/391]: Loss 0.01607646234333515\n",
      "[Epoch 29] Training Batch [299/391]: Loss 0.0070371972396969795\n",
      "[Epoch 29] Training Batch [300/391]: Loss 0.006873076315969229\n",
      "[Epoch 29] Training Batch [301/391]: Loss 0.0160958431661129\n",
      "[Epoch 29] Training Batch [302/391]: Loss 0.0035915998741984367\n",
      "[Epoch 29] Training Batch [303/391]: Loss 0.003042567754164338\n",
      "[Epoch 29] Training Batch [304/391]: Loss 0.004628585185855627\n",
      "[Epoch 29] Training Batch [305/391]: Loss 0.0040170568972826\n",
      "[Epoch 29] Training Batch [306/391]: Loss 0.013753939419984818\n",
      "[Epoch 29] Training Batch [307/391]: Loss 0.0036075389944016933\n",
      "[Epoch 29] Training Batch [308/391]: Loss 0.0179805438965559\n",
      "[Epoch 29] Training Batch [309/391]: Loss 0.030777230858802795\n",
      "[Epoch 29] Training Batch [310/391]: Loss 0.012326564639806747\n",
      "[Epoch 29] Training Batch [311/391]: Loss 0.008099881932139397\n",
      "[Epoch 29] Training Batch [312/391]: Loss 0.022841254249215126\n",
      "[Epoch 29] Training Batch [313/391]: Loss 0.00523552019149065\n",
      "[Epoch 29] Training Batch [314/391]: Loss 0.012843724340200424\n",
      "[Epoch 29] Training Batch [315/391]: Loss 0.006902001332491636\n",
      "[Epoch 29] Training Batch [316/391]: Loss 0.005208176095038652\n",
      "[Epoch 29] Training Batch [317/391]: Loss 0.0037023029290139675\n",
      "[Epoch 29] Training Batch [318/391]: Loss 0.01665753684937954\n",
      "[Epoch 29] Training Batch [319/391]: Loss 0.0038624885492026806\n",
      "[Epoch 29] Training Batch [320/391]: Loss 0.0030964426696300507\n",
      "[Epoch 29] Training Batch [321/391]: Loss 0.0015707361744716763\n",
      "[Epoch 29] Training Batch [322/391]: Loss 0.016673754900693893\n",
      "[Epoch 29] Training Batch [323/391]: Loss 0.001953384606167674\n",
      "[Epoch 29] Training Batch [324/391]: Loss 0.010274450294673443\n",
      "[Epoch 29] Training Batch [325/391]: Loss 0.0010390501702204347\n",
      "[Epoch 29] Training Batch [326/391]: Loss 0.010345169343054295\n",
      "[Epoch 29] Training Batch [327/391]: Loss 0.004760713316500187\n",
      "[Epoch 29] Training Batch [328/391]: Loss 0.0046018376015126705\n",
      "[Epoch 29] Training Batch [329/391]: Loss 0.004086531698703766\n",
      "[Epoch 29] Training Batch [330/391]: Loss 0.000795060012023896\n",
      "[Epoch 29] Training Batch [331/391]: Loss 0.07324770838022232\n",
      "[Epoch 29] Training Batch [332/391]: Loss 0.016070937737822533\n",
      "[Epoch 29] Training Batch [333/391]: Loss 0.006180834956467152\n",
      "[Epoch 29] Training Batch [334/391]: Loss 0.008816340006887913\n",
      "[Epoch 29] Training Batch [335/391]: Loss 0.010307577438652515\n",
      "[Epoch 29] Training Batch [336/391]: Loss 0.006107510533183813\n",
      "[Epoch 29] Training Batch [337/391]: Loss 0.0034789845813065767\n",
      "[Epoch 29] Training Batch [338/391]: Loss 0.0014974350342527032\n",
      "[Epoch 29] Training Batch [339/391]: Loss 0.0035239248536527157\n",
      "[Epoch 29] Training Batch [340/391]: Loss 0.01653551124036312\n",
      "[Epoch 29] Training Batch [341/391]: Loss 0.0022690086625516415\n",
      "[Epoch 29] Training Batch [342/391]: Loss 0.030156932771205902\n",
      "[Epoch 29] Training Batch [343/391]: Loss 0.0053103771060705185\n",
      "[Epoch 29] Training Batch [344/391]: Loss 0.007709226105362177\n",
      "[Epoch 29] Training Batch [345/391]: Loss 0.0044799502938985825\n",
      "[Epoch 29] Training Batch [346/391]: Loss 0.013858330436050892\n",
      "[Epoch 29] Training Batch [347/391]: Loss 0.004564232658594847\n",
      "[Epoch 29] Training Batch [348/391]: Loss 0.005444212816655636\n",
      "[Epoch 29] Training Batch [349/391]: Loss 0.005234910175204277\n",
      "[Epoch 29] Training Batch [350/391]: Loss 0.006179284304380417\n",
      "[Epoch 29] Training Batch [351/391]: Loss 0.001793185598216951\n",
      "[Epoch 29] Training Batch [352/391]: Loss 0.0026761179324239492\n",
      "[Epoch 29] Training Batch [353/391]: Loss 0.029027391225099564\n",
      "[Epoch 29] Training Batch [354/391]: Loss 0.010757479816675186\n",
      "[Epoch 29] Training Batch [355/391]: Loss 0.0037946528755128384\n",
      "[Epoch 29] Training Batch [356/391]: Loss 0.007418781518936157\n",
      "[Epoch 29] Training Batch [357/391]: Loss 0.014289767481386662\n",
      "[Epoch 29] Training Batch [358/391]: Loss 0.013512794859707355\n",
      "[Epoch 29] Training Batch [359/391]: Loss 0.004698161967098713\n",
      "[Epoch 29] Training Batch [360/391]: Loss 0.014806554652750492\n",
      "[Epoch 29] Training Batch [361/391]: Loss 0.001362467766739428\n",
      "[Epoch 29] Training Batch [362/391]: Loss 0.0013058301992714405\n",
      "[Epoch 29] Training Batch [363/391]: Loss 0.0042078737169504166\n",
      "[Epoch 29] Training Batch [364/391]: Loss 0.0353713296353817\n",
      "[Epoch 29] Training Batch [365/391]: Loss 0.003949697595089674\n",
      "[Epoch 29] Training Batch [366/391]: Loss 0.0022990729194134474\n",
      "[Epoch 29] Training Batch [367/391]: Loss 0.002077049110084772\n",
      "[Epoch 29] Training Batch [368/391]: Loss 0.038883909583091736\n",
      "[Epoch 29] Training Batch [369/391]: Loss 0.005336159840226173\n",
      "[Epoch 29] Training Batch [370/391]: Loss 0.006412837654352188\n",
      "[Epoch 29] Training Batch [371/391]: Loss 0.003314791712909937\n",
      "[Epoch 29] Training Batch [372/391]: Loss 0.004255206789821386\n",
      "[Epoch 29] Training Batch [373/391]: Loss 0.0012617434840649366\n",
      "[Epoch 29] Training Batch [374/391]: Loss 0.006209203042089939\n",
      "[Epoch 29] Training Batch [375/391]: Loss 0.012011153623461723\n",
      "[Epoch 29] Training Batch [376/391]: Loss 0.005665866658091545\n",
      "[Epoch 29] Training Batch [377/391]: Loss 0.01915483921766281\n",
      "[Epoch 29] Training Batch [378/391]: Loss 0.003490826115012169\n",
      "[Epoch 29] Training Batch [379/391]: Loss 0.004033954814076424\n",
      "[Epoch 29] Training Batch [380/391]: Loss 0.012315474450588226\n",
      "[Epoch 29] Training Batch [381/391]: Loss 0.005602579098194838\n",
      "[Epoch 29] Training Batch [382/391]: Loss 0.008126867935061455\n",
      "[Epoch 29] Training Batch [383/391]: Loss 0.010944202542304993\n",
      "[Epoch 29] Training Batch [384/391]: Loss 0.005209540482610464\n",
      "[Epoch 29] Training Batch [385/391]: Loss 0.04491625353693962\n",
      "[Epoch 29] Training Batch [386/391]: Loss 0.002241266891360283\n",
      "[Epoch 29] Training Batch [387/391]: Loss 0.034605804830789566\n",
      "[Epoch 29] Training Batch [388/391]: Loss 0.0058551961556077\n",
      "[Epoch 29] Training Batch [389/391]: Loss 0.004276476334780455\n",
      "[Epoch 29] Training Batch [390/391]: Loss 0.006508427672088146\n",
      "[Epoch 29] Training Batch [391/391]: Loss 0.06164110451936722\n",
      "Epoch 29 - Train Loss: 0.0148\n",
      "*********  Epoch 30/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Training Batch [1/391]: Loss 0.0013445004587993026\n",
      "[Epoch 30] Training Batch [2/391]: Loss 0.004691297188401222\n",
      "[Epoch 30] Training Batch [3/391]: Loss 0.000561201071832329\n",
      "[Epoch 30] Training Batch [4/391]: Loss 0.0012692715972661972\n",
      "[Epoch 30] Training Batch [5/391]: Loss 0.00889219157397747\n",
      "[Epoch 30] Training Batch [6/391]: Loss 0.006621664389967918\n",
      "[Epoch 30] Training Batch [7/391]: Loss 0.00982052180916071\n",
      "[Epoch 30] Training Batch [8/391]: Loss 0.006259569898247719\n",
      "[Epoch 30] Training Batch [9/391]: Loss 0.0054356008768081665\n",
      "[Epoch 30] Training Batch [10/391]: Loss 0.0015889396890997887\n",
      "[Epoch 30] Training Batch [11/391]: Loss 0.0017322148196399212\n",
      "[Epoch 30] Training Batch [12/391]: Loss 0.009411865845322609\n",
      "[Epoch 30] Training Batch [13/391]: Loss 0.0008179934811778367\n",
      "[Epoch 30] Training Batch [14/391]: Loss 0.015571234747767448\n",
      "[Epoch 30] Training Batch [15/391]: Loss 0.0018013903172686696\n",
      "[Epoch 30] Training Batch [16/391]: Loss 0.03090190328657627\n",
      "[Epoch 30] Training Batch [17/391]: Loss 0.0021742244716733694\n",
      "[Epoch 30] Training Batch [18/391]: Loss 0.0020356017630547285\n",
      "[Epoch 30] Training Batch [19/391]: Loss 0.004282331094145775\n",
      "[Epoch 30] Training Batch [20/391]: Loss 0.016027623787522316\n",
      "[Epoch 30] Training Batch [21/391]: Loss 0.0006290434394031763\n",
      "[Epoch 30] Training Batch [22/391]: Loss 0.004289114382117987\n",
      "[Epoch 30] Training Batch [23/391]: Loss 0.03805389255285263\n",
      "[Epoch 30] Training Batch [24/391]: Loss 0.014228200539946556\n",
      "[Epoch 30] Training Batch [25/391]: Loss 0.007308724336326122\n",
      "[Epoch 30] Training Batch [26/391]: Loss 0.006874222308397293\n",
      "[Epoch 30] Training Batch [27/391]: Loss 0.017430955544114113\n",
      "[Epoch 30] Training Batch [28/391]: Loss 0.0007084678509272635\n",
      "[Epoch 30] Training Batch [29/391]: Loss 0.004211227409541607\n",
      "[Epoch 30] Training Batch [30/391]: Loss 0.002488872269168496\n",
      "[Epoch 30] Training Batch [31/391]: Loss 0.0025541516952216625\n",
      "[Epoch 30] Training Batch [32/391]: Loss 0.003006930463016033\n",
      "[Epoch 30] Training Batch [33/391]: Loss 0.0015999289462342858\n",
      "[Epoch 30] Training Batch [34/391]: Loss 0.01958460733294487\n",
      "[Epoch 30] Training Batch [35/391]: Loss 0.00798336137086153\n",
      "[Epoch 30] Training Batch [36/391]: Loss 0.04587547853589058\n",
      "[Epoch 30] Training Batch [37/391]: Loss 0.0026025502011179924\n",
      "[Epoch 30] Training Batch [38/391]: Loss 0.005507500842213631\n",
      "[Epoch 30] Training Batch [39/391]: Loss 0.0015809946926310658\n",
      "[Epoch 30] Training Batch [40/391]: Loss 0.007011653855443001\n",
      "[Epoch 30] Training Batch [41/391]: Loss 0.012041156180202961\n",
      "[Epoch 30] Training Batch [42/391]: Loss 0.02117508463561535\n",
      "[Epoch 30] Training Batch [43/391]: Loss 0.004298471380025148\n",
      "[Epoch 30] Training Batch [44/391]: Loss 0.012061157263815403\n",
      "[Epoch 30] Training Batch [45/391]: Loss 0.0017290001269429922\n",
      "[Epoch 30] Training Batch [46/391]: Loss 0.003711957950145006\n",
      "[Epoch 30] Training Batch [47/391]: Loss 0.03794165328145027\n",
      "[Epoch 30] Training Batch [48/391]: Loss 0.0014555586967617273\n",
      "[Epoch 30] Training Batch [49/391]: Loss 0.010713443160057068\n",
      "[Epoch 30] Training Batch [50/391]: Loss 0.010918290354311466\n",
      "[Epoch 30] Training Batch [51/391]: Loss 0.006396625190973282\n",
      "[Epoch 30] Training Batch [52/391]: Loss 0.010316130705177784\n",
      "[Epoch 30] Training Batch [53/391]: Loss 0.002636369550600648\n",
      "[Epoch 30] Training Batch [54/391]: Loss 0.006751119624823332\n",
      "[Epoch 30] Training Batch [55/391]: Loss 0.0026692666579037905\n",
      "[Epoch 30] Training Batch [56/391]: Loss 0.002266115276142955\n",
      "[Epoch 30] Training Batch [57/391]: Loss 0.0042000566609203815\n",
      "[Epoch 30] Training Batch [58/391]: Loss 0.02482074871659279\n",
      "[Epoch 30] Training Batch [59/391]: Loss 0.005600303411483765\n",
      "[Epoch 30] Training Batch [60/391]: Loss 0.0030067863408476114\n",
      "[Epoch 30] Training Batch [61/391]: Loss 0.00817148108035326\n",
      "[Epoch 30] Training Batch [62/391]: Loss 0.0011773129226639867\n",
      "[Epoch 30] Training Batch [63/391]: Loss 0.01359599269926548\n",
      "[Epoch 30] Training Batch [64/391]: Loss 0.0027776979841291904\n",
      "[Epoch 30] Training Batch [65/391]: Loss 0.0012931867968291044\n",
      "[Epoch 30] Training Batch [66/391]: Loss 0.006427614949643612\n",
      "[Epoch 30] Training Batch [67/391]: Loss 0.007838116958737373\n",
      "[Epoch 30] Training Batch [68/391]: Loss 0.024984130635857582\n",
      "[Epoch 30] Training Batch [69/391]: Loss 0.002934421878308058\n",
      "[Epoch 30] Training Batch [70/391]: Loss 0.0022515503223985434\n",
      "[Epoch 30] Training Batch [71/391]: Loss 0.008507931604981422\n",
      "[Epoch 30] Training Batch [72/391]: Loss 0.001964655239135027\n",
      "[Epoch 30] Training Batch [73/391]: Loss 0.0024790619499981403\n",
      "[Epoch 30] Training Batch [74/391]: Loss 0.0007266933680512011\n",
      "[Epoch 30] Training Batch [75/391]: Loss 0.0016664732247591019\n",
      "[Epoch 30] Training Batch [76/391]: Loss 0.0075098066590726376\n",
      "[Epoch 30] Training Batch [77/391]: Loss 0.0038587951567023993\n",
      "[Epoch 30] Training Batch [78/391]: Loss 0.008192075416445732\n",
      "[Epoch 30] Training Batch [79/391]: Loss 0.006647696252912283\n",
      "[Epoch 30] Training Batch [80/391]: Loss 0.003362278686836362\n",
      "[Epoch 30] Training Batch [81/391]: Loss 0.002508326666429639\n",
      "[Epoch 30] Training Batch [82/391]: Loss 0.024335360154509544\n",
      "[Epoch 30] Training Batch [83/391]: Loss 0.001934272120706737\n",
      "[Epoch 30] Training Batch [84/391]: Loss 0.0006600177730433643\n",
      "[Epoch 30] Training Batch [85/391]: Loss 0.008744976483285427\n",
      "[Epoch 30] Training Batch [86/391]: Loss 0.0018986486829817295\n",
      "[Epoch 30] Training Batch [87/391]: Loss 0.0010936417384073138\n",
      "[Epoch 30] Training Batch [88/391]: Loss 0.00024845387088134885\n",
      "[Epoch 30] Training Batch [89/391]: Loss 0.005751045420765877\n",
      "[Epoch 30] Training Batch [90/391]: Loss 0.0017570571508258581\n",
      "[Epoch 30] Training Batch [91/391]: Loss 0.016638513654470444\n",
      "[Epoch 30] Training Batch [92/391]: Loss 0.002855857601389289\n",
      "[Epoch 30] Training Batch [93/391]: Loss 0.0016472255811095238\n",
      "[Epoch 30] Training Batch [94/391]: Loss 0.0018400761764496565\n",
      "[Epoch 30] Training Batch [95/391]: Loss 0.012214424088597298\n",
      "[Epoch 30] Training Batch [96/391]: Loss 0.00194819294847548\n",
      "[Epoch 30] Training Batch [97/391]: Loss 0.000902129162568599\n",
      "[Epoch 30] Training Batch [98/391]: Loss 0.002605099231004715\n",
      "[Epoch 30] Training Batch [99/391]: Loss 0.0016986490227282047\n",
      "[Epoch 30] Training Batch [100/391]: Loss 0.014744565822184086\n",
      "[Epoch 30] Training Batch [101/391]: Loss 0.0017481064423918724\n",
      "[Epoch 30] Training Batch [102/391]: Loss 0.0022754317615181208\n",
      "[Epoch 30] Training Batch [103/391]: Loss 0.01153363473713398\n",
      "[Epoch 30] Training Batch [104/391]: Loss 0.021783148869872093\n",
      "[Epoch 30] Training Batch [105/391]: Loss 0.00205868948251009\n",
      "[Epoch 30] Training Batch [106/391]: Loss 0.001123366178944707\n",
      "[Epoch 30] Training Batch [107/391]: Loss 0.004151264671236277\n",
      "[Epoch 30] Training Batch [108/391]: Loss 0.0020920594688504934\n",
      "[Epoch 30] Training Batch [109/391]: Loss 0.002092899288982153\n",
      "[Epoch 30] Training Batch [110/391]: Loss 0.0016440256731584668\n",
      "[Epoch 30] Training Batch [111/391]: Loss 0.0013863003114238381\n",
      "[Epoch 30] Training Batch [112/391]: Loss 0.00686778174713254\n",
      "[Epoch 30] Training Batch [113/391]: Loss 0.004283020738512278\n",
      "[Epoch 30] Training Batch [114/391]: Loss 0.0013839612947776914\n",
      "[Epoch 30] Training Batch [115/391]: Loss 0.0006335470243357122\n",
      "[Epoch 30] Training Batch [116/391]: Loss 0.0004436085291672498\n",
      "[Epoch 30] Training Batch [117/391]: Loss 0.01790187880396843\n",
      "[Epoch 30] Training Batch [118/391]: Loss 0.0024465692695230246\n",
      "[Epoch 30] Training Batch [119/391]: Loss 0.002184092765673995\n",
      "[Epoch 30] Training Batch [120/391]: Loss 0.007271081209182739\n",
      "[Epoch 30] Training Batch [121/391]: Loss 0.0014501416590064764\n",
      "[Epoch 30] Training Batch [122/391]: Loss 0.0027885837480425835\n",
      "[Epoch 30] Training Batch [123/391]: Loss 0.001437108963727951\n",
      "[Epoch 30] Training Batch [124/391]: Loss 0.0021343196276575327\n",
      "[Epoch 30] Training Batch [125/391]: Loss 0.0004953057505190372\n",
      "[Epoch 30] Training Batch [126/391]: Loss 0.0013277693651616573\n",
      "[Epoch 30] Training Batch [127/391]: Loss 0.007357494439929724\n",
      "[Epoch 30] Training Batch [128/391]: Loss 0.0060305409133434296\n",
      "[Epoch 30] Training Batch [129/391]: Loss 0.008183055557310581\n",
      "[Epoch 30] Training Batch [130/391]: Loss 0.0018566529033705592\n",
      "[Epoch 30] Training Batch [131/391]: Loss 0.003852532245218754\n",
      "[Epoch 30] Training Batch [132/391]: Loss 0.0022039494942873716\n",
      "[Epoch 30] Training Batch [133/391]: Loss 0.0012771146139129996\n",
      "[Epoch 30] Training Batch [134/391]: Loss 0.003778191516175866\n",
      "[Epoch 30] Training Batch [135/391]: Loss 0.0011191394878551364\n",
      "[Epoch 30] Training Batch [136/391]: Loss 0.00257509620860219\n",
      "[Epoch 30] Training Batch [137/391]: Loss 0.011791273020207882\n",
      "[Epoch 30] Training Batch [138/391]: Loss 0.01153597328811884\n",
      "[Epoch 30] Training Batch [139/391]: Loss 0.007187069859355688\n",
      "[Epoch 30] Training Batch [140/391]: Loss 0.0011729646939784288\n",
      "[Epoch 30] Training Batch [141/391]: Loss 0.0014124666340649128\n",
      "[Epoch 30] Training Batch [142/391]: Loss 0.0014189183712005615\n",
      "[Epoch 30] Training Batch [143/391]: Loss 0.003727593459188938\n",
      "[Epoch 30] Training Batch [144/391]: Loss 0.00593099370598793\n",
      "[Epoch 30] Training Batch [145/391]: Loss 0.000897975463885814\n",
      "[Epoch 30] Training Batch [146/391]: Loss 0.0032208082266151905\n",
      "[Epoch 30] Training Batch [147/391]: Loss 0.00172113673761487\n",
      "[Epoch 30] Training Batch [148/391]: Loss 0.0014927075244486332\n",
      "[Epoch 30] Training Batch [149/391]: Loss 0.0006091228569857776\n",
      "[Epoch 30] Training Batch [150/391]: Loss 0.0005700182518921793\n",
      "[Epoch 30] Training Batch [151/391]: Loss 0.0019920438062399626\n",
      "[Epoch 30] Training Batch [152/391]: Loss 0.0010429160902276635\n",
      "[Epoch 30] Training Batch [153/391]: Loss 0.0025017564184963703\n",
      "[Epoch 30] Training Batch [154/391]: Loss 0.0014191246591508389\n",
      "[Epoch 30] Training Batch [155/391]: Loss 0.0006256061024032533\n",
      "[Epoch 30] Training Batch [156/391]: Loss 0.0007896870956756175\n",
      "[Epoch 30] Training Batch [157/391]: Loss 0.0037540749181061983\n",
      "[Epoch 30] Training Batch [158/391]: Loss 0.001844465034082532\n",
      "[Epoch 30] Training Batch [159/391]: Loss 0.006086681969463825\n",
      "[Epoch 30] Training Batch [160/391]: Loss 0.00022384147450793535\n",
      "[Epoch 30] Training Batch [161/391]: Loss 0.0008287621312774718\n",
      "[Epoch 30] Training Batch [162/391]: Loss 0.002093867864459753\n",
      "[Epoch 30] Training Batch [163/391]: Loss 0.0013439018512144685\n",
      "[Epoch 30] Training Batch [164/391]: Loss 0.0002935688535217196\n",
      "[Epoch 30] Training Batch [165/391]: Loss 0.0029568325262516737\n",
      "[Epoch 30] Training Batch [166/391]: Loss 0.005448878277093172\n",
      "[Epoch 30] Training Batch [167/391]: Loss 0.001186563284136355\n",
      "[Epoch 30] Training Batch [168/391]: Loss 0.0047411080449819565\n",
      "[Epoch 30] Training Batch [169/391]: Loss 0.0020160034764558077\n",
      "[Epoch 30] Training Batch [170/391]: Loss 0.0009525488130748272\n",
      "[Epoch 30] Training Batch [171/391]: Loss 0.004062298219650984\n",
      "[Epoch 30] Training Batch [172/391]: Loss 0.0003494725388009101\n",
      "[Epoch 30] Training Batch [173/391]: Loss 0.0008270294638350606\n",
      "[Epoch 30] Training Batch [174/391]: Loss 0.0010761512676253915\n",
      "[Epoch 30] Training Batch [175/391]: Loss 0.007963370531797409\n",
      "[Epoch 30] Training Batch [176/391]: Loss 0.0012770146131515503\n",
      "[Epoch 30] Training Batch [177/391]: Loss 0.0021227963734418154\n",
      "[Epoch 30] Training Batch [178/391]: Loss 0.001670956495217979\n",
      "[Epoch 30] Training Batch [179/391]: Loss 0.003100774949416518\n",
      "[Epoch 30] Training Batch [180/391]: Loss 0.002313953125849366\n",
      "[Epoch 30] Training Batch [181/391]: Loss 0.0006066469941288233\n",
      "[Epoch 30] Training Batch [182/391]: Loss 0.0005191268865019083\n",
      "[Epoch 30] Training Batch [183/391]: Loss 0.002424980280920863\n",
      "[Epoch 30] Training Batch [184/391]: Loss 0.030833890661597252\n",
      "[Epoch 30] Training Batch [185/391]: Loss 0.0007274274830706418\n",
      "[Epoch 30] Training Batch [186/391]: Loss 0.005182539112865925\n",
      "[Epoch 30] Training Batch [187/391]: Loss 0.007925184443593025\n",
      "[Epoch 30] Training Batch [188/391]: Loss 0.005758332554250956\n",
      "[Epoch 30] Training Batch [189/391]: Loss 0.0008428394212387502\n",
      "[Epoch 30] Training Batch [190/391]: Loss 0.00026276931748725474\n",
      "[Epoch 30] Training Batch [191/391]: Loss 0.0030449535697698593\n",
      "[Epoch 30] Training Batch [192/391]: Loss 0.0034757188986986876\n",
      "[Epoch 30] Training Batch [193/391]: Loss 0.0008582478039897978\n",
      "[Epoch 30] Training Batch [194/391]: Loss 0.001972511410713196\n",
      "[Epoch 30] Training Batch [195/391]: Loss 0.0074060820043087006\n",
      "[Epoch 30] Training Batch [196/391]: Loss 0.001261602621525526\n",
      "[Epoch 30] Training Batch [197/391]: Loss 0.0005705671501345932\n",
      "[Epoch 30] Training Batch [198/391]: Loss 0.0009427475742995739\n",
      "[Epoch 30] Training Batch [199/391]: Loss 0.0006571915582753718\n",
      "[Epoch 30] Training Batch [200/391]: Loss 0.0006393030053004622\n",
      "[Epoch 30] Training Batch [201/391]: Loss 0.003028166713193059\n",
      "[Epoch 30] Training Batch [202/391]: Loss 0.0009570373804308474\n",
      "[Epoch 30] Training Batch [203/391]: Loss 0.0014987126924097538\n",
      "[Epoch 30] Training Batch [204/391]: Loss 0.00655378308147192\n",
      "[Epoch 30] Training Batch [205/391]: Loss 0.007198646664619446\n",
      "[Epoch 30] Training Batch [206/391]: Loss 0.0002704793878365308\n",
      "[Epoch 30] Training Batch [207/391]: Loss 0.001198961166664958\n",
      "[Epoch 30] Training Batch [208/391]: Loss 0.0015833897050470114\n",
      "[Epoch 30] Training Batch [209/391]: Loss 0.0021832454949617386\n",
      "[Epoch 30] Training Batch [210/391]: Loss 0.0005621518357656896\n",
      "[Epoch 30] Training Batch [211/391]: Loss 0.027202418074011803\n",
      "[Epoch 30] Training Batch [212/391]: Loss 0.0005595451802946627\n",
      "[Epoch 30] Training Batch [213/391]: Loss 0.0030510672368109226\n",
      "[Epoch 30] Training Batch [214/391]: Loss 0.001976901199668646\n",
      "[Epoch 30] Training Batch [215/391]: Loss 0.00288560101762414\n",
      "[Epoch 30] Training Batch [216/391]: Loss 0.005379367619752884\n",
      "[Epoch 30] Training Batch [217/391]: Loss 0.020999107509851456\n",
      "[Epoch 30] Training Batch [218/391]: Loss 0.004889705218374729\n",
      "[Epoch 30] Training Batch [219/391]: Loss 0.0006826702738180757\n",
      "[Epoch 30] Training Batch [220/391]: Loss 0.01855849102139473\n",
      "[Epoch 30] Training Batch [221/391]: Loss 0.000599304330535233\n",
      "[Epoch 30] Training Batch [222/391]: Loss 0.0037565066013485193\n",
      "[Epoch 30] Training Batch [223/391]: Loss 0.004213547799736261\n",
      "[Epoch 30] Training Batch [224/391]: Loss 0.013106520287692547\n",
      "[Epoch 30] Training Batch [225/391]: Loss 0.0005894655478186905\n",
      "[Epoch 30] Training Batch [226/391]: Loss 0.006128415465354919\n",
      "[Epoch 30] Training Batch [227/391]: Loss 0.0009478593710809946\n",
      "[Epoch 30] Training Batch [228/391]: Loss 0.0004990087472833693\n",
      "[Epoch 30] Training Batch [229/391]: Loss 0.0004064619424752891\n",
      "[Epoch 30] Training Batch [230/391]: Loss 0.0023952731862664223\n",
      "[Epoch 30] Training Batch [231/391]: Loss 0.0008884372073225677\n",
      "[Epoch 30] Training Batch [232/391]: Loss 0.002251460449770093\n",
      "[Epoch 30] Training Batch [233/391]: Loss 0.0043707494623959064\n",
      "[Epoch 30] Training Batch [234/391]: Loss 0.0006101443432271481\n",
      "[Epoch 30] Training Batch [235/391]: Loss 0.003378319088369608\n",
      "[Epoch 30] Training Batch [236/391]: Loss 0.024415338411927223\n",
      "[Epoch 30] Training Batch [237/391]: Loss 0.007132606580853462\n",
      "[Epoch 30] Training Batch [238/391]: Loss 0.004688817076385021\n",
      "[Epoch 30] Training Batch [239/391]: Loss 0.009192480705678463\n",
      "[Epoch 30] Training Batch [240/391]: Loss 0.0006388386245816946\n",
      "[Epoch 30] Training Batch [241/391]: Loss 0.001751414849422872\n",
      "[Epoch 30] Training Batch [242/391]: Loss 0.008913888595998287\n",
      "[Epoch 30] Training Batch [243/391]: Loss 0.0003790667688008398\n",
      "[Epoch 30] Training Batch [244/391]: Loss 0.0009218704653903842\n",
      "[Epoch 30] Training Batch [245/391]: Loss 0.02567148208618164\n",
      "[Epoch 30] Training Batch [246/391]: Loss 0.0032305915374308825\n",
      "[Epoch 30] Training Batch [247/391]: Loss 0.026423992589116096\n",
      "[Epoch 30] Training Batch [248/391]: Loss 0.001543282181955874\n",
      "[Epoch 30] Training Batch [249/391]: Loss 0.0018758022924885154\n",
      "[Epoch 30] Training Batch [250/391]: Loss 0.0010516757611185312\n",
      "[Epoch 30] Training Batch [251/391]: Loss 0.00320973782800138\n",
      "[Epoch 30] Training Batch [252/391]: Loss 0.006475272122770548\n",
      "[Epoch 30] Training Batch [253/391]: Loss 0.005038413684815168\n",
      "[Epoch 30] Training Batch [254/391]: Loss 0.005013284273445606\n",
      "[Epoch 30] Training Batch [255/391]: Loss 0.003546309657394886\n",
      "[Epoch 30] Training Batch [256/391]: Loss 0.014770328998565674\n",
      "[Epoch 30] Training Batch [257/391]: Loss 0.015627479180693626\n",
      "[Epoch 30] Training Batch [258/391]: Loss 0.01960531622171402\n",
      "[Epoch 30] Training Batch [259/391]: Loss 0.0005955524975433946\n",
      "[Epoch 30] Training Batch [260/391]: Loss 0.020226454362273216\n",
      "[Epoch 30] Training Batch [261/391]: Loss 0.0024100474547594786\n",
      "[Epoch 30] Training Batch [262/391]: Loss 0.0006451862282119691\n",
      "[Epoch 30] Training Batch [263/391]: Loss 0.004132400266826153\n",
      "[Epoch 30] Training Batch [264/391]: Loss 0.005134141072630882\n",
      "[Epoch 30] Training Batch [265/391]: Loss 0.01999713107943535\n",
      "[Epoch 30] Training Batch [266/391]: Loss 0.003463947447016835\n",
      "[Epoch 30] Training Batch [267/391]: Loss 0.001704339636489749\n",
      "[Epoch 30] Training Batch [268/391]: Loss 0.006745927967131138\n",
      "[Epoch 30] Training Batch [269/391]: Loss 0.0010437784949317575\n",
      "[Epoch 30] Training Batch [270/391]: Loss 0.0018959559965878725\n",
      "[Epoch 30] Training Batch [271/391]: Loss 0.00042260412010364234\n",
      "[Epoch 30] Training Batch [272/391]: Loss 0.0021298823412507772\n",
      "[Epoch 30] Training Batch [273/391]: Loss 0.046192023903131485\n",
      "[Epoch 30] Training Batch [274/391]: Loss 0.011927561834454536\n",
      "[Epoch 30] Training Batch [275/391]: Loss 0.009533625096082687\n",
      "[Epoch 30] Training Batch [276/391]: Loss 0.0006272188620641828\n",
      "[Epoch 30] Training Batch [277/391]: Loss 0.020062768831849098\n",
      "[Epoch 30] Training Batch [278/391]: Loss 0.001107910880818963\n",
      "[Epoch 30] Training Batch [279/391]: Loss 0.0006194222369231284\n",
      "[Epoch 30] Training Batch [280/391]: Loss 0.004887258168309927\n",
      "[Epoch 30] Training Batch [281/391]: Loss 0.017252204939723015\n",
      "[Epoch 30] Training Batch [282/391]: Loss 0.0011006515705958009\n",
      "[Epoch 30] Training Batch [283/391]: Loss 0.002993239788338542\n",
      "[Epoch 30] Training Batch [284/391]: Loss 0.009515488520264626\n",
      "[Epoch 30] Training Batch [285/391]: Loss 0.009606963954865932\n",
      "[Epoch 30] Training Batch [286/391]: Loss 0.0031243092380464077\n",
      "[Epoch 30] Training Batch [287/391]: Loss 0.006106163375079632\n",
      "[Epoch 30] Training Batch [288/391]: Loss 0.0266812052577734\n",
      "[Epoch 30] Training Batch [289/391]: Loss 0.007105737458914518\n",
      "[Epoch 30] Training Batch [290/391]: Loss 0.0105436397716403\n",
      "[Epoch 30] Training Batch [291/391]: Loss 0.0023726257495582104\n",
      "[Epoch 30] Training Batch [292/391]: Loss 0.01581895351409912\n",
      "[Epoch 30] Training Batch [293/391]: Loss 0.0016778933349996805\n",
      "[Epoch 30] Training Batch [294/391]: Loss 0.009247998706996441\n",
      "[Epoch 30] Training Batch [295/391]: Loss 0.002908275695517659\n",
      "[Epoch 30] Training Batch [296/391]: Loss 0.005178057588636875\n",
      "[Epoch 30] Training Batch [297/391]: Loss 0.0017255673883482814\n",
      "[Epoch 30] Training Batch [298/391]: Loss 0.01618393138051033\n",
      "[Epoch 30] Training Batch [299/391]: Loss 0.011783130466938019\n",
      "[Epoch 30] Training Batch [300/391]: Loss 0.00421550078317523\n",
      "[Epoch 30] Training Batch [301/391]: Loss 0.002233779290691018\n",
      "[Epoch 30] Training Batch [302/391]: Loss 0.0074556246399879456\n",
      "[Epoch 30] Training Batch [303/391]: Loss 0.0008638588478788733\n",
      "[Epoch 30] Training Batch [304/391]: Loss 0.005398129113018513\n",
      "[Epoch 30] Training Batch [305/391]: Loss 0.0012936487328261137\n",
      "[Epoch 30] Training Batch [306/391]: Loss 0.0009793981444090605\n",
      "[Epoch 30] Training Batch [307/391]: Loss 0.003547642845660448\n",
      "[Epoch 30] Training Batch [308/391]: Loss 0.018080946058034897\n",
      "[Epoch 30] Training Batch [309/391]: Loss 0.019636211916804314\n",
      "[Epoch 30] Training Batch [310/391]: Loss 0.03088238276541233\n",
      "[Epoch 30] Training Batch [311/391]: Loss 0.03358311951160431\n",
      "[Epoch 30] Training Batch [312/391]: Loss 0.02341761626303196\n",
      "[Epoch 30] Training Batch [313/391]: Loss 0.0010637263767421246\n",
      "[Epoch 30] Training Batch [314/391]: Loss 0.0005320376367308199\n",
      "[Epoch 30] Training Batch [315/391]: Loss 0.001473010634072125\n",
      "[Epoch 30] Training Batch [316/391]: Loss 0.0005399916553869843\n",
      "[Epoch 30] Training Batch [317/391]: Loss 0.0775885358452797\n",
      "[Epoch 30] Training Batch [318/391]: Loss 0.002269389573484659\n",
      "[Epoch 30] Training Batch [319/391]: Loss 0.012079252861440182\n",
      "[Epoch 30] Training Batch [320/391]: Loss 0.003643395844846964\n",
      "[Epoch 30] Training Batch [321/391]: Loss 0.0009181082714349031\n",
      "[Epoch 30] Training Batch [322/391]: Loss 0.0007994687766768038\n",
      "[Epoch 30] Training Batch [323/391]: Loss 0.00688573531806469\n",
      "[Epoch 30] Training Batch [324/391]: Loss 0.006416121032088995\n",
      "[Epoch 30] Training Batch [325/391]: Loss 0.013082664459943771\n",
      "[Epoch 30] Training Batch [326/391]: Loss 0.006988876964896917\n",
      "[Epoch 30] Training Batch [327/391]: Loss 0.021995237097144127\n",
      "[Epoch 30] Training Batch [328/391]: Loss 0.05348258465528488\n",
      "[Epoch 30] Training Batch [329/391]: Loss 0.005807617679238319\n",
      "[Epoch 30] Training Batch [330/391]: Loss 0.001610275125131011\n",
      "[Epoch 30] Training Batch [331/391]: Loss 0.0011973566142842174\n",
      "[Epoch 30] Training Batch [332/391]: Loss 0.01985258050262928\n",
      "[Epoch 30] Training Batch [333/391]: Loss 0.003544054226949811\n",
      "[Epoch 30] Training Batch [334/391]: Loss 0.005268764216452837\n",
      "[Epoch 30] Training Batch [335/391]: Loss 0.006877250969409943\n",
      "[Epoch 30] Training Batch [336/391]: Loss 0.00471657095476985\n",
      "[Epoch 30] Training Batch [337/391]: Loss 0.014546296559274197\n",
      "[Epoch 30] Training Batch [338/391]: Loss 0.026436002925038338\n",
      "[Epoch 30] Training Batch [339/391]: Loss 0.01842997595667839\n",
      "[Epoch 30] Training Batch [340/391]: Loss 0.013263884000480175\n",
      "[Epoch 30] Training Batch [341/391]: Loss 0.007398262154310942\n",
      "[Epoch 30] Training Batch [342/391]: Loss 0.002620112616568804\n",
      "[Epoch 30] Training Batch [343/391]: Loss 0.01215702760964632\n",
      "[Epoch 30] Training Batch [344/391]: Loss 0.01690620183944702\n",
      "[Epoch 30] Training Batch [345/391]: Loss 0.001081376220099628\n",
      "[Epoch 30] Training Batch [346/391]: Loss 0.0017997511895373464\n",
      "[Epoch 30] Training Batch [347/391]: Loss 0.015835050493478775\n",
      "[Epoch 30] Training Batch [348/391]: Loss 0.0009483926114626229\n",
      "[Epoch 30] Training Batch [349/391]: Loss 0.006770233623683453\n",
      "[Epoch 30] Training Batch [350/391]: Loss 0.0015460565919056535\n",
      "[Epoch 30] Training Batch [351/391]: Loss 0.00917941052466631\n",
      "[Epoch 30] Training Batch [352/391]: Loss 0.0024502284359186888\n",
      "[Epoch 30] Training Batch [353/391]: Loss 0.0033336577471345663\n",
      "[Epoch 30] Training Batch [354/391]: Loss 0.00480923056602478\n",
      "[Epoch 30] Training Batch [355/391]: Loss 0.0024488915223628283\n",
      "[Epoch 30] Training Batch [356/391]: Loss 0.0040671853348612785\n",
      "[Epoch 30] Training Batch [357/391]: Loss 0.004881836473941803\n",
      "[Epoch 30] Training Batch [358/391]: Loss 0.00811874307692051\n",
      "[Epoch 30] Training Batch [359/391]: Loss 0.000864449655637145\n",
      "[Epoch 30] Training Batch [360/391]: Loss 0.0006625233800150454\n",
      "[Epoch 30] Training Batch [361/391]: Loss 0.0008541415445506573\n",
      "[Epoch 30] Training Batch [362/391]: Loss 0.001145143061876297\n",
      "[Epoch 30] Training Batch [363/391]: Loss 0.01672682911157608\n",
      "[Epoch 30] Training Batch [364/391]: Loss 0.0025425117928534746\n",
      "[Epoch 30] Training Batch [365/391]: Loss 0.012142093852162361\n",
      "[Epoch 30] Training Batch [366/391]: Loss 0.005007189232856035\n",
      "[Epoch 30] Training Batch [367/391]: Loss 0.002972595626488328\n",
      "[Epoch 30] Training Batch [368/391]: Loss 0.008640005253255367\n",
      "[Epoch 30] Training Batch [369/391]: Loss 0.0019550456199795008\n",
      "[Epoch 30] Training Batch [370/391]: Loss 0.003449982963502407\n",
      "[Epoch 30] Training Batch [371/391]: Loss 0.02164503186941147\n",
      "[Epoch 30] Training Batch [372/391]: Loss 0.0013090797001495957\n",
      "[Epoch 30] Training Batch [373/391]: Loss 0.00860948022454977\n",
      "[Epoch 30] Training Batch [374/391]: Loss 0.020983697846531868\n",
      "[Epoch 30] Training Batch [375/391]: Loss 0.0021639566402882338\n",
      "[Epoch 30] Training Batch [376/391]: Loss 0.009917938150465488\n",
      "[Epoch 30] Training Batch [377/391]: Loss 0.0025512550491839647\n",
      "[Epoch 30] Training Batch [378/391]: Loss 0.0014214601833373308\n",
      "[Epoch 30] Training Batch [379/391]: Loss 0.04750490561127663\n",
      "[Epoch 30] Training Batch [380/391]: Loss 0.001531204441562295\n",
      "[Epoch 30] Training Batch [381/391]: Loss 0.005895839538425207\n",
      "[Epoch 30] Training Batch [382/391]: Loss 0.013504021801054478\n",
      "[Epoch 30] Training Batch [383/391]: Loss 0.001580320647917688\n",
      "[Epoch 30] Training Batch [384/391]: Loss 0.002290587406605482\n",
      "[Epoch 30] Training Batch [385/391]: Loss 0.0071150255389511585\n",
      "[Epoch 30] Training Batch [386/391]: Loss 0.008539274334907532\n",
      "[Epoch 30] Training Batch [387/391]: Loss 0.011890300549566746\n",
      "[Epoch 30] Training Batch [388/391]: Loss 0.013625051826238632\n",
      "[Epoch 30] Training Batch [389/391]: Loss 0.010015636682510376\n",
      "[Epoch 30] Training Batch [390/391]: Loss 0.01462644524872303\n",
      "[Epoch 30] Training Batch [391/391]: Loss 0.05208779126405716\n",
      "Epoch 30 - Train Loss: 0.0068\n",
      "*********  Epoch 31/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Training Batch [1/391]: Loss 0.006794104818254709\n",
      "[Epoch 31] Training Batch [2/391]: Loss 0.0011183375027030706\n",
      "[Epoch 31] Training Batch [3/391]: Loss 0.0007732238154858351\n",
      "[Epoch 31] Training Batch [4/391]: Loss 0.0014824345707893372\n",
      "[Epoch 31] Training Batch [5/391]: Loss 0.0048453365452587605\n",
      "[Epoch 31] Training Batch [6/391]: Loss 0.005534053314477205\n",
      "[Epoch 31] Training Batch [7/391]: Loss 0.021530384197831154\n",
      "[Epoch 31] Training Batch [8/391]: Loss 0.0015200760681182146\n",
      "[Epoch 31] Training Batch [9/391]: Loss 0.05393184721469879\n",
      "[Epoch 31] Training Batch [10/391]: Loss 0.0012181185884401202\n",
      "[Epoch 31] Training Batch [11/391]: Loss 0.08413393050432205\n",
      "[Epoch 31] Training Batch [12/391]: Loss 0.005757824517786503\n",
      "[Epoch 31] Training Batch [13/391]: Loss 0.007284941151738167\n",
      "[Epoch 31] Training Batch [14/391]: Loss 0.002381441416218877\n",
      "[Epoch 31] Training Batch [15/391]: Loss 0.00819276925176382\n",
      "[Epoch 31] Training Batch [16/391]: Loss 0.0006948801456019282\n",
      "[Epoch 31] Training Batch [17/391]: Loss 0.005982203409075737\n",
      "[Epoch 31] Training Batch [18/391]: Loss 0.003161101136356592\n",
      "[Epoch 31] Training Batch [19/391]: Loss 0.0010329654905945063\n",
      "[Epoch 31] Training Batch [20/391]: Loss 0.004701134283095598\n",
      "[Epoch 31] Training Batch [21/391]: Loss 0.006769589614123106\n",
      "[Epoch 31] Training Batch [22/391]: Loss 0.002198378322646022\n",
      "[Epoch 31] Training Batch [23/391]: Loss 0.028409214690327644\n",
      "[Epoch 31] Training Batch [24/391]: Loss 0.0038024899549782276\n",
      "[Epoch 31] Training Batch [25/391]: Loss 0.023726005107164383\n",
      "[Epoch 31] Training Batch [26/391]: Loss 0.028031405061483383\n",
      "[Epoch 31] Training Batch [27/391]: Loss 0.014683591201901436\n",
      "[Epoch 31] Training Batch [28/391]: Loss 0.0005220503662712872\n",
      "[Epoch 31] Training Batch [29/391]: Loss 0.012561045587062836\n",
      "[Epoch 31] Training Batch [30/391]: Loss 0.011137588880956173\n",
      "[Epoch 31] Training Batch [31/391]: Loss 0.03089146688580513\n",
      "[Epoch 31] Training Batch [32/391]: Loss 0.0029414573218673468\n",
      "[Epoch 31] Training Batch [33/391]: Loss 0.0007062029908411205\n",
      "[Epoch 31] Training Batch [34/391]: Loss 0.002774736611172557\n",
      "[Epoch 31] Training Batch [35/391]: Loss 0.004375234711915255\n",
      "[Epoch 31] Training Batch [36/391]: Loss 0.016967622563242912\n",
      "[Epoch 31] Training Batch [37/391]: Loss 0.017781289294362068\n",
      "[Epoch 31] Training Batch [38/391]: Loss 0.0038999542593955994\n",
      "[Epoch 31] Training Batch [39/391]: Loss 0.008407294750213623\n",
      "[Epoch 31] Training Batch [40/391]: Loss 0.0007591109024360776\n",
      "[Epoch 31] Training Batch [41/391]: Loss 0.0058318316005170345\n",
      "[Epoch 31] Training Batch [42/391]: Loss 0.011612137779593468\n",
      "[Epoch 31] Training Batch [43/391]: Loss 0.011009693145751953\n",
      "[Epoch 31] Training Batch [44/391]: Loss 0.006198198534548283\n",
      "[Epoch 31] Training Batch [45/391]: Loss 0.0017277589067816734\n",
      "[Epoch 31] Training Batch [46/391]: Loss 0.005750725045800209\n",
      "[Epoch 31] Training Batch [47/391]: Loss 0.002677604788914323\n",
      "[Epoch 31] Training Batch [48/391]: Loss 0.010889658704400063\n",
      "[Epoch 31] Training Batch [49/391]: Loss 0.004957443103194237\n",
      "[Epoch 31] Training Batch [50/391]: Loss 0.002123659709468484\n",
      "[Epoch 31] Training Batch [51/391]: Loss 0.010557562112808228\n",
      "[Epoch 31] Training Batch [52/391]: Loss 0.003159543499350548\n",
      "[Epoch 31] Training Batch [53/391]: Loss 0.020374463871121407\n",
      "[Epoch 31] Training Batch [54/391]: Loss 0.004515206441283226\n",
      "[Epoch 31] Training Batch [55/391]: Loss 0.014935747720301151\n",
      "[Epoch 31] Training Batch [56/391]: Loss 0.0014100488042458892\n",
      "[Epoch 31] Training Batch [57/391]: Loss 0.003829589579254389\n",
      "[Epoch 31] Training Batch [58/391]: Loss 0.0008434132323600352\n",
      "[Epoch 31] Training Batch [59/391]: Loss 0.00548348156735301\n",
      "[Epoch 31] Training Batch [60/391]: Loss 0.003044093493372202\n",
      "[Epoch 31] Training Batch [61/391]: Loss 0.012426387518644333\n",
      "[Epoch 31] Training Batch [62/391]: Loss 0.005343735218048096\n",
      "[Epoch 31] Training Batch [63/391]: Loss 0.0029354009311646223\n",
      "[Epoch 31] Training Batch [64/391]: Loss 0.005229013040661812\n",
      "[Epoch 31] Training Batch [65/391]: Loss 0.0039026730228215456\n",
      "[Epoch 31] Training Batch [66/391]: Loss 0.00838736817240715\n",
      "[Epoch 31] Training Batch [67/391]: Loss 0.0013615208445116878\n",
      "[Epoch 31] Training Batch [68/391]: Loss 0.0010129681322723627\n",
      "[Epoch 31] Training Batch [69/391]: Loss 0.0007576496573165059\n",
      "[Epoch 31] Training Batch [70/391]: Loss 0.0052053057588636875\n",
      "[Epoch 31] Training Batch [71/391]: Loss 0.026509089395403862\n",
      "[Epoch 31] Training Batch [72/391]: Loss 0.02240884117782116\n",
      "[Epoch 31] Training Batch [73/391]: Loss 0.0024754612240940332\n",
      "[Epoch 31] Training Batch [74/391]: Loss 0.005520440638065338\n",
      "[Epoch 31] Training Batch [75/391]: Loss 0.002589978976175189\n",
      "[Epoch 31] Training Batch [76/391]: Loss 0.003393161576241255\n",
      "[Epoch 31] Training Batch [77/391]: Loss 0.0005131037905812263\n",
      "[Epoch 31] Training Batch [78/391]: Loss 0.002266339026391506\n",
      "[Epoch 31] Training Batch [79/391]: Loss 0.0032990791369229555\n",
      "[Epoch 31] Training Batch [80/391]: Loss 0.0007005039369687438\n",
      "[Epoch 31] Training Batch [81/391]: Loss 0.00508482800796628\n",
      "[Epoch 31] Training Batch [82/391]: Loss 0.005585449282079935\n",
      "[Epoch 31] Training Batch [83/391]: Loss 0.003106177318841219\n",
      "[Epoch 31] Training Batch [84/391]: Loss 0.006039897911250591\n",
      "[Epoch 31] Training Batch [85/391]: Loss 0.0014487453736364841\n",
      "[Epoch 31] Training Batch [86/391]: Loss 0.0030355181079357862\n",
      "[Epoch 31] Training Batch [87/391]: Loss 0.0014833980239927769\n",
      "[Epoch 31] Training Batch [88/391]: Loss 0.0025176245253533125\n",
      "[Epoch 31] Training Batch [89/391]: Loss 0.01758914440870285\n",
      "[Epoch 31] Training Batch [90/391]: Loss 0.002987779676914215\n",
      "[Epoch 31] Training Batch [91/391]: Loss 0.0008217637659981847\n",
      "[Epoch 31] Training Batch [92/391]: Loss 0.017031265422701836\n",
      "[Epoch 31] Training Batch [93/391]: Loss 0.0028731541242450476\n",
      "[Epoch 31] Training Batch [94/391]: Loss 0.0015033817617222667\n",
      "[Epoch 31] Training Batch [95/391]: Loss 0.004891511518508196\n",
      "[Epoch 31] Training Batch [96/391]: Loss 0.03637346625328064\n",
      "[Epoch 31] Training Batch [97/391]: Loss 0.006946322042495012\n",
      "[Epoch 31] Training Batch [98/391]: Loss 0.0014095917576923966\n",
      "[Epoch 31] Training Batch [99/391]: Loss 0.0015320898965001106\n",
      "[Epoch 31] Training Batch [100/391]: Loss 0.001521991565823555\n",
      "[Epoch 31] Training Batch [101/391]: Loss 0.00411301339045167\n",
      "[Epoch 31] Training Batch [102/391]: Loss 0.0012327989097684622\n",
      "[Epoch 31] Training Batch [103/391]: Loss 0.0014522293349727988\n",
      "[Epoch 31] Training Batch [104/391]: Loss 0.0007106774719431996\n",
      "[Epoch 31] Training Batch [105/391]: Loss 0.007531686220318079\n",
      "[Epoch 31] Training Batch [106/391]: Loss 0.0012512481771409512\n",
      "[Epoch 31] Training Batch [107/391]: Loss 0.005379372276365757\n",
      "[Epoch 31] Training Batch [108/391]: Loss 0.0009505735943093896\n",
      "[Epoch 31] Training Batch [109/391]: Loss 0.0033569252118468285\n",
      "[Epoch 31] Training Batch [110/391]: Loss 0.002377675846219063\n",
      "[Epoch 31] Training Batch [111/391]: Loss 0.0012020111316815019\n",
      "[Epoch 31] Training Batch [112/391]: Loss 0.004624659661203623\n",
      "[Epoch 31] Training Batch [113/391]: Loss 0.0049262698739767075\n",
      "[Epoch 31] Training Batch [114/391]: Loss 0.002081258688122034\n",
      "[Epoch 31] Training Batch [115/391]: Loss 0.0006395238451659679\n",
      "[Epoch 31] Training Batch [116/391]: Loss 0.0004411651170812547\n",
      "[Epoch 31] Training Batch [117/391]: Loss 0.00031777197727933526\n",
      "[Epoch 31] Training Batch [118/391]: Loss 0.016256058588624\n",
      "[Epoch 31] Training Batch [119/391]: Loss 0.0009346138685941696\n",
      "[Epoch 31] Training Batch [120/391]: Loss 0.022567367181181908\n",
      "[Epoch 31] Training Batch [121/391]: Loss 0.0038651018403470516\n",
      "[Epoch 31] Training Batch [122/391]: Loss 0.0008443195838481188\n",
      "[Epoch 31] Training Batch [123/391]: Loss 0.0006285944837145507\n",
      "[Epoch 31] Training Batch [124/391]: Loss 0.005148905795067549\n",
      "[Epoch 31] Training Batch [125/391]: Loss 0.018245242536067963\n",
      "[Epoch 31] Training Batch [126/391]: Loss 0.0007319587748497725\n",
      "[Epoch 31] Training Batch [127/391]: Loss 0.0008104491862468421\n",
      "[Epoch 31] Training Batch [128/391]: Loss 0.0029185970779508352\n",
      "[Epoch 31] Training Batch [129/391]: Loss 0.005085493437945843\n",
      "[Epoch 31] Training Batch [130/391]: Loss 0.0003111982368864119\n",
      "[Epoch 31] Training Batch [131/391]: Loss 0.0051021878607571125\n",
      "[Epoch 31] Training Batch [132/391]: Loss 0.011291653849184513\n",
      "[Epoch 31] Training Batch [133/391]: Loss 0.0013663519639521837\n",
      "[Epoch 31] Training Batch [134/391]: Loss 0.0005171144148334861\n",
      "[Epoch 31] Training Batch [135/391]: Loss 0.0019815522246062756\n",
      "[Epoch 31] Training Batch [136/391]: Loss 0.0008723298087716103\n",
      "[Epoch 31] Training Batch [137/391]: Loss 0.004811437800526619\n",
      "[Epoch 31] Training Batch [138/391]: Loss 0.0064806854352355\n",
      "[Epoch 31] Training Batch [139/391]: Loss 0.00741447601467371\n",
      "[Epoch 31] Training Batch [140/391]: Loss 0.0020461997482925653\n",
      "[Epoch 31] Training Batch [141/391]: Loss 0.006160322111099958\n",
      "[Epoch 31] Training Batch [142/391]: Loss 0.005261803511530161\n",
      "[Epoch 31] Training Batch [143/391]: Loss 0.004800430499017239\n",
      "[Epoch 31] Training Batch [144/391]: Loss 0.0035234629176557064\n",
      "[Epoch 31] Training Batch [145/391]: Loss 0.000623641419224441\n",
      "[Epoch 31] Training Batch [146/391]: Loss 0.02982979081571102\n",
      "[Epoch 31] Training Batch [147/391]: Loss 0.01563532091677189\n",
      "[Epoch 31] Training Batch [148/391]: Loss 0.0009387623285874724\n",
      "[Epoch 31] Training Batch [149/391]: Loss 0.0028210508171468973\n",
      "[Epoch 31] Training Batch [150/391]: Loss 0.007651710417121649\n",
      "[Epoch 31] Training Batch [151/391]: Loss 0.0012320327805355191\n",
      "[Epoch 31] Training Batch [152/391]: Loss 0.0029114321805536747\n",
      "[Epoch 31] Training Batch [153/391]: Loss 0.0026991774793714285\n",
      "[Epoch 31] Training Batch [154/391]: Loss 0.0029413863085210323\n",
      "[Epoch 31] Training Batch [155/391]: Loss 0.0017471379833295941\n",
      "[Epoch 31] Training Batch [156/391]: Loss 0.0007974591571837664\n",
      "[Epoch 31] Training Batch [157/391]: Loss 0.020904796198010445\n",
      "[Epoch 31] Training Batch [158/391]: Loss 0.0008698301971890032\n",
      "[Epoch 31] Training Batch [159/391]: Loss 0.0029346963856369257\n",
      "[Epoch 31] Training Batch [160/391]: Loss 0.0011992418440058827\n",
      "[Epoch 31] Training Batch [161/391]: Loss 0.0009529097005724907\n",
      "[Epoch 31] Training Batch [162/391]: Loss 0.007479443214833736\n",
      "[Epoch 31] Training Batch [163/391]: Loss 0.0004543869581539184\n",
      "[Epoch 31] Training Batch [164/391]: Loss 0.004531748127192259\n",
      "[Epoch 31] Training Batch [165/391]: Loss 0.006952464580535889\n",
      "[Epoch 31] Training Batch [166/391]: Loss 0.002205571858212352\n",
      "[Epoch 31] Training Batch [167/391]: Loss 0.0059694405645132065\n",
      "[Epoch 31] Training Batch [168/391]: Loss 0.000645805208478123\n",
      "[Epoch 31] Training Batch [169/391]: Loss 0.0016724368324503303\n",
      "[Epoch 31] Training Batch [170/391]: Loss 0.0014141775900498033\n",
      "[Epoch 31] Training Batch [171/391]: Loss 0.00230969930998981\n",
      "[Epoch 31] Training Batch [172/391]: Loss 0.003922948148101568\n",
      "[Epoch 31] Training Batch [173/391]: Loss 0.0008263207855634391\n",
      "[Epoch 31] Training Batch [174/391]: Loss 0.0007337955757975578\n",
      "[Epoch 31] Training Batch [175/391]: Loss 0.002096841111779213\n",
      "[Epoch 31] Training Batch [176/391]: Loss 0.004779027309268713\n",
      "[Epoch 31] Training Batch [177/391]: Loss 0.003550490364432335\n",
      "[Epoch 31] Training Batch [178/391]: Loss 0.0007002888596616685\n",
      "[Epoch 31] Training Batch [179/391]: Loss 0.001609145081602037\n",
      "[Epoch 31] Training Batch [180/391]: Loss 0.0006504444172605872\n",
      "[Epoch 31] Training Batch [181/391]: Loss 0.006054051220417023\n",
      "[Epoch 31] Training Batch [182/391]: Loss 0.0683763399720192\n",
      "[Epoch 31] Training Batch [183/391]: Loss 0.0016919324407353997\n",
      "[Epoch 31] Training Batch [184/391]: Loss 0.0013408424565568566\n",
      "[Epoch 31] Training Batch [185/391]: Loss 0.00961021427065134\n",
      "[Epoch 31] Training Batch [186/391]: Loss 0.0005468946183100343\n",
      "[Epoch 31] Training Batch [187/391]: Loss 0.001153014600276947\n",
      "[Epoch 31] Training Batch [188/391]: Loss 0.01989542692899704\n",
      "[Epoch 31] Training Batch [189/391]: Loss 0.0008991471258923411\n",
      "[Epoch 31] Training Batch [190/391]: Loss 0.0010345977498218417\n",
      "[Epoch 31] Training Batch [191/391]: Loss 0.0038594931829720736\n",
      "[Epoch 31] Training Batch [192/391]: Loss 0.0004919646307826042\n",
      "[Epoch 31] Training Batch [193/391]: Loss 0.019128767773509026\n",
      "[Epoch 31] Training Batch [194/391]: Loss 0.005817529279738665\n",
      "[Epoch 31] Training Batch [195/391]: Loss 0.004406271502375603\n",
      "[Epoch 31] Training Batch [196/391]: Loss 0.00489317299798131\n",
      "[Epoch 31] Training Batch [197/391]: Loss 0.0008934468496590853\n",
      "[Epoch 31] Training Batch [198/391]: Loss 0.011055275797843933\n",
      "[Epoch 31] Training Batch [199/391]: Loss 0.0011984402080997825\n",
      "[Epoch 31] Training Batch [200/391]: Loss 0.002896303776651621\n",
      "[Epoch 31] Training Batch [201/391]: Loss 0.003023139899596572\n",
      "[Epoch 31] Training Batch [202/391]: Loss 0.0018201597267761827\n",
      "[Epoch 31] Training Batch [203/391]: Loss 0.00043212666059844196\n",
      "[Epoch 31] Training Batch [204/391]: Loss 0.0024956197012215853\n",
      "[Epoch 31] Training Batch [205/391]: Loss 0.004374469630420208\n",
      "[Epoch 31] Training Batch [206/391]: Loss 0.000741588301025331\n",
      "[Epoch 31] Training Batch [207/391]: Loss 0.0051727392710745335\n",
      "[Epoch 31] Training Batch [208/391]: Loss 0.012457076460123062\n",
      "[Epoch 31] Training Batch [209/391]: Loss 0.0006308864685706794\n",
      "[Epoch 31] Training Batch [210/391]: Loss 0.0008744766819290817\n",
      "[Epoch 31] Training Batch [211/391]: Loss 0.009641717188060284\n",
      "[Epoch 31] Training Batch [212/391]: Loss 0.017490020021796227\n",
      "[Epoch 31] Training Batch [213/391]: Loss 0.001925457501783967\n",
      "[Epoch 31] Training Batch [214/391]: Loss 0.0009842757135629654\n",
      "[Epoch 31] Training Batch [215/391]: Loss 0.0014051367761567235\n",
      "[Epoch 31] Training Batch [216/391]: Loss 0.0010290466016158462\n",
      "[Epoch 31] Training Batch [217/391]: Loss 0.015258187428116798\n",
      "[Epoch 31] Training Batch [218/391]: Loss 0.00350548536516726\n",
      "[Epoch 31] Training Batch [219/391]: Loss 0.0012402958236634731\n",
      "[Epoch 31] Training Batch [220/391]: Loss 0.05040798336267471\n",
      "[Epoch 31] Training Batch [221/391]: Loss 0.0005748637486249208\n",
      "[Epoch 31] Training Batch [222/391]: Loss 0.0017605277244001627\n",
      "[Epoch 31] Training Batch [223/391]: Loss 0.0012339004315435886\n",
      "[Epoch 31] Training Batch [224/391]: Loss 0.0004169337044004351\n",
      "[Epoch 31] Training Batch [225/391]: Loss 0.0016456068260595202\n",
      "[Epoch 31] Training Batch [226/391]: Loss 0.002928184112533927\n",
      "[Epoch 31] Training Batch [227/391]: Loss 0.0013158534420654178\n",
      "[Epoch 31] Training Batch [228/391]: Loss 0.0004190672480035573\n",
      "[Epoch 31] Training Batch [229/391]: Loss 0.004084059968590736\n",
      "[Epoch 31] Training Batch [230/391]: Loss 0.0005524957668967545\n",
      "[Epoch 31] Training Batch [231/391]: Loss 0.0036710717249661684\n",
      "[Epoch 31] Training Batch [232/391]: Loss 0.0038176947273314\n",
      "[Epoch 31] Training Batch [233/391]: Loss 0.016833223402500153\n",
      "[Epoch 31] Training Batch [234/391]: Loss 0.0012362177949398756\n",
      "[Epoch 31] Training Batch [235/391]: Loss 0.0029139232356101274\n",
      "[Epoch 31] Training Batch [236/391]: Loss 0.0010240493575111032\n",
      "[Epoch 31] Training Batch [237/391]: Loss 0.002192050451412797\n",
      "[Epoch 31] Training Batch [238/391]: Loss 0.002505695912986994\n",
      "[Epoch 31] Training Batch [239/391]: Loss 0.0033786622807383537\n",
      "[Epoch 31] Training Batch [240/391]: Loss 0.0147685706615448\n",
      "[Epoch 31] Training Batch [241/391]: Loss 0.0009417942492291331\n",
      "[Epoch 31] Training Batch [242/391]: Loss 0.0005331956781446934\n",
      "[Epoch 31] Training Batch [243/391]: Loss 0.007396081928163767\n",
      "[Epoch 31] Training Batch [244/391]: Loss 0.0011632689274847507\n",
      "[Epoch 31] Training Batch [245/391]: Loss 0.0011791102588176727\n",
      "[Epoch 31] Training Batch [246/391]: Loss 0.0017974867951124907\n",
      "[Epoch 31] Training Batch [247/391]: Loss 0.004938354250043631\n",
      "[Epoch 31] Training Batch [248/391]: Loss 0.003413144266232848\n",
      "[Epoch 31] Training Batch [249/391]: Loss 0.0009118742891587317\n",
      "[Epoch 31] Training Batch [250/391]: Loss 0.004370782524347305\n",
      "[Epoch 31] Training Batch [251/391]: Loss 0.005259701982140541\n",
      "[Epoch 31] Training Batch [252/391]: Loss 0.0006516482681035995\n",
      "[Epoch 31] Training Batch [253/391]: Loss 0.0006431594956666231\n",
      "[Epoch 31] Training Batch [254/391]: Loss 0.03302864730358124\n",
      "[Epoch 31] Training Batch [255/391]: Loss 0.0030570400413125753\n",
      "[Epoch 31] Training Batch [256/391]: Loss 0.0009529188391752541\n",
      "[Epoch 31] Training Batch [257/391]: Loss 0.00043213789467699826\n",
      "[Epoch 31] Training Batch [258/391]: Loss 0.0007167866569943726\n",
      "[Epoch 31] Training Batch [259/391]: Loss 0.0021065338514745235\n",
      "[Epoch 31] Training Batch [260/391]: Loss 0.0018656125757843256\n",
      "[Epoch 31] Training Batch [261/391]: Loss 0.007496465928852558\n",
      "[Epoch 31] Training Batch [262/391]: Loss 0.002037945669144392\n",
      "[Epoch 31] Training Batch [263/391]: Loss 0.0015061090234667063\n",
      "[Epoch 31] Training Batch [264/391]: Loss 0.006294303108006716\n",
      "[Epoch 31] Training Batch [265/391]: Loss 0.0017217444255948067\n",
      "[Epoch 31] Training Batch [266/391]: Loss 0.018487827852368355\n",
      "[Epoch 31] Training Batch [267/391]: Loss 0.0007021832279860973\n",
      "[Epoch 31] Training Batch [268/391]: Loss 0.009565987624228\n",
      "[Epoch 31] Training Batch [269/391]: Loss 0.0002240089379483834\n",
      "[Epoch 31] Training Batch [270/391]: Loss 0.033334881067276\n",
      "[Epoch 31] Training Batch [271/391]: Loss 0.004664110951125622\n",
      "[Epoch 31] Training Batch [272/391]: Loss 0.04000882804393768\n",
      "[Epoch 31] Training Batch [273/391]: Loss 0.002403860678896308\n",
      "[Epoch 31] Training Batch [274/391]: Loss 0.0005157695850357413\n",
      "[Epoch 31] Training Batch [275/391]: Loss 0.0009357195813208818\n",
      "[Epoch 31] Training Batch [276/391]: Loss 0.00569302449002862\n",
      "[Epoch 31] Training Batch [277/391]: Loss 0.0010037565371021628\n",
      "[Epoch 31] Training Batch [278/391]: Loss 0.0033365655690431595\n",
      "[Epoch 31] Training Batch [279/391]: Loss 0.02123359590768814\n",
      "[Epoch 31] Training Batch [280/391]: Loss 0.0258585587143898\n",
      "[Epoch 31] Training Batch [281/391]: Loss 0.010599183849990368\n",
      "[Epoch 31] Training Batch [282/391]: Loss 0.0020191040821373463\n",
      "[Epoch 31] Training Batch [283/391]: Loss 0.002347490517422557\n",
      "[Epoch 31] Training Batch [284/391]: Loss 0.0007968799327500165\n",
      "[Epoch 31] Training Batch [285/391]: Loss 0.0016005452489480376\n",
      "[Epoch 31] Training Batch [286/391]: Loss 0.0007128595607355237\n",
      "[Epoch 31] Training Batch [287/391]: Loss 0.003877143142744899\n",
      "[Epoch 31] Training Batch [288/391]: Loss 0.05432981625199318\n",
      "[Epoch 31] Training Batch [289/391]: Loss 0.03131033480167389\n",
      "[Epoch 31] Training Batch [290/391]: Loss 0.01243225671350956\n",
      "[Epoch 31] Training Batch [291/391]: Loss 0.04722138121724129\n",
      "[Epoch 31] Training Batch [292/391]: Loss 0.006988626439124346\n",
      "[Epoch 31] Training Batch [293/391]: Loss 0.017153212800621986\n",
      "[Epoch 31] Training Batch [294/391]: Loss 0.0015896373661234975\n",
      "[Epoch 31] Training Batch [295/391]: Loss 0.004631925839930773\n",
      "[Epoch 31] Training Batch [296/391]: Loss 0.005007933359593153\n",
      "[Epoch 31] Training Batch [297/391]: Loss 0.005433345213532448\n",
      "[Epoch 31] Training Batch [298/391]: Loss 0.01030645240098238\n",
      "[Epoch 31] Training Batch [299/391]: Loss 0.009829707443714142\n",
      "[Epoch 31] Training Batch [300/391]: Loss 0.01677907258272171\n",
      "[Epoch 31] Training Batch [301/391]: Loss 0.02222726121544838\n",
      "[Epoch 31] Training Batch [302/391]: Loss 0.0009703685063868761\n",
      "[Epoch 31] Training Batch [303/391]: Loss 0.003042475553229451\n",
      "[Epoch 31] Training Batch [304/391]: Loss 0.0022393614053726196\n",
      "[Epoch 31] Training Batch [305/391]: Loss 0.0005805494147352874\n",
      "[Epoch 31] Training Batch [306/391]: Loss 0.013442497700452805\n",
      "[Epoch 31] Training Batch [307/391]: Loss 0.0018808812601491809\n",
      "[Epoch 31] Training Batch [308/391]: Loss 0.010113248601555824\n",
      "[Epoch 31] Training Batch [309/391]: Loss 0.006546420510858297\n",
      "[Epoch 31] Training Batch [310/391]: Loss 0.0011151102371513844\n",
      "[Epoch 31] Training Batch [311/391]: Loss 0.015477928332984447\n",
      "[Epoch 31] Training Batch [312/391]: Loss 0.017178142443299294\n",
      "[Epoch 31] Training Batch [313/391]: Loss 0.0035412146244198084\n",
      "[Epoch 31] Training Batch [314/391]: Loss 0.0013418906601145864\n",
      "[Epoch 31] Training Batch [315/391]: Loss 0.014114225283265114\n",
      "[Epoch 31] Training Batch [316/391]: Loss 0.00527446623891592\n",
      "[Epoch 31] Training Batch [317/391]: Loss 0.02611004188656807\n",
      "[Epoch 31] Training Batch [318/391]: Loss 0.0004979294026270509\n",
      "[Epoch 31] Training Batch [319/391]: Loss 0.001910559949465096\n",
      "[Epoch 31] Training Batch [320/391]: Loss 0.0009072190150618553\n",
      "[Epoch 31] Training Batch [321/391]: Loss 0.000631412782240659\n",
      "[Epoch 31] Training Batch [322/391]: Loss 0.03200899809598923\n",
      "[Epoch 31] Training Batch [323/391]: Loss 0.0009226338006556034\n",
      "[Epoch 31] Training Batch [324/391]: Loss 0.0010167739819735289\n",
      "[Epoch 31] Training Batch [325/391]: Loss 0.01203808095306158\n",
      "[Epoch 31] Training Batch [326/391]: Loss 0.0031126157846301794\n",
      "[Epoch 31] Training Batch [327/391]: Loss 0.014263267628848553\n",
      "[Epoch 31] Training Batch [328/391]: Loss 0.0014459026278927922\n",
      "[Epoch 31] Training Batch [329/391]: Loss 0.0009236365440301597\n",
      "[Epoch 31] Training Batch [330/391]: Loss 0.007140809204429388\n",
      "[Epoch 31] Training Batch [331/391]: Loss 0.0030058822594583035\n",
      "[Epoch 31] Training Batch [332/391]: Loss 0.00022782356245443225\n",
      "[Epoch 31] Training Batch [333/391]: Loss 0.0013413535198196769\n",
      "[Epoch 31] Training Batch [334/391]: Loss 0.0012629108969122171\n",
      "[Epoch 31] Training Batch [335/391]: Loss 0.0013438541209325194\n",
      "[Epoch 31] Training Batch [336/391]: Loss 0.0020737177692353725\n",
      "[Epoch 31] Training Batch [337/391]: Loss 0.0007714784587733448\n",
      "[Epoch 31] Training Batch [338/391]: Loss 0.0022815873380750418\n",
      "[Epoch 31] Training Batch [339/391]: Loss 0.01369384117424488\n",
      "[Epoch 31] Training Batch [340/391]: Loss 0.00018746776913758367\n",
      "[Epoch 31] Training Batch [341/391]: Loss 0.0008360739448107779\n",
      "[Epoch 31] Training Batch [342/391]: Loss 0.05291813984513283\n",
      "[Epoch 31] Training Batch [343/391]: Loss 0.0018931608647108078\n",
      "[Epoch 31] Training Batch [344/391]: Loss 0.0026087320875376463\n",
      "[Epoch 31] Training Batch [345/391]: Loss 0.0014969260664656758\n",
      "[Epoch 31] Training Batch [346/391]: Loss 0.009608481079339981\n",
      "[Epoch 31] Training Batch [347/391]: Loss 0.0008617084240540862\n",
      "[Epoch 31] Training Batch [348/391]: Loss 0.00046300425310619175\n",
      "[Epoch 31] Training Batch [349/391]: Loss 0.0019538514316082\n",
      "[Epoch 31] Training Batch [350/391]: Loss 0.0005312410648912191\n",
      "[Epoch 31] Training Batch [351/391]: Loss 0.017616024240851402\n",
      "[Epoch 31] Training Batch [352/391]: Loss 0.002560930559411645\n",
      "[Epoch 31] Training Batch [353/391]: Loss 0.01968492567539215\n",
      "[Epoch 31] Training Batch [354/391]: Loss 0.006581481546163559\n",
      "[Epoch 31] Training Batch [355/391]: Loss 0.024867836385965347\n",
      "[Epoch 31] Training Batch [356/391]: Loss 0.0010170250898227096\n",
      "[Epoch 31] Training Batch [357/391]: Loss 0.010101371444761753\n",
      "[Epoch 31] Training Batch [358/391]: Loss 0.005260039586573839\n",
      "[Epoch 31] Training Batch [359/391]: Loss 0.0012529564555734396\n",
      "[Epoch 31] Training Batch [360/391]: Loss 0.000628656183835119\n",
      "[Epoch 31] Training Batch [361/391]: Loss 0.010409251786768436\n",
      "[Epoch 31] Training Batch [362/391]: Loss 0.0035401186905801296\n",
      "[Epoch 31] Training Batch [363/391]: Loss 0.004541118163615465\n",
      "[Epoch 31] Training Batch [364/391]: Loss 0.02925361506640911\n",
      "[Epoch 31] Training Batch [365/391]: Loss 0.005834191106259823\n",
      "[Epoch 31] Training Batch [366/391]: Loss 0.002423725789412856\n",
      "[Epoch 31] Training Batch [367/391]: Loss 0.0011997238034382463\n",
      "[Epoch 31] Training Batch [368/391]: Loss 0.0010704720625653863\n",
      "[Epoch 31] Training Batch [369/391]: Loss 0.0028649524319916964\n",
      "[Epoch 31] Training Batch [370/391]: Loss 0.008346923626959324\n",
      "[Epoch 31] Training Batch [371/391]: Loss 0.0066065299324691296\n",
      "[Epoch 31] Training Batch [372/391]: Loss 0.006223926320672035\n",
      "[Epoch 31] Training Batch [373/391]: Loss 0.005617458838969469\n",
      "[Epoch 31] Training Batch [374/391]: Loss 0.006483353674411774\n",
      "[Epoch 31] Training Batch [375/391]: Loss 0.0008656856371089816\n",
      "[Epoch 31] Training Batch [376/391]: Loss 0.006814887747168541\n",
      "[Epoch 31] Training Batch [377/391]: Loss 0.012839952483773232\n",
      "[Epoch 31] Training Batch [378/391]: Loss 0.00020794231386389583\n",
      "[Epoch 31] Training Batch [379/391]: Loss 0.004809337668120861\n",
      "[Epoch 31] Training Batch [380/391]: Loss 0.0011458990629762411\n",
      "[Epoch 31] Training Batch [381/391]: Loss 0.0011832391610369086\n",
      "[Epoch 31] Training Batch [382/391]: Loss 0.0003181598731316626\n",
      "[Epoch 31] Training Batch [383/391]: Loss 0.0006093918345868587\n",
      "[Epoch 31] Training Batch [384/391]: Loss 0.004424462094902992\n",
      "[Epoch 31] Training Batch [385/391]: Loss 0.0034048070665448904\n",
      "[Epoch 31] Training Batch [386/391]: Loss 0.0007886821986176074\n",
      "[Epoch 31] Training Batch [387/391]: Loss 0.0014190308284014463\n",
      "[Epoch 31] Training Batch [388/391]: Loss 0.002808541525155306\n",
      "[Epoch 31] Training Batch [389/391]: Loss 0.028027456253767014\n",
      "[Epoch 31] Training Batch [390/391]: Loss 0.004004094749689102\n",
      "[Epoch 31] Training Batch [391/391]: Loss 0.0020785913802683353\n",
      "Epoch 31 - Train Loss: 0.0067\n",
      "*********  Epoch 32/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32] Training Batch [1/391]: Loss 0.0004325330664869398\n",
      "[Epoch 32] Training Batch [2/391]: Loss 0.001042500720359385\n",
      "[Epoch 32] Training Batch [3/391]: Loss 0.0011214015539735556\n",
      "[Epoch 32] Training Batch [4/391]: Loss 0.0005921369884163141\n",
      "[Epoch 32] Training Batch [5/391]: Loss 0.004748920910060406\n",
      "[Epoch 32] Training Batch [6/391]: Loss 0.0033736582845449448\n",
      "[Epoch 32] Training Batch [7/391]: Loss 0.000513021252118051\n",
      "[Epoch 32] Training Batch [8/391]: Loss 0.0005980840069241822\n",
      "[Epoch 32] Training Batch [9/391]: Loss 0.0012316468637436628\n",
      "[Epoch 32] Training Batch [10/391]: Loss 0.0007758854189887643\n",
      "[Epoch 32] Training Batch [11/391]: Loss 0.005512571427971125\n",
      "[Epoch 32] Training Batch [12/391]: Loss 0.0010054206941276789\n",
      "[Epoch 32] Training Batch [13/391]: Loss 0.0021433811634778976\n",
      "[Epoch 32] Training Batch [14/391]: Loss 0.0018657381879165769\n",
      "[Epoch 32] Training Batch [15/391]: Loss 0.00015818131214473397\n",
      "[Epoch 32] Training Batch [16/391]: Loss 0.005161772016435862\n",
      "[Epoch 32] Training Batch [17/391]: Loss 0.02839500643312931\n",
      "[Epoch 32] Training Batch [18/391]: Loss 0.001677046180702746\n",
      "[Epoch 32] Training Batch [19/391]: Loss 0.001150714117102325\n",
      "[Epoch 32] Training Batch [20/391]: Loss 0.0034030722454190254\n",
      "[Epoch 32] Training Batch [21/391]: Loss 0.0018086679046973586\n",
      "[Epoch 32] Training Batch [22/391]: Loss 0.0004111173329874873\n",
      "[Epoch 32] Training Batch [23/391]: Loss 0.0021634160075336695\n",
      "[Epoch 32] Training Batch [24/391]: Loss 0.0006139547913335264\n",
      "[Epoch 32] Training Batch [25/391]: Loss 0.010389615781605244\n",
      "[Epoch 32] Training Batch [26/391]: Loss 0.0008149572531692684\n",
      "[Epoch 32] Training Batch [27/391]: Loss 0.0009345132275484502\n",
      "[Epoch 32] Training Batch [28/391]: Loss 0.0030353113543242216\n",
      "[Epoch 32] Training Batch [29/391]: Loss 0.004450464621186256\n",
      "[Epoch 32] Training Batch [30/391]: Loss 0.0006042651366442442\n",
      "[Epoch 32] Training Batch [31/391]: Loss 0.005100567825138569\n",
      "[Epoch 32] Training Batch [32/391]: Loss 0.012306476011872292\n",
      "[Epoch 32] Training Batch [33/391]: Loss 0.0023468201979994774\n",
      "[Epoch 32] Training Batch [34/391]: Loss 0.002224448136985302\n",
      "[Epoch 32] Training Batch [35/391]: Loss 0.00123401521705091\n",
      "[Epoch 32] Training Batch [36/391]: Loss 0.001815205905586481\n",
      "[Epoch 32] Training Batch [37/391]: Loss 0.0089600570499897\n",
      "[Epoch 32] Training Batch [38/391]: Loss 0.005265112966299057\n",
      "[Epoch 32] Training Batch [39/391]: Loss 0.001430870615877211\n",
      "[Epoch 32] Training Batch [40/391]: Loss 0.01153547316789627\n",
      "[Epoch 32] Training Batch [41/391]: Loss 0.015099597163498402\n",
      "[Epoch 32] Training Batch [42/391]: Loss 0.0012554229469969869\n",
      "[Epoch 32] Training Batch [43/391]: Loss 0.0024032448418438435\n",
      "[Epoch 32] Training Batch [44/391]: Loss 0.001613956643268466\n",
      "[Epoch 32] Training Batch [45/391]: Loss 0.01869136467576027\n",
      "[Epoch 32] Training Batch [46/391]: Loss 0.0034483852796256542\n",
      "[Epoch 32] Training Batch [47/391]: Loss 0.006401670631021261\n",
      "[Epoch 32] Training Batch [48/391]: Loss 0.0007147765136323869\n",
      "[Epoch 32] Training Batch [49/391]: Loss 0.001055979635566473\n",
      "[Epoch 32] Training Batch [50/391]: Loss 0.0014854812761768699\n",
      "[Epoch 32] Training Batch [51/391]: Loss 0.0012188025284558535\n",
      "[Epoch 32] Training Batch [52/391]: Loss 0.0008236171561293304\n",
      "[Epoch 32] Training Batch [53/391]: Loss 0.005664574448019266\n",
      "[Epoch 32] Training Batch [54/391]: Loss 0.0008243779302574694\n",
      "[Epoch 32] Training Batch [55/391]: Loss 0.001572518376633525\n",
      "[Epoch 32] Training Batch [56/391]: Loss 0.028590280562639236\n",
      "[Epoch 32] Training Batch [57/391]: Loss 0.00686594657599926\n",
      "[Epoch 32] Training Batch [58/391]: Loss 0.0014694900019094348\n",
      "[Epoch 32] Training Batch [59/391]: Loss 0.008580449037253857\n",
      "[Epoch 32] Training Batch [60/391]: Loss 0.011415991000831127\n",
      "[Epoch 32] Training Batch [61/391]: Loss 0.0019284754525870085\n",
      "[Epoch 32] Training Batch [62/391]: Loss 0.0345754399895668\n",
      "[Epoch 32] Training Batch [63/391]: Loss 0.0017974941292777658\n",
      "[Epoch 32] Training Batch [64/391]: Loss 0.0020458127837628126\n",
      "[Epoch 32] Training Batch [65/391]: Loss 0.0009926969651132822\n",
      "[Epoch 32] Training Batch [66/391]: Loss 0.0012766310246661305\n",
      "[Epoch 32] Training Batch [67/391]: Loss 0.0016516153700649738\n",
      "[Epoch 32] Training Batch [68/391]: Loss 0.0014870751183480024\n",
      "[Epoch 32] Training Batch [69/391]: Loss 0.0013771502999588847\n",
      "[Epoch 32] Training Batch [70/391]: Loss 0.003811785951256752\n",
      "[Epoch 32] Training Batch [71/391]: Loss 0.0014572992222383618\n",
      "[Epoch 32] Training Batch [72/391]: Loss 0.0018136139260604978\n",
      "[Epoch 32] Training Batch [73/391]: Loss 0.010597426444292068\n",
      "[Epoch 32] Training Batch [74/391]: Loss 0.004016770515590906\n",
      "[Epoch 32] Training Batch [75/391]: Loss 0.00041506646084599197\n",
      "[Epoch 32] Training Batch [76/391]: Loss 0.001666099182330072\n",
      "[Epoch 32] Training Batch [77/391]: Loss 0.005277125630527735\n",
      "[Epoch 32] Training Batch [78/391]: Loss 0.0008298613247461617\n",
      "[Epoch 32] Training Batch [79/391]: Loss 0.0013615047791972756\n",
      "[Epoch 32] Training Batch [80/391]: Loss 0.004738449119031429\n",
      "[Epoch 32] Training Batch [81/391]: Loss 0.0014287012163549662\n",
      "[Epoch 32] Training Batch [82/391]: Loss 0.0005678781890310347\n",
      "[Epoch 32] Training Batch [83/391]: Loss 0.0010960770305246115\n",
      "[Epoch 32] Training Batch [84/391]: Loss 0.005069599486887455\n",
      "[Epoch 32] Training Batch [85/391]: Loss 0.0010063534136861563\n",
      "[Epoch 32] Training Batch [86/391]: Loss 0.0015911590307950974\n",
      "[Epoch 32] Training Batch [87/391]: Loss 0.00031404447508975863\n",
      "[Epoch 32] Training Batch [88/391]: Loss 0.00902647152543068\n",
      "[Epoch 32] Training Batch [89/391]: Loss 0.0014157993718981743\n",
      "[Epoch 32] Training Batch [90/391]: Loss 0.0006135737639851868\n",
      "[Epoch 32] Training Batch [91/391]: Loss 0.00975849200040102\n",
      "[Epoch 32] Training Batch [92/391]: Loss 0.005104187875986099\n",
      "[Epoch 32] Training Batch [93/391]: Loss 0.003530662041157484\n",
      "[Epoch 32] Training Batch [94/391]: Loss 0.021148638799786568\n",
      "[Epoch 32] Training Batch [95/391]: Loss 0.002325064968317747\n",
      "[Epoch 32] Training Batch [96/391]: Loss 0.03145259991288185\n",
      "[Epoch 32] Training Batch [97/391]: Loss 0.002109687775373459\n",
      "[Epoch 32] Training Batch [98/391]: Loss 0.0008256183937191963\n",
      "[Epoch 32] Training Batch [99/391]: Loss 0.0017003023531287909\n",
      "[Epoch 32] Training Batch [100/391]: Loss 0.000985757797025144\n",
      "[Epoch 32] Training Batch [101/391]: Loss 0.0004251083009876311\n",
      "[Epoch 32] Training Batch [102/391]: Loss 0.000317569327307865\n",
      "[Epoch 32] Training Batch [103/391]: Loss 0.0007882434292696416\n",
      "[Epoch 32] Training Batch [104/391]: Loss 0.001829817658290267\n",
      "[Epoch 32] Training Batch [105/391]: Loss 0.0011727118398994207\n",
      "[Epoch 32] Training Batch [106/391]: Loss 0.002150157233700156\n",
      "[Epoch 32] Training Batch [107/391]: Loss 0.0006168154068291187\n",
      "[Epoch 32] Training Batch [108/391]: Loss 0.0007940221112221479\n",
      "[Epoch 32] Training Batch [109/391]: Loss 0.000578352133743465\n",
      "[Epoch 32] Training Batch [110/391]: Loss 0.0015645466046407819\n",
      "[Epoch 32] Training Batch [111/391]: Loss 0.0008336597238667309\n",
      "[Epoch 32] Training Batch [112/391]: Loss 0.00035217514960095286\n",
      "[Epoch 32] Training Batch [113/391]: Loss 0.0003453932877164334\n",
      "[Epoch 32] Training Batch [114/391]: Loss 0.0017114662332460284\n",
      "[Epoch 32] Training Batch [115/391]: Loss 0.006503749173134565\n",
      "[Epoch 32] Training Batch [116/391]: Loss 0.002376148011535406\n",
      "[Epoch 32] Training Batch [117/391]: Loss 0.000307548267301172\n",
      "[Epoch 32] Training Batch [118/391]: Loss 0.0016541221411898732\n",
      "[Epoch 32] Training Batch [119/391]: Loss 0.005210637114942074\n",
      "[Epoch 32] Training Batch [120/391]: Loss 0.0011770696146413684\n",
      "[Epoch 32] Training Batch [121/391]: Loss 0.0029874355532228947\n",
      "[Epoch 32] Training Batch [122/391]: Loss 0.010491177439689636\n",
      "[Epoch 32] Training Batch [123/391]: Loss 0.0014203067403286695\n",
      "[Epoch 32] Training Batch [124/391]: Loss 0.0013131716987118125\n",
      "[Epoch 32] Training Batch [125/391]: Loss 0.00077700155088678\n",
      "[Epoch 32] Training Batch [126/391]: Loss 0.0009344868594780564\n",
      "[Epoch 32] Training Batch [127/391]: Loss 0.0029603135772049427\n",
      "[Epoch 32] Training Batch [128/391]: Loss 0.0004283208982087672\n",
      "[Epoch 32] Training Batch [129/391]: Loss 0.0015354662900790572\n",
      "[Epoch 32] Training Batch [130/391]: Loss 0.000918691570404917\n",
      "[Epoch 32] Training Batch [131/391]: Loss 0.0008605668554082513\n",
      "[Epoch 32] Training Batch [132/391]: Loss 0.0007836855365894735\n",
      "[Epoch 32] Training Batch [133/391]: Loss 0.0014976889360696077\n",
      "[Epoch 32] Training Batch [134/391]: Loss 0.0006587081006728113\n",
      "[Epoch 32] Training Batch [135/391]: Loss 0.002851797267794609\n",
      "[Epoch 32] Training Batch [136/391]: Loss 0.0011840257793664932\n",
      "[Epoch 32] Training Batch [137/391]: Loss 0.009961048141121864\n",
      "[Epoch 32] Training Batch [138/391]: Loss 0.002392868045717478\n",
      "[Epoch 32] Training Batch [139/391]: Loss 0.016814853996038437\n",
      "[Epoch 32] Training Batch [140/391]: Loss 0.015174711123108864\n",
      "[Epoch 32] Training Batch [141/391]: Loss 0.0004812137922272086\n",
      "[Epoch 32] Training Batch [142/391]: Loss 0.0030651723500341177\n",
      "[Epoch 32] Training Batch [143/391]: Loss 0.00039157576975412667\n",
      "[Epoch 32] Training Batch [144/391]: Loss 0.00048471998888999224\n",
      "[Epoch 32] Training Batch [145/391]: Loss 0.0023070971947163343\n",
      "[Epoch 32] Training Batch [146/391]: Loss 0.001961474772542715\n",
      "[Epoch 32] Training Batch [147/391]: Loss 0.001527799409814179\n",
      "[Epoch 32] Training Batch [148/391]: Loss 0.0008349780109710991\n",
      "[Epoch 32] Training Batch [149/391]: Loss 0.0011856224155053496\n",
      "[Epoch 32] Training Batch [150/391]: Loss 0.020771175622940063\n",
      "[Epoch 32] Training Batch [151/391]: Loss 0.005239945370703936\n",
      "[Epoch 32] Training Batch [152/391]: Loss 0.0047547472640872\n",
      "[Epoch 32] Training Batch [153/391]: Loss 0.001133606885559857\n",
      "[Epoch 32] Training Batch [154/391]: Loss 0.0013223466230556369\n",
      "[Epoch 32] Training Batch [155/391]: Loss 0.002246672287583351\n",
      "[Epoch 32] Training Batch [156/391]: Loss 0.005194682162255049\n",
      "[Epoch 32] Training Batch [157/391]: Loss 0.025274861603975296\n",
      "[Epoch 32] Training Batch [158/391]: Loss 0.009059417992830276\n",
      "[Epoch 32] Training Batch [159/391]: Loss 0.011434471234679222\n",
      "[Epoch 32] Training Batch [160/391]: Loss 0.0014142945874482393\n",
      "[Epoch 32] Training Batch [161/391]: Loss 0.002010227646678686\n",
      "[Epoch 32] Training Batch [162/391]: Loss 0.0009089793893508613\n",
      "[Epoch 32] Training Batch [163/391]: Loss 0.002024482935667038\n",
      "[Epoch 32] Training Batch [164/391]: Loss 0.0011475024512037635\n",
      "[Epoch 32] Training Batch [165/391]: Loss 0.0006161871133372188\n",
      "[Epoch 32] Training Batch [166/391]: Loss 0.0005653751431964338\n",
      "[Epoch 32] Training Batch [167/391]: Loss 0.00327324983663857\n",
      "[Epoch 32] Training Batch [168/391]: Loss 0.0009021709556691349\n",
      "[Epoch 32] Training Batch [169/391]: Loss 0.0008626625058241189\n",
      "[Epoch 32] Training Batch [170/391]: Loss 0.002370609203353524\n",
      "[Epoch 32] Training Batch [171/391]: Loss 0.023215360939502716\n",
      "[Epoch 32] Training Batch [172/391]: Loss 0.0024673528969287872\n",
      "[Epoch 32] Training Batch [173/391]: Loss 0.00449552433565259\n",
      "[Epoch 32] Training Batch [174/391]: Loss 0.002312221098691225\n",
      "[Epoch 32] Training Batch [175/391]: Loss 0.009951084852218628\n",
      "[Epoch 32] Training Batch [176/391]: Loss 0.000817392545286566\n",
      "[Epoch 32] Training Batch [177/391]: Loss 0.00747989397495985\n",
      "[Epoch 32] Training Batch [178/391]: Loss 0.005491588730365038\n",
      "[Epoch 32] Training Batch [179/391]: Loss 0.00047314338735304773\n",
      "[Epoch 32] Training Batch [180/391]: Loss 0.000935482035856694\n",
      "[Epoch 32] Training Batch [181/391]: Loss 0.0067881811410188675\n",
      "[Epoch 32] Training Batch [182/391]: Loss 0.0023963472340255976\n",
      "[Epoch 32] Training Batch [183/391]: Loss 0.01055322214961052\n",
      "[Epoch 32] Training Batch [184/391]: Loss 0.0006188292754814029\n",
      "[Epoch 32] Training Batch [185/391]: Loss 0.0004538749053608626\n",
      "[Epoch 32] Training Batch [186/391]: Loss 0.0023730082903057337\n",
      "[Epoch 32] Training Batch [187/391]: Loss 0.005211438983678818\n",
      "[Epoch 32] Training Batch [188/391]: Loss 0.004459662362933159\n",
      "[Epoch 32] Training Batch [189/391]: Loss 0.0007164977723732591\n",
      "[Epoch 32] Training Batch [190/391]: Loss 0.005074556451290846\n",
      "[Epoch 32] Training Batch [191/391]: Loss 0.0009928183862939477\n",
      "[Epoch 32] Training Batch [192/391]: Loss 0.00164031982421875\n",
      "[Epoch 32] Training Batch [193/391]: Loss 0.0005013216286897659\n",
      "[Epoch 32] Training Batch [194/391]: Loss 0.003376598935574293\n",
      "[Epoch 32] Training Batch [195/391]: Loss 0.00020018058421555907\n",
      "[Epoch 32] Training Batch [196/391]: Loss 0.0006359296385198832\n",
      "[Epoch 32] Training Batch [197/391]: Loss 0.000359651749022305\n",
      "[Epoch 32] Training Batch [198/391]: Loss 0.0013718095142394304\n",
      "[Epoch 32] Training Batch [199/391]: Loss 0.0003251524467486888\n",
      "[Epoch 32] Training Batch [200/391]: Loss 0.0008382688392885029\n",
      "[Epoch 32] Training Batch [201/391]: Loss 0.0018811689224094152\n",
      "[Epoch 32] Training Batch [202/391]: Loss 0.0005331491120159626\n",
      "[Epoch 32] Training Batch [203/391]: Loss 0.0005058113601990044\n",
      "[Epoch 32] Training Batch [204/391]: Loss 0.0037649553269147873\n",
      "[Epoch 32] Training Batch [205/391]: Loss 0.001707683433778584\n",
      "[Epoch 32] Training Batch [206/391]: Loss 0.0009410445927642286\n",
      "[Epoch 32] Training Batch [207/391]: Loss 0.007564850151538849\n",
      "[Epoch 32] Training Batch [208/391]: Loss 0.0010725936153903604\n",
      "[Epoch 32] Training Batch [209/391]: Loss 0.0004368577792774886\n",
      "[Epoch 32] Training Batch [210/391]: Loss 0.0019536656327545643\n",
      "[Epoch 32] Training Batch [211/391]: Loss 0.0003351341583766043\n",
      "[Epoch 32] Training Batch [212/391]: Loss 0.0019167705904692411\n",
      "[Epoch 32] Training Batch [213/391]: Loss 0.0010860244510695338\n",
      "[Epoch 32] Training Batch [214/391]: Loss 0.001019756426103413\n",
      "[Epoch 32] Training Batch [215/391]: Loss 0.0033261121716350317\n",
      "[Epoch 32] Training Batch [216/391]: Loss 0.01089804619550705\n",
      "[Epoch 32] Training Batch [217/391]: Loss 0.0009028316708281636\n",
      "[Epoch 32] Training Batch [218/391]: Loss 0.002837068634107709\n",
      "[Epoch 32] Training Batch [219/391]: Loss 0.0036026593297719955\n",
      "[Epoch 32] Training Batch [220/391]: Loss 0.00034332607174292207\n",
      "[Epoch 32] Training Batch [221/391]: Loss 0.00041376243461854756\n",
      "[Epoch 32] Training Batch [222/391]: Loss 0.001111903809942305\n",
      "[Epoch 32] Training Batch [223/391]: Loss 0.0004752522800117731\n",
      "[Epoch 32] Training Batch [224/391]: Loss 0.0018884255550801754\n",
      "[Epoch 32] Training Batch [225/391]: Loss 0.009850239381194115\n",
      "[Epoch 32] Training Batch [226/391]: Loss 0.00036575470585376024\n",
      "[Epoch 32] Training Batch [227/391]: Loss 0.00013762313756160438\n",
      "[Epoch 32] Training Batch [228/391]: Loss 0.02509530819952488\n",
      "[Epoch 32] Training Batch [229/391]: Loss 0.013836827129125595\n",
      "[Epoch 32] Training Batch [230/391]: Loss 0.008925007656216621\n",
      "[Epoch 32] Training Batch [231/391]: Loss 0.0009562086779624224\n",
      "[Epoch 32] Training Batch [232/391]: Loss 0.0003208873386029154\n",
      "[Epoch 32] Training Batch [233/391]: Loss 0.0004923824453726411\n",
      "[Epoch 32] Training Batch [234/391]: Loss 0.026152128353714943\n",
      "[Epoch 32] Training Batch [235/391]: Loss 0.0007494592573493719\n",
      "[Epoch 32] Training Batch [236/391]: Loss 0.000431967549957335\n",
      "[Epoch 32] Training Batch [237/391]: Loss 0.0018025239696726203\n",
      "[Epoch 32] Training Batch [238/391]: Loss 0.0036219265311956406\n",
      "[Epoch 32] Training Batch [239/391]: Loss 0.0012161944760009646\n",
      "[Epoch 32] Training Batch [240/391]: Loss 0.001526497770100832\n",
      "[Epoch 32] Training Batch [241/391]: Loss 0.0008486022124998271\n",
      "[Epoch 32] Training Batch [242/391]: Loss 0.0006244629621505737\n",
      "[Epoch 32] Training Batch [243/391]: Loss 0.0005132368532940745\n",
      "[Epoch 32] Training Batch [244/391]: Loss 0.0003670925507321954\n",
      "[Epoch 32] Training Batch [245/391]: Loss 0.001224720967002213\n",
      "[Epoch 32] Training Batch [246/391]: Loss 0.003277036128565669\n",
      "[Epoch 32] Training Batch [247/391]: Loss 0.002322897780686617\n",
      "[Epoch 32] Training Batch [248/391]: Loss 0.024394754320383072\n",
      "[Epoch 32] Training Batch [249/391]: Loss 0.004032985307276249\n",
      "[Epoch 32] Training Batch [250/391]: Loss 0.007905101403594017\n",
      "[Epoch 32] Training Batch [251/391]: Loss 0.0017023732652887702\n",
      "[Epoch 32] Training Batch [252/391]: Loss 0.0005755485617555678\n",
      "[Epoch 32] Training Batch [253/391]: Loss 0.0004266528703738004\n",
      "[Epoch 32] Training Batch [254/391]: Loss 0.00570721784606576\n",
      "[Epoch 32] Training Batch [255/391]: Loss 0.0019422952318564057\n",
      "[Epoch 32] Training Batch [256/391]: Loss 0.0024845164734870195\n",
      "[Epoch 32] Training Batch [257/391]: Loss 0.0011416267370805144\n",
      "[Epoch 32] Training Batch [258/391]: Loss 0.0022213764023035765\n",
      "[Epoch 32] Training Batch [259/391]: Loss 0.0013893337454646826\n",
      "[Epoch 32] Training Batch [260/391]: Loss 0.0019449173705652356\n",
      "[Epoch 32] Training Batch [261/391]: Loss 0.007212318480014801\n",
      "[Epoch 32] Training Batch [262/391]: Loss 0.0009735784842632711\n",
      "[Epoch 32] Training Batch [263/391]: Loss 0.0011597846169024706\n",
      "[Epoch 32] Training Batch [264/391]: Loss 0.009968123398721218\n",
      "[Epoch 32] Training Batch [265/391]: Loss 0.0006484407349489629\n",
      "[Epoch 32] Training Batch [266/391]: Loss 0.0008997946861200035\n",
      "[Epoch 32] Training Batch [267/391]: Loss 0.005395149812102318\n",
      "[Epoch 32] Training Batch [268/391]: Loss 0.018499108031392097\n",
      "[Epoch 32] Training Batch [269/391]: Loss 0.004534852225333452\n",
      "[Epoch 32] Training Batch [270/391]: Loss 0.001226644148118794\n",
      "[Epoch 32] Training Batch [271/391]: Loss 0.00036141666350886226\n",
      "[Epoch 32] Training Batch [272/391]: Loss 0.0014603890012949705\n",
      "[Epoch 32] Training Batch [273/391]: Loss 0.002242814749479294\n",
      "[Epoch 32] Training Batch [274/391]: Loss 0.0044352272525429726\n",
      "[Epoch 32] Training Batch [275/391]: Loss 0.0006431021029129624\n",
      "[Epoch 32] Training Batch [276/391]: Loss 0.0006089471862651408\n",
      "[Epoch 32] Training Batch [277/391]: Loss 0.0017821103101596236\n",
      "[Epoch 32] Training Batch [278/391]: Loss 0.005219487007707357\n",
      "[Epoch 32] Training Batch [279/391]: Loss 0.0011907974258065224\n",
      "[Epoch 32] Training Batch [280/391]: Loss 0.0006949812523089349\n",
      "[Epoch 32] Training Batch [281/391]: Loss 0.0009437850094400346\n",
      "[Epoch 32] Training Batch [282/391]: Loss 0.0005830810987390578\n",
      "[Epoch 32] Training Batch [283/391]: Loss 0.0022009732201695442\n",
      "[Epoch 32] Training Batch [284/391]: Loss 0.0013715794775635004\n",
      "[Epoch 32] Training Batch [285/391]: Loss 0.0014348143013194203\n",
      "[Epoch 32] Training Batch [286/391]: Loss 0.00788646750152111\n",
      "[Epoch 32] Training Batch [287/391]: Loss 0.003060603979974985\n",
      "[Epoch 32] Training Batch [288/391]: Loss 0.0005209672381170094\n",
      "[Epoch 32] Training Batch [289/391]: Loss 0.0016572754830121994\n",
      "[Epoch 32] Training Batch [290/391]: Loss 0.0024370213504880667\n",
      "[Epoch 32] Training Batch [291/391]: Loss 0.003493271768093109\n",
      "[Epoch 32] Training Batch [292/391]: Loss 0.0007545146509073675\n",
      "[Epoch 32] Training Batch [293/391]: Loss 0.0031517460010945797\n",
      "[Epoch 32] Training Batch [294/391]: Loss 0.0006767748855054379\n",
      "[Epoch 32] Training Batch [295/391]: Loss 0.0013039320474490523\n",
      "[Epoch 32] Training Batch [296/391]: Loss 0.02779211662709713\n",
      "[Epoch 32] Training Batch [297/391]: Loss 0.00032166449818760157\n",
      "[Epoch 32] Training Batch [298/391]: Loss 0.0041466667316854\n",
      "[Epoch 32] Training Batch [299/391]: Loss 0.0022559165954589844\n",
      "[Epoch 32] Training Batch [300/391]: Loss 0.0005593789392150939\n",
      "[Epoch 32] Training Batch [301/391]: Loss 0.0045669348910450935\n",
      "[Epoch 32] Training Batch [302/391]: Loss 0.001007714425213635\n",
      "[Epoch 32] Training Batch [303/391]: Loss 0.000448919425252825\n",
      "[Epoch 32] Training Batch [304/391]: Loss 0.0004042238579131663\n",
      "[Epoch 32] Training Batch [305/391]: Loss 0.0015085276681929827\n",
      "[Epoch 32] Training Batch [306/391]: Loss 0.0009018057608045638\n",
      "[Epoch 32] Training Batch [307/391]: Loss 0.0005873912014067173\n",
      "[Epoch 32] Training Batch [308/391]: Loss 0.002470816019922495\n",
      "[Epoch 32] Training Batch [309/391]: Loss 0.0012058853171765804\n",
      "[Epoch 32] Training Batch [310/391]: Loss 0.001504907850176096\n",
      "[Epoch 32] Training Batch [311/391]: Loss 0.0012607825919985771\n",
      "[Epoch 32] Training Batch [312/391]: Loss 0.002583978697657585\n",
      "[Epoch 32] Training Batch [313/391]: Loss 0.001912447507493198\n",
      "[Epoch 32] Training Batch [314/391]: Loss 0.0013017412275075912\n",
      "[Epoch 32] Training Batch [315/391]: Loss 0.00014341391215566546\n",
      "[Epoch 32] Training Batch [316/391]: Loss 0.0012351337354630232\n",
      "[Epoch 32] Training Batch [317/391]: Loss 0.00869592372328043\n",
      "[Epoch 32] Training Batch [318/391]: Loss 0.0005518097314052284\n",
      "[Epoch 32] Training Batch [319/391]: Loss 0.0012990140821784735\n",
      "[Epoch 32] Training Batch [320/391]: Loss 0.0014613120583817363\n",
      "[Epoch 32] Training Batch [321/391]: Loss 0.01102922111749649\n",
      "[Epoch 32] Training Batch [322/391]: Loss 0.0015144671779125929\n",
      "[Epoch 32] Training Batch [323/391]: Loss 0.001115631777793169\n",
      "[Epoch 32] Training Batch [324/391]: Loss 0.0011497230734676123\n",
      "[Epoch 32] Training Batch [325/391]: Loss 0.0003392101498320699\n",
      "[Epoch 32] Training Batch [326/391]: Loss 0.004654636140912771\n",
      "[Epoch 32] Training Batch [327/391]: Loss 0.001968684373423457\n",
      "[Epoch 32] Training Batch [328/391]: Loss 0.0029068822041153908\n",
      "[Epoch 32] Training Batch [329/391]: Loss 0.0004094172618351877\n",
      "[Epoch 32] Training Batch [330/391]: Loss 0.0003125739749521017\n",
      "[Epoch 32] Training Batch [331/391]: Loss 0.005083878058940172\n",
      "[Epoch 32] Training Batch [332/391]: Loss 0.0004260667192284018\n",
      "[Epoch 32] Training Batch [333/391]: Loss 0.01051406655460596\n",
      "[Epoch 32] Training Batch [334/391]: Loss 0.0019022747874259949\n",
      "[Epoch 32] Training Batch [335/391]: Loss 0.000315422541461885\n",
      "[Epoch 32] Training Batch [336/391]: Loss 0.00127626012545079\n",
      "[Epoch 32] Training Batch [337/391]: Loss 0.0039239367470145226\n",
      "[Epoch 32] Training Batch [338/391]: Loss 0.0003521995386108756\n",
      "[Epoch 32] Training Batch [339/391]: Loss 0.00029608578188344836\n",
      "[Epoch 32] Training Batch [340/391]: Loss 0.002503997879102826\n",
      "[Epoch 32] Training Batch [341/391]: Loss 0.00019992007582914084\n",
      "[Epoch 32] Training Batch [342/391]: Loss 0.0007946924306452274\n",
      "[Epoch 32] Training Batch [343/391]: Loss 0.0020938480738550425\n",
      "[Epoch 32] Training Batch [344/391]: Loss 0.0008949508774094284\n",
      "[Epoch 32] Training Batch [345/391]: Loss 0.008384020999073982\n",
      "[Epoch 32] Training Batch [346/391]: Loss 0.0022555245086550713\n",
      "[Epoch 32] Training Batch [347/391]: Loss 0.00024351304455194622\n",
      "[Epoch 32] Training Batch [348/391]: Loss 0.00972716137766838\n",
      "[Epoch 32] Training Batch [349/391]: Loss 0.0013965022517368197\n",
      "[Epoch 32] Training Batch [350/391]: Loss 0.0010537055786699057\n",
      "[Epoch 32] Training Batch [351/391]: Loss 0.0005104248994030058\n",
      "[Epoch 32] Training Batch [352/391]: Loss 0.012181062251329422\n",
      "[Epoch 32] Training Batch [353/391]: Loss 0.001977979438379407\n",
      "[Epoch 32] Training Batch [354/391]: Loss 0.0006287258584052324\n",
      "[Epoch 32] Training Batch [355/391]: Loss 0.001843432430177927\n",
      "[Epoch 32] Training Batch [356/391]: Loss 0.0013763036113232374\n",
      "[Epoch 32] Training Batch [357/391]: Loss 0.000909284979570657\n",
      "[Epoch 32] Training Batch [358/391]: Loss 0.0018573288107290864\n",
      "[Epoch 32] Training Batch [359/391]: Loss 0.0009952259715646505\n",
      "[Epoch 32] Training Batch [360/391]: Loss 0.012695777229964733\n",
      "[Epoch 32] Training Batch [361/391]: Loss 0.00014953836216591299\n",
      "[Epoch 32] Training Batch [362/391]: Loss 0.0004286833282094449\n",
      "[Epoch 32] Training Batch [363/391]: Loss 0.0065026674419641495\n",
      "[Epoch 32] Training Batch [364/391]: Loss 0.0004128666769247502\n",
      "[Epoch 32] Training Batch [365/391]: Loss 0.0049474951811134815\n",
      "[Epoch 32] Training Batch [366/391]: Loss 0.0019945933017879725\n",
      "[Epoch 32] Training Batch [367/391]: Loss 0.0003913096443284303\n",
      "[Epoch 32] Training Batch [368/391]: Loss 0.0003668513090815395\n",
      "[Epoch 32] Training Batch [369/391]: Loss 0.0013649336760863662\n",
      "[Epoch 32] Training Batch [370/391]: Loss 0.00059937295736745\n",
      "[Epoch 32] Training Batch [371/391]: Loss 0.008941720239818096\n",
      "[Epoch 32] Training Batch [372/391]: Loss 0.0002563271555118263\n",
      "[Epoch 32] Training Batch [373/391]: Loss 0.03649923950433731\n",
      "[Epoch 32] Training Batch [374/391]: Loss 0.00036244798684492707\n",
      "[Epoch 32] Training Batch [375/391]: Loss 0.0007472046418115497\n",
      "[Epoch 32] Training Batch [376/391]: Loss 0.0014281345065683126\n",
      "[Epoch 32] Training Batch [377/391]: Loss 0.005217214114964008\n",
      "[Epoch 32] Training Batch [378/391]: Loss 0.00043717195512726903\n",
      "[Epoch 32] Training Batch [379/391]: Loss 0.006724555045366287\n",
      "[Epoch 32] Training Batch [380/391]: Loss 0.0005798230413347483\n",
      "[Epoch 32] Training Batch [381/391]: Loss 0.0006891915109008551\n",
      "[Epoch 32] Training Batch [382/391]: Loss 0.0028210454620420933\n",
      "[Epoch 32] Training Batch [383/391]: Loss 0.0005329762934707105\n",
      "[Epoch 32] Training Batch [384/391]: Loss 0.002034717006608844\n",
      "[Epoch 32] Training Batch [385/391]: Loss 0.0029570229817181826\n",
      "[Epoch 32] Training Batch [386/391]: Loss 0.016343016177415848\n",
      "[Epoch 32] Training Batch [387/391]: Loss 0.0016072152648121119\n",
      "[Epoch 32] Training Batch [388/391]: Loss 0.003473365679383278\n",
      "[Epoch 32] Training Batch [389/391]: Loss 0.010390511713922024\n",
      "[Epoch 32] Training Batch [390/391]: Loss 0.0002559433924034238\n",
      "[Epoch 32] Training Batch [391/391]: Loss 0.0034048394300043583\n",
      "Epoch 32 - Train Loss: 0.0036\n",
      "*********  Epoch 33/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33] Training Batch [1/391]: Loss 0.00019444036297500134\n",
      "[Epoch 33] Training Batch [2/391]: Loss 0.0008123490260913968\n",
      "[Epoch 33] Training Batch [3/391]: Loss 0.0006455474067479372\n",
      "[Epoch 33] Training Batch [4/391]: Loss 0.0015317653305828571\n",
      "[Epoch 33] Training Batch [5/391]: Loss 0.00449574226513505\n",
      "[Epoch 33] Training Batch [6/391]: Loss 0.0013557685306295753\n",
      "[Epoch 33] Training Batch [7/391]: Loss 0.001800547237507999\n",
      "[Epoch 33] Training Batch [8/391]: Loss 0.0022548979613929987\n",
      "[Epoch 33] Training Batch [9/391]: Loss 0.0010434890864416957\n",
      "[Epoch 33] Training Batch [10/391]: Loss 0.0012275160988792777\n",
      "[Epoch 33] Training Batch [11/391]: Loss 0.0052453926764428616\n",
      "[Epoch 33] Training Batch [12/391]: Loss 0.0009053064859472215\n",
      "[Epoch 33] Training Batch [13/391]: Loss 0.0066094109788537025\n",
      "[Epoch 33] Training Batch [14/391]: Loss 0.0018908458296209574\n",
      "[Epoch 33] Training Batch [15/391]: Loss 0.00317478459328413\n",
      "[Epoch 33] Training Batch [16/391]: Loss 0.0007110872538760304\n",
      "[Epoch 33] Training Batch [17/391]: Loss 0.0006097324658185244\n",
      "[Epoch 33] Training Batch [18/391]: Loss 0.0010976976482197642\n",
      "[Epoch 33] Training Batch [19/391]: Loss 0.0005454031634144485\n",
      "[Epoch 33] Training Batch [20/391]: Loss 0.0005364857497625053\n",
      "[Epoch 33] Training Batch [21/391]: Loss 0.0006703967228531837\n",
      "[Epoch 33] Training Batch [22/391]: Loss 0.0010703164152801037\n",
      "[Epoch 33] Training Batch [23/391]: Loss 0.000414879439631477\n",
      "[Epoch 33] Training Batch [24/391]: Loss 0.0007675429806113243\n",
      "[Epoch 33] Training Batch [25/391]: Loss 0.00043122287024743855\n",
      "[Epoch 33] Training Batch [26/391]: Loss 0.004339458420872688\n",
      "[Epoch 33] Training Batch [27/391]: Loss 0.0018792978953570127\n",
      "[Epoch 33] Training Batch [28/391]: Loss 0.0004706188919954002\n",
      "[Epoch 33] Training Batch [29/391]: Loss 0.04273217171430588\n",
      "[Epoch 33] Training Batch [30/391]: Loss 0.0002820422814693302\n",
      "[Epoch 33] Training Batch [31/391]: Loss 0.0005127612967044115\n",
      "[Epoch 33] Training Batch [32/391]: Loss 0.0004056260804645717\n",
      "[Epoch 33] Training Batch [33/391]: Loss 0.0004240252892486751\n",
      "[Epoch 33] Training Batch [34/391]: Loss 0.00042354484321549535\n",
      "[Epoch 33] Training Batch [35/391]: Loss 0.0012354622595012188\n",
      "[Epoch 33] Training Batch [36/391]: Loss 0.0007029955158941448\n",
      "[Epoch 33] Training Batch [37/391]: Loss 0.0005396646447479725\n",
      "[Epoch 33] Training Batch [38/391]: Loss 0.000488280231365934\n",
      "[Epoch 33] Training Batch [39/391]: Loss 0.0042212302796542645\n",
      "[Epoch 33] Training Batch [40/391]: Loss 0.00406707264482975\n",
      "[Epoch 33] Training Batch [41/391]: Loss 0.02654535323381424\n",
      "[Epoch 33] Training Batch [42/391]: Loss 0.007765706162899733\n",
      "[Epoch 33] Training Batch [43/391]: Loss 0.0004498889611568302\n",
      "[Epoch 33] Training Batch [44/391]: Loss 0.003664313582703471\n",
      "[Epoch 33] Training Batch [45/391]: Loss 0.000644807587377727\n",
      "[Epoch 33] Training Batch [46/391]: Loss 0.0023800055496394634\n",
      "[Epoch 33] Training Batch [47/391]: Loss 0.00124730309471488\n",
      "[Epoch 33] Training Batch [48/391]: Loss 0.00036492335493676364\n",
      "[Epoch 33] Training Batch [49/391]: Loss 0.0011722407070919871\n",
      "[Epoch 33] Training Batch [50/391]: Loss 0.006912780459970236\n",
      "[Epoch 33] Training Batch [51/391]: Loss 0.0018289596773684025\n",
      "[Epoch 33] Training Batch [52/391]: Loss 0.0008948518079705536\n",
      "[Epoch 33] Training Batch [53/391]: Loss 0.0007444130023941398\n",
      "[Epoch 33] Training Batch [54/391]: Loss 0.0007130648591555655\n",
      "[Epoch 33] Training Batch [55/391]: Loss 0.002020785817876458\n",
      "[Epoch 33] Training Batch [56/391]: Loss 0.0005600425065495074\n",
      "[Epoch 33] Training Batch [57/391]: Loss 0.04240873083472252\n",
      "[Epoch 33] Training Batch [58/391]: Loss 0.005125533789396286\n",
      "[Epoch 33] Training Batch [59/391]: Loss 0.030789971351623535\n",
      "[Epoch 33] Training Batch [60/391]: Loss 0.011407939717173576\n",
      "[Epoch 33] Training Batch [61/391]: Loss 0.0004799755406565964\n",
      "[Epoch 33] Training Batch [62/391]: Loss 0.0028144209645688534\n",
      "[Epoch 33] Training Batch [63/391]: Loss 0.0006788552855141461\n",
      "[Epoch 33] Training Batch [64/391]: Loss 0.0011265617795288563\n",
      "[Epoch 33] Training Batch [65/391]: Loss 0.003928199876099825\n",
      "[Epoch 33] Training Batch [66/391]: Loss 0.0010123394895344973\n",
      "[Epoch 33] Training Batch [67/391]: Loss 0.0010904634837061167\n",
      "[Epoch 33] Training Batch [68/391]: Loss 0.0009851785143837333\n",
      "[Epoch 33] Training Batch [69/391]: Loss 0.001907872036099434\n",
      "[Epoch 33] Training Batch [70/391]: Loss 0.0006105494685471058\n",
      "[Epoch 33] Training Batch [71/391]: Loss 0.001347132958471775\n",
      "[Epoch 33] Training Batch [72/391]: Loss 0.0020339731127023697\n",
      "[Epoch 33] Training Batch [73/391]: Loss 0.013007879257202148\n",
      "[Epoch 33] Training Batch [74/391]: Loss 0.0007418757886625826\n",
      "[Epoch 33] Training Batch [75/391]: Loss 0.0031799825374037027\n",
      "[Epoch 33] Training Batch [76/391]: Loss 0.002500646747648716\n",
      "[Epoch 33] Training Batch [77/391]: Loss 0.011426297016441822\n",
      "[Epoch 33] Training Batch [78/391]: Loss 0.0003035524277947843\n",
      "[Epoch 33] Training Batch [79/391]: Loss 0.0006675151525996625\n",
      "[Epoch 33] Training Batch [80/391]: Loss 0.0009242905071005225\n",
      "[Epoch 33] Training Batch [81/391]: Loss 0.005449448246508837\n",
      "[Epoch 33] Training Batch [82/391]: Loss 0.0014420474180951715\n",
      "[Epoch 33] Training Batch [83/391]: Loss 0.0002985586761496961\n",
      "[Epoch 33] Training Batch [84/391]: Loss 0.0002593206590972841\n",
      "[Epoch 33] Training Batch [85/391]: Loss 0.00023128771863412112\n",
      "[Epoch 33] Training Batch [86/391]: Loss 0.004163400735706091\n",
      "[Epoch 33] Training Batch [87/391]: Loss 0.0006988011882640421\n",
      "[Epoch 33] Training Batch [88/391]: Loss 0.0036324411630630493\n",
      "[Epoch 33] Training Batch [89/391]: Loss 0.0014651438686996698\n",
      "[Epoch 33] Training Batch [90/391]: Loss 0.0012931324308738112\n",
      "[Epoch 33] Training Batch [91/391]: Loss 0.002817462431266904\n",
      "[Epoch 33] Training Batch [92/391]: Loss 0.0034572300501167774\n",
      "[Epoch 33] Training Batch [93/391]: Loss 0.05190804600715637\n",
      "[Epoch 33] Training Batch [94/391]: Loss 0.00045179855078458786\n",
      "[Epoch 33] Training Batch [95/391]: Loss 0.00833690445870161\n",
      "[Epoch 33] Training Batch [96/391]: Loss 0.0011669148225337267\n",
      "[Epoch 33] Training Batch [97/391]: Loss 0.00034379796124994755\n",
      "[Epoch 33] Training Batch [98/391]: Loss 0.0006670008879154921\n",
      "[Epoch 33] Training Batch [99/391]: Loss 0.00048328947741538286\n",
      "[Epoch 33] Training Batch [100/391]: Loss 0.000831158715300262\n",
      "[Epoch 33] Training Batch [101/391]: Loss 0.00038570514880120754\n",
      "[Epoch 33] Training Batch [102/391]: Loss 0.00517986249178648\n",
      "[Epoch 33] Training Batch [103/391]: Loss 0.001348690828308463\n",
      "[Epoch 33] Training Batch [104/391]: Loss 0.0007662647985853255\n",
      "[Epoch 33] Training Batch [105/391]: Loss 0.0003385992022231221\n",
      "[Epoch 33] Training Batch [106/391]: Loss 0.018483415246009827\n",
      "[Epoch 33] Training Batch [107/391]: Loss 0.0021571414545178413\n",
      "[Epoch 33] Training Batch [108/391]: Loss 0.0005483575514517725\n",
      "[Epoch 33] Training Batch [109/391]: Loss 0.006726717110723257\n",
      "[Epoch 33] Training Batch [110/391]: Loss 0.0006960147875361145\n",
      "[Epoch 33] Training Batch [111/391]: Loss 0.00047279399586841464\n",
      "[Epoch 33] Training Batch [112/391]: Loss 0.0022522355429828167\n",
      "[Epoch 33] Training Batch [113/391]: Loss 0.0007345394697040319\n",
      "[Epoch 33] Training Batch [114/391]: Loss 0.0006002752925269306\n",
      "[Epoch 33] Training Batch [115/391]: Loss 0.0015215316088870168\n",
      "[Epoch 33] Training Batch [116/391]: Loss 0.001008761115372181\n",
      "[Epoch 33] Training Batch [117/391]: Loss 0.001586709637194872\n",
      "[Epoch 33] Training Batch [118/391]: Loss 0.004721310921013355\n",
      "[Epoch 33] Training Batch [119/391]: Loss 0.00036352756433188915\n",
      "[Epoch 33] Training Batch [120/391]: Loss 0.0014026393182575703\n",
      "[Epoch 33] Training Batch [121/391]: Loss 0.0004266267060302198\n",
      "[Epoch 33] Training Batch [122/391]: Loss 0.0007671063067391515\n",
      "[Epoch 33] Training Batch [123/391]: Loss 0.0010794850531965494\n",
      "[Epoch 33] Training Batch [124/391]: Loss 0.00314401276409626\n",
      "[Epoch 33] Training Batch [125/391]: Loss 0.0011662228498607874\n",
      "[Epoch 33] Training Batch [126/391]: Loss 0.003515000222250819\n",
      "[Epoch 33] Training Batch [127/391]: Loss 0.00273495982401073\n",
      "[Epoch 33] Training Batch [128/391]: Loss 0.005509043578058481\n",
      "[Epoch 33] Training Batch [129/391]: Loss 0.0025495043955743313\n",
      "[Epoch 33] Training Batch [130/391]: Loss 0.007191438693553209\n",
      "[Epoch 33] Training Batch [131/391]: Loss 0.0010482684010639787\n",
      "[Epoch 33] Training Batch [132/391]: Loss 0.0008779283380135894\n",
      "[Epoch 33] Training Batch [133/391]: Loss 0.0055968607775866985\n",
      "[Epoch 33] Training Batch [134/391]: Loss 0.0010620994726195931\n",
      "[Epoch 33] Training Batch [135/391]: Loss 0.0005707688396796584\n",
      "[Epoch 33] Training Batch [136/391]: Loss 0.001107129268348217\n",
      "[Epoch 33] Training Batch [137/391]: Loss 0.0002515437954571098\n",
      "[Epoch 33] Training Batch [138/391]: Loss 0.0005421516252681613\n",
      "[Epoch 33] Training Batch [139/391]: Loss 0.0008608722127974033\n",
      "[Epoch 33] Training Batch [140/391]: Loss 0.003111368976533413\n",
      "[Epoch 33] Training Batch [141/391]: Loss 0.0006281787646003067\n",
      "[Epoch 33] Training Batch [142/391]: Loss 0.0009684636606834829\n",
      "[Epoch 33] Training Batch [143/391]: Loss 0.011679857037961483\n",
      "[Epoch 33] Training Batch [144/391]: Loss 0.0006111440015956759\n",
      "[Epoch 33] Training Batch [145/391]: Loss 0.0003625430690590292\n",
      "[Epoch 33] Training Batch [146/391]: Loss 0.00042456999653950334\n",
      "[Epoch 33] Training Batch [147/391]: Loss 0.0007356397691182792\n",
      "[Epoch 33] Training Batch [148/391]: Loss 0.000620627892203629\n",
      "[Epoch 33] Training Batch [149/391]: Loss 0.005379851907491684\n",
      "[Epoch 33] Training Batch [150/391]: Loss 0.005789201706647873\n",
      "[Epoch 33] Training Batch [151/391]: Loss 0.0007773268152959645\n",
      "[Epoch 33] Training Batch [152/391]: Loss 0.008451086468994617\n",
      "[Epoch 33] Training Batch [153/391]: Loss 0.0009399054106324911\n",
      "[Epoch 33] Training Batch [154/391]: Loss 0.0012162041384726763\n",
      "[Epoch 33] Training Batch [155/391]: Loss 0.00034393215901218355\n",
      "[Epoch 33] Training Batch [156/391]: Loss 0.001889629871584475\n",
      "[Epoch 33] Training Batch [157/391]: Loss 0.008343935012817383\n",
      "[Epoch 33] Training Batch [158/391]: Loss 0.00036880586412735283\n",
      "[Epoch 33] Training Batch [159/391]: Loss 0.0014572972431778908\n",
      "[Epoch 33] Training Batch [160/391]: Loss 0.0011244978522881866\n",
      "[Epoch 33] Training Batch [161/391]: Loss 0.000771242892369628\n",
      "[Epoch 33] Training Batch [162/391]: Loss 0.0033713802695274353\n",
      "[Epoch 33] Training Batch [163/391]: Loss 0.005613852292299271\n",
      "[Epoch 33] Training Batch [164/391]: Loss 0.003777590114623308\n",
      "[Epoch 33] Training Batch [165/391]: Loss 0.004481287207454443\n",
      "[Epoch 33] Training Batch [166/391]: Loss 0.007151453290134668\n",
      "[Epoch 33] Training Batch [167/391]: Loss 0.0011952375061810017\n",
      "[Epoch 33] Training Batch [168/391]: Loss 0.0006331940530799329\n",
      "[Epoch 33] Training Batch [169/391]: Loss 0.0007444252842105925\n",
      "[Epoch 33] Training Batch [170/391]: Loss 0.0018948226934298873\n",
      "[Epoch 33] Training Batch [171/391]: Loss 0.0004609049065038562\n",
      "[Epoch 33] Training Batch [172/391]: Loss 0.0018042679876089096\n",
      "[Epoch 33] Training Batch [173/391]: Loss 0.0010679038241505623\n",
      "[Epoch 33] Training Batch [174/391]: Loss 0.00020685257914010435\n",
      "[Epoch 33] Training Batch [175/391]: Loss 0.0005375859327614307\n",
      "[Epoch 33] Training Batch [176/391]: Loss 0.0045881373807787895\n",
      "[Epoch 33] Training Batch [177/391]: Loss 0.00018115813145413995\n",
      "[Epoch 33] Training Batch [178/391]: Loss 0.0013086646795272827\n",
      "[Epoch 33] Training Batch [179/391]: Loss 0.001274996087886393\n",
      "[Epoch 33] Training Batch [180/391]: Loss 0.0003365679003763944\n",
      "[Epoch 33] Training Batch [181/391]: Loss 0.0006758790696039796\n",
      "[Epoch 33] Training Batch [182/391]: Loss 0.004847059026360512\n",
      "[Epoch 33] Training Batch [183/391]: Loss 0.00027007004246115685\n",
      "[Epoch 33] Training Batch [184/391]: Loss 0.0014148743357509375\n",
      "[Epoch 33] Training Batch [185/391]: Loss 0.002218575682491064\n",
      "[Epoch 33] Training Batch [186/391]: Loss 0.0006589507684111595\n",
      "[Epoch 33] Training Batch [187/391]: Loss 0.00029287138022482395\n",
      "[Epoch 33] Training Batch [188/391]: Loss 0.0005689843674190342\n",
      "[Epoch 33] Training Batch [189/391]: Loss 0.0012387934839352965\n",
      "[Epoch 33] Training Batch [190/391]: Loss 0.0005688685341738164\n",
      "[Epoch 33] Training Batch [191/391]: Loss 0.0003896771522704512\n",
      "[Epoch 33] Training Batch [192/391]: Loss 0.0005015904316678643\n",
      "[Epoch 33] Training Batch [193/391]: Loss 0.00037752260686829686\n",
      "[Epoch 33] Training Batch [194/391]: Loss 0.000452083331765607\n",
      "[Epoch 33] Training Batch [195/391]: Loss 0.008875115774571896\n",
      "[Epoch 33] Training Batch [196/391]: Loss 0.0009262450039386749\n",
      "[Epoch 33] Training Batch [197/391]: Loss 0.001184406690299511\n",
      "[Epoch 33] Training Batch [198/391]: Loss 0.0003994900907855481\n",
      "[Epoch 33] Training Batch [199/391]: Loss 0.0011927593732252717\n",
      "[Epoch 33] Training Batch [200/391]: Loss 0.0004201621632091701\n",
      "[Epoch 33] Training Batch [201/391]: Loss 0.0002788821002468467\n",
      "[Epoch 33] Training Batch [202/391]: Loss 0.0003445176116656512\n",
      "[Epoch 33] Training Batch [203/391]: Loss 0.023299729451537132\n",
      "[Epoch 33] Training Batch [204/391]: Loss 0.0003505500790197402\n",
      "[Epoch 33] Training Batch [205/391]: Loss 0.005570974200963974\n",
      "[Epoch 33] Training Batch [206/391]: Loss 0.00015230393910314888\n",
      "[Epoch 33] Training Batch [207/391]: Loss 0.0004637794045265764\n",
      "[Epoch 33] Training Batch [208/391]: Loss 0.0015708195278421044\n",
      "[Epoch 33] Training Batch [209/391]: Loss 0.0012166438391432166\n",
      "[Epoch 33] Training Batch [210/391]: Loss 0.0005147600895725191\n",
      "[Epoch 33] Training Batch [211/391]: Loss 0.0012714674230664968\n",
      "[Epoch 33] Training Batch [212/391]: Loss 0.00035507400752976537\n",
      "[Epoch 33] Training Batch [213/391]: Loss 0.0009626094251871109\n",
      "[Epoch 33] Training Batch [214/391]: Loss 0.0016156438505277038\n",
      "[Epoch 33] Training Batch [215/391]: Loss 0.0006163885700516403\n",
      "[Epoch 33] Training Batch [216/391]: Loss 0.0004230771155562252\n",
      "[Epoch 33] Training Batch [217/391]: Loss 0.0017522095004096627\n",
      "[Epoch 33] Training Batch [218/391]: Loss 0.0003790926712099463\n",
      "[Epoch 33] Training Batch [219/391]: Loss 0.0009655379690229893\n",
      "[Epoch 33] Training Batch [220/391]: Loss 0.00048759308992885053\n",
      "[Epoch 33] Training Batch [221/391]: Loss 0.0007856368320062757\n",
      "[Epoch 33] Training Batch [222/391]: Loss 0.00019095669267699122\n",
      "[Epoch 33] Training Batch [223/391]: Loss 0.0013530865544453263\n",
      "[Epoch 33] Training Batch [224/391]: Loss 0.000454987894045189\n",
      "[Epoch 33] Training Batch [225/391]: Loss 0.006729864049702883\n",
      "[Epoch 33] Training Batch [226/391]: Loss 0.0005845797713845968\n",
      "[Epoch 33] Training Batch [227/391]: Loss 0.0004781857132911682\n",
      "[Epoch 33] Training Batch [228/391]: Loss 0.0010589765151962638\n",
      "[Epoch 33] Training Batch [229/391]: Loss 0.0011512176133692265\n",
      "[Epoch 33] Training Batch [230/391]: Loss 0.0015005854656919837\n",
      "[Epoch 33] Training Batch [231/391]: Loss 0.001810594229027629\n",
      "[Epoch 33] Training Batch [232/391]: Loss 0.0002691060653887689\n",
      "[Epoch 33] Training Batch [233/391]: Loss 0.0064729731529951096\n",
      "[Epoch 33] Training Batch [234/391]: Loss 0.0009079884621314704\n",
      "[Epoch 33] Training Batch [235/391]: Loss 0.0007302192389033735\n",
      "[Epoch 33] Training Batch [236/391]: Loss 0.003155873389914632\n",
      "[Epoch 33] Training Batch [237/391]: Loss 0.000897642457857728\n",
      "[Epoch 33] Training Batch [238/391]: Loss 0.00021655419550370425\n",
      "[Epoch 33] Training Batch [239/391]: Loss 0.0004379862220957875\n",
      "[Epoch 33] Training Batch [240/391]: Loss 0.002573821460828185\n",
      "[Epoch 33] Training Batch [241/391]: Loss 0.0016985620604828\n",
      "[Epoch 33] Training Batch [242/391]: Loss 0.00021974294213578105\n",
      "[Epoch 33] Training Batch [243/391]: Loss 0.00012876407708972692\n",
      "[Epoch 33] Training Batch [244/391]: Loss 0.0004429798573255539\n",
      "[Epoch 33] Training Batch [245/391]: Loss 0.0010212573688477278\n",
      "[Epoch 33] Training Batch [246/391]: Loss 0.005469080526381731\n",
      "[Epoch 33] Training Batch [247/391]: Loss 0.0007756364648230374\n",
      "[Epoch 33] Training Batch [248/391]: Loss 0.0013583466643467546\n",
      "[Epoch 33] Training Batch [249/391]: Loss 9.042014426086098e-05\n",
      "[Epoch 33] Training Batch [250/391]: Loss 0.00012307144061196595\n",
      "[Epoch 33] Training Batch [251/391]: Loss 0.00014911845210008323\n",
      "[Epoch 33] Training Batch [252/391]: Loss 0.000606275862082839\n",
      "[Epoch 33] Training Batch [253/391]: Loss 0.0006757351802662015\n",
      "[Epoch 33] Training Batch [254/391]: Loss 0.00016593330656178296\n",
      "[Epoch 33] Training Batch [255/391]: Loss 0.00017453081090934575\n",
      "[Epoch 33] Training Batch [256/391]: Loss 0.001088915509171784\n",
      "[Epoch 33] Training Batch [257/391]: Loss 0.0035230370704084635\n",
      "[Epoch 33] Training Batch [258/391]: Loss 0.0003515042189974338\n",
      "[Epoch 33] Training Batch [259/391]: Loss 9.475539991399273e-05\n",
      "[Epoch 33] Training Batch [260/391]: Loss 0.00038409215630963445\n",
      "[Epoch 33] Training Batch [261/391]: Loss 0.002722351113334298\n",
      "[Epoch 33] Training Batch [262/391]: Loss 0.0003514048585202545\n",
      "[Epoch 33] Training Batch [263/391]: Loss 0.00039257598109543324\n",
      "[Epoch 33] Training Batch [264/391]: Loss 0.0002788333222270012\n",
      "[Epoch 33] Training Batch [265/391]: Loss 0.003570261877030134\n",
      "[Epoch 33] Training Batch [266/391]: Loss 0.0007682516588829458\n",
      "[Epoch 33] Training Batch [267/391]: Loss 0.0011323014041408896\n",
      "[Epoch 33] Training Batch [268/391]: Loss 3.958743764087558e-05\n",
      "[Epoch 33] Training Batch [269/391]: Loss 0.00014132887008599937\n",
      "[Epoch 33] Training Batch [270/391]: Loss 0.0007945374818518758\n",
      "[Epoch 33] Training Batch [271/391]: Loss 0.00027886941097676754\n",
      "[Epoch 33] Training Batch [272/391]: Loss 0.0004178239614702761\n",
      "[Epoch 33] Training Batch [273/391]: Loss 0.0003857344563584775\n",
      "[Epoch 33] Training Batch [274/391]: Loss 0.00029650909709744155\n",
      "[Epoch 33] Training Batch [275/391]: Loss 0.0007231603958643973\n",
      "[Epoch 33] Training Batch [276/391]: Loss 0.0007638644892722368\n",
      "[Epoch 33] Training Batch [277/391]: Loss 0.0006573827704414725\n",
      "[Epoch 33] Training Batch [278/391]: Loss 0.0009353476925753057\n",
      "[Epoch 33] Training Batch [279/391]: Loss 0.00015444612654391676\n",
      "[Epoch 33] Training Batch [280/391]: Loss 0.00017023635155055672\n",
      "[Epoch 33] Training Batch [281/391]: Loss 0.0002740754571277648\n",
      "[Epoch 33] Training Batch [282/391]: Loss 0.0005562003934755921\n",
      "[Epoch 33] Training Batch [283/391]: Loss 0.0013297379482537508\n",
      "[Epoch 33] Training Batch [284/391]: Loss 0.0008036380168050528\n",
      "[Epoch 33] Training Batch [285/391]: Loss 0.0006172977155074477\n",
      "[Epoch 33] Training Batch [286/391]: Loss 0.00017283842316828668\n",
      "[Epoch 33] Training Batch [287/391]: Loss 0.00028550505521707237\n",
      "[Epoch 33] Training Batch [288/391]: Loss 0.00014518655370920897\n",
      "[Epoch 33] Training Batch [289/391]: Loss 0.0005289767286740243\n",
      "[Epoch 33] Training Batch [290/391]: Loss 0.0043152072466909885\n",
      "[Epoch 33] Training Batch [291/391]: Loss 0.0008243783377110958\n",
      "[Epoch 33] Training Batch [292/391]: Loss 0.00037231401074677706\n",
      "[Epoch 33] Training Batch [293/391]: Loss 0.0003276699280831963\n",
      "[Epoch 33] Training Batch [294/391]: Loss 7.302734593395144e-05\n",
      "[Epoch 33] Training Batch [295/391]: Loss 0.0016500886995345354\n",
      "[Epoch 33] Training Batch [296/391]: Loss 0.0006473253015428782\n",
      "[Epoch 33] Training Batch [297/391]: Loss 0.0007377041620202363\n",
      "[Epoch 33] Training Batch [298/391]: Loss 0.0004602430562954396\n",
      "[Epoch 33] Training Batch [299/391]: Loss 0.00015143015480134636\n",
      "[Epoch 33] Training Batch [300/391]: Loss 0.0003128330281469971\n",
      "[Epoch 33] Training Batch [301/391]: Loss 0.00019129460270050913\n",
      "[Epoch 33] Training Batch [302/391]: Loss 0.00042726119863800704\n",
      "[Epoch 33] Training Batch [303/391]: Loss 0.00018119219748768955\n",
      "[Epoch 33] Training Batch [304/391]: Loss 0.0006378680700436234\n",
      "[Epoch 33] Training Batch [305/391]: Loss 9.349269384983927e-05\n",
      "[Epoch 33] Training Batch [306/391]: Loss 5.5999662436079234e-05\n",
      "[Epoch 33] Training Batch [307/391]: Loss 0.000314613978844136\n",
      "[Epoch 33] Training Batch [308/391]: Loss 0.0006119479658082128\n",
      "[Epoch 33] Training Batch [309/391]: Loss 0.0005307671381160617\n",
      "[Epoch 33] Training Batch [310/391]: Loss 0.0035978599917143583\n",
      "[Epoch 33] Training Batch [311/391]: Loss 0.004016967490315437\n",
      "[Epoch 33] Training Batch [312/391]: Loss 0.00012809928739443421\n",
      "[Epoch 33] Training Batch [313/391]: Loss 0.001632828381843865\n",
      "[Epoch 33] Training Batch [314/391]: Loss 9.555066208122298e-05\n",
      "[Epoch 33] Training Batch [315/391]: Loss 0.0002617414284031838\n",
      "[Epoch 33] Training Batch [316/391]: Loss 0.00026722793700173497\n",
      "[Epoch 33] Training Batch [317/391]: Loss 0.0003275348281022161\n",
      "[Epoch 33] Training Batch [318/391]: Loss 0.00013357556599657983\n",
      "[Epoch 33] Training Batch [319/391]: Loss 0.0009506257483735681\n",
      "[Epoch 33] Training Batch [320/391]: Loss 0.0007921288488432765\n",
      "[Epoch 33] Training Batch [321/391]: Loss 0.00045749288983643055\n",
      "[Epoch 33] Training Batch [322/391]: Loss 0.0008270781254395843\n",
      "[Epoch 33] Training Batch [323/391]: Loss 0.00025876934523694217\n",
      "[Epoch 33] Training Batch [324/391]: Loss 0.006293933838605881\n",
      "[Epoch 33] Training Batch [325/391]: Loss 0.0008795812027528882\n",
      "[Epoch 33] Training Batch [326/391]: Loss 9.124103235080838e-05\n",
      "[Epoch 33] Training Batch [327/391]: Loss 6.540217145811766e-05\n",
      "[Epoch 33] Training Batch [328/391]: Loss 0.00014593465311918408\n",
      "[Epoch 33] Training Batch [329/391]: Loss 0.0001542989630252123\n",
      "[Epoch 33] Training Batch [330/391]: Loss 0.0003693858743645251\n",
      "[Epoch 33] Training Batch [331/391]: Loss 9.062737808562815e-05\n",
      "[Epoch 33] Training Batch [332/391]: Loss 0.00019514546147547662\n",
      "[Epoch 33] Training Batch [333/391]: Loss 0.0001306730991927907\n",
      "[Epoch 33] Training Batch [334/391]: Loss 0.0012461248552426696\n",
      "[Epoch 33] Training Batch [335/391]: Loss 0.0008309569675475359\n",
      "[Epoch 33] Training Batch [336/391]: Loss 0.0002531352511141449\n",
      "[Epoch 33] Training Batch [337/391]: Loss 0.0006756075308658183\n",
      "[Epoch 33] Training Batch [338/391]: Loss 0.0008102529682219028\n",
      "[Epoch 33] Training Batch [339/391]: Loss 0.003012954955920577\n",
      "[Epoch 33] Training Batch [340/391]: Loss 0.00014621656737290323\n",
      "[Epoch 33] Training Batch [341/391]: Loss 0.00019825728668365628\n",
      "[Epoch 33] Training Batch [342/391]: Loss 0.00014215496776159853\n",
      "[Epoch 33] Training Batch [343/391]: Loss 0.00041667066398076713\n",
      "[Epoch 33] Training Batch [344/391]: Loss 0.00013212428893893957\n",
      "[Epoch 33] Training Batch [345/391]: Loss 0.002020928543061018\n",
      "[Epoch 33] Training Batch [346/391]: Loss 0.0023856095504015684\n",
      "[Epoch 33] Training Batch [347/391]: Loss 0.00021393803763203323\n",
      "[Epoch 33] Training Batch [348/391]: Loss 0.00012381080887280405\n",
      "[Epoch 33] Training Batch [349/391]: Loss 0.00046960782492533326\n",
      "[Epoch 33] Training Batch [350/391]: Loss 0.0010271969949826598\n",
      "[Epoch 33] Training Batch [351/391]: Loss 7.298790296772495e-05\n",
      "[Epoch 33] Training Batch [352/391]: Loss 0.00014185623149387538\n",
      "[Epoch 33] Training Batch [353/391]: Loss 0.0004521572554949671\n",
      "[Epoch 33] Training Batch [354/391]: Loss 0.002271704375743866\n",
      "[Epoch 33] Training Batch [355/391]: Loss 0.00033247709507122636\n",
      "[Epoch 33] Training Batch [356/391]: Loss 0.00012536333815660328\n",
      "[Epoch 33] Training Batch [357/391]: Loss 0.0017070858739316463\n",
      "[Epoch 33] Training Batch [358/391]: Loss 0.00046733752242289484\n",
      "[Epoch 33] Training Batch [359/391]: Loss 0.0009202734800055623\n",
      "[Epoch 33] Training Batch [360/391]: Loss 0.0001017489266814664\n",
      "[Epoch 33] Training Batch [361/391]: Loss 9.142499038716778e-05\n",
      "[Epoch 33] Training Batch [362/391]: Loss 0.00013732234947383404\n",
      "[Epoch 33] Training Batch [363/391]: Loss 0.00018037080008070916\n",
      "[Epoch 33] Training Batch [364/391]: Loss 8.844456897350028e-05\n",
      "[Epoch 33] Training Batch [365/391]: Loss 0.0003379581030458212\n",
      "[Epoch 33] Training Batch [366/391]: Loss 0.009937619790434837\n",
      "[Epoch 33] Training Batch [367/391]: Loss 8.073675417108461e-05\n",
      "[Epoch 33] Training Batch [368/391]: Loss 0.00030811040778644383\n",
      "[Epoch 33] Training Batch [369/391]: Loss 0.0003128034877590835\n",
      "[Epoch 33] Training Batch [370/391]: Loss 0.0002570981450844556\n",
      "[Epoch 33] Training Batch [371/391]: Loss 0.00020454759942367673\n",
      "[Epoch 33] Training Batch [372/391]: Loss 0.00018148345407098532\n",
      "[Epoch 33] Training Batch [373/391]: Loss 0.0009429564233869314\n",
      "[Epoch 33] Training Batch [374/391]: Loss 0.0008554948726668954\n",
      "[Epoch 33] Training Batch [375/391]: Loss 0.0002946194726973772\n",
      "[Epoch 33] Training Batch [376/391]: Loss 0.004451347980648279\n",
      "[Epoch 33] Training Batch [377/391]: Loss 0.0007679212721996009\n",
      "[Epoch 33] Training Batch [378/391]: Loss 9.08378278836608e-05\n",
      "[Epoch 33] Training Batch [379/391]: Loss 0.00021718493371736258\n",
      "[Epoch 33] Training Batch [380/391]: Loss 0.00013970150030218065\n",
      "[Epoch 33] Training Batch [381/391]: Loss 0.005131579004228115\n",
      "[Epoch 33] Training Batch [382/391]: Loss 0.001963169313967228\n",
      "[Epoch 33] Training Batch [383/391]: Loss 0.00026631346554495394\n",
      "[Epoch 33] Training Batch [384/391]: Loss 0.00021303942776285112\n",
      "[Epoch 33] Training Batch [385/391]: Loss 0.000824177055619657\n",
      "[Epoch 33] Training Batch [386/391]: Loss 0.0009738493245095015\n",
      "[Epoch 33] Training Batch [387/391]: Loss 0.0003652429149951786\n",
      "[Epoch 33] Training Batch [388/391]: Loss 0.000890325871296227\n",
      "[Epoch 33] Training Batch [389/391]: Loss 0.0002883743145503104\n",
      "[Epoch 33] Training Batch [390/391]: Loss 0.0010704707819968462\n",
      "[Epoch 33] Training Batch [391/391]: Loss 0.00020334447617642581\n",
      "Epoch 33 - Train Loss: 0.0021\n",
      "*********  Epoch 34/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34] Training Batch [1/391]: Loss 0.0007897350587882102\n",
      "[Epoch 34] Training Batch [2/391]: Loss 0.0007272869697771966\n",
      "[Epoch 34] Training Batch [3/391]: Loss 0.00024381792172789574\n",
      "[Epoch 34] Training Batch [4/391]: Loss 0.00010954982280964032\n",
      "[Epoch 34] Training Batch [5/391]: Loss 0.00011113935761386529\n",
      "[Epoch 34] Training Batch [6/391]: Loss 0.0001774326228769496\n",
      "[Epoch 34] Training Batch [7/391]: Loss 0.0006035808473825455\n",
      "[Epoch 34] Training Batch [8/391]: Loss 0.00026112041086889803\n",
      "[Epoch 34] Training Batch [9/391]: Loss 0.000176833345904015\n",
      "[Epoch 34] Training Batch [10/391]: Loss 0.0007307013147510588\n",
      "[Epoch 34] Training Batch [11/391]: Loss 0.0002592642849776894\n",
      "[Epoch 34] Training Batch [12/391]: Loss 0.0004114951880183071\n",
      "[Epoch 34] Training Batch [13/391]: Loss 0.0006353852222673595\n",
      "[Epoch 34] Training Batch [14/391]: Loss 0.00010480477794772014\n",
      "[Epoch 34] Training Batch [15/391]: Loss 0.00011598016862990335\n",
      "[Epoch 34] Training Batch [16/391]: Loss 0.00038764000055380166\n",
      "[Epoch 34] Training Batch [17/391]: Loss 0.00024569715606048703\n",
      "[Epoch 34] Training Batch [18/391]: Loss 0.0002064372383756563\n",
      "[Epoch 34] Training Batch [19/391]: Loss 0.0003668051795102656\n",
      "[Epoch 34] Training Batch [20/391]: Loss 0.0003878175048157573\n",
      "[Epoch 34] Training Batch [21/391]: Loss 0.00018384333816356957\n",
      "[Epoch 34] Training Batch [22/391]: Loss 0.0007744253380224109\n",
      "[Epoch 34] Training Batch [23/391]: Loss 0.00016206904547289014\n",
      "[Epoch 34] Training Batch [24/391]: Loss 0.00016702934226486832\n",
      "[Epoch 34] Training Batch [25/391]: Loss 0.00013145456614438444\n",
      "[Epoch 34] Training Batch [26/391]: Loss 0.0002390792651567608\n",
      "[Epoch 34] Training Batch [27/391]: Loss 0.0002536772226449102\n",
      "[Epoch 34] Training Batch [28/391]: Loss 0.00013919688353780657\n",
      "[Epoch 34] Training Batch [29/391]: Loss 6.617853796342388e-05\n",
      "[Epoch 34] Training Batch [30/391]: Loss 0.0004604072601068765\n",
      "[Epoch 34] Training Batch [31/391]: Loss 0.0002086055901600048\n",
      "[Epoch 34] Training Batch [32/391]: Loss 0.00014975610247347504\n",
      "[Epoch 34] Training Batch [33/391]: Loss 0.0008847046992741525\n",
      "[Epoch 34] Training Batch [34/391]: Loss 0.00018578663002699614\n",
      "[Epoch 34] Training Batch [35/391]: Loss 0.00020067687728442252\n",
      "[Epoch 34] Training Batch [36/391]: Loss 0.00010459929762873799\n",
      "[Epoch 34] Training Batch [37/391]: Loss 0.0006429032073356211\n",
      "[Epoch 34] Training Batch [38/391]: Loss 7.356053538387641e-05\n",
      "[Epoch 34] Training Batch [39/391]: Loss 9.37476652325131e-05\n",
      "[Epoch 34] Training Batch [40/391]: Loss 0.0004845743824262172\n",
      "[Epoch 34] Training Batch [41/391]: Loss 0.0006350863841362298\n",
      "[Epoch 34] Training Batch [42/391]: Loss 0.000142498713103123\n",
      "[Epoch 34] Training Batch [43/391]: Loss 8.852840983308852e-05\n",
      "[Epoch 34] Training Batch [44/391]: Loss 8.799271017778665e-05\n",
      "[Epoch 34] Training Batch [45/391]: Loss 0.00014027563156560063\n",
      "[Epoch 34] Training Batch [46/391]: Loss 0.00019816412532236427\n",
      "[Epoch 34] Training Batch [47/391]: Loss 0.00031664653215557337\n",
      "[Epoch 34] Training Batch [48/391]: Loss 0.00015510330558754504\n",
      "[Epoch 34] Training Batch [49/391]: Loss 0.00013786909403279424\n",
      "[Epoch 34] Training Batch [50/391]: Loss 0.0001880488998722285\n",
      "[Epoch 34] Training Batch [51/391]: Loss 0.0001869078987510875\n",
      "[Epoch 34] Training Batch [52/391]: Loss 0.00018270616419613361\n",
      "[Epoch 34] Training Batch [53/391]: Loss 9.56801013671793e-05\n",
      "[Epoch 34] Training Batch [54/391]: Loss 0.00017324500367976725\n",
      "[Epoch 34] Training Batch [55/391]: Loss 8.898612577468157e-05\n",
      "[Epoch 34] Training Batch [56/391]: Loss 0.00020808292902074754\n",
      "[Epoch 34] Training Batch [57/391]: Loss 0.0006140824407339096\n",
      "[Epoch 34] Training Batch [58/391]: Loss 0.00015248989802785218\n",
      "[Epoch 34] Training Batch [59/391]: Loss 0.00016623707779217511\n",
      "[Epoch 34] Training Batch [60/391]: Loss 0.00030422667623497546\n",
      "[Epoch 34] Training Batch [61/391]: Loss 0.0003559600736480206\n",
      "[Epoch 34] Training Batch [62/391]: Loss 8.232848631450906e-05\n",
      "[Epoch 34] Training Batch [63/391]: Loss 0.00028054541326127946\n",
      "[Epoch 34] Training Batch [64/391]: Loss 0.00031354965176433325\n",
      "[Epoch 34] Training Batch [65/391]: Loss 6.274167390074581e-05\n",
      "[Epoch 34] Training Batch [66/391]: Loss 9.768985910341144e-05\n",
      "[Epoch 34] Training Batch [67/391]: Loss 6.821612623753026e-05\n",
      "[Epoch 34] Training Batch [68/391]: Loss 0.0002766741090454161\n",
      "[Epoch 34] Training Batch [69/391]: Loss 0.00013034779112786055\n",
      "[Epoch 34] Training Batch [70/391]: Loss 7.936757174320519e-05\n",
      "[Epoch 34] Training Batch [71/391]: Loss 6.629237759625539e-05\n",
      "[Epoch 34] Training Batch [72/391]: Loss 0.0001345191994914785\n",
      "[Epoch 34] Training Batch [73/391]: Loss 0.000106760737253353\n",
      "[Epoch 34] Training Batch [74/391]: Loss 0.00024661151110194623\n",
      "[Epoch 34] Training Batch [75/391]: Loss 0.00013393584231380373\n",
      "[Epoch 34] Training Batch [76/391]: Loss 0.00026321905897930264\n",
      "[Epoch 34] Training Batch [77/391]: Loss 0.00017833115998655558\n",
      "[Epoch 34] Training Batch [78/391]: Loss 0.0008063511340878904\n",
      "[Epoch 34] Training Batch [79/391]: Loss 0.00021211143757682294\n",
      "[Epoch 34] Training Batch [80/391]: Loss 0.00010044792725238949\n",
      "[Epoch 34] Training Batch [81/391]: Loss 0.00014214093971531838\n",
      "[Epoch 34] Training Batch [82/391]: Loss 0.00039288619882427156\n",
      "[Epoch 34] Training Batch [83/391]: Loss 0.00030086562037467957\n",
      "[Epoch 34] Training Batch [84/391]: Loss 7.968926365720108e-05\n",
      "[Epoch 34] Training Batch [85/391]: Loss 0.0002601051819510758\n",
      "[Epoch 34] Training Batch [86/391]: Loss 5.2208604756742716e-05\n",
      "[Epoch 34] Training Batch [87/391]: Loss 0.0004571559256874025\n",
      "[Epoch 34] Training Batch [88/391]: Loss 0.0006646292167715728\n",
      "[Epoch 34] Training Batch [89/391]: Loss 0.0002548099437262863\n",
      "[Epoch 34] Training Batch [90/391]: Loss 0.0007960857474245131\n",
      "[Epoch 34] Training Batch [91/391]: Loss 9.543790656607598e-05\n",
      "[Epoch 34] Training Batch [92/391]: Loss 0.0002387615095358342\n",
      "[Epoch 34] Training Batch [93/391]: Loss 0.00013255845988169312\n",
      "[Epoch 34] Training Batch [94/391]: Loss 0.00019800371956080198\n",
      "[Epoch 34] Training Batch [95/391]: Loss 0.0001586659054737538\n",
      "[Epoch 34] Training Batch [96/391]: Loss 0.0003305002464912832\n",
      "[Epoch 34] Training Batch [97/391]: Loss 0.00014591256331186742\n",
      "[Epoch 34] Training Batch [98/391]: Loss 8.994142262963578e-05\n",
      "[Epoch 34] Training Batch [99/391]: Loss 0.00024072366068139672\n",
      "[Epoch 34] Training Batch [100/391]: Loss 0.0002457328955642879\n",
      "[Epoch 34] Training Batch [101/391]: Loss 0.00048243068158626556\n",
      "[Epoch 34] Training Batch [102/391]: Loss 0.0002616318524815142\n",
      "[Epoch 34] Training Batch [103/391]: Loss 0.0003178844344802201\n",
      "[Epoch 34] Training Batch [104/391]: Loss 0.00027729940484277904\n",
      "[Epoch 34] Training Batch [105/391]: Loss 5.1941628044005483e-05\n",
      "[Epoch 34] Training Batch [106/391]: Loss 9.582537313690409e-05\n",
      "[Epoch 34] Training Batch [107/391]: Loss 0.00012556678848341107\n",
      "[Epoch 34] Training Batch [108/391]: Loss 3.0754643375985324e-05\n",
      "[Epoch 34] Training Batch [109/391]: Loss 0.0001934676110977307\n",
      "[Epoch 34] Training Batch [110/391]: Loss 9.494584082858637e-05\n",
      "[Epoch 34] Training Batch [111/391]: Loss 4.290134893381037e-05\n",
      "[Epoch 34] Training Batch [112/391]: Loss 9.68977328739129e-05\n",
      "[Epoch 34] Training Batch [113/391]: Loss 0.00010443580686114728\n",
      "[Epoch 34] Training Batch [114/391]: Loss 0.0001588873128639534\n",
      "[Epoch 34] Training Batch [115/391]: Loss 0.00012999372847843915\n",
      "[Epoch 34] Training Batch [116/391]: Loss 0.00012546966900117695\n",
      "[Epoch 34] Training Batch [117/391]: Loss 0.00023547900491394103\n",
      "[Epoch 34] Training Batch [118/391]: Loss 0.0001666064781602472\n",
      "[Epoch 34] Training Batch [119/391]: Loss 6.526864308398217e-05\n",
      "[Epoch 34] Training Batch [120/391]: Loss 0.00018376187654212117\n",
      "[Epoch 34] Training Batch [121/391]: Loss 0.00024214947188738734\n",
      "[Epoch 34] Training Batch [122/391]: Loss 0.00015028439520392567\n",
      "[Epoch 34] Training Batch [123/391]: Loss 6.220902287168428e-05\n",
      "[Epoch 34] Training Batch [124/391]: Loss 0.00014219817239791155\n",
      "[Epoch 34] Training Batch [125/391]: Loss 0.00031143860542215407\n",
      "[Epoch 34] Training Batch [126/391]: Loss 0.00017061956168618053\n",
      "[Epoch 34] Training Batch [127/391]: Loss 0.00011392032320145518\n",
      "[Epoch 34] Training Batch [128/391]: Loss 0.0002820389636326581\n",
      "[Epoch 34] Training Batch [129/391]: Loss 0.00032432982698082924\n",
      "[Epoch 34] Training Batch [130/391]: Loss 0.00038566687726415694\n",
      "[Epoch 34] Training Batch [131/391]: Loss 0.0003034558321814984\n",
      "[Epoch 34] Training Batch [132/391]: Loss 0.00027371422038413584\n",
      "[Epoch 34] Training Batch [133/391]: Loss 9.588579268893227e-05\n",
      "[Epoch 34] Training Batch [134/391]: Loss 0.00013930824934504926\n",
      "[Epoch 34] Training Batch [135/391]: Loss 0.0003109170065727085\n",
      "[Epoch 34] Training Batch [136/391]: Loss 0.00012180541671114042\n",
      "[Epoch 34] Training Batch [137/391]: Loss 0.00017772179853636771\n",
      "[Epoch 34] Training Batch [138/391]: Loss 0.0004117689677514136\n",
      "[Epoch 34] Training Batch [139/391]: Loss 0.00019326181791257113\n",
      "[Epoch 34] Training Batch [140/391]: Loss 9.580185724189505e-05\n",
      "[Epoch 34] Training Batch [141/391]: Loss 0.00023346368107013404\n",
      "[Epoch 34] Training Batch [142/391]: Loss 0.0002113320806529373\n",
      "[Epoch 34] Training Batch [143/391]: Loss 0.00037386358599178493\n",
      "[Epoch 34] Training Batch [144/391]: Loss 0.000185536322533153\n",
      "[Epoch 34] Training Batch [145/391]: Loss 0.00011920170800294727\n",
      "[Epoch 34] Training Batch [146/391]: Loss 0.00011884506238857284\n",
      "[Epoch 34] Training Batch [147/391]: Loss 0.00011676496069412678\n",
      "[Epoch 34] Training Batch [148/391]: Loss 0.0001519379875389859\n",
      "[Epoch 34] Training Batch [149/391]: Loss 4.969046494807117e-05\n",
      "[Epoch 34] Training Batch [150/391]: Loss 0.00016844669880811125\n",
      "[Epoch 34] Training Batch [151/391]: Loss 0.00018891907529905438\n",
      "[Epoch 34] Training Batch [152/391]: Loss 0.0003412816149648279\n",
      "[Epoch 34] Training Batch [153/391]: Loss 0.00020670960657298565\n",
      "[Epoch 34] Training Batch [154/391]: Loss 0.00019950908608734608\n",
      "[Epoch 34] Training Batch [155/391]: Loss 7.70015612943098e-05\n",
      "[Epoch 34] Training Batch [156/391]: Loss 6.332836346700788e-05\n",
      "[Epoch 34] Training Batch [157/391]: Loss 0.0001447480171918869\n",
      "[Epoch 34] Training Batch [158/391]: Loss 3.8791680708527565e-05\n",
      "[Epoch 34] Training Batch [159/391]: Loss 0.00010953309538308531\n",
      "[Epoch 34] Training Batch [160/391]: Loss 6.070421659387648e-05\n",
      "[Epoch 34] Training Batch [161/391]: Loss 0.00018802481645252556\n",
      "[Epoch 34] Training Batch [162/391]: Loss 5.9680161939468235e-05\n",
      "[Epoch 34] Training Batch [163/391]: Loss 0.00010380338790128008\n",
      "[Epoch 34] Training Batch [164/391]: Loss 7.506106339860708e-05\n",
      "[Epoch 34] Training Batch [165/391]: Loss 0.00019947886175941676\n",
      "[Epoch 34] Training Batch [166/391]: Loss 3.5090713936369866e-05\n",
      "[Epoch 34] Training Batch [167/391]: Loss 0.0001769835507730022\n",
      "[Epoch 34] Training Batch [168/391]: Loss 4.278133565094322e-05\n",
      "[Epoch 34] Training Batch [169/391]: Loss 0.00038308813236653805\n",
      "[Epoch 34] Training Batch [170/391]: Loss 4.744173202197999e-05\n",
      "[Epoch 34] Training Batch [171/391]: Loss 0.0002528524782974273\n",
      "[Epoch 34] Training Batch [172/391]: Loss 0.00019377267744857818\n",
      "[Epoch 34] Training Batch [173/391]: Loss 0.0005659263697452843\n",
      "[Epoch 34] Training Batch [174/391]: Loss 9.886711632134393e-05\n",
      "[Epoch 34] Training Batch [175/391]: Loss 0.0001113261969294399\n",
      "[Epoch 34] Training Batch [176/391]: Loss 0.0005159031716175377\n",
      "[Epoch 34] Training Batch [177/391]: Loss 0.00014950399054214358\n",
      "[Epoch 34] Training Batch [178/391]: Loss 7.486364484066144e-05\n",
      "[Epoch 34] Training Batch [179/391]: Loss 6.48146087769419e-05\n",
      "[Epoch 34] Training Batch [180/391]: Loss 0.00011691153486026451\n",
      "[Epoch 34] Training Batch [181/391]: Loss 9.890587534755468e-05\n",
      "[Epoch 34] Training Batch [182/391]: Loss 8.540535782231018e-05\n",
      "[Epoch 34] Training Batch [183/391]: Loss 0.00034703174605965614\n",
      "[Epoch 34] Training Batch [184/391]: Loss 6.283930997597054e-05\n",
      "[Epoch 34] Training Batch [185/391]: Loss 8.378372876904905e-05\n",
      "[Epoch 34] Training Batch [186/391]: Loss 0.00014555262168869376\n",
      "[Epoch 34] Training Batch [187/391]: Loss 0.00023028386931400746\n",
      "[Epoch 34] Training Batch [188/391]: Loss 0.00015916259144432843\n",
      "[Epoch 34] Training Batch [189/391]: Loss 0.000224593241000548\n",
      "[Epoch 34] Training Batch [190/391]: Loss 8.897815860109404e-05\n",
      "[Epoch 34] Training Batch [191/391]: Loss 4.7677880502305925e-05\n",
      "[Epoch 34] Training Batch [192/391]: Loss 0.0001724778994685039\n",
      "[Epoch 34] Training Batch [193/391]: Loss 6.45749387331307e-05\n",
      "[Epoch 34] Training Batch [194/391]: Loss 0.0001230921916430816\n",
      "[Epoch 34] Training Batch [195/391]: Loss 9.595121082384139e-05\n",
      "[Epoch 34] Training Batch [196/391]: Loss 0.00020402116933837533\n",
      "[Epoch 34] Training Batch [197/391]: Loss 6.412896618712693e-05\n",
      "[Epoch 34] Training Batch [198/391]: Loss 0.00010858140740310773\n",
      "[Epoch 34] Training Batch [199/391]: Loss 7.856081356294453e-05\n",
      "[Epoch 34] Training Batch [200/391]: Loss 0.00042171459062956274\n",
      "[Epoch 34] Training Batch [201/391]: Loss 0.00020576412498485297\n",
      "[Epoch 34] Training Batch [202/391]: Loss 0.00013882121129427105\n",
      "[Epoch 34] Training Batch [203/391]: Loss 0.00016363701433874667\n",
      "[Epoch 34] Training Batch [204/391]: Loss 0.00014078390086069703\n",
      "[Epoch 34] Training Batch [205/391]: Loss 0.00010462848149472848\n",
      "[Epoch 34] Training Batch [206/391]: Loss 5.8367630117572844e-05\n",
      "[Epoch 34] Training Batch [207/391]: Loss 3.321700933156535e-05\n",
      "[Epoch 34] Training Batch [208/391]: Loss 0.00010028581164078787\n",
      "[Epoch 34] Training Batch [209/391]: Loss 0.00014988609473221004\n",
      "[Epoch 34] Training Batch [210/391]: Loss 0.00012782486737705767\n",
      "[Epoch 34] Training Batch [211/391]: Loss 3.248884968343191e-05\n",
      "[Epoch 34] Training Batch [212/391]: Loss 2.82744258583989e-05\n",
      "[Epoch 34] Training Batch [213/391]: Loss 0.00032304905471391976\n",
      "[Epoch 34] Training Batch [214/391]: Loss 6.77940042805858e-05\n",
      "[Epoch 34] Training Batch [215/391]: Loss 9.102839248953387e-05\n",
      "[Epoch 34] Training Batch [216/391]: Loss 0.00013358588330447674\n",
      "[Epoch 34] Training Batch [217/391]: Loss 4.4996409997111186e-05\n",
      "[Epoch 34] Training Batch [218/391]: Loss 9.612890426069498e-05\n",
      "[Epoch 34] Training Batch [219/391]: Loss 9.905094339046627e-05\n",
      "[Epoch 34] Training Batch [220/391]: Loss 0.0007766469498164952\n",
      "[Epoch 34] Training Batch [221/391]: Loss 0.00023054270423017442\n",
      "[Epoch 34] Training Batch [222/391]: Loss 6.449350621551275e-05\n",
      "[Epoch 34] Training Batch [223/391]: Loss 0.00015332442126236856\n",
      "[Epoch 34] Training Batch [224/391]: Loss 5.28684031451121e-05\n",
      "[Epoch 34] Training Batch [225/391]: Loss 5.4093525250209495e-05\n",
      "[Epoch 34] Training Batch [226/391]: Loss 0.00010667052265489474\n",
      "[Epoch 34] Training Batch [227/391]: Loss 0.0015476028202101588\n",
      "[Epoch 34] Training Batch [228/391]: Loss 0.00020871343440376222\n",
      "[Epoch 34] Training Batch [229/391]: Loss 0.00011832520249299705\n",
      "[Epoch 34] Training Batch [230/391]: Loss 0.00013919665070716292\n",
      "[Epoch 34] Training Batch [231/391]: Loss 0.00016822163888718933\n",
      "[Epoch 34] Training Batch [232/391]: Loss 0.0003068772202823311\n",
      "[Epoch 34] Training Batch [233/391]: Loss 9.454548853682354e-05\n",
      "[Epoch 34] Training Batch [234/391]: Loss 0.00012579449685290456\n",
      "[Epoch 34] Training Batch [235/391]: Loss 9.70058172242716e-05\n",
      "[Epoch 34] Training Batch [236/391]: Loss 4.511796578299254e-05\n",
      "[Epoch 34] Training Batch [237/391]: Loss 3.2373871363233775e-05\n",
      "[Epoch 34] Training Batch [238/391]: Loss 6.49735375191085e-05\n",
      "[Epoch 34] Training Batch [239/391]: Loss 8.938056271290407e-05\n",
      "[Epoch 34] Training Batch [240/391]: Loss 0.000505467876791954\n",
      "[Epoch 34] Training Batch [241/391]: Loss 0.0036881226114928722\n",
      "[Epoch 34] Training Batch [242/391]: Loss 0.00017136606038548052\n",
      "[Epoch 34] Training Batch [243/391]: Loss 0.00011344063386786729\n",
      "[Epoch 34] Training Batch [244/391]: Loss 3.2801781344460323e-05\n",
      "[Epoch 34] Training Batch [245/391]: Loss 0.00047566110151819885\n",
      "[Epoch 34] Training Batch [246/391]: Loss 0.0001327710342593491\n",
      "[Epoch 34] Training Batch [247/391]: Loss 0.00017273236881010234\n",
      "[Epoch 34] Training Batch [248/391]: Loss 5.230216993368231e-05\n",
      "[Epoch 34] Training Batch [249/391]: Loss 0.00011978007387369871\n",
      "[Epoch 34] Training Batch [250/391]: Loss 4.67354038846679e-05\n",
      "[Epoch 34] Training Batch [251/391]: Loss 0.0001298124116146937\n",
      "[Epoch 34] Training Batch [252/391]: Loss 7.184638525359333e-05\n",
      "[Epoch 34] Training Batch [253/391]: Loss 9.864305320661515e-05\n",
      "[Epoch 34] Training Batch [254/391]: Loss 0.00010963175736833364\n",
      "[Epoch 34] Training Batch [255/391]: Loss 9.237985796062276e-05\n",
      "[Epoch 34] Training Batch [256/391]: Loss 0.00022770193754695356\n",
      "[Epoch 34] Training Batch [257/391]: Loss 0.0002234850253444165\n",
      "[Epoch 34] Training Batch [258/391]: Loss 0.00020821375073865056\n",
      "[Epoch 34] Training Batch [259/391]: Loss 0.00021278468193486333\n",
      "[Epoch 34] Training Batch [260/391]: Loss 0.00010717767872847617\n",
      "[Epoch 34] Training Batch [261/391]: Loss 0.00026715989224612713\n",
      "[Epoch 34] Training Batch [262/391]: Loss 0.00010395437857368961\n",
      "[Epoch 34] Training Batch [263/391]: Loss 0.00023556547239422798\n",
      "[Epoch 34] Training Batch [264/391]: Loss 5.202424290473573e-05\n",
      "[Epoch 34] Training Batch [265/391]: Loss 0.0001499722566222772\n",
      "[Epoch 34] Training Batch [266/391]: Loss 0.0001607742888154462\n",
      "[Epoch 34] Training Batch [267/391]: Loss 0.000235462372074835\n",
      "[Epoch 34] Training Batch [268/391]: Loss 0.0004043016815558076\n",
      "[Epoch 34] Training Batch [269/391]: Loss 7.272406219271943e-05\n",
      "[Epoch 34] Training Batch [270/391]: Loss 0.00016483156650792807\n",
      "[Epoch 34] Training Batch [271/391]: Loss 0.00014363577065523714\n",
      "[Epoch 34] Training Batch [272/391]: Loss 0.0002935788070317358\n",
      "[Epoch 34] Training Batch [273/391]: Loss 0.00020554111688397825\n",
      "[Epoch 34] Training Batch [274/391]: Loss 3.856132389046252e-05\n",
      "[Epoch 34] Training Batch [275/391]: Loss 0.0005590588552877307\n",
      "[Epoch 34] Training Batch [276/391]: Loss 0.00021838024258613586\n",
      "[Epoch 34] Training Batch [277/391]: Loss 0.000209810197702609\n",
      "[Epoch 34] Training Batch [278/391]: Loss 6.079999729990959e-05\n",
      "[Epoch 34] Training Batch [279/391]: Loss 5.637248250422999e-05\n",
      "[Epoch 34] Training Batch [280/391]: Loss 0.0014135680394247174\n",
      "[Epoch 34] Training Batch [281/391]: Loss 0.0002595769474282861\n",
      "[Epoch 34] Training Batch [282/391]: Loss 0.00046734962961636484\n",
      "[Epoch 34] Training Batch [283/391]: Loss 7.284855382749811e-05\n",
      "[Epoch 34] Training Batch [284/391]: Loss 0.00021704440587200224\n",
      "[Epoch 34] Training Batch [285/391]: Loss 0.00012168944522272795\n",
      "[Epoch 34] Training Batch [286/391]: Loss 0.00023518470698036253\n",
      "[Epoch 34] Training Batch [287/391]: Loss 7.769278454361483e-05\n",
      "[Epoch 34] Training Batch [288/391]: Loss 0.0003297196526546031\n",
      "[Epoch 34] Training Batch [289/391]: Loss 0.00011026662832591683\n",
      "[Epoch 34] Training Batch [290/391]: Loss 8.583333692513406e-05\n",
      "[Epoch 34] Training Batch [291/391]: Loss 0.00025979767087846994\n",
      "[Epoch 34] Training Batch [292/391]: Loss 0.0002453132183291018\n",
      "[Epoch 34] Training Batch [293/391]: Loss 8.3620980149135e-05\n",
      "[Epoch 34] Training Batch [294/391]: Loss 0.00017117544484790415\n",
      "[Epoch 34] Training Batch [295/391]: Loss 7.453596481354907e-05\n",
      "[Epoch 34] Training Batch [296/391]: Loss 0.0005681494367308915\n",
      "[Epoch 34] Training Batch [297/391]: Loss 0.00016092794248834252\n",
      "[Epoch 34] Training Batch [298/391]: Loss 2.9188537155278027e-05\n",
      "[Epoch 34] Training Batch [299/391]: Loss 9.083751501748338e-05\n",
      "[Epoch 34] Training Batch [300/391]: Loss 0.00019268236064817756\n",
      "[Epoch 34] Training Batch [301/391]: Loss 0.00010069373092846945\n",
      "[Epoch 34] Training Batch [302/391]: Loss 0.00013320519065018743\n",
      "[Epoch 34] Training Batch [303/391]: Loss 0.00014655012637376785\n",
      "[Epoch 34] Training Batch [304/391]: Loss 4.0913957491284236e-05\n",
      "[Epoch 34] Training Batch [305/391]: Loss 8.730719855520874e-05\n",
      "[Epoch 34] Training Batch [306/391]: Loss 9.048247011378407e-05\n",
      "[Epoch 34] Training Batch [307/391]: Loss 9.891670197248459e-05\n",
      "[Epoch 34] Training Batch [308/391]: Loss 3.745483627426438e-05\n",
      "[Epoch 34] Training Batch [309/391]: Loss 0.000295096862828359\n",
      "[Epoch 34] Training Batch [310/391]: Loss 0.00011661607277346775\n",
      "[Epoch 34] Training Batch [311/391]: Loss 0.0002457822556607425\n",
      "[Epoch 34] Training Batch [312/391]: Loss 5.6161326938308775e-05\n",
      "[Epoch 34] Training Batch [313/391]: Loss 0.0003183262888342142\n",
      "[Epoch 34] Training Batch [314/391]: Loss 4.810334576177411e-05\n",
      "[Epoch 34] Training Batch [315/391]: Loss 4.693774098996073e-05\n",
      "[Epoch 34] Training Batch [316/391]: Loss 5.393896935856901e-05\n",
      "[Epoch 34] Training Batch [317/391]: Loss 0.00013836559082847089\n",
      "[Epoch 34] Training Batch [318/391]: Loss 6.50909569230862e-05\n",
      "[Epoch 34] Training Batch [319/391]: Loss 9.377624519402161e-05\n",
      "[Epoch 34] Training Batch [320/391]: Loss 9.998664609156549e-05\n",
      "[Epoch 34] Training Batch [321/391]: Loss 5.394004983827472e-05\n",
      "[Epoch 34] Training Batch [322/391]: Loss 0.00016731934738345444\n",
      "[Epoch 34] Training Batch [323/391]: Loss 0.00021644732623826712\n",
      "[Epoch 34] Training Batch [324/391]: Loss 0.00012181429337942973\n",
      "[Epoch 34] Training Batch [325/391]: Loss 0.00019397676805965602\n",
      "[Epoch 34] Training Batch [326/391]: Loss 0.00010609636956360191\n",
      "[Epoch 34] Training Batch [327/391]: Loss 0.00023111786867957562\n",
      "[Epoch 34] Training Batch [328/391]: Loss 0.00012598409375641495\n",
      "[Epoch 34] Training Batch [329/391]: Loss 0.0001726407208479941\n",
      "[Epoch 34] Training Batch [330/391]: Loss 0.0001579468371346593\n",
      "[Epoch 34] Training Batch [331/391]: Loss 7.010924309724942e-05\n",
      "[Epoch 34] Training Batch [332/391]: Loss 6.266440323088318e-05\n",
      "[Epoch 34] Training Batch [333/391]: Loss 3.8814680010546e-05\n",
      "[Epoch 34] Training Batch [334/391]: Loss 7.596098294015974e-05\n",
      "[Epoch 34] Training Batch [335/391]: Loss 0.00013328659406397492\n",
      "[Epoch 34] Training Batch [336/391]: Loss 4.995496055926196e-05\n",
      "[Epoch 34] Training Batch [337/391]: Loss 4.995912968297489e-05\n",
      "[Epoch 34] Training Batch [338/391]: Loss 0.0001348111982224509\n",
      "[Epoch 34] Training Batch [339/391]: Loss 8.992861694423482e-05\n",
      "[Epoch 34] Training Batch [340/391]: Loss 5.712943311664276e-05\n",
      "[Epoch 34] Training Batch [341/391]: Loss 0.0001510903239250183\n",
      "[Epoch 34] Training Batch [342/391]: Loss 0.000154976369231008\n",
      "[Epoch 34] Training Batch [343/391]: Loss 0.00016905840311665088\n",
      "[Epoch 34] Training Batch [344/391]: Loss 0.00011329697736073285\n",
      "[Epoch 34] Training Batch [345/391]: Loss 6.204804230947047e-05\n",
      "[Epoch 34] Training Batch [346/391]: Loss 8.776271715760231e-05\n",
      "[Epoch 34] Training Batch [347/391]: Loss 0.00019118466298095882\n",
      "[Epoch 34] Training Batch [348/391]: Loss 3.377355096745305e-05\n",
      "[Epoch 34] Training Batch [349/391]: Loss 7.066743273753673e-05\n",
      "[Epoch 34] Training Batch [350/391]: Loss 4.898529005004093e-05\n",
      "[Epoch 34] Training Batch [351/391]: Loss 0.0006923829205334187\n",
      "[Epoch 34] Training Batch [352/391]: Loss 9.456292173126712e-05\n",
      "[Epoch 34] Training Batch [353/391]: Loss 0.00010782815661514178\n",
      "[Epoch 34] Training Batch [354/391]: Loss 0.00010302895680069923\n",
      "[Epoch 34] Training Batch [355/391]: Loss 8.031709876377136e-05\n",
      "[Epoch 34] Training Batch [356/391]: Loss 0.0001357192813884467\n",
      "[Epoch 34] Training Batch [357/391]: Loss 0.00021742995886597782\n",
      "[Epoch 34] Training Batch [358/391]: Loss 5.02296861668583e-05\n",
      "[Epoch 34] Training Batch [359/391]: Loss 5.6194327044067904e-05\n",
      "[Epoch 34] Training Batch [360/391]: Loss 6.85106570017524e-05\n",
      "[Epoch 34] Training Batch [361/391]: Loss 0.0001230988564202562\n",
      "[Epoch 34] Training Batch [362/391]: Loss 0.00020411568402778357\n",
      "[Epoch 34] Training Batch [363/391]: Loss 8.95374541869387e-05\n",
      "[Epoch 34] Training Batch [364/391]: Loss 0.00025165427359752357\n",
      "[Epoch 34] Training Batch [365/391]: Loss 0.00017011939780786633\n",
      "[Epoch 34] Training Batch [366/391]: Loss 8.499791147187352e-05\n",
      "[Epoch 34] Training Batch [367/391]: Loss 8.822204836178571e-05\n",
      "[Epoch 34] Training Batch [368/391]: Loss 8.538876863894984e-05\n",
      "[Epoch 34] Training Batch [369/391]: Loss 0.00028304453007876873\n",
      "[Epoch 34] Training Batch [370/391]: Loss 8.786987018538639e-05\n",
      "[Epoch 34] Training Batch [371/391]: Loss 0.00020813212904613465\n",
      "[Epoch 34] Training Batch [372/391]: Loss 0.0002071125927614048\n",
      "[Epoch 34] Training Batch [373/391]: Loss 6.419180863304064e-05\n",
      "[Epoch 34] Training Batch [374/391]: Loss 0.0001418266911059618\n",
      "[Epoch 34] Training Batch [375/391]: Loss 0.0001111910241888836\n",
      "[Epoch 34] Training Batch [376/391]: Loss 0.000283173518255353\n",
      "[Epoch 34] Training Batch [377/391]: Loss 0.0004313447279855609\n",
      "[Epoch 34] Training Batch [378/391]: Loss 0.00016578682698309422\n",
      "[Epoch 34] Training Batch [379/391]: Loss 0.00010696848039515316\n",
      "[Epoch 34] Training Batch [380/391]: Loss 0.00013760148431174457\n",
      "[Epoch 34] Training Batch [381/391]: Loss 7.438209286192432e-05\n",
      "[Epoch 34] Training Batch [382/391]: Loss 0.00014134137018118054\n",
      "[Epoch 34] Training Batch [383/391]: Loss 0.00010154493793379515\n",
      "[Epoch 34] Training Batch [384/391]: Loss 0.0001451054704375565\n",
      "[Epoch 34] Training Batch [385/391]: Loss 5.681759648723528e-05\n",
      "[Epoch 34] Training Batch [386/391]: Loss 9.275266347685829e-05\n",
      "[Epoch 34] Training Batch [387/391]: Loss 6.906581984367222e-05\n",
      "[Epoch 34] Training Batch [388/391]: Loss 7.364108751062304e-05\n",
      "[Epoch 34] Training Batch [389/391]: Loss 3.808844485320151e-05\n",
      "[Epoch 34] Training Batch [390/391]: Loss 0.00014621854643337429\n",
      "[Epoch 34] Training Batch [391/391]: Loss 5.708118624170311e-05\n",
      "Epoch 34 - Train Loss: 0.0002\n",
      "*********  Epoch 35/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35] Training Batch [1/391]: Loss 3.7044123018858954e-05\n",
      "[Epoch 35] Training Batch [2/391]: Loss 0.00012055013212375343\n",
      "[Epoch 35] Training Batch [3/391]: Loss 8.132977382047102e-05\n",
      "[Epoch 35] Training Batch [4/391]: Loss 0.0001374354906147346\n",
      "[Epoch 35] Training Batch [5/391]: Loss 6.856150866951793e-05\n",
      "[Epoch 35] Training Batch [6/391]: Loss 6.769682659069076e-05\n",
      "[Epoch 35] Training Batch [7/391]: Loss 2.002613473450765e-05\n",
      "[Epoch 35] Training Batch [8/391]: Loss 6.465386104537174e-05\n",
      "[Epoch 35] Training Batch [9/391]: Loss 0.00020667945500463247\n",
      "[Epoch 35] Training Batch [10/391]: Loss 0.00010968978313030675\n",
      "[Epoch 35] Training Batch [11/391]: Loss 8.427272405242547e-05\n",
      "[Epoch 35] Training Batch [12/391]: Loss 5.5619773775106296e-05\n",
      "[Epoch 35] Training Batch [13/391]: Loss 4.283758971723728e-05\n",
      "[Epoch 35] Training Batch [14/391]: Loss 7.870474655646831e-05\n",
      "[Epoch 35] Training Batch [15/391]: Loss 8.42369845486246e-05\n",
      "[Epoch 35] Training Batch [16/391]: Loss 5.09264282300137e-05\n",
      "[Epoch 35] Training Batch [17/391]: Loss 8.94034601515159e-05\n",
      "[Epoch 35] Training Batch [18/391]: Loss 3.9315244066528976e-05\n",
      "[Epoch 35] Training Batch [19/391]: Loss 7.535694749094546e-05\n",
      "[Epoch 35] Training Batch [20/391]: Loss 5.323354707797989e-05\n",
      "[Epoch 35] Training Batch [21/391]: Loss 6.592002318939194e-05\n",
      "[Epoch 35] Training Batch [22/391]: Loss 5.9112513554282486e-05\n",
      "[Epoch 35] Training Batch [23/391]: Loss 6.62344682496041e-05\n",
      "[Epoch 35] Training Batch [24/391]: Loss 0.00010566200944595039\n",
      "[Epoch 35] Training Batch [25/391]: Loss 3.4148997656302527e-05\n",
      "[Epoch 35] Training Batch [26/391]: Loss 8.86455673025921e-05\n",
      "[Epoch 35] Training Batch [27/391]: Loss 7.812616968294606e-05\n",
      "[Epoch 35] Training Batch [28/391]: Loss 0.0003148226242046803\n",
      "[Epoch 35] Training Batch [29/391]: Loss 9.581230551702902e-05\n",
      "[Epoch 35] Training Batch [30/391]: Loss 5.117786713526584e-05\n",
      "[Epoch 35] Training Batch [31/391]: Loss 0.00011103317228844389\n",
      "[Epoch 35] Training Batch [32/391]: Loss 4.7258290578611195e-05\n",
      "[Epoch 35] Training Batch [33/391]: Loss 6.987952656345442e-05\n",
      "[Epoch 35] Training Batch [34/391]: Loss 3.506290522636846e-05\n",
      "[Epoch 35] Training Batch [35/391]: Loss 3.585901140468195e-05\n",
      "[Epoch 35] Training Batch [36/391]: Loss 9.200542262988165e-05\n",
      "[Epoch 35] Training Batch [37/391]: Loss 6.155923620099202e-05\n",
      "[Epoch 35] Training Batch [38/391]: Loss 8.981496648630127e-05\n",
      "[Epoch 35] Training Batch [39/391]: Loss 7.580341480206698e-05\n",
      "[Epoch 35] Training Batch [40/391]: Loss 0.00012045909534208477\n",
      "[Epoch 35] Training Batch [41/391]: Loss 6.565689545823261e-05\n",
      "[Epoch 35] Training Batch [42/391]: Loss 8.68854476721026e-05\n",
      "[Epoch 35] Training Batch [43/391]: Loss 0.00011528834875207394\n",
      "[Epoch 35] Training Batch [44/391]: Loss 9.168917313218117e-05\n",
      "[Epoch 35] Training Batch [45/391]: Loss 4.268393968231976e-05\n",
      "[Epoch 35] Training Batch [46/391]: Loss 4.112919486942701e-05\n",
      "[Epoch 35] Training Batch [47/391]: Loss 5.10541649418883e-05\n",
      "[Epoch 35] Training Batch [48/391]: Loss 8.367189002456143e-05\n",
      "[Epoch 35] Training Batch [49/391]: Loss 8.879075903678313e-05\n",
      "[Epoch 35] Training Batch [50/391]: Loss 5.922205309616402e-05\n",
      "[Epoch 35] Training Batch [51/391]: Loss 5.004371632821858e-05\n",
      "[Epoch 35] Training Batch [52/391]: Loss 9.979495371226221e-05\n",
      "[Epoch 35] Training Batch [53/391]: Loss 8.834776963340119e-05\n",
      "[Epoch 35] Training Batch [54/391]: Loss 9.067956125363708e-05\n",
      "[Epoch 35] Training Batch [55/391]: Loss 5.637417052639648e-05\n",
      "[Epoch 35] Training Batch [56/391]: Loss 5.138586857356131e-05\n",
      "[Epoch 35] Training Batch [57/391]: Loss 7.997775537660345e-05\n",
      "[Epoch 35] Training Batch [58/391]: Loss 7.193729106802493e-05\n",
      "[Epoch 35] Training Batch [59/391]: Loss 9.58624659688212e-05\n",
      "[Epoch 35] Training Batch [60/391]: Loss 5.312060966389254e-05\n",
      "[Epoch 35] Training Batch [61/391]: Loss 4.5178792788647115e-05\n",
      "[Epoch 35] Training Batch [62/391]: Loss 9.80489858193323e-05\n",
      "[Epoch 35] Training Batch [63/391]: Loss 4.063510277774185e-05\n",
      "[Epoch 35] Training Batch [64/391]: Loss 0.00010266884783050045\n",
      "[Epoch 35] Training Batch [65/391]: Loss 3.834741801256314e-05\n",
      "[Epoch 35] Training Batch [66/391]: Loss 5.308211984811351e-05\n",
      "[Epoch 35] Training Batch [67/391]: Loss 4.880813503405079e-05\n",
      "[Epoch 35] Training Batch [68/391]: Loss 5.08591256220825e-05\n",
      "[Epoch 35] Training Batch [69/391]: Loss 6.376778037520126e-05\n",
      "[Epoch 35] Training Batch [70/391]: Loss 5.213975964579731e-05\n",
      "[Epoch 35] Training Batch [71/391]: Loss 0.00012911035446450114\n",
      "[Epoch 35] Training Batch [72/391]: Loss 5.47627751075197e-05\n",
      "[Epoch 35] Training Batch [73/391]: Loss 3.976816879003309e-05\n",
      "[Epoch 35] Training Batch [74/391]: Loss 7.790262316120788e-05\n",
      "[Epoch 35] Training Batch [75/391]: Loss 0.00010050188575405627\n",
      "[Epoch 35] Training Batch [76/391]: Loss 8.332265133503824e-05\n",
      "[Epoch 35] Training Batch [77/391]: Loss 0.00011900492245331407\n",
      "[Epoch 35] Training Batch [78/391]: Loss 8.062800043262541e-05\n",
      "[Epoch 35] Training Batch [79/391]: Loss 4.4336287828627974e-05\n",
      "[Epoch 35] Training Batch [80/391]: Loss 7.54741849959828e-05\n",
      "[Epoch 35] Training Batch [81/391]: Loss 4.627406451618299e-05\n",
      "[Epoch 35] Training Batch [82/391]: Loss 4.6241821110015735e-05\n",
      "[Epoch 35] Training Batch [83/391]: Loss 4.0046044887276366e-05\n",
      "[Epoch 35] Training Batch [84/391]: Loss 5.74994228372816e-05\n",
      "[Epoch 35] Training Batch [85/391]: Loss 6.0273250710451975e-05\n",
      "[Epoch 35] Training Batch [86/391]: Loss 3.559616379789077e-05\n",
      "[Epoch 35] Training Batch [87/391]: Loss 5.183168832445517e-05\n",
      "[Epoch 35] Training Batch [88/391]: Loss 6.755581853212789e-05\n",
      "[Epoch 35] Training Batch [89/391]: Loss 0.00015035764954518527\n",
      "[Epoch 35] Training Batch [90/391]: Loss 8.32408040878363e-05\n",
      "[Epoch 35] Training Batch [91/391]: Loss 4.633255593944341e-05\n",
      "[Epoch 35] Training Batch [92/391]: Loss 2.4173612473532557e-05\n",
      "[Epoch 35] Training Batch [93/391]: Loss 0.0001876255264505744\n",
      "[Epoch 35] Training Batch [94/391]: Loss 0.00010152917820960283\n",
      "[Epoch 35] Training Batch [95/391]: Loss 4.823401468456723e-05\n",
      "[Epoch 35] Training Batch [96/391]: Loss 0.00010073611338157207\n",
      "[Epoch 35] Training Batch [97/391]: Loss 0.00011021541286027059\n",
      "[Epoch 35] Training Batch [98/391]: Loss 0.00013495064922608435\n",
      "[Epoch 35] Training Batch [99/391]: Loss 5.7867044233717024e-05\n",
      "[Epoch 35] Training Batch [100/391]: Loss 8.337252074852586e-05\n",
      "[Epoch 35] Training Batch [101/391]: Loss 0.00016123287787195295\n",
      "[Epoch 35] Training Batch [102/391]: Loss 6.570693949470297e-05\n",
      "[Epoch 35] Training Batch [103/391]: Loss 9.855981625150889e-05\n",
      "[Epoch 35] Training Batch [104/391]: Loss 8.03786824690178e-05\n",
      "[Epoch 35] Training Batch [105/391]: Loss 1.6544432583032176e-05\n",
      "[Epoch 35] Training Batch [106/391]: Loss 5.75051162741147e-05\n",
      "[Epoch 35] Training Batch [107/391]: Loss 4.210649058222771e-05\n",
      "[Epoch 35] Training Batch [108/391]: Loss 3.189271956216544e-05\n",
      "[Epoch 35] Training Batch [109/391]: Loss 5.982690709060989e-05\n",
      "[Epoch 35] Training Batch [110/391]: Loss 6.553225830430165e-05\n",
      "[Epoch 35] Training Batch [111/391]: Loss 4.895140227745287e-05\n",
      "[Epoch 35] Training Batch [112/391]: Loss 0.00024413940263912082\n",
      "[Epoch 35] Training Batch [113/391]: Loss 4.40850890299771e-05\n",
      "[Epoch 35] Training Batch [114/391]: Loss 4.2188927181996405e-05\n",
      "[Epoch 35] Training Batch [115/391]: Loss 4.820213871425949e-05\n",
      "[Epoch 35] Training Batch [116/391]: Loss 3.7638299545506015e-05\n",
      "[Epoch 35] Training Batch [117/391]: Loss 0.00015671111759729683\n",
      "[Epoch 35] Training Batch [118/391]: Loss 4.485093813855201e-05\n",
      "[Epoch 35] Training Batch [119/391]: Loss 9.770900214789435e-05\n",
      "[Epoch 35] Training Batch [120/391]: Loss 4.038446058984846e-05\n",
      "[Epoch 35] Training Batch [121/391]: Loss 8.240877650678158e-05\n",
      "[Epoch 35] Training Batch [122/391]: Loss 6.737183139193803e-05\n",
      "[Epoch 35] Training Batch [123/391]: Loss 6.902108725626022e-05\n",
      "[Epoch 35] Training Batch [124/391]: Loss 7.383310003206134e-05\n",
      "[Epoch 35] Training Batch [125/391]: Loss 4.7117966460064054e-05\n",
      "[Epoch 35] Training Batch [126/391]: Loss 4.039470513816923e-05\n",
      "[Epoch 35] Training Batch [127/391]: Loss 5.3939278586767614e-05\n",
      "[Epoch 35] Training Batch [128/391]: Loss 5.559420969802886e-05\n",
      "[Epoch 35] Training Batch [129/391]: Loss 6.246880366234109e-05\n",
      "[Epoch 35] Training Batch [130/391]: Loss 0.0001598319795448333\n",
      "[Epoch 35] Training Batch [131/391]: Loss 7.105019176378846e-05\n",
      "[Epoch 35] Training Batch [132/391]: Loss 8.551761857233942e-05\n",
      "[Epoch 35] Training Batch [133/391]: Loss 0.00011524935689521953\n",
      "[Epoch 35] Training Batch [134/391]: Loss 5.312150460667908e-05\n",
      "[Epoch 35] Training Batch [135/391]: Loss 9.065798076335341e-05\n",
      "[Epoch 35] Training Batch [136/391]: Loss 4.1963539842981845e-05\n",
      "[Epoch 35] Training Batch [137/391]: Loss 2.5654275304987095e-05\n",
      "[Epoch 35] Training Batch [138/391]: Loss 5.9727870393544436e-05\n",
      "[Epoch 35] Training Batch [139/391]: Loss 4.751383312395774e-05\n",
      "[Epoch 35] Training Batch [140/391]: Loss 0.00013710802886635065\n",
      "[Epoch 35] Training Batch [141/391]: Loss 5.218504156800918e-05\n",
      "[Epoch 35] Training Batch [142/391]: Loss 0.00011404062388464808\n",
      "[Epoch 35] Training Batch [143/391]: Loss 0.00018842074496205896\n",
      "[Epoch 35] Training Batch [144/391]: Loss 4.466294194571674e-05\n",
      "[Epoch 35] Training Batch [145/391]: Loss 7.262303552124649e-05\n",
      "[Epoch 35] Training Batch [146/391]: Loss 8.462474215775728e-05\n",
      "[Epoch 35] Training Batch [147/391]: Loss 7.198286766652018e-05\n",
      "[Epoch 35] Training Batch [148/391]: Loss 0.0003170813142787665\n",
      "[Epoch 35] Training Batch [149/391]: Loss 0.00010087720875162631\n",
      "[Epoch 35] Training Batch [150/391]: Loss 3.465453119133599e-05\n",
      "[Epoch 35] Training Batch [151/391]: Loss 3.387588367331773e-05\n",
      "[Epoch 35] Training Batch [152/391]: Loss 0.00011790839926106855\n",
      "[Epoch 35] Training Batch [153/391]: Loss 7.217785605462268e-05\n",
      "[Epoch 35] Training Batch [154/391]: Loss 4.4817068555857986e-05\n",
      "[Epoch 35] Training Batch [155/391]: Loss 8.412398165091872e-05\n",
      "[Epoch 35] Training Batch [156/391]: Loss 6.837822002125904e-05\n",
      "[Epoch 35] Training Batch [157/391]: Loss 9.670430154073983e-05\n",
      "[Epoch 35] Training Batch [158/391]: Loss 6.357596430461854e-05\n",
      "[Epoch 35] Training Batch [159/391]: Loss 7.797564467182383e-05\n",
      "[Epoch 35] Training Batch [160/391]: Loss 7.25858990335837e-05\n",
      "[Epoch 35] Training Batch [161/391]: Loss 8.676228753756732e-05\n",
      "[Epoch 35] Training Batch [162/391]: Loss 0.00011171807273058221\n",
      "[Epoch 35] Training Batch [163/391]: Loss 7.537359488196671e-05\n",
      "[Epoch 35] Training Batch [164/391]: Loss 2.3861957743065432e-05\n",
      "[Epoch 35] Training Batch [165/391]: Loss 9.765545837581158e-05\n",
      "[Epoch 35] Training Batch [166/391]: Loss 0.0001020417403196916\n",
      "[Epoch 35] Training Batch [167/391]: Loss 0.00012998768943361938\n",
      "[Epoch 35] Training Batch [168/391]: Loss 6.048008071957156e-05\n",
      "[Epoch 35] Training Batch [169/391]: Loss 3.401735739316791e-05\n",
      "[Epoch 35] Training Batch [170/391]: Loss 4.219741822453216e-05\n",
      "[Epoch 35] Training Batch [171/391]: Loss 2.5185447157127783e-05\n",
      "[Epoch 35] Training Batch [172/391]: Loss 7.050159911159426e-05\n",
      "[Epoch 35] Training Batch [173/391]: Loss 6.635542376898229e-05\n",
      "[Epoch 35] Training Batch [174/391]: Loss 7.549663132522255e-05\n",
      "[Epoch 35] Training Batch [175/391]: Loss 5.023779522161931e-05\n",
      "[Epoch 35] Training Batch [176/391]: Loss 4.513581006904133e-05\n",
      "[Epoch 35] Training Batch [177/391]: Loss 9.215260070050135e-05\n",
      "[Epoch 35] Training Batch [178/391]: Loss 8.493325731251389e-05\n",
      "[Epoch 35] Training Batch [179/391]: Loss 4.23200435761828e-05\n",
      "[Epoch 35] Training Batch [180/391]: Loss 5.1389841246418655e-05\n",
      "[Epoch 35] Training Batch [181/391]: Loss 9.198018960887566e-05\n",
      "[Epoch 35] Training Batch [182/391]: Loss 2.693624446692411e-05\n",
      "[Epoch 35] Training Batch [183/391]: Loss 2.3084636268322356e-05\n",
      "[Epoch 35] Training Batch [184/391]: Loss 4.7918303607730195e-05\n",
      "[Epoch 35] Training Batch [185/391]: Loss 3.63693579856772e-05\n",
      "[Epoch 35] Training Batch [186/391]: Loss 8.298414468299598e-05\n",
      "[Epoch 35] Training Batch [187/391]: Loss 7.022639329079539e-05\n",
      "[Epoch 35] Training Batch [188/391]: Loss 4.447879473445937e-05\n",
      "[Epoch 35] Training Batch [189/391]: Loss 6.460420263465494e-05\n",
      "[Epoch 35] Training Batch [190/391]: Loss 4.421755511430092e-05\n",
      "[Epoch 35] Training Batch [191/391]: Loss 5.651785249938257e-05\n",
      "[Epoch 35] Training Batch [192/391]: Loss 0.00013476292951963842\n",
      "[Epoch 35] Training Batch [193/391]: Loss 4.528392310021445e-05\n",
      "[Epoch 35] Training Batch [194/391]: Loss 0.00015415351663250476\n",
      "[Epoch 35] Training Batch [195/391]: Loss 3.0131741368677467e-05\n",
      "[Epoch 35] Training Batch [196/391]: Loss 4.5857330405851826e-05\n",
      "[Epoch 35] Training Batch [197/391]: Loss 4.5493317884393036e-05\n",
      "[Epoch 35] Training Batch [198/391]: Loss 7.413493585772812e-05\n",
      "[Epoch 35] Training Batch [199/391]: Loss 0.00012992425763513893\n",
      "[Epoch 35] Training Batch [200/391]: Loss 4.171106411376968e-05\n",
      "[Epoch 35] Training Batch [201/391]: Loss 3.980563997174613e-05\n",
      "[Epoch 35] Training Batch [202/391]: Loss 6.081955143599771e-05\n",
      "[Epoch 35] Training Batch [203/391]: Loss 9.136394510278478e-05\n",
      "[Epoch 35] Training Batch [204/391]: Loss 7.542168896179646e-05\n",
      "[Epoch 35] Training Batch [205/391]: Loss 4.87773068016395e-05\n",
      "[Epoch 35] Training Batch [206/391]: Loss 0.00011196764535270631\n",
      "[Epoch 35] Training Batch [207/391]: Loss 8.78492501215078e-05\n",
      "[Epoch 35] Training Batch [208/391]: Loss 7.265758904395625e-05\n",
      "[Epoch 35] Training Batch [209/391]: Loss 4.5107281039236113e-05\n",
      "[Epoch 35] Training Batch [210/391]: Loss 8.098205580608919e-05\n",
      "[Epoch 35] Training Batch [211/391]: Loss 6.305870192591101e-05\n",
      "[Epoch 35] Training Batch [212/391]: Loss 8.397873898502439e-05\n",
      "[Epoch 35] Training Batch [213/391]: Loss 6.820823182351887e-05\n",
      "[Epoch 35] Training Batch [214/391]: Loss 2.6612149667926133e-05\n",
      "[Epoch 35] Training Batch [215/391]: Loss 1.6728426999179646e-05\n",
      "[Epoch 35] Training Batch [216/391]: Loss 4.178118615527637e-05\n",
      "[Epoch 35] Training Batch [217/391]: Loss 9.979549213312566e-05\n",
      "[Epoch 35] Training Batch [218/391]: Loss 5.8778088714461774e-05\n",
      "[Epoch 35] Training Batch [219/391]: Loss 4.942230589222163e-05\n",
      "[Epoch 35] Training Batch [220/391]: Loss 3.442791057750583e-05\n",
      "[Epoch 35] Training Batch [221/391]: Loss 5.8476958656683564e-05\n",
      "[Epoch 35] Training Batch [222/391]: Loss 5.628124199574813e-05\n",
      "[Epoch 35] Training Batch [223/391]: Loss 3.7070803955430165e-05\n",
      "[Epoch 35] Training Batch [224/391]: Loss 5.3116320486878976e-05\n",
      "[Epoch 35] Training Batch [225/391]: Loss 0.00014600444410461932\n",
      "[Epoch 35] Training Batch [226/391]: Loss 6.093000774853863e-05\n",
      "[Epoch 35] Training Batch [227/391]: Loss 3.620560892159119e-05\n",
      "[Epoch 35] Training Batch [228/391]: Loss 6.890264921821654e-05\n",
      "[Epoch 35] Training Batch [229/391]: Loss 4.4683012674795464e-05\n",
      "[Epoch 35] Training Batch [230/391]: Loss 0.00019545515533536673\n",
      "[Epoch 35] Training Batch [231/391]: Loss 8.655818237457424e-05\n",
      "[Epoch 35] Training Batch [232/391]: Loss 2.6987840101355687e-05\n",
      "[Epoch 35] Training Batch [233/391]: Loss 4.610316318576224e-05\n",
      "[Epoch 35] Training Batch [234/391]: Loss 3.3712560252752155e-05\n",
      "[Epoch 35] Training Batch [235/391]: Loss 8.024102135095745e-05\n",
      "[Epoch 35] Training Batch [236/391]: Loss 8.773705485509709e-05\n",
      "[Epoch 35] Training Batch [237/391]: Loss 0.00010734359966591\n",
      "[Epoch 35] Training Batch [238/391]: Loss 7.357959111686796e-05\n",
      "[Epoch 35] Training Batch [239/391]: Loss 0.00010517771443119273\n",
      "[Epoch 35] Training Batch [240/391]: Loss 2.3654074539081194e-05\n",
      "[Epoch 35] Training Batch [241/391]: Loss 3.0446644814219326e-05\n",
      "[Epoch 35] Training Batch [242/391]: Loss 8.826198609312996e-05\n",
      "[Epoch 35] Training Batch [243/391]: Loss 2.6752675694297068e-05\n",
      "[Epoch 35] Training Batch [244/391]: Loss 6.241395749384537e-05\n",
      "[Epoch 35] Training Batch [245/391]: Loss 9.82887067948468e-05\n",
      "[Epoch 35] Training Batch [246/391]: Loss 0.00010709997150115669\n",
      "[Epoch 35] Training Batch [247/391]: Loss 6.599311745958403e-05\n",
      "[Epoch 35] Training Batch [248/391]: Loss 0.00011790091957664117\n",
      "[Epoch 35] Training Batch [249/391]: Loss 5.3377621952677146e-05\n",
      "[Epoch 35] Training Batch [250/391]: Loss 5.375001273932867e-05\n",
      "[Epoch 35] Training Batch [251/391]: Loss 7.146274583647028e-05\n",
      "[Epoch 35] Training Batch [252/391]: Loss 3.676483902381733e-05\n",
      "[Epoch 35] Training Batch [253/391]: Loss 0.0001255788083653897\n",
      "[Epoch 35] Training Batch [254/391]: Loss 6.380774721037596e-05\n",
      "[Epoch 35] Training Batch [255/391]: Loss 6.509968807222322e-05\n",
      "[Epoch 35] Training Batch [256/391]: Loss 1.6805333871161565e-05\n",
      "[Epoch 35] Training Batch [257/391]: Loss 4.005429218523204e-05\n",
      "[Epoch 35] Training Batch [258/391]: Loss 5.265774598228745e-05\n",
      "[Epoch 35] Training Batch [259/391]: Loss 4.3554373405640945e-05\n",
      "[Epoch 35] Training Batch [260/391]: Loss 0.00010461891361046582\n",
      "[Epoch 35] Training Batch [261/391]: Loss 4.441451528691687e-05\n",
      "[Epoch 35] Training Batch [262/391]: Loss 6.031052544130944e-05\n",
      "[Epoch 35] Training Batch [263/391]: Loss 4.0873856050893664e-05\n",
      "[Epoch 35] Training Batch [264/391]: Loss 8.44154492369853e-05\n",
      "[Epoch 35] Training Batch [265/391]: Loss 5.1979179261252284e-05\n",
      "[Epoch 35] Training Batch [266/391]: Loss 6.036538252374157e-05\n",
      "[Epoch 35] Training Batch [267/391]: Loss 3.263985490775667e-05\n",
      "[Epoch 35] Training Batch [268/391]: Loss 5.146935291122645e-05\n",
      "[Epoch 35] Training Batch [269/391]: Loss 6.629205745412037e-05\n",
      "[Epoch 35] Training Batch [270/391]: Loss 8.217190043069422e-05\n",
      "[Epoch 35] Training Batch [271/391]: Loss 5.309718108037487e-05\n",
      "[Epoch 35] Training Batch [272/391]: Loss 5.6166194553952664e-05\n",
      "[Epoch 35] Training Batch [273/391]: Loss 3.881945667671971e-05\n",
      "[Epoch 35] Training Batch [274/391]: Loss 0.00010745074541773647\n",
      "[Epoch 35] Training Batch [275/391]: Loss 6.253819447010756e-05\n",
      "[Epoch 35] Training Batch [276/391]: Loss 9.025262261275202e-05\n",
      "[Epoch 35] Training Batch [277/391]: Loss 9.569283429300413e-05\n",
      "[Epoch 35] Training Batch [278/391]: Loss 7.453484431607649e-05\n",
      "[Epoch 35] Training Batch [279/391]: Loss 2.6324762075091712e-05\n",
      "[Epoch 35] Training Batch [280/391]: Loss 7.179325621109456e-05\n",
      "[Epoch 35] Training Batch [281/391]: Loss 0.00012496694398578256\n",
      "[Epoch 35] Training Batch [282/391]: Loss 0.00013852315896656364\n",
      "[Epoch 35] Training Batch [283/391]: Loss 9.801660053199157e-05\n",
      "[Epoch 35] Training Batch [284/391]: Loss 4.028884359286167e-05\n",
      "[Epoch 35] Training Batch [285/391]: Loss 0.00011399270442780107\n",
      "[Epoch 35] Training Batch [286/391]: Loss 6.594251317437738e-05\n",
      "[Epoch 35] Training Batch [287/391]: Loss 2.726950333453715e-05\n",
      "[Epoch 35] Training Batch [288/391]: Loss 7.41743206162937e-05\n",
      "[Epoch 35] Training Batch [289/391]: Loss 3.9522681618109345e-05\n",
      "[Epoch 35] Training Batch [290/391]: Loss 6.81015444570221e-05\n",
      "[Epoch 35] Training Batch [291/391]: Loss 8.091248309938237e-05\n",
      "[Epoch 35] Training Batch [292/391]: Loss 5.149698336026631e-05\n",
      "[Epoch 35] Training Batch [293/391]: Loss 7.399448077194393e-05\n",
      "[Epoch 35] Training Batch [294/391]: Loss 7.411890692310408e-05\n",
      "[Epoch 35] Training Batch [295/391]: Loss 0.00011312918650219217\n",
      "[Epoch 35] Training Batch [296/391]: Loss 5.600575968855992e-05\n",
      "[Epoch 35] Training Batch [297/391]: Loss 3.875324910040945e-05\n",
      "[Epoch 35] Training Batch [298/391]: Loss 6.26546679995954e-05\n",
      "[Epoch 35] Training Batch [299/391]: Loss 5.696751395589672e-05\n",
      "[Epoch 35] Training Batch [300/391]: Loss 6.359774124575779e-05\n",
      "[Epoch 35] Training Batch [301/391]: Loss 3.9583486795891076e-05\n",
      "[Epoch 35] Training Batch [302/391]: Loss 4.952051676809788e-05\n",
      "[Epoch 35] Training Batch [303/391]: Loss 5.395683183451183e-05\n",
      "[Epoch 35] Training Batch [304/391]: Loss 0.00010242007556371391\n",
      "[Epoch 35] Training Batch [305/391]: Loss 3.2688036299077794e-05\n",
      "[Epoch 35] Training Batch [306/391]: Loss 0.00016091605357360095\n",
      "[Epoch 35] Training Batch [307/391]: Loss 6.868417403893545e-05\n",
      "[Epoch 35] Training Batch [308/391]: Loss 6.432936788769439e-05\n",
      "[Epoch 35] Training Batch [309/391]: Loss 4.1963252442656085e-05\n",
      "[Epoch 35] Training Batch [310/391]: Loss 3.436557381064631e-05\n",
      "[Epoch 35] Training Batch [311/391]: Loss 1.2261165466043167e-05\n",
      "[Epoch 35] Training Batch [312/391]: Loss 9.286856948165223e-05\n",
      "[Epoch 35] Training Batch [313/391]: Loss 0.00020265448256395757\n",
      "[Epoch 35] Training Batch [314/391]: Loss 8.33787998999469e-05\n",
      "[Epoch 35] Training Batch [315/391]: Loss 0.00010162985563511029\n",
      "[Epoch 35] Training Batch [316/391]: Loss 5.348795457393862e-05\n",
      "[Epoch 35] Training Batch [317/391]: Loss 3.303513949504122e-05\n",
      "[Epoch 35] Training Batch [318/391]: Loss 7.700437708990648e-05\n",
      "[Epoch 35] Training Batch [319/391]: Loss 4.453928704606369e-05\n",
      "[Epoch 35] Training Batch [320/391]: Loss 5.362396768759936e-05\n",
      "[Epoch 35] Training Batch [321/391]: Loss 5.56799495825544e-05\n",
      "[Epoch 35] Training Batch [322/391]: Loss 5.6483386288164183e-05\n",
      "[Epoch 35] Training Batch [323/391]: Loss 2.727416722336784e-05\n",
      "[Epoch 35] Training Batch [324/391]: Loss 6.044948167982511e-05\n",
      "[Epoch 35] Training Batch [325/391]: Loss 6.106340151745826e-05\n",
      "[Epoch 35] Training Batch [326/391]: Loss 5.669764141202904e-05\n",
      "[Epoch 35] Training Batch [327/391]: Loss 9.73743517533876e-05\n",
      "[Epoch 35] Training Batch [328/391]: Loss 2.828061224136036e-05\n",
      "[Epoch 35] Training Batch [329/391]: Loss 4.5998946006875485e-05\n",
      "[Epoch 35] Training Batch [330/391]: Loss 7.525855471612886e-05\n",
      "[Epoch 35] Training Batch [331/391]: Loss 3.643362651928328e-05\n",
      "[Epoch 35] Training Batch [332/391]: Loss 5.5175885790959e-05\n",
      "[Epoch 35] Training Batch [333/391]: Loss 6.639748607994989e-05\n",
      "[Epoch 35] Training Batch [334/391]: Loss 6.381746788974851e-05\n",
      "[Epoch 35] Training Batch [335/391]: Loss 3.4071519621647894e-05\n",
      "[Epoch 35] Training Batch [336/391]: Loss 7.762175664538518e-05\n",
      "[Epoch 35] Training Batch [337/391]: Loss 9.722292452352121e-05\n",
      "[Epoch 35] Training Batch [338/391]: Loss 5.326896280166693e-05\n",
      "[Epoch 35] Training Batch [339/391]: Loss 0.00010419705358799547\n",
      "[Epoch 35] Training Batch [340/391]: Loss 8.450067252852023e-05\n",
      "[Epoch 35] Training Batch [341/391]: Loss 4.23610836151056e-05\n",
      "[Epoch 35] Training Batch [342/391]: Loss 6.11174400546588e-05\n",
      "[Epoch 35] Training Batch [343/391]: Loss 4.896067184745334e-05\n",
      "[Epoch 35] Training Batch [344/391]: Loss 0.00010462210775585845\n",
      "[Epoch 35] Training Batch [345/391]: Loss 4.8203182814177126e-05\n",
      "[Epoch 35] Training Batch [346/391]: Loss 1.5163533134909812e-05\n",
      "[Epoch 35] Training Batch [347/391]: Loss 7.283829472726211e-05\n",
      "[Epoch 35] Training Batch [348/391]: Loss 5.2277991926530376e-05\n",
      "[Epoch 35] Training Batch [349/391]: Loss 5.924854849581607e-05\n",
      "[Epoch 35] Training Batch [350/391]: Loss 4.013286888948642e-05\n",
      "[Epoch 35] Training Batch [351/391]: Loss 6.597305764444172e-05\n",
      "[Epoch 35] Training Batch [352/391]: Loss 5.9211386542301625e-05\n",
      "[Epoch 35] Training Batch [353/391]: Loss 2.2508773326990195e-05\n",
      "[Epoch 35] Training Batch [354/391]: Loss 5.503808642970398e-05\n",
      "[Epoch 35] Training Batch [355/391]: Loss 3.622851363616064e-05\n",
      "[Epoch 35] Training Batch [356/391]: Loss 8.404079562751576e-05\n",
      "[Epoch 35] Training Batch [357/391]: Loss 9.422694711247459e-05\n",
      "[Epoch 35] Training Batch [358/391]: Loss 8.477140363538638e-05\n",
      "[Epoch 35] Training Batch [359/391]: Loss 2.2205800632946193e-05\n",
      "[Epoch 35] Training Batch [360/391]: Loss 2.3962966224644333e-05\n",
      "[Epoch 35] Training Batch [361/391]: Loss 2.8916985684190877e-05\n",
      "[Epoch 35] Training Batch [362/391]: Loss 5.222636536927894e-05\n",
      "[Epoch 35] Training Batch [363/391]: Loss 6.146486703073606e-05\n",
      "[Epoch 35] Training Batch [364/391]: Loss 0.00012323769624345005\n",
      "[Epoch 35] Training Batch [365/391]: Loss 3.6851106415269896e-05\n",
      "[Epoch 35] Training Batch [366/391]: Loss 3.9017319068079814e-05\n",
      "[Epoch 35] Training Batch [367/391]: Loss 7.897892646724358e-05\n",
      "[Epoch 35] Training Batch [368/391]: Loss 2.9587950848508626e-05\n",
      "[Epoch 35] Training Batch [369/391]: Loss 7.946733239805326e-05\n",
      "[Epoch 35] Training Batch [370/391]: Loss 7.956588524393737e-05\n",
      "[Epoch 35] Training Batch [371/391]: Loss 4.3220134102739394e-05\n",
      "[Epoch 35] Training Batch [372/391]: Loss 8.155110117513686e-05\n",
      "[Epoch 35] Training Batch [373/391]: Loss 5.0609352911124006e-05\n",
      "[Epoch 35] Training Batch [374/391]: Loss 2.8299817131482996e-05\n",
      "[Epoch 35] Training Batch [375/391]: Loss 1.766448258422315e-05\n",
      "[Epoch 35] Training Batch [376/391]: Loss 2.0634688553400338e-05\n",
      "[Epoch 35] Training Batch [377/391]: Loss 6.95864437147975e-05\n",
      "[Epoch 35] Training Batch [378/391]: Loss 3.4860335290431976e-05\n",
      "[Epoch 35] Training Batch [379/391]: Loss 3.4058335586450994e-05\n",
      "[Epoch 35] Training Batch [380/391]: Loss 8.198509749490768e-05\n",
      "[Epoch 35] Training Batch [381/391]: Loss 5.732799036195502e-05\n",
      "[Epoch 35] Training Batch [382/391]: Loss 5.3525101975537837e-05\n",
      "[Epoch 35] Training Batch [383/391]: Loss 7.625181024195626e-05\n",
      "[Epoch 35] Training Batch [384/391]: Loss 3.0911669455235824e-05\n",
      "[Epoch 35] Training Batch [385/391]: Loss 4.8687434173189104e-05\n",
      "[Epoch 35] Training Batch [386/391]: Loss 2.784326352411881e-05\n",
      "[Epoch 35] Training Batch [387/391]: Loss 5.44302529306151e-05\n",
      "[Epoch 35] Training Batch [388/391]: Loss 6.434974784497172e-05\n",
      "[Epoch 35] Training Batch [389/391]: Loss 2.3435930415871553e-05\n",
      "[Epoch 35] Training Batch [390/391]: Loss 5.428220174508169e-05\n",
      "[Epoch 35] Training Batch [391/391]: Loss 0.00012460476136766374\n",
      "Epoch 35 - Train Loss: 0.0001\n",
      "*********  Epoch 36/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36] Training Batch [1/391]: Loss 4.360368620837107e-05\n",
      "[Epoch 36] Training Batch [2/391]: Loss 6.496247078757733e-05\n",
      "[Epoch 36] Training Batch [3/391]: Loss 9.781327389646322e-05\n",
      "[Epoch 36] Training Batch [4/391]: Loss 5.0860646297223866e-05\n",
      "[Epoch 36] Training Batch [5/391]: Loss 2.9778673706459813e-05\n",
      "[Epoch 36] Training Batch [6/391]: Loss 4.5353972382145e-05\n",
      "[Epoch 36] Training Batch [7/391]: Loss 3.7044574128231034e-05\n",
      "[Epoch 36] Training Batch [8/391]: Loss 2.0552946807583794e-05\n",
      "[Epoch 36] Training Batch [9/391]: Loss 2.0399676941451617e-05\n",
      "[Epoch 36] Training Batch [10/391]: Loss 4.074260505149141e-05\n",
      "[Epoch 36] Training Batch [11/391]: Loss 3.729899617610499e-05\n",
      "[Epoch 36] Training Batch [12/391]: Loss 6.0252212279010564e-05\n",
      "[Epoch 36] Training Batch [13/391]: Loss 2.6801435524248518e-05\n",
      "[Epoch 36] Training Batch [14/391]: Loss 9.073340333998203e-05\n",
      "[Epoch 36] Training Batch [15/391]: Loss 3.5997356462758034e-05\n",
      "[Epoch 36] Training Batch [16/391]: Loss 4.57005480711814e-05\n",
      "[Epoch 36] Training Batch [17/391]: Loss 4.9460526497568935e-05\n",
      "[Epoch 36] Training Batch [18/391]: Loss 3.33521056745667e-05\n",
      "[Epoch 36] Training Batch [19/391]: Loss 5.0442733481759205e-05\n",
      "[Epoch 36] Training Batch [20/391]: Loss 3.4348617191426456e-05\n",
      "[Epoch 36] Training Batch [21/391]: Loss 2.3869726646807976e-05\n",
      "[Epoch 36] Training Batch [22/391]: Loss 3.6710818676510826e-05\n",
      "[Epoch 36] Training Batch [23/391]: Loss 3.0343107937369496e-05\n",
      "[Epoch 36] Training Batch [24/391]: Loss 2.109958222717978e-05\n",
      "[Epoch 36] Training Batch [25/391]: Loss 3.569152613636106e-05\n",
      "[Epoch 36] Training Batch [26/391]: Loss 6.0296068113530055e-05\n",
      "[Epoch 36] Training Batch [27/391]: Loss 4.061115760123357e-05\n",
      "[Epoch 36] Training Batch [28/391]: Loss 5.9308807976776734e-05\n",
      "[Epoch 36] Training Batch [29/391]: Loss 2.9195856768637896e-05\n",
      "[Epoch 36] Training Batch [30/391]: Loss 5.880851676920429e-05\n",
      "[Epoch 36] Training Batch [31/391]: Loss 8.93029136932455e-05\n",
      "[Epoch 36] Training Batch [32/391]: Loss 6.891061639180407e-05\n",
      "[Epoch 36] Training Batch [33/391]: Loss 7.707339682383463e-05\n",
      "[Epoch 36] Training Batch [34/391]: Loss 8.084022556431592e-05\n",
      "[Epoch 36] Training Batch [35/391]: Loss 5.105310265207663e-05\n",
      "[Epoch 36] Training Batch [36/391]: Loss 3.1394454708788544e-05\n",
      "[Epoch 36] Training Batch [37/391]: Loss 3.0036359021323733e-05\n",
      "[Epoch 36] Training Batch [38/391]: Loss 3.95003444282338e-05\n",
      "[Epoch 36] Training Batch [39/391]: Loss 5.365673132473603e-05\n",
      "[Epoch 36] Training Batch [40/391]: Loss 8.284358045784757e-05\n",
      "[Epoch 36] Training Batch [41/391]: Loss 4.374669515527785e-05\n",
      "[Epoch 36] Training Batch [42/391]: Loss 4.661920320359059e-05\n",
      "[Epoch 36] Training Batch [43/391]: Loss 3.902511525666341e-05\n",
      "[Epoch 36] Training Batch [44/391]: Loss 4.12391746067442e-05\n",
      "[Epoch 36] Training Batch [45/391]: Loss 7.875610026530921e-05\n",
      "[Epoch 36] Training Batch [46/391]: Loss 5.850744491908699e-05\n",
      "[Epoch 36] Training Batch [47/391]: Loss 7.227071182569489e-05\n",
      "[Epoch 36] Training Batch [48/391]: Loss 2.6742136469692923e-05\n",
      "[Epoch 36] Training Batch [49/391]: Loss 3.651412043836899e-05\n",
      "[Epoch 36] Training Batch [50/391]: Loss 5.507253808900714e-05\n",
      "[Epoch 36] Training Batch [51/391]: Loss 4.840991459786892e-05\n",
      "[Epoch 36] Training Batch [52/391]: Loss 1.7191867300425656e-05\n",
      "[Epoch 36] Training Batch [53/391]: Loss 4.482819713302888e-05\n",
      "[Epoch 36] Training Batch [54/391]: Loss 3.5537312214728445e-05\n",
      "[Epoch 36] Training Batch [55/391]: Loss 6.469572690548375e-05\n",
      "[Epoch 36] Training Batch [56/391]: Loss 3.748179005924612e-05\n",
      "[Epoch 36] Training Batch [57/391]: Loss 6.566989759448916e-05\n",
      "[Epoch 36] Training Batch [58/391]: Loss 5.5372147471643984e-05\n",
      "[Epoch 36] Training Batch [59/391]: Loss 6.783741264371201e-05\n",
      "[Epoch 36] Training Batch [60/391]: Loss 0.00014112357166595757\n",
      "[Epoch 36] Training Batch [61/391]: Loss 5.4801035730633885e-05\n",
      "[Epoch 36] Training Batch [62/391]: Loss 4.6502296754624695e-05\n",
      "[Epoch 36] Training Batch [63/391]: Loss 3.761915286304429e-05\n",
      "[Epoch 36] Training Batch [64/391]: Loss 4.6549805119866505e-05\n",
      "[Epoch 36] Training Batch [65/391]: Loss 4.656940291170031e-05\n",
      "[Epoch 36] Training Batch [66/391]: Loss 2.824056355166249e-05\n",
      "[Epoch 36] Training Batch [67/391]: Loss 7.821223698556423e-05\n",
      "[Epoch 36] Training Batch [68/391]: Loss 1.886102654680144e-05\n",
      "[Epoch 36] Training Batch [69/391]: Loss 3.870697400998324e-05\n",
      "[Epoch 36] Training Batch [70/391]: Loss 4.143817932344973e-05\n",
      "[Epoch 36] Training Batch [71/391]: Loss 4.313670433475636e-05\n",
      "[Epoch 36] Training Batch [72/391]: Loss 5.5175180023070425e-05\n",
      "[Epoch 36] Training Batch [73/391]: Loss 4.8775371396914124e-05\n",
      "[Epoch 36] Training Batch [74/391]: Loss 5.4279924370348454e-05\n",
      "[Epoch 36] Training Batch [75/391]: Loss 6.056228812667541e-05\n",
      "[Epoch 36] Training Batch [76/391]: Loss 4.994788469048217e-05\n",
      "[Epoch 36] Training Batch [77/391]: Loss 8.937925304053351e-05\n",
      "[Epoch 36] Training Batch [78/391]: Loss 4.207644087728113e-05\n",
      "[Epoch 36] Training Batch [79/391]: Loss 3.9717033359920606e-05\n",
      "[Epoch 36] Training Batch [80/391]: Loss 5.936366505920887e-05\n",
      "[Epoch 36] Training Batch [81/391]: Loss 4.5080236304784194e-05\n",
      "[Epoch 36] Training Batch [82/391]: Loss 6.100800965214148e-05\n",
      "[Epoch 36] Training Batch [83/391]: Loss 5.1708444516407326e-05\n",
      "[Epoch 36] Training Batch [84/391]: Loss 6.482526805484667e-05\n",
      "[Epoch 36] Training Batch [85/391]: Loss 3.770799230551347e-05\n",
      "[Epoch 36] Training Batch [86/391]: Loss 4.397517841425724e-05\n",
      "[Epoch 36] Training Batch [87/391]: Loss 4.530247679213062e-05\n",
      "[Epoch 36] Training Batch [88/391]: Loss 6.227278208825737e-05\n",
      "[Epoch 36] Training Batch [89/391]: Loss 4.9370613851351663e-05\n",
      "[Epoch 36] Training Batch [90/391]: Loss 3.7481975596165285e-05\n",
      "[Epoch 36] Training Batch [91/391]: Loss 4.042137152282521e-05\n",
      "[Epoch 36] Training Batch [92/391]: Loss 8.599464490544051e-05\n",
      "[Epoch 36] Training Batch [93/391]: Loss 6.630861025769264e-05\n",
      "[Epoch 36] Training Batch [94/391]: Loss 4.280541179468855e-05\n",
      "[Epoch 36] Training Batch [95/391]: Loss 2.6634173991624266e-05\n",
      "[Epoch 36] Training Batch [96/391]: Loss 4.801484101335518e-05\n",
      "[Epoch 36] Training Batch [97/391]: Loss 5.311395580065437e-05\n",
      "[Epoch 36] Training Batch [98/391]: Loss 8.805825927993283e-05\n",
      "[Epoch 36] Training Batch [99/391]: Loss 9.25592757994309e-05\n",
      "[Epoch 36] Training Batch [100/391]: Loss 6.630785355810076e-05\n",
      "[Epoch 36] Training Batch [101/391]: Loss 4.248284312780015e-05\n",
      "[Epoch 36] Training Batch [102/391]: Loss 2.928274443547707e-05\n",
      "[Epoch 36] Training Batch [103/391]: Loss 2.588451752671972e-05\n",
      "[Epoch 36] Training Batch [104/391]: Loss 5.84496337978635e-05\n",
      "[Epoch 36] Training Batch [105/391]: Loss 6.651868170592934e-05\n",
      "[Epoch 36] Training Batch [106/391]: Loss 3.806700260611251e-05\n",
      "[Epoch 36] Training Batch [107/391]: Loss 3.365939483046532e-05\n",
      "[Epoch 36] Training Batch [108/391]: Loss 9.001897706184536e-05\n",
      "[Epoch 36] Training Batch [109/391]: Loss 2.2982612790656276e-05\n",
      "[Epoch 36] Training Batch [110/391]: Loss 2.2804675609222613e-05\n",
      "[Epoch 36] Training Batch [111/391]: Loss 2.4900878997868858e-05\n",
      "[Epoch 36] Training Batch [112/391]: Loss 5.348579725250602e-05\n",
      "[Epoch 36] Training Batch [113/391]: Loss 4.3488198571139947e-05\n",
      "[Epoch 36] Training Batch [114/391]: Loss 7.901538629084826e-05\n",
      "[Epoch 36] Training Batch [115/391]: Loss 3.305685822851956e-05\n",
      "[Epoch 36] Training Batch [116/391]: Loss 7.041044591460377e-05\n",
      "[Epoch 36] Training Batch [117/391]: Loss 4.4088868889957666e-05\n",
      "[Epoch 36] Training Batch [118/391]: Loss 2.0915513232466765e-05\n",
      "[Epoch 36] Training Batch [119/391]: Loss 3.636610563262366e-05\n",
      "[Epoch 36] Training Batch [120/391]: Loss 4.730674481834285e-05\n",
      "[Epoch 36] Training Batch [121/391]: Loss 1.0391156138211954e-05\n",
      "[Epoch 36] Training Batch [122/391]: Loss 4.618897946784273e-05\n",
      "[Epoch 36] Training Batch [123/391]: Loss 3.1808765925234184e-05\n",
      "[Epoch 36] Training Batch [124/391]: Loss 5.221011451794766e-05\n",
      "[Epoch 36] Training Batch [125/391]: Loss 7.972023740876466e-05\n",
      "[Epoch 36] Training Batch [126/391]: Loss 6.41467995592393e-05\n",
      "[Epoch 36] Training Batch [127/391]: Loss 3.935503991669975e-05\n",
      "[Epoch 36] Training Batch [128/391]: Loss 5.018903539166786e-05\n",
      "[Epoch 36] Training Batch [129/391]: Loss 5.818207864649594e-05\n",
      "[Epoch 36] Training Batch [130/391]: Loss 4.3141288188053295e-05\n",
      "[Epoch 36] Training Batch [131/391]: Loss 5.065503137302585e-05\n",
      "[Epoch 36] Training Batch [132/391]: Loss 3.136412487947382e-05\n",
      "[Epoch 36] Training Batch [133/391]: Loss 9.662785305408761e-05\n",
      "[Epoch 36] Training Batch [134/391]: Loss 3.162416396662593e-05\n",
      "[Epoch 36] Training Batch [135/391]: Loss 6.123768980614841e-05\n",
      "[Epoch 36] Training Batch [136/391]: Loss 5.577171759796329e-05\n",
      "[Epoch 36] Training Batch [137/391]: Loss 7.623591227456927e-05\n",
      "[Epoch 36] Training Batch [138/391]: Loss 5.8103865740122274e-05\n",
      "[Epoch 36] Training Batch [139/391]: Loss 3.3701100619509816e-05\n",
      "[Epoch 36] Training Batch [140/391]: Loss 9.977669833460823e-05\n",
      "[Epoch 36] Training Batch [141/391]: Loss 3.310442480142228e-05\n",
      "[Epoch 36] Training Batch [142/391]: Loss 5.698044697055593e-05\n",
      "[Epoch 36] Training Batch [143/391]: Loss 2.5737990654306486e-05\n",
      "[Epoch 36] Training Batch [144/391]: Loss 3.799359910772182e-05\n",
      "[Epoch 36] Training Batch [145/391]: Loss 5.7519257097737864e-05\n",
      "[Epoch 36] Training Batch [146/391]: Loss 7.013264985289425e-05\n",
      "[Epoch 36] Training Batch [147/391]: Loss 5.388333011069335e-05\n",
      "[Epoch 36] Training Batch [148/391]: Loss 5.1109389460179955e-05\n",
      "[Epoch 36] Training Batch [149/391]: Loss 8.419495861744508e-05\n",
      "[Epoch 36] Training Batch [150/391]: Loss 2.9175813324400224e-05\n",
      "[Epoch 36] Training Batch [151/391]: Loss 2.1076182747492567e-05\n",
      "[Epoch 36] Training Batch [152/391]: Loss 5.4100506531540304e-05\n",
      "[Epoch 36] Training Batch [153/391]: Loss 1.3787953321298119e-05\n",
      "[Epoch 36] Training Batch [154/391]: Loss 8.324903319589794e-05\n",
      "[Epoch 36] Training Batch [155/391]: Loss 4.231725324643776e-05\n",
      "[Epoch 36] Training Batch [156/391]: Loss 4.4346732465783134e-05\n",
      "[Epoch 36] Training Batch [157/391]: Loss 1.9519888155627996e-05\n",
      "[Epoch 36] Training Batch [158/391]: Loss 4.949554204358719e-05\n",
      "[Epoch 36] Training Batch [159/391]: Loss 3.391782593098469e-05\n",
      "[Epoch 36] Training Batch [160/391]: Loss 7.345349877141416e-05\n",
      "[Epoch 36] Training Batch [161/391]: Loss 7.778814324410632e-05\n",
      "[Epoch 36] Training Batch [162/391]: Loss 6.868889613542706e-05\n",
      "[Epoch 36] Training Batch [163/391]: Loss 4.745065962197259e-05\n",
      "[Epoch 36] Training Batch [164/391]: Loss 3.7482961488422006e-05\n",
      "[Epoch 36] Training Batch [165/391]: Loss 4.768926737597212e-05\n",
      "[Epoch 36] Training Batch [166/391]: Loss 4.0862611058400944e-05\n",
      "[Epoch 36] Training Batch [167/391]: Loss 5.8175239246338606e-05\n",
      "[Epoch 36] Training Batch [168/391]: Loss 9.394077642355114e-05\n",
      "[Epoch 36] Training Batch [169/391]: Loss 7.580732926726341e-05\n",
      "[Epoch 36] Training Batch [170/391]: Loss 2.971090543724131e-05\n",
      "[Epoch 36] Training Batch [171/391]: Loss 5.256265649222769e-05\n",
      "[Epoch 36] Training Batch [172/391]: Loss 4.1814972064457834e-05\n",
      "[Epoch 36] Training Batch [173/391]: Loss 3.556972660589963e-05\n",
      "[Epoch 36] Training Batch [174/391]: Loss 5.6822515034582466e-05\n",
      "[Epoch 36] Training Batch [175/391]: Loss 6.258851499296725e-05\n",
      "[Epoch 36] Training Batch [176/391]: Loss 2.6895839255303144e-05\n",
      "[Epoch 36] Training Batch [177/391]: Loss 3.529183959471993e-05\n",
      "[Epoch 36] Training Batch [178/391]: Loss 0.0001030803396133706\n",
      "[Epoch 36] Training Batch [179/391]: Loss 6.795415538363159e-05\n",
      "[Epoch 36] Training Batch [180/391]: Loss 6.774627399863675e-05\n",
      "[Epoch 36] Training Batch [181/391]: Loss 7.951912266435102e-05\n",
      "[Epoch 36] Training Batch [182/391]: Loss 3.208890120731667e-05\n",
      "[Epoch 36] Training Batch [183/391]: Loss 4.771646490553394e-05\n",
      "[Epoch 36] Training Batch [184/391]: Loss 4.150216409470886e-05\n",
      "[Epoch 36] Training Batch [185/391]: Loss 5.377293564379215e-05\n",
      "[Epoch 36] Training Batch [186/391]: Loss 3.9650723920203745e-05\n",
      "[Epoch 36] Training Batch [187/391]: Loss 3.152863428113051e-05\n",
      "[Epoch 36] Training Batch [188/391]: Loss 4.964936670148745e-05\n",
      "[Epoch 36] Training Batch [189/391]: Loss 5.309890548232943e-05\n",
      "[Epoch 36] Training Batch [190/391]: Loss 3.76004827558063e-05\n",
      "[Epoch 36] Training Batch [191/391]: Loss 2.9192959118518047e-05\n",
      "[Epoch 36] Training Batch [192/391]: Loss 4.096984412171878e-05\n",
      "[Epoch 36] Training Batch [193/391]: Loss 8.303732465719804e-05\n",
      "[Epoch 36] Training Batch [194/391]: Loss 4.0851238736649975e-05\n",
      "[Epoch 36] Training Batch [195/391]: Loss 7.08465013303794e-05\n",
      "[Epoch 36] Training Batch [196/391]: Loss 2.764096461760346e-05\n",
      "[Epoch 36] Training Batch [197/391]: Loss 4.256114334566519e-05\n",
      "[Epoch 36] Training Batch [198/391]: Loss 5.596837218035944e-05\n",
      "[Epoch 36] Training Batch [199/391]: Loss 2.920355836977251e-05\n",
      "[Epoch 36] Training Batch [200/391]: Loss 0.00011853419709950686\n",
      "[Epoch 36] Training Batch [201/391]: Loss 5.6850949476938695e-05\n",
      "[Epoch 36] Training Batch [202/391]: Loss 6.558353925356641e-05\n",
      "[Epoch 36] Training Batch [203/391]: Loss 5.3468214900931343e-05\n",
      "[Epoch 36] Training Batch [204/391]: Loss 6.648155249422416e-05\n",
      "[Epoch 36] Training Batch [205/391]: Loss 2.274256621603854e-05\n",
      "[Epoch 36] Training Batch [206/391]: Loss 5.262313788989559e-05\n",
      "[Epoch 36] Training Batch [207/391]: Loss 5.1151007937733084e-05\n",
      "[Epoch 36] Training Batch [208/391]: Loss 4.218814137857407e-05\n",
      "[Epoch 36] Training Batch [209/391]: Loss 4.99461566505488e-05\n",
      "[Epoch 36] Training Batch [210/391]: Loss 4.110612644581124e-05\n",
      "[Epoch 36] Training Batch [211/391]: Loss 4.225522934575565e-05\n",
      "[Epoch 36] Training Batch [212/391]: Loss 4.541070302366279e-05\n",
      "[Epoch 36] Training Batch [213/391]: Loss 3.587118771974929e-05\n",
      "[Epoch 36] Training Batch [214/391]: Loss 3.779729377129115e-05\n",
      "[Epoch 36] Training Batch [215/391]: Loss 6.616006430704147e-05\n",
      "[Epoch 36] Training Batch [216/391]: Loss 4.272091609891504e-05\n",
      "[Epoch 36] Training Batch [217/391]: Loss 2.9034858016530052e-05\n",
      "[Epoch 36] Training Batch [218/391]: Loss 3.6241945053916425e-05\n",
      "[Epoch 36] Training Batch [219/391]: Loss 7.913477747933939e-05\n",
      "[Epoch 36] Training Batch [220/391]: Loss 7.874995935708284e-05\n",
      "[Epoch 36] Training Batch [221/391]: Loss 5.603690442512743e-05\n",
      "[Epoch 36] Training Batch [222/391]: Loss 7.381824252661318e-05\n",
      "[Epoch 36] Training Batch [223/391]: Loss 3.479240695014596e-05\n",
      "[Epoch 36] Training Batch [224/391]: Loss 2.3260443413164467e-05\n",
      "[Epoch 36] Training Batch [225/391]: Loss 4.427059684530832e-05\n",
      "[Epoch 36] Training Batch [226/391]: Loss 3.637866393546574e-05\n",
      "[Epoch 36] Training Batch [227/391]: Loss 3.869513966492377e-05\n",
      "[Epoch 36] Training Batch [228/391]: Loss 6.595243758056313e-05\n",
      "[Epoch 36] Training Batch [229/391]: Loss 7.666557212360203e-05\n",
      "[Epoch 36] Training Batch [230/391]: Loss 2.3905553462100215e-05\n",
      "[Epoch 36] Training Batch [231/391]: Loss 5.741618952015415e-05\n",
      "[Epoch 36] Training Batch [232/391]: Loss 1.8526277926866896e-05\n",
      "[Epoch 36] Training Batch [233/391]: Loss 8.173135574907064e-05\n",
      "[Epoch 36] Training Batch [234/391]: Loss 2.071211019938346e-05\n",
      "[Epoch 36] Training Batch [235/391]: Loss 2.5900069886120036e-05\n",
      "[Epoch 36] Training Batch [236/391]: Loss 5.311551649356261e-05\n",
      "[Epoch 36] Training Batch [237/391]: Loss 9.151880658464506e-05\n",
      "[Epoch 36] Training Batch [238/391]: Loss 6.248857243917882e-05\n",
      "[Epoch 36] Training Batch [239/391]: Loss 6.804293661843985e-05\n",
      "[Epoch 36] Training Batch [240/391]: Loss 3.854209353448823e-05\n",
      "[Epoch 36] Training Batch [241/391]: Loss 4.011563214589842e-05\n",
      "[Epoch 36] Training Batch [242/391]: Loss 3.898757859133184e-05\n",
      "[Epoch 36] Training Batch [243/391]: Loss 7.811541581759229e-05\n",
      "[Epoch 36] Training Batch [244/391]: Loss 2.4483935703756288e-05\n",
      "[Epoch 36] Training Batch [245/391]: Loss 3.6736026231665164e-05\n",
      "[Epoch 36] Training Batch [246/391]: Loss 4.5442095142789185e-05\n",
      "[Epoch 36] Training Batch [247/391]: Loss 5.981998401694e-05\n",
      "[Epoch 36] Training Batch [248/391]: Loss 4.911636278848164e-05\n",
      "[Epoch 36] Training Batch [249/391]: Loss 6.359934195643291e-05\n",
      "[Epoch 36] Training Batch [250/391]: Loss 5.010214590583928e-05\n",
      "[Epoch 36] Training Batch [251/391]: Loss 3.8989946915535256e-05\n",
      "[Epoch 36] Training Batch [252/391]: Loss 2.9151591661502607e-05\n",
      "[Epoch 36] Training Batch [253/391]: Loss 7.41707845008932e-05\n",
      "[Epoch 36] Training Batch [254/391]: Loss 2.314411904080771e-05\n",
      "[Epoch 36] Training Batch [255/391]: Loss 3.610505518736318e-05\n",
      "[Epoch 36] Training Batch [256/391]: Loss 4.3282063415972516e-05\n",
      "[Epoch 36] Training Batch [257/391]: Loss 4.4962325773667544e-05\n",
      "[Epoch 36] Training Batch [258/391]: Loss 5.370310464059003e-05\n",
      "[Epoch 36] Training Batch [259/391]: Loss 5.5156229791464284e-05\n",
      "[Epoch 36] Training Batch [260/391]: Loss 4.2801268136827275e-05\n",
      "[Epoch 36] Training Batch [261/391]: Loss 8.154795796144754e-05\n",
      "[Epoch 36] Training Batch [262/391]: Loss 2.0472480173339136e-05\n",
      "[Epoch 36] Training Batch [263/391]: Loss 3.918211950804107e-05\n",
      "[Epoch 36] Training Batch [264/391]: Loss 3.1137904443312436e-05\n",
      "[Epoch 36] Training Batch [265/391]: Loss 2.4639792172820307e-05\n",
      "[Epoch 36] Training Batch [266/391]: Loss 5.616022463073023e-05\n",
      "[Epoch 36] Training Batch [267/391]: Loss 4.257646287442185e-05\n",
      "[Epoch 36] Training Batch [268/391]: Loss 2.584320645837579e-05\n",
      "[Epoch 36] Training Batch [269/391]: Loss 1.296081882173894e-05\n",
      "[Epoch 36] Training Batch [270/391]: Loss 3.120091787423007e-05\n",
      "[Epoch 36] Training Batch [271/391]: Loss 7.709074270678684e-05\n",
      "[Epoch 36] Training Batch [272/391]: Loss 5.6843655329430476e-05\n",
      "[Epoch 36] Training Batch [273/391]: Loss 4.5923065044917166e-05\n",
      "[Epoch 36] Training Batch [274/391]: Loss 2.730909181991592e-05\n",
      "[Epoch 36] Training Batch [275/391]: Loss 7.218764949357137e-05\n",
      "[Epoch 36] Training Batch [276/391]: Loss 5.637876165565103e-05\n",
      "[Epoch 36] Training Batch [277/391]: Loss 3.103018752881326e-05\n",
      "[Epoch 36] Training Batch [278/391]: Loss 2.305731140950229e-05\n",
      "[Epoch 36] Training Batch [279/391]: Loss 3.200202263542451e-05\n",
      "[Epoch 36] Training Batch [280/391]: Loss 4.036254540551454e-05\n",
      "[Epoch 36] Training Batch [281/391]: Loss 0.00010056048631668091\n",
      "[Epoch 36] Training Batch [282/391]: Loss 1.7008616850944236e-05\n",
      "[Epoch 36] Training Batch [283/391]: Loss 7.818048470653594e-05\n",
      "[Epoch 36] Training Batch [284/391]: Loss 5.4466869187308475e-05\n",
      "[Epoch 36] Training Batch [285/391]: Loss 3.1738076359033585e-05\n",
      "[Epoch 36] Training Batch [286/391]: Loss 3.5884950193576515e-05\n",
      "[Epoch 36] Training Batch [287/391]: Loss 3.230708898627199e-05\n",
      "[Epoch 36] Training Batch [288/391]: Loss 5.6263976148329675e-05\n",
      "[Epoch 36] Training Batch [289/391]: Loss 7.5650961662177e-05\n",
      "[Epoch 36] Training Batch [290/391]: Loss 4.4544936827151105e-05\n",
      "[Epoch 36] Training Batch [291/391]: Loss 2.9460878067766316e-05\n",
      "[Epoch 36] Training Batch [292/391]: Loss 7.309130887733772e-05\n",
      "[Epoch 36] Training Batch [293/391]: Loss 3.533015478751622e-05\n",
      "[Epoch 36] Training Batch [294/391]: Loss 3.872759043588303e-05\n",
      "[Epoch 36] Training Batch [295/391]: Loss 3.987843228969723e-05\n",
      "[Epoch 36] Training Batch [296/391]: Loss 2.5542703951941803e-05\n",
      "[Epoch 36] Training Batch [297/391]: Loss 2.7227606551605277e-05\n",
      "[Epoch 36] Training Batch [298/391]: Loss 2.7250289349467494e-05\n",
      "[Epoch 36] Training Batch [299/391]: Loss 4.501055445871316e-05\n",
      "[Epoch 36] Training Batch [300/391]: Loss 4.3154846935067326e-05\n",
      "[Epoch 36] Training Batch [301/391]: Loss 4.68294856545981e-05\n",
      "[Epoch 36] Training Batch [302/391]: Loss 2.3798404072294943e-05\n",
      "[Epoch 36] Training Batch [303/391]: Loss 3.518801531754434e-05\n",
      "[Epoch 36] Training Batch [304/391]: Loss 4.953528696205467e-05\n",
      "[Epoch 36] Training Batch [305/391]: Loss 4.0767539758235216e-05\n",
      "[Epoch 36] Training Batch [306/391]: Loss 4.259481283952482e-05\n",
      "[Epoch 36] Training Batch [307/391]: Loss 4.599846579367295e-05\n",
      "[Epoch 36] Training Batch [308/391]: Loss 1.4460628335655201e-05\n",
      "[Epoch 36] Training Batch [309/391]: Loss 5.301426426740363e-05\n",
      "[Epoch 36] Training Batch [310/391]: Loss 2.744614721450489e-05\n",
      "[Epoch 36] Training Batch [311/391]: Loss 4.1231411159969866e-05\n",
      "[Epoch 36] Training Batch [312/391]: Loss 3.056681453017518e-05\n",
      "[Epoch 36] Training Batch [313/391]: Loss 5.4281899792840704e-05\n",
      "[Epoch 36] Training Batch [314/391]: Loss 6.122725608292967e-05\n",
      "[Epoch 36] Training Batch [315/391]: Loss 1.103057547879871e-05\n",
      "[Epoch 36] Training Batch [316/391]: Loss 3.7172612792346627e-05\n",
      "[Epoch 36] Training Batch [317/391]: Loss 6.166948878671974e-05\n",
      "[Epoch 36] Training Batch [318/391]: Loss 2.6945497666019946e-05\n",
      "[Epoch 36] Training Batch [319/391]: Loss 5.9887937823077664e-05\n",
      "[Epoch 36] Training Batch [320/391]: Loss 5.745624366682023e-05\n",
      "[Epoch 36] Training Batch [321/391]: Loss 7.245894812513143e-05\n",
      "[Epoch 36] Training Batch [322/391]: Loss 6.369329639710486e-05\n",
      "[Epoch 36] Training Batch [323/391]: Loss 5.4152355005498976e-05\n",
      "[Epoch 36] Training Batch [324/391]: Loss 6.925739580765367e-05\n",
      "[Epoch 36] Training Batch [325/391]: Loss 4.598690429702401e-05\n",
      "[Epoch 36] Training Batch [326/391]: Loss 9.417249384569004e-05\n",
      "[Epoch 36] Training Batch [327/391]: Loss 3.4147924452554435e-05\n",
      "[Epoch 36] Training Batch [328/391]: Loss 6.436614057747647e-05\n",
      "[Epoch 36] Training Batch [329/391]: Loss 6.12812364124693e-05\n",
      "[Epoch 36] Training Batch [330/391]: Loss 1.5761490431032144e-05\n",
      "[Epoch 36] Training Batch [331/391]: Loss 4.387734588817693e-05\n",
      "[Epoch 36] Training Batch [332/391]: Loss 4.0126004023477435e-05\n",
      "[Epoch 36] Training Batch [333/391]: Loss 3.519012898323126e-05\n",
      "[Epoch 36] Training Batch [334/391]: Loss 5.496068115462549e-05\n",
      "[Epoch 36] Training Batch [335/391]: Loss 3.128164098598063e-05\n",
      "[Epoch 36] Training Batch [336/391]: Loss 3.436073166085407e-05\n",
      "[Epoch 36] Training Batch [337/391]: Loss 7.186755101429299e-05\n",
      "[Epoch 36] Training Batch [338/391]: Loss 4.163641278864816e-05\n",
      "[Epoch 36] Training Batch [339/391]: Loss 5.628215876640752e-05\n",
      "[Epoch 36] Training Batch [340/391]: Loss 3.380179623491131e-05\n",
      "[Epoch 36] Training Batch [341/391]: Loss 6.66615305817686e-05\n",
      "[Epoch 36] Training Batch [342/391]: Loss 3.674940307973884e-05\n",
      "[Epoch 36] Training Batch [343/391]: Loss 4.821811307920143e-05\n",
      "[Epoch 36] Training Batch [344/391]: Loss 6.677231431240216e-05\n",
      "[Epoch 36] Training Batch [345/391]: Loss 3.9273676520679146e-05\n",
      "[Epoch 36] Training Batch [346/391]: Loss 5.030012835050002e-05\n",
      "[Epoch 36] Training Batch [347/391]: Loss 9.414022497367114e-05\n",
      "[Epoch 36] Training Batch [348/391]: Loss 5.220980892772786e-05\n",
      "[Epoch 36] Training Batch [349/391]: Loss 4.5199496526038274e-05\n",
      "[Epoch 36] Training Batch [350/391]: Loss 7.872422429500148e-05\n",
      "[Epoch 36] Training Batch [351/391]: Loss 5.5521479225717485e-05\n",
      "[Epoch 36] Training Batch [352/391]: Loss 7.275670213857666e-05\n",
      "[Epoch 36] Training Batch [353/391]: Loss 5.9334786783438176e-05\n",
      "[Epoch 36] Training Batch [354/391]: Loss 4.926579276798293e-05\n",
      "[Epoch 36] Training Batch [355/391]: Loss 4.053568409290165e-05\n",
      "[Epoch 36] Training Batch [356/391]: Loss 3.7086603697389364e-05\n",
      "[Epoch 36] Training Batch [357/391]: Loss 6.343444692902267e-05\n",
      "[Epoch 36] Training Batch [358/391]: Loss 6.99096272001043e-05\n",
      "[Epoch 36] Training Batch [359/391]: Loss 2.08405836019665e-05\n",
      "[Epoch 36] Training Batch [360/391]: Loss 3.451502198004164e-05\n",
      "[Epoch 36] Training Batch [361/391]: Loss 4.217937384964898e-05\n",
      "[Epoch 36] Training Batch [362/391]: Loss 6.830236816313118e-05\n",
      "[Epoch 36] Training Batch [363/391]: Loss 3.0091068765614182e-05\n",
      "[Epoch 36] Training Batch [364/391]: Loss 6.781593401683494e-05\n",
      "[Epoch 36] Training Batch [365/391]: Loss 3.136018858640455e-05\n",
      "[Epoch 36] Training Batch [366/391]: Loss 2.924380351032596e-05\n",
      "[Epoch 36] Training Batch [367/391]: Loss 1.8949427612824365e-05\n",
      "[Epoch 36] Training Batch [368/391]: Loss 2.6795703888637945e-05\n",
      "[Epoch 36] Training Batch [369/391]: Loss 9.239308565156534e-05\n",
      "[Epoch 36] Training Batch [370/391]: Loss 5.198276267037727e-05\n",
      "[Epoch 36] Training Batch [371/391]: Loss 3.441810258664191e-05\n",
      "[Epoch 36] Training Batch [372/391]: Loss 3.826312240562402e-05\n",
      "[Epoch 36] Training Batch [373/391]: Loss 5.272549969959073e-05\n",
      "[Epoch 36] Training Batch [374/391]: Loss 3.9282494981307536e-05\n",
      "[Epoch 36] Training Batch [375/391]: Loss 3.1899518944555894e-05\n",
      "[Epoch 36] Training Batch [376/391]: Loss 5.742820940213278e-05\n",
      "[Epoch 36] Training Batch [377/391]: Loss 2.5950075723812915e-05\n",
      "[Epoch 36] Training Batch [378/391]: Loss 4.8579091526335105e-05\n",
      "[Epoch 36] Training Batch [379/391]: Loss 3.9223879866767675e-05\n",
      "[Epoch 36] Training Batch [380/391]: Loss 4.2918927647406235e-05\n",
      "[Epoch 36] Training Batch [381/391]: Loss 6.823986041126773e-05\n",
      "[Epoch 36] Training Batch [382/391]: Loss 7.451278361259028e-05\n",
      "[Epoch 36] Training Batch [383/391]: Loss 2.9428354537230916e-05\n",
      "[Epoch 36] Training Batch [384/391]: Loss 5.193276592763141e-05\n",
      "[Epoch 36] Training Batch [385/391]: Loss 4.992887988919392e-05\n",
      "[Epoch 36] Training Batch [386/391]: Loss 3.722191831911914e-05\n",
      "[Epoch 36] Training Batch [387/391]: Loss 3.522183033055626e-05\n",
      "[Epoch 36] Training Batch [388/391]: Loss 5.7952693168772385e-05\n",
      "[Epoch 36] Training Batch [389/391]: Loss 4.84289375890512e-05\n",
      "[Epoch 36] Training Batch [390/391]: Loss 4.4815729779656976e-05\n",
      "[Epoch 36] Training Batch [391/391]: Loss 3.8774767745053396e-05\n",
      "Epoch 36 - Train Loss: 0.0000\n",
      "*********  Epoch 37/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37] Training Batch [1/391]: Loss 7.171597826527432e-05\n",
      "[Epoch 37] Training Batch [2/391]: Loss 4.9001522711478174e-05\n",
      "[Epoch 37] Training Batch [3/391]: Loss 6.359748658724129e-05\n",
      "[Epoch 37] Training Batch [4/391]: Loss 4.8093403165694326e-05\n",
      "[Epoch 37] Training Batch [5/391]: Loss 5.196608981350437e-05\n",
      "[Epoch 37] Training Batch [6/391]: Loss 5.9972884628223255e-05\n",
      "[Epoch 37] Training Batch [7/391]: Loss 2.5899013053276576e-05\n",
      "[Epoch 37] Training Batch [8/391]: Loss 3.232197923352942e-05\n",
      "[Epoch 37] Training Batch [9/391]: Loss 5.429506200016476e-05\n",
      "[Epoch 37] Training Batch [10/391]: Loss 4.040412022732198e-05\n",
      "[Epoch 37] Training Batch [11/391]: Loss 6.890502118039876e-05\n",
      "[Epoch 37] Training Batch [12/391]: Loss 3.82216639991384e-05\n",
      "[Epoch 37] Training Batch [13/391]: Loss 2.8596981792361476e-05\n",
      "[Epoch 37] Training Batch [14/391]: Loss 2.1772108084405772e-05\n",
      "[Epoch 37] Training Batch [15/391]: Loss 1.9867129594786093e-05\n",
      "[Epoch 37] Training Batch [16/391]: Loss 4.4874530431116e-05\n",
      "[Epoch 37] Training Batch [17/391]: Loss 6.780192779842764e-05\n",
      "[Epoch 37] Training Batch [18/391]: Loss 3.4633900213520974e-05\n",
      "[Epoch 37] Training Batch [19/391]: Loss 2.7307472919346765e-05\n",
      "[Epoch 37] Training Batch [20/391]: Loss 4.740149597637355e-05\n",
      "[Epoch 37] Training Batch [21/391]: Loss 5.768693517893553e-05\n",
      "[Epoch 37] Training Batch [22/391]: Loss 4.959078069077805e-05\n",
      "[Epoch 37] Training Batch [23/391]: Loss 4.166050348430872e-05\n",
      "[Epoch 37] Training Batch [24/391]: Loss 4.707144762505777e-05\n",
      "[Epoch 37] Training Batch [25/391]: Loss 2.2864342099637724e-05\n",
      "[Epoch 37] Training Batch [26/391]: Loss 4.1030529246199876e-05\n",
      "[Epoch 37] Training Batch [27/391]: Loss 2.907259840867482e-05\n",
      "[Epoch 37] Training Batch [28/391]: Loss 6.854782259324566e-05\n",
      "[Epoch 37] Training Batch [29/391]: Loss 3.571041816030629e-05\n",
      "[Epoch 37] Training Batch [30/391]: Loss 4.390103276818991e-05\n",
      "[Epoch 37] Training Batch [31/391]: Loss 1.3091457731206901e-05\n",
      "[Epoch 37] Training Batch [32/391]: Loss 5.083027281216346e-05\n",
      "[Epoch 37] Training Batch [33/391]: Loss 5.283144491841085e-05\n",
      "[Epoch 37] Training Batch [34/391]: Loss 3.839694909402169e-05\n",
      "[Epoch 37] Training Batch [35/391]: Loss 2.903690437960904e-05\n",
      "[Epoch 37] Training Batch [36/391]: Loss 4.3234842451056466e-05\n",
      "[Epoch 37] Training Batch [37/391]: Loss 4.254857049090788e-05\n",
      "[Epoch 37] Training Batch [38/391]: Loss 5.299000986269675e-05\n",
      "[Epoch 37] Training Batch [39/391]: Loss 3.343472781125456e-05\n",
      "[Epoch 37] Training Batch [40/391]: Loss 2.9825869205524214e-05\n",
      "[Epoch 37] Training Batch [41/391]: Loss 2.9422644729493186e-05\n",
      "[Epoch 37] Training Batch [42/391]: Loss 3.226744229323231e-05\n",
      "[Epoch 37] Training Batch [43/391]: Loss 6.483894685516134e-05\n",
      "[Epoch 37] Training Batch [44/391]: Loss 1.536117088107858e-05\n",
      "[Epoch 37] Training Batch [45/391]: Loss 4.730857472168282e-05\n",
      "[Epoch 37] Training Batch [46/391]: Loss 5.3092353482497856e-05\n",
      "[Epoch 37] Training Batch [47/391]: Loss 2.470389699738007e-05\n",
      "[Epoch 37] Training Batch [48/391]: Loss 5.590072396444157e-05\n",
      "[Epoch 37] Training Batch [49/391]: Loss 2.8636737624765374e-05\n",
      "[Epoch 37] Training Batch [50/391]: Loss 3.523223494994454e-05\n",
      "[Epoch 37] Training Batch [51/391]: Loss 5.1041977712884545e-05\n",
      "[Epoch 37] Training Batch [52/391]: Loss 4.285223621991463e-05\n",
      "[Epoch 37] Training Batch [53/391]: Loss 3.931442188331857e-05\n",
      "[Epoch 37] Training Batch [54/391]: Loss 2.9765100407530554e-05\n",
      "[Epoch 37] Training Batch [55/391]: Loss 4.874071237281896e-05\n",
      "[Epoch 37] Training Batch [56/391]: Loss 3.114859282504767e-05\n",
      "[Epoch 37] Training Batch [57/391]: Loss 1.5113751942408271e-05\n",
      "[Epoch 37] Training Batch [58/391]: Loss 4.3017324060201645e-05\n",
      "[Epoch 37] Training Batch [59/391]: Loss 4.6642846427857876e-05\n",
      "[Epoch 37] Training Batch [60/391]: Loss 7.854335126467049e-05\n",
      "[Epoch 37] Training Batch [61/391]: Loss 2.769966158666648e-05\n",
      "[Epoch 37] Training Batch [62/391]: Loss 4.153262489126064e-05\n",
      "[Epoch 37] Training Batch [63/391]: Loss 5.1088347390759736e-05\n",
      "[Epoch 37] Training Batch [64/391]: Loss 4.223454016027972e-05\n",
      "[Epoch 37] Training Batch [65/391]: Loss 2.8710568585665897e-05\n",
      "[Epoch 37] Training Batch [66/391]: Loss 3.613503940869123e-05\n",
      "[Epoch 37] Training Batch [67/391]: Loss 6.949137605261058e-05\n",
      "[Epoch 37] Training Batch [68/391]: Loss 4.19021125708241e-05\n",
      "[Epoch 37] Training Batch [69/391]: Loss 5.027464430895634e-05\n",
      "[Epoch 37] Training Batch [70/391]: Loss 3.375204323674552e-05\n",
      "[Epoch 37] Training Batch [71/391]: Loss 6.180032505653799e-05\n",
      "[Epoch 37] Training Batch [72/391]: Loss 4.835557047044858e-05\n",
      "[Epoch 37] Training Batch [73/391]: Loss 4.5046122977510095e-05\n",
      "[Epoch 37] Training Batch [74/391]: Loss 2.979560122184921e-05\n",
      "[Epoch 37] Training Batch [75/391]: Loss 1.370056088489946e-05\n",
      "[Epoch 37] Training Batch [76/391]: Loss 4.791973333340138e-05\n",
      "[Epoch 37] Training Batch [77/391]: Loss 3.7913061532890424e-05\n",
      "[Epoch 37] Training Batch [78/391]: Loss 4.425803490448743e-05\n",
      "[Epoch 37] Training Batch [79/391]: Loss 2.5782568627619185e-05\n",
      "[Epoch 37] Training Batch [80/391]: Loss 2.51616020250367e-05\n",
      "[Epoch 37] Training Batch [81/391]: Loss 4.7013665607664734e-05\n",
      "[Epoch 37] Training Batch [82/391]: Loss 3.055046545341611e-05\n",
      "[Epoch 37] Training Batch [83/391]: Loss 5.802380837849341e-05\n",
      "[Epoch 37] Training Batch [84/391]: Loss 5.154857353772968e-05\n",
      "[Epoch 37] Training Batch [85/391]: Loss 4.19690586568322e-05\n",
      "[Epoch 37] Training Batch [86/391]: Loss 4.4417582103051245e-05\n",
      "[Epoch 37] Training Batch [87/391]: Loss 5.831452290294692e-05\n",
      "[Epoch 37] Training Batch [88/391]: Loss 3.309962266939692e-05\n",
      "[Epoch 37] Training Batch [89/391]: Loss 4.476464164326899e-05\n",
      "[Epoch 37] Training Batch [90/391]: Loss 2.9340615583350882e-05\n",
      "[Epoch 37] Training Batch [91/391]: Loss 2.958756340376567e-05\n",
      "[Epoch 37] Training Batch [92/391]: Loss 5.5090567911975086e-05\n",
      "[Epoch 37] Training Batch [93/391]: Loss 3.86636056646239e-05\n",
      "[Epoch 37] Training Batch [94/391]: Loss 7.294042006833479e-05\n",
      "[Epoch 37] Training Batch [95/391]: Loss 2.7024127120967023e-05\n",
      "[Epoch 37] Training Batch [96/391]: Loss 4.217116656946018e-05\n",
      "[Epoch 37] Training Batch [97/391]: Loss 2.9082295441185124e-05\n",
      "[Epoch 37] Training Batch [98/391]: Loss 3.6259843909647316e-05\n",
      "[Epoch 37] Training Batch [99/391]: Loss 2.0326560843386687e-05\n",
      "[Epoch 37] Training Batch [100/391]: Loss 4.899445048067719e-05\n",
      "[Epoch 37] Training Batch [101/391]: Loss 4.389182504382916e-05\n",
      "[Epoch 37] Training Batch [102/391]: Loss 2.5794515750021674e-05\n",
      "[Epoch 37] Training Batch [103/391]: Loss 3.2567899324931204e-05\n",
      "[Epoch 37] Training Batch [104/391]: Loss 3.093749182880856e-05\n",
      "[Epoch 37] Training Batch [105/391]: Loss 7.238153193611652e-05\n",
      "[Epoch 37] Training Batch [106/391]: Loss 6.524025229737163e-05\n",
      "[Epoch 37] Training Batch [107/391]: Loss 4.620536128641106e-05\n",
      "[Epoch 37] Training Batch [108/391]: Loss 2.776028668449726e-05\n",
      "[Epoch 37] Training Batch [109/391]: Loss 3.8645073800580576e-05\n",
      "[Epoch 37] Training Batch [110/391]: Loss 4.562395406537689e-05\n",
      "[Epoch 37] Training Batch [111/391]: Loss 4.2406933062011376e-05\n",
      "[Epoch 37] Training Batch [112/391]: Loss 3.579428812372498e-05\n",
      "[Epoch 37] Training Batch [113/391]: Loss 4.673139483202249e-05\n",
      "[Epoch 37] Training Batch [114/391]: Loss 2.2943100702832453e-05\n",
      "[Epoch 37] Training Batch [115/391]: Loss 5.9350673836888745e-05\n",
      "[Epoch 37] Training Batch [116/391]: Loss 4.133921174798161e-05\n",
      "[Epoch 37] Training Batch [117/391]: Loss 2.5811485102167353e-05\n",
      "[Epoch 37] Training Batch [118/391]: Loss 3.257466232753359e-05\n",
      "[Epoch 37] Training Batch [119/391]: Loss 2.93529374175705e-05\n",
      "[Epoch 37] Training Batch [120/391]: Loss 4.827231168746948e-05\n",
      "[Epoch 37] Training Batch [121/391]: Loss 5.8179983170703053e-05\n",
      "[Epoch 37] Training Batch [122/391]: Loss 1.9000197426066734e-05\n",
      "[Epoch 37] Training Batch [123/391]: Loss 4.583998452289961e-05\n",
      "[Epoch 37] Training Batch [124/391]: Loss 6.423537706723437e-05\n",
      "[Epoch 37] Training Batch [125/391]: Loss 3.861402001348324e-05\n",
      "[Epoch 37] Training Batch [126/391]: Loss 5.04112322232686e-05\n",
      "[Epoch 37] Training Batch [127/391]: Loss 5.3913547162665054e-05\n",
      "[Epoch 37] Training Batch [128/391]: Loss 2.382625825703144e-05\n",
      "[Epoch 37] Training Batch [129/391]: Loss 4.199654722469859e-05\n",
      "[Epoch 37] Training Batch [130/391]: Loss 4.564168193610385e-05\n",
      "[Epoch 37] Training Batch [131/391]: Loss 3.329626269987784e-05\n",
      "[Epoch 37] Training Batch [132/391]: Loss 3.791298877331428e-05\n",
      "[Epoch 37] Training Batch [133/391]: Loss 3.3235264709219337e-05\n",
      "[Epoch 37] Training Batch [134/391]: Loss 4.283984162611887e-05\n",
      "[Epoch 37] Training Batch [135/391]: Loss 4.9823345761979e-05\n",
      "[Epoch 37] Training Batch [136/391]: Loss 3.372407809365541e-05\n",
      "[Epoch 37] Training Batch [137/391]: Loss 5.874348426004872e-05\n",
      "[Epoch 37] Training Batch [138/391]: Loss 5.7510947954142466e-05\n",
      "[Epoch 37] Training Batch [139/391]: Loss 3.771984120248817e-05\n",
      "[Epoch 37] Training Batch [140/391]: Loss 3.221715087420307e-05\n",
      "[Epoch 37] Training Batch [141/391]: Loss 3.139329783152789e-05\n",
      "[Epoch 37] Training Batch [142/391]: Loss 2.709543696255423e-05\n",
      "[Epoch 37] Training Batch [143/391]: Loss 3.831364665529691e-05\n",
      "[Epoch 37] Training Batch [144/391]: Loss 2.9604289011331275e-05\n",
      "[Epoch 37] Training Batch [145/391]: Loss 1.7261296306969598e-05\n",
      "[Epoch 37] Training Batch [146/391]: Loss 3.367446333868429e-05\n",
      "[Epoch 37] Training Batch [147/391]: Loss 3.443563400651328e-05\n",
      "[Epoch 37] Training Batch [148/391]: Loss 2.6382145733805373e-05\n",
      "[Epoch 37] Training Batch [149/391]: Loss 2.3477334252675064e-05\n",
      "[Epoch 37] Training Batch [150/391]: Loss 5.393668106989935e-05\n",
      "[Epoch 37] Training Batch [151/391]: Loss 4.3665429984685034e-05\n",
      "[Epoch 37] Training Batch [152/391]: Loss 3.159481639158912e-05\n",
      "[Epoch 37] Training Batch [153/391]: Loss 8.829060971038416e-05\n",
      "[Epoch 37] Training Batch [154/391]: Loss 5.386420161812566e-05\n",
      "[Epoch 37] Training Batch [155/391]: Loss 3.402161746635102e-05\n",
      "[Epoch 37] Training Batch [156/391]: Loss 6.798790855100378e-05\n",
      "[Epoch 37] Training Batch [157/391]: Loss 6.603012298000976e-05\n",
      "[Epoch 37] Training Batch [158/391]: Loss 5.371330553316511e-05\n",
      "[Epoch 37] Training Batch [159/391]: Loss 2.9838429327355698e-05\n",
      "[Epoch 37] Training Batch [160/391]: Loss 1.5088014151842799e-05\n",
      "[Epoch 37] Training Batch [161/391]: Loss 3.3898417314048856e-05\n",
      "[Epoch 37] Training Batch [162/391]: Loss 5.4080377594800666e-05\n",
      "[Epoch 37] Training Batch [163/391]: Loss 3.466917041805573e-05\n",
      "[Epoch 37] Training Batch [164/391]: Loss 7.014104630798101e-05\n",
      "[Epoch 37] Training Batch [165/391]: Loss 4.736287519335747e-05\n",
      "[Epoch 37] Training Batch [166/391]: Loss 6.97885116096586e-05\n",
      "[Epoch 37] Training Batch [167/391]: Loss 2.9489610824384727e-05\n",
      "[Epoch 37] Training Batch [168/391]: Loss 3.306677535874769e-05\n",
      "[Epoch 37] Training Batch [169/391]: Loss 2.763981137832161e-05\n",
      "[Epoch 37] Training Batch [170/391]: Loss 4.513422754826024e-05\n",
      "[Epoch 37] Training Batch [171/391]: Loss 6.110023241490126e-05\n",
      "[Epoch 37] Training Batch [172/391]: Loss 3.5514614864951e-05\n",
      "[Epoch 37] Training Batch [173/391]: Loss 7.016518793534487e-05\n",
      "[Epoch 37] Training Batch [174/391]: Loss 2.5472159904893488e-05\n",
      "[Epoch 37] Training Batch [175/391]: Loss 2.2766316760680638e-05\n",
      "[Epoch 37] Training Batch [176/391]: Loss 2.57146057265345e-05\n",
      "[Epoch 37] Training Batch [177/391]: Loss 6.298869993770495e-05\n",
      "[Epoch 37] Training Batch [178/391]: Loss 4.547974094748497e-05\n",
      "[Epoch 37] Training Batch [179/391]: Loss 4.006371455034241e-05\n",
      "[Epoch 37] Training Batch [180/391]: Loss 6.49203357170336e-05\n",
      "[Epoch 37] Training Batch [181/391]: Loss 4.625305882655084e-05\n",
      "[Epoch 37] Training Batch [182/391]: Loss 6.126156949903816e-05\n",
      "[Epoch 37] Training Batch [183/391]: Loss 2.9067181458231062e-05\n",
      "[Epoch 37] Training Batch [184/391]: Loss 4.1643572330940515e-05\n",
      "[Epoch 37] Training Batch [185/391]: Loss 1.1681867363222409e-05\n",
      "[Epoch 37] Training Batch [186/391]: Loss 4.30991240136791e-05\n",
      "[Epoch 37] Training Batch [187/391]: Loss 3.773510252358392e-05\n",
      "[Epoch 37] Training Batch [188/391]: Loss 7.956654735608026e-05\n",
      "[Epoch 37] Training Batch [189/391]: Loss 3.054286207770929e-05\n",
      "[Epoch 37] Training Batch [190/391]: Loss 2.4807328372844495e-05\n",
      "[Epoch 37] Training Batch [191/391]: Loss 5.524218067876063e-05\n",
      "[Epoch 37] Training Batch [192/391]: Loss 5.5556542065460235e-05\n",
      "[Epoch 37] Training Batch [193/391]: Loss 4.032822107546963e-05\n",
      "[Epoch 37] Training Batch [194/391]: Loss 1.5017620171420276e-05\n",
      "[Epoch 37] Training Batch [195/391]: Loss 4.375097341835499e-05\n",
      "[Epoch 37] Training Batch [196/391]: Loss 5.4288138926494867e-05\n",
      "[Epoch 37] Training Batch [197/391]: Loss 4.130630986765027e-05\n",
      "[Epoch 37] Training Batch [198/391]: Loss 3.907643258571625e-05\n",
      "[Epoch 37] Training Batch [199/391]: Loss 4.7674730012658983e-05\n",
      "[Epoch 37] Training Batch [200/391]: Loss 5.9419035096652806e-05\n",
      "[Epoch 37] Training Batch [201/391]: Loss 6.139796460047364e-05\n",
      "[Epoch 37] Training Batch [202/391]: Loss 3.9898528484627604e-05\n",
      "[Epoch 37] Training Batch [203/391]: Loss 5.051874541095458e-05\n",
      "[Epoch 37] Training Batch [204/391]: Loss 2.3387909095617943e-05\n",
      "[Epoch 37] Training Batch [205/391]: Loss 4.292407174943946e-05\n",
      "[Epoch 37] Training Batch [206/391]: Loss 4.214326327200979e-05\n",
      "[Epoch 37] Training Batch [207/391]: Loss 4.2030264012282714e-05\n",
      "[Epoch 37] Training Batch [208/391]: Loss 4.296862243791111e-05\n",
      "[Epoch 37] Training Batch [209/391]: Loss 5.382270319387317e-05\n",
      "[Epoch 37] Training Batch [210/391]: Loss 6.0315218433970585e-05\n",
      "[Epoch 37] Training Batch [211/391]: Loss 1.2116726793465205e-05\n",
      "[Epoch 37] Training Batch [212/391]: Loss 3.3442891435697675e-05\n",
      "[Epoch 37] Training Batch [213/391]: Loss 3.838819247903302e-05\n",
      "[Epoch 37] Training Batch [214/391]: Loss 3.0686042009619996e-05\n",
      "[Epoch 37] Training Batch [215/391]: Loss 3.0051780413486995e-05\n",
      "[Epoch 37] Training Batch [216/391]: Loss 3.701866080518812e-05\n",
      "[Epoch 37] Training Batch [217/391]: Loss 1.3431665138341486e-05\n",
      "[Epoch 37] Training Batch [218/391]: Loss 5.683076597051695e-05\n",
      "[Epoch 37] Training Batch [219/391]: Loss 2.889839743147604e-05\n",
      "[Epoch 37] Training Batch [220/391]: Loss 2.664653402462136e-05\n",
      "[Epoch 37] Training Batch [221/391]: Loss 3.0323968530865386e-05\n",
      "[Epoch 37] Training Batch [222/391]: Loss 8.528836769983172e-05\n",
      "[Epoch 37] Training Batch [223/391]: Loss 2.3433154638041742e-05\n",
      "[Epoch 37] Training Batch [224/391]: Loss 4.686573083745316e-05\n",
      "[Epoch 37] Training Batch [225/391]: Loss 3.673583341878839e-05\n",
      "[Epoch 37] Training Batch [226/391]: Loss 6.325943832052872e-05\n",
      "[Epoch 37] Training Batch [227/391]: Loss 3.3513842936372384e-05\n",
      "[Epoch 37] Training Batch [228/391]: Loss 2.6807696485775523e-05\n",
      "[Epoch 37] Training Batch [229/391]: Loss 3.746380389202386e-05\n",
      "[Epoch 37] Training Batch [230/391]: Loss 2.3917345970403403e-05\n",
      "[Epoch 37] Training Batch [231/391]: Loss 2.0964933355571702e-05\n",
      "[Epoch 37] Training Batch [232/391]: Loss 4.162265395279974e-05\n",
      "[Epoch 37] Training Batch [233/391]: Loss 2.0499734091572464e-05\n",
      "[Epoch 37] Training Batch [234/391]: Loss 2.1411415218608454e-05\n",
      "[Epoch 37] Training Batch [235/391]: Loss 2.862201290554367e-05\n",
      "[Epoch 37] Training Batch [236/391]: Loss 7.426703814417124e-05\n",
      "[Epoch 37] Training Batch [237/391]: Loss 6.771072366973385e-05\n",
      "[Epoch 37] Training Batch [238/391]: Loss 2.8337666662991978e-05\n",
      "[Epoch 37] Training Batch [239/391]: Loss 3.602612923714332e-05\n",
      "[Epoch 37] Training Batch [240/391]: Loss 4.20841206505429e-05\n",
      "[Epoch 37] Training Batch [241/391]: Loss 2.065967237285804e-05\n",
      "[Epoch 37] Training Batch [242/391]: Loss 2.8716636734316126e-05\n",
      "[Epoch 37] Training Batch [243/391]: Loss 2.9101862310199067e-05\n",
      "[Epoch 37] Training Batch [244/391]: Loss 7.020375778665766e-05\n",
      "[Epoch 37] Training Batch [245/391]: Loss 2.4303039026563056e-05\n",
      "[Epoch 37] Training Batch [246/391]: Loss 4.033809818793088e-05\n",
      "[Epoch 37] Training Batch [247/391]: Loss 6.064643457648344e-05\n",
      "[Epoch 37] Training Batch [248/391]: Loss 3.271761306677945e-05\n",
      "[Epoch 37] Training Batch [249/391]: Loss 5.204660556046292e-05\n",
      "[Epoch 37] Training Batch [250/391]: Loss 3.095322608714923e-05\n",
      "[Epoch 37] Training Batch [251/391]: Loss 3.0870676710037515e-05\n",
      "[Epoch 37] Training Batch [252/391]: Loss 1.752180833136663e-05\n",
      "[Epoch 37] Training Batch [253/391]: Loss 3.0010372938704677e-05\n",
      "[Epoch 37] Training Batch [254/391]: Loss 2.47037587541854e-05\n",
      "[Epoch 37] Training Batch [255/391]: Loss 5.998776032356545e-05\n",
      "[Epoch 37] Training Batch [256/391]: Loss 5.055473229731433e-05\n",
      "[Epoch 37] Training Batch [257/391]: Loss 4.1334325942443684e-05\n",
      "[Epoch 37] Training Batch [258/391]: Loss 2.8871427275589667e-05\n",
      "[Epoch 37] Training Batch [259/391]: Loss 2.5475737857050262e-05\n",
      "[Epoch 37] Training Batch [260/391]: Loss 3.44782201864291e-05\n",
      "[Epoch 37] Training Batch [261/391]: Loss 5.629849692923017e-05\n",
      "[Epoch 37] Training Batch [262/391]: Loss 1.7413334717275575e-05\n",
      "[Epoch 37] Training Batch [263/391]: Loss 5.630425221170299e-05\n",
      "[Epoch 37] Training Batch [264/391]: Loss 5.1698501920327544e-05\n",
      "[Epoch 37] Training Batch [265/391]: Loss 4.20033247792162e-05\n",
      "[Epoch 37] Training Batch [266/391]: Loss 3.441162334638648e-05\n",
      "[Epoch 37] Training Batch [267/391]: Loss 4.0728049498284236e-05\n",
      "[Epoch 37] Training Batch [268/391]: Loss 2.087133179884404e-05\n",
      "[Epoch 37] Training Batch [269/391]: Loss 3.7464462366187945e-05\n",
      "[Epoch 37] Training Batch [270/391]: Loss 5.1260583859402686e-05\n",
      "[Epoch 37] Training Batch [271/391]: Loss 4.44331526523456e-05\n",
      "[Epoch 37] Training Batch [272/391]: Loss 5.0353570259176195e-05\n",
      "[Epoch 37] Training Batch [273/391]: Loss 2.3638342099729925e-05\n",
      "[Epoch 37] Training Batch [274/391]: Loss 2.5662558982730843e-05\n",
      "[Epoch 37] Training Batch [275/391]: Loss 3.481890962575562e-05\n",
      "[Epoch 37] Training Batch [276/391]: Loss 4.786371937370859e-05\n",
      "[Epoch 37] Training Batch [277/391]: Loss 3.557270611054264e-05\n",
      "[Epoch 37] Training Batch [278/391]: Loss 3.4508957469370216e-05\n",
      "[Epoch 37] Training Batch [279/391]: Loss 4.6224660763982683e-05\n",
      "[Epoch 37] Training Batch [280/391]: Loss 2.0296101865824312e-05\n",
      "[Epoch 37] Training Batch [281/391]: Loss 1.8003893273998983e-05\n",
      "[Epoch 37] Training Batch [282/391]: Loss 3.9363567339023575e-05\n",
      "[Epoch 37] Training Batch [283/391]: Loss 3.6961348087061197e-05\n",
      "[Epoch 37] Training Batch [284/391]: Loss 8.000882371561602e-05\n",
      "[Epoch 37] Training Batch [285/391]: Loss 2.3516327928518876e-05\n",
      "[Epoch 37] Training Batch [286/391]: Loss 3.678008943097666e-05\n",
      "[Epoch 37] Training Batch [287/391]: Loss 2.507832323317416e-05\n",
      "[Epoch 37] Training Batch [288/391]: Loss 3.7383179005701095e-05\n",
      "[Epoch 37] Training Batch [289/391]: Loss 2.0734407371492125e-05\n",
      "[Epoch 37] Training Batch [290/391]: Loss 2.5779650968615897e-05\n",
      "[Epoch 37] Training Batch [291/391]: Loss 5.8060446463059634e-05\n",
      "[Epoch 37] Training Batch [292/391]: Loss 3.010675936820917e-05\n",
      "[Epoch 37] Training Batch [293/391]: Loss 4.970972804585472e-05\n",
      "[Epoch 37] Training Batch [294/391]: Loss 3.123795977444388e-05\n",
      "[Epoch 37] Training Batch [295/391]: Loss 2.823448448907584e-05\n",
      "[Epoch 37] Training Batch [296/391]: Loss 5.4169180657481775e-05\n",
      "[Epoch 37] Training Batch [297/391]: Loss 5.679631431121379e-05\n",
      "[Epoch 37] Training Batch [298/391]: Loss 2.2778425773140043e-05\n",
      "[Epoch 37] Training Batch [299/391]: Loss 5.1014081691391766e-05\n",
      "[Epoch 37] Training Batch [300/391]: Loss 2.617150494188536e-05\n",
      "[Epoch 37] Training Batch [301/391]: Loss 6.105860666139051e-05\n",
      "[Epoch 37] Training Batch [302/391]: Loss 5.380058064474724e-05\n",
      "[Epoch 37] Training Batch [303/391]: Loss 3.677793574752286e-05\n",
      "[Epoch 37] Training Batch [304/391]: Loss 4.621325570042245e-05\n",
      "[Epoch 37] Training Batch [305/391]: Loss 3.620223651523702e-05\n",
      "[Epoch 37] Training Batch [306/391]: Loss 7.49299579183571e-05\n",
      "[Epoch 37] Training Batch [307/391]: Loss 1.7412601664545946e-05\n",
      "[Epoch 37] Training Batch [308/391]: Loss 5.1687416998902336e-05\n",
      "[Epoch 37] Training Batch [309/391]: Loss 6.006874900776893e-05\n",
      "[Epoch 37] Training Batch [310/391]: Loss 6.655859760940075e-05\n",
      "[Epoch 37] Training Batch [311/391]: Loss 2.9375833037192933e-05\n",
      "[Epoch 37] Training Batch [312/391]: Loss 4.8971749492920935e-05\n",
      "[Epoch 37] Training Batch [313/391]: Loss 2.9953080229461193e-05\n",
      "[Epoch 37] Training Batch [314/391]: Loss 2.4521094019291922e-05\n",
      "[Epoch 37] Training Batch [315/391]: Loss 3.417677362449467e-05\n",
      "[Epoch 37] Training Batch [316/391]: Loss 3.189250855939463e-05\n",
      "[Epoch 37] Training Batch [317/391]: Loss 2.714744368859101e-05\n",
      "[Epoch 37] Training Batch [318/391]: Loss 5.65579321119003e-05\n",
      "[Epoch 37] Training Batch [319/391]: Loss 2.40305616898695e-05\n",
      "[Epoch 37] Training Batch [320/391]: Loss 3.752524571609683e-05\n",
      "[Epoch 37] Training Batch [321/391]: Loss 5.223789412411861e-05\n",
      "[Epoch 37] Training Batch [322/391]: Loss 6.857524567749351e-05\n",
      "[Epoch 37] Training Batch [323/391]: Loss 3.778817335842177e-05\n",
      "[Epoch 37] Training Batch [324/391]: Loss 3.387972537893802e-05\n",
      "[Epoch 37] Training Batch [325/391]: Loss 2.5965131499106064e-05\n",
      "[Epoch 37] Training Batch [326/391]: Loss 4.645589069696143e-05\n",
      "[Epoch 37] Training Batch [327/391]: Loss 5.011956818634644e-05\n",
      "[Epoch 37] Training Batch [328/391]: Loss 3.406727046240121e-05\n",
      "[Epoch 37] Training Batch [329/391]: Loss 3.837433541775681e-05\n",
      "[Epoch 37] Training Batch [330/391]: Loss 3.375350934220478e-05\n",
      "[Epoch 37] Training Batch [331/391]: Loss 3.488011134322733e-05\n",
      "[Epoch 37] Training Batch [332/391]: Loss 2.569334719737526e-05\n",
      "[Epoch 37] Training Batch [333/391]: Loss 2.0992345525883138e-05\n",
      "[Epoch 37] Training Batch [334/391]: Loss 2.6600018827593885e-05\n",
      "[Epoch 37] Training Batch [335/391]: Loss 4.2263167415512726e-05\n",
      "[Epoch 37] Training Batch [336/391]: Loss 3.2922791433520615e-05\n",
      "[Epoch 37] Training Batch [337/391]: Loss 3.659766662167385e-05\n",
      "[Epoch 37] Training Batch [338/391]: Loss 3.980955443694256e-05\n",
      "[Epoch 37] Training Batch [339/391]: Loss 2.771762228803709e-05\n",
      "[Epoch 37] Training Batch [340/391]: Loss 4.2797804781002924e-05\n",
      "[Epoch 37] Training Batch [341/391]: Loss 3.268233922426589e-05\n",
      "[Epoch 37] Training Batch [342/391]: Loss 2.6105768483830616e-05\n",
      "[Epoch 37] Training Batch [343/391]: Loss 3.588264007703401e-05\n",
      "[Epoch 37] Training Batch [344/391]: Loss 3.0262726795626804e-05\n",
      "[Epoch 37] Training Batch [345/391]: Loss 4.8347726988140494e-05\n",
      "[Epoch 37] Training Batch [346/391]: Loss 3.451477823546156e-05\n",
      "[Epoch 37] Training Batch [347/391]: Loss 1.6478601537528448e-05\n",
      "[Epoch 37] Training Batch [348/391]: Loss 8.754050213610753e-06\n",
      "[Epoch 37] Training Batch [349/391]: Loss 5.754495578003116e-05\n",
      "[Epoch 37] Training Batch [350/391]: Loss 3.541009937180206e-05\n",
      "[Epoch 37] Training Batch [351/391]: Loss 3.543600905686617e-05\n",
      "[Epoch 37] Training Batch [352/391]: Loss 3.0639348551630974e-05\n",
      "[Epoch 37] Training Batch [353/391]: Loss 5.206698551774025e-05\n",
      "[Epoch 37] Training Batch [354/391]: Loss 5.977835098747164e-05\n",
      "[Epoch 37] Training Batch [355/391]: Loss 3.0654806323582307e-05\n",
      "[Epoch 37] Training Batch [356/391]: Loss 4.272600563126616e-05\n",
      "[Epoch 37] Training Batch [357/391]: Loss 4.804678610526025e-05\n",
      "[Epoch 37] Training Batch [358/391]: Loss 2.633404619700741e-05\n",
      "[Epoch 37] Training Batch [359/391]: Loss 3.594136069295928e-05\n",
      "[Epoch 37] Training Batch [360/391]: Loss 4.27567028964404e-05\n",
      "[Epoch 37] Training Batch [361/391]: Loss 1.8859052943298593e-05\n",
      "[Epoch 37] Training Batch [362/391]: Loss 3.524935164023191e-05\n",
      "[Epoch 37] Training Batch [363/391]: Loss 2.894298995670397e-05\n",
      "[Epoch 37] Training Batch [364/391]: Loss 2.655428215803113e-05\n",
      "[Epoch 37] Training Batch [365/391]: Loss 3.183357694069855e-05\n",
      "[Epoch 37] Training Batch [366/391]: Loss 2.6661720767151564e-05\n",
      "[Epoch 37] Training Batch [367/391]: Loss 6.177605246193707e-05\n",
      "[Epoch 37] Training Batch [368/391]: Loss 8.219339360948652e-05\n",
      "[Epoch 37] Training Batch [369/391]: Loss 3.210065187886357e-05\n",
      "[Epoch 37] Training Batch [370/391]: Loss 2.789441896311473e-05\n",
      "[Epoch 37] Training Batch [371/391]: Loss 4.325890768086538e-05\n",
      "[Epoch 37] Training Batch [372/391]: Loss 3.450562144280411e-05\n",
      "[Epoch 37] Training Batch [373/391]: Loss 1.9512324797688052e-05\n",
      "[Epoch 37] Training Batch [374/391]: Loss 5.199667430133559e-05\n",
      "[Epoch 37] Training Batch [375/391]: Loss 2.6543781132204458e-05\n",
      "[Epoch 37] Training Batch [376/391]: Loss 3.8436726754298434e-05\n",
      "[Epoch 37] Training Batch [377/391]: Loss 3.2415424357168376e-05\n",
      "[Epoch 37] Training Batch [378/391]: Loss 4.475439709494822e-05\n",
      "[Epoch 37] Training Batch [379/391]: Loss 2.636047065607272e-05\n",
      "[Epoch 37] Training Batch [380/391]: Loss 3.769312024815008e-05\n",
      "[Epoch 37] Training Batch [381/391]: Loss 1.6908516045077704e-05\n",
      "[Epoch 37] Training Batch [382/391]: Loss 4.679548510466702e-05\n",
      "[Epoch 37] Training Batch [383/391]: Loss 4.456669557839632e-05\n",
      "[Epoch 37] Training Batch [384/391]: Loss 5.33677484781947e-05\n",
      "[Epoch 37] Training Batch [385/391]: Loss 3.764728899113834e-05\n",
      "[Epoch 37] Training Batch [386/391]: Loss 2.3643293388886377e-05\n",
      "[Epoch 37] Training Batch [387/391]: Loss 2.5209814339177683e-05\n",
      "[Epoch 37] Training Batch [388/391]: Loss 2.9946259019197896e-05\n",
      "[Epoch 37] Training Batch [389/391]: Loss 3.514243508107029e-05\n",
      "[Epoch 37] Training Batch [390/391]: Loss 2.601708001748193e-05\n",
      "[Epoch 37] Training Batch [391/391]: Loss 3.8104299164842814e-05\n",
      "Epoch 37 - Train Loss: 0.0000\n",
      "*********  Epoch 38/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38] Training Batch [1/391]: Loss 1.3666237464349251e-05\n",
      "[Epoch 38] Training Batch [2/391]: Loss 2.277188286825549e-05\n",
      "[Epoch 38] Training Batch [3/391]: Loss 3.4434178814990446e-05\n",
      "[Epoch 38] Training Batch [4/391]: Loss 2.8640570235438645e-05\n",
      "[Epoch 38] Training Batch [5/391]: Loss 4.499860733631067e-05\n",
      "[Epoch 38] Training Batch [6/391]: Loss 3.842460864689201e-05\n",
      "[Epoch 38] Training Batch [7/391]: Loss 1.907478872453794e-05\n",
      "[Epoch 38] Training Batch [8/391]: Loss 2.073114774248097e-05\n",
      "[Epoch 38] Training Batch [9/391]: Loss 3.81103964173235e-05\n",
      "[Epoch 38] Training Batch [10/391]: Loss 6.384948210325092e-05\n",
      "[Epoch 38] Training Batch [11/391]: Loss 2.0017831047880463e-05\n",
      "[Epoch 38] Training Batch [12/391]: Loss 2.0216451957821846e-05\n",
      "[Epoch 38] Training Batch [13/391]: Loss 2.5289016775786877e-05\n",
      "[Epoch 38] Training Batch [14/391]: Loss 3.135723090963438e-05\n",
      "[Epoch 38] Training Batch [15/391]: Loss 2.3700162273598835e-05\n",
      "[Epoch 38] Training Batch [16/391]: Loss 1.7543599824421108e-05\n",
      "[Epoch 38] Training Batch [17/391]: Loss 1.7367685359204188e-05\n",
      "[Epoch 38] Training Batch [18/391]: Loss 5.087733006803319e-05\n",
      "[Epoch 38] Training Batch [19/391]: Loss 3.119935354334302e-05\n",
      "[Epoch 38] Training Batch [20/391]: Loss 2.3324901121668518e-05\n",
      "[Epoch 38] Training Batch [21/391]: Loss 3.478592770989053e-05\n",
      "[Epoch 38] Training Batch [22/391]: Loss 2.8477761588874273e-05\n",
      "[Epoch 38] Training Batch [23/391]: Loss 2.3828004486858845e-05\n",
      "[Epoch 38] Training Batch [24/391]: Loss 3.916523201041855e-05\n",
      "[Epoch 38] Training Batch [25/391]: Loss 3.16119912895374e-05\n",
      "[Epoch 38] Training Batch [26/391]: Loss 3.356124580022879e-05\n",
      "[Epoch 38] Training Batch [27/391]: Loss 3.8527665310539305e-05\n",
      "[Epoch 38] Training Batch [28/391]: Loss 2.936161581601482e-05\n",
      "[Epoch 38] Training Batch [29/391]: Loss 3.269024091423489e-05\n",
      "[Epoch 38] Training Batch [30/391]: Loss 3.923957046936266e-05\n",
      "[Epoch 38] Training Batch [31/391]: Loss 4.018730396637693e-05\n",
      "[Epoch 38] Training Batch [32/391]: Loss 4.5019027311354876e-05\n",
      "[Epoch 38] Training Batch [33/391]: Loss 9.19995454751188e-06\n",
      "[Epoch 38] Training Batch [34/391]: Loss 3.819880657829344e-05\n",
      "[Epoch 38] Training Batch [35/391]: Loss 3.9030855987221e-05\n",
      "[Epoch 38] Training Batch [36/391]: Loss 4.269186683814041e-05\n",
      "[Epoch 38] Training Batch [37/391]: Loss 3.266119529143907e-05\n",
      "[Epoch 38] Training Batch [38/391]: Loss 1.5821829947526567e-05\n",
      "[Epoch 38] Training Batch [39/391]: Loss 1.8819180695572868e-05\n",
      "[Epoch 38] Training Batch [40/391]: Loss 3.4640754165593535e-05\n",
      "[Epoch 38] Training Batch [41/391]: Loss 4.223239375278354e-05\n",
      "[Epoch 38] Training Batch [42/391]: Loss 4.275655373930931e-05\n",
      "[Epoch 38] Training Batch [43/391]: Loss 2.3935526769491844e-05\n",
      "[Epoch 38] Training Batch [44/391]: Loss 3.973063940065913e-05\n",
      "[Epoch 38] Training Batch [45/391]: Loss 4.221390190650709e-05\n",
      "[Epoch 38] Training Batch [46/391]: Loss 1.1582022125367075e-05\n",
      "[Epoch 38] Training Batch [47/391]: Loss 1.3184686395106837e-05\n",
      "[Epoch 38] Training Batch [48/391]: Loss 3.010035288752988e-05\n",
      "[Epoch 38] Training Batch [49/391]: Loss 5.38874592166394e-05\n",
      "[Epoch 38] Training Batch [50/391]: Loss 5.133094600751065e-05\n",
      "[Epoch 38] Training Batch [51/391]: Loss 1.6682148270774633e-05\n",
      "[Epoch 38] Training Batch [52/391]: Loss 2.1026151443948038e-05\n",
      "[Epoch 38] Training Batch [53/391]: Loss 3.175938400090672e-05\n",
      "[Epoch 38] Training Batch [54/391]: Loss 4.189881292404607e-05\n",
      "[Epoch 38] Training Batch [55/391]: Loss 5.600181975751184e-05\n",
      "[Epoch 38] Training Batch [56/391]: Loss 2.8335640308796428e-05\n",
      "[Epoch 38] Training Batch [57/391]: Loss 8.137308759614825e-05\n",
      "[Epoch 38] Training Batch [58/391]: Loss 4.4367334339767694e-05\n",
      "[Epoch 38] Training Batch [59/391]: Loss 2.2228443413041532e-05\n",
      "[Epoch 38] Training Batch [60/391]: Loss 2.3193519155029207e-05\n",
      "[Epoch 38] Training Batch [61/391]: Loss 3.0351115128723904e-05\n",
      "[Epoch 38] Training Batch [62/391]: Loss 5.724341099266894e-05\n",
      "[Epoch 38] Training Batch [63/391]: Loss 1.669823359407019e-05\n",
      "[Epoch 38] Training Batch [64/391]: Loss 3.490105154924095e-05\n",
      "[Epoch 38] Training Batch [65/391]: Loss 3.7806159525644034e-05\n",
      "[Epoch 38] Training Batch [66/391]: Loss 1.2353050806268584e-05\n",
      "[Epoch 38] Training Batch [67/391]: Loss 3.69418557966128e-05\n",
      "[Epoch 38] Training Batch [68/391]: Loss 3.855040995404124e-05\n",
      "[Epoch 38] Training Batch [69/391]: Loss 1.1671613719954621e-05\n",
      "[Epoch 38] Training Batch [70/391]: Loss 2.7876527383341454e-05\n",
      "[Epoch 38] Training Batch [71/391]: Loss 2.485221193637699e-05\n",
      "[Epoch 38] Training Batch [72/391]: Loss 2.546143878134899e-05\n",
      "[Epoch 38] Training Batch [73/391]: Loss 4.1854822484310716e-05\n",
      "[Epoch 38] Training Batch [74/391]: Loss 1.495384185545845e-05\n",
      "[Epoch 38] Training Batch [75/391]: Loss 3.653900057543069e-05\n",
      "[Epoch 38] Training Batch [76/391]: Loss 2.7717838747776113e-05\n",
      "[Epoch 38] Training Batch [77/391]: Loss 2.9980941690155305e-05\n",
      "[Epoch 38] Training Batch [78/391]: Loss 5.738526306231506e-05\n",
      "[Epoch 38] Training Batch [79/391]: Loss 4.4471296860137954e-05\n",
      "[Epoch 38] Training Batch [80/391]: Loss 4.3745334551203996e-05\n",
      "[Epoch 38] Training Batch [81/391]: Loss 1.8753906260826625e-05\n",
      "[Epoch 38] Training Batch [82/391]: Loss 2.146220504073426e-05\n",
      "[Epoch 38] Training Batch [83/391]: Loss 3.253318209317513e-05\n",
      "[Epoch 38] Training Batch [84/391]: Loss 5.725311712012626e-05\n",
      "[Epoch 38] Training Batch [85/391]: Loss 3.641397051978856e-05\n",
      "[Epoch 38] Training Batch [86/391]: Loss 1.8915245163952932e-05\n",
      "[Epoch 38] Training Batch [87/391]: Loss 4.228417310514487e-05\n",
      "[Epoch 38] Training Batch [88/391]: Loss 2.9205033570178784e-05\n",
      "[Epoch 38] Training Batch [89/391]: Loss 2.6848527340916917e-05\n",
      "[Epoch 38] Training Batch [90/391]: Loss 3.716759238159284e-05\n",
      "[Epoch 38] Training Batch [91/391]: Loss 2.9376977181527764e-05\n",
      "[Epoch 38] Training Batch [92/391]: Loss 3.07292903016787e-05\n",
      "[Epoch 38] Training Batch [93/391]: Loss 4.632899072021246e-05\n",
      "[Epoch 38] Training Batch [94/391]: Loss 1.8775066564558074e-05\n",
      "[Epoch 38] Training Batch [95/391]: Loss 3.0406714358832687e-05\n",
      "[Epoch 38] Training Batch [96/391]: Loss 4.5697917812503874e-05\n",
      "[Epoch 38] Training Batch [97/391]: Loss 2.3865310140536167e-05\n",
      "[Epoch 38] Training Batch [98/391]: Loss 2.7252117433818057e-05\n",
      "[Epoch 38] Training Batch [99/391]: Loss 4.242386421537958e-05\n",
      "[Epoch 38] Training Batch [100/391]: Loss 4.2149611545028165e-05\n",
      "[Epoch 38] Training Batch [101/391]: Loss 4.245535092195496e-05\n",
      "[Epoch 38] Training Batch [102/391]: Loss 3.084707714151591e-05\n",
      "[Epoch 38] Training Batch [103/391]: Loss 4.168151644989848e-05\n",
      "[Epoch 38] Training Batch [104/391]: Loss 4.002268542535603e-05\n",
      "[Epoch 38] Training Batch [105/391]: Loss 2.3154801965574734e-05\n",
      "[Epoch 38] Training Batch [106/391]: Loss 4.077535049873404e-05\n",
      "[Epoch 38] Training Batch [107/391]: Loss 5.6310120271518826e-05\n",
      "[Epoch 38] Training Batch [108/391]: Loss 4.2969739297404885e-05\n",
      "[Epoch 38] Training Batch [109/391]: Loss 3.888270657625981e-05\n",
      "[Epoch 38] Training Batch [110/391]: Loss 1.532077294541523e-05\n",
      "[Epoch 38] Training Batch [111/391]: Loss 3.85308449040167e-05\n",
      "[Epoch 38] Training Batch [112/391]: Loss 1.702586632745806e-05\n",
      "[Epoch 38] Training Batch [113/391]: Loss 4.969275323674083e-05\n",
      "[Epoch 38] Training Batch [114/391]: Loss 4.954816176905297e-05\n",
      "[Epoch 38] Training Batch [115/391]: Loss 3.455592741374858e-05\n",
      "[Epoch 38] Training Batch [116/391]: Loss 6.358658720273525e-05\n",
      "[Epoch 38] Training Batch [117/391]: Loss 4.0155424358090386e-05\n",
      "[Epoch 38] Training Batch [118/391]: Loss 3.5947570722782984e-05\n",
      "[Epoch 38] Training Batch [119/391]: Loss 3.0860683182254434e-05\n",
      "[Epoch 38] Training Batch [120/391]: Loss 7.580760575365275e-05\n",
      "[Epoch 38] Training Batch [121/391]: Loss 2.008487172133755e-05\n",
      "[Epoch 38] Training Batch [122/391]: Loss 3.359191396157257e-05\n",
      "[Epoch 38] Training Batch [123/391]: Loss 4.066298060934059e-05\n",
      "[Epoch 38] Training Batch [124/391]: Loss 2.9168570108595304e-05\n",
      "[Epoch 38] Training Batch [125/391]: Loss 3.875747279380448e-05\n",
      "[Epoch 38] Training Batch [126/391]: Loss 1.80495062522823e-05\n",
      "[Epoch 38] Training Batch [127/391]: Loss 3.711927274707705e-05\n",
      "[Epoch 38] Training Batch [128/391]: Loss 3.841337820631452e-05\n",
      "[Epoch 38] Training Batch [129/391]: Loss 3.199055208824575e-05\n",
      "[Epoch 38] Training Batch [130/391]: Loss 4.13513153034728e-05\n",
      "[Epoch 38] Training Batch [131/391]: Loss 4.483650263864547e-05\n",
      "[Epoch 38] Training Batch [132/391]: Loss 2.692054840736091e-05\n",
      "[Epoch 38] Training Batch [133/391]: Loss 3.5753004340222105e-05\n",
      "[Epoch 38] Training Batch [134/391]: Loss 3.739177918760106e-05\n",
      "[Epoch 38] Training Batch [135/391]: Loss 2.8810683943447657e-05\n",
      "[Epoch 38] Training Batch [136/391]: Loss 2.6017190975835547e-05\n",
      "[Epoch 38] Training Batch [137/391]: Loss 3.1128838600125164e-05\n",
      "[Epoch 38] Training Batch [138/391]: Loss 3.520771861076355e-05\n",
      "[Epoch 38] Training Batch [139/391]: Loss 3.276404459029436e-05\n",
      "[Epoch 38] Training Batch [140/391]: Loss 4.940055077895522e-05\n",
      "[Epoch 38] Training Batch [141/391]: Loss 5.070973202236928e-05\n",
      "[Epoch 38] Training Batch [142/391]: Loss 5.276276351651177e-05\n",
      "[Epoch 38] Training Batch [143/391]: Loss 1.908131889649667e-05\n",
      "[Epoch 38] Training Batch [144/391]: Loss 2.3395312382490374e-05\n",
      "[Epoch 38] Training Batch [145/391]: Loss 3.778986501856707e-05\n",
      "[Epoch 38] Training Batch [146/391]: Loss 3.592115535866469e-05\n",
      "[Epoch 38] Training Batch [147/391]: Loss 3.603069490054622e-05\n",
      "[Epoch 38] Training Batch [148/391]: Loss 2.2507127141579986e-05\n",
      "[Epoch 38] Training Batch [149/391]: Loss 3.614824527176097e-05\n",
      "[Epoch 38] Training Batch [150/391]: Loss 1.8084740077028982e-05\n",
      "[Epoch 38] Training Batch [151/391]: Loss 2.5868452212307602e-05\n",
      "[Epoch 38] Training Batch [152/391]: Loss 3.786059460253455e-05\n",
      "[Epoch 38] Training Batch [153/391]: Loss 4.1038987546926364e-05\n",
      "[Epoch 38] Training Batch [154/391]: Loss 2.914875585702248e-05\n",
      "[Epoch 38] Training Batch [155/391]: Loss 3.681502857943997e-05\n",
      "[Epoch 38] Training Batch [156/391]: Loss 4.305792026570998e-05\n",
      "[Epoch 38] Training Batch [157/391]: Loss 3.6557834391715005e-05\n",
      "[Epoch 38] Training Batch [158/391]: Loss 2.7586725991568528e-05\n",
      "[Epoch 38] Training Batch [159/391]: Loss 3.603382356232032e-05\n",
      "[Epoch 38] Training Batch [160/391]: Loss 3.270735396654345e-05\n",
      "[Epoch 38] Training Batch [161/391]: Loss 2.9994329452165402e-05\n",
      "[Epoch 38] Training Batch [162/391]: Loss 3.998333340859972e-05\n",
      "[Epoch 38] Training Batch [163/391]: Loss 2.03675044758711e-05\n",
      "[Epoch 38] Training Batch [164/391]: Loss 3.6831268516834825e-05\n",
      "[Epoch 38] Training Batch [165/391]: Loss 4.566346251522191e-05\n",
      "[Epoch 38] Training Batch [166/391]: Loss 4.8941590648610145e-05\n",
      "[Epoch 38] Training Batch [167/391]: Loss 3.487123467493802e-05\n",
      "[Epoch 38] Training Batch [168/391]: Loss 5.332979708327912e-05\n",
      "[Epoch 38] Training Batch [169/391]: Loss 3.800494960159995e-05\n",
      "[Epoch 38] Training Batch [170/391]: Loss 3.17244375764858e-05\n",
      "[Epoch 38] Training Batch [171/391]: Loss 3.490154631435871e-05\n",
      "[Epoch 38] Training Batch [172/391]: Loss 3.367157478351146e-05\n",
      "[Epoch 38] Training Batch [173/391]: Loss 4.796460416400805e-05\n",
      "[Epoch 38] Training Batch [174/391]: Loss 3.75242525478825e-05\n",
      "[Epoch 38] Training Batch [175/391]: Loss 3.314284185762517e-05\n",
      "[Epoch 38] Training Batch [176/391]: Loss 2.811745616781991e-05\n",
      "[Epoch 38] Training Batch [177/391]: Loss 3.7785055610584095e-05\n",
      "[Epoch 38] Training Batch [178/391]: Loss 1.0646363989508245e-05\n",
      "[Epoch 38] Training Batch [179/391]: Loss 5.058610258856788e-05\n",
      "[Epoch 38] Training Batch [180/391]: Loss 1.299110590480268e-05\n",
      "[Epoch 38] Training Batch [181/391]: Loss 2.3138212782214396e-05\n",
      "[Epoch 38] Training Batch [182/391]: Loss 2.7332467652740888e-05\n",
      "[Epoch 38] Training Batch [183/391]: Loss 2.3085773136699572e-05\n",
      "[Epoch 38] Training Batch [184/391]: Loss 2.8631260647671297e-05\n",
      "[Epoch 38] Training Batch [185/391]: Loss 1.844675534812268e-05\n",
      "[Epoch 38] Training Batch [186/391]: Loss 3.8510166632477194e-05\n",
      "[Epoch 38] Training Batch [187/391]: Loss 4.4598185922950506e-05\n",
      "[Epoch 38] Training Batch [188/391]: Loss 4.64749064121861e-05\n",
      "[Epoch 38] Training Batch [189/391]: Loss 2.4165321519831195e-05\n",
      "[Epoch 38] Training Batch [190/391]: Loss 2.678265627764631e-05\n",
      "[Epoch 38] Training Batch [191/391]: Loss 5.086125383968465e-05\n",
      "[Epoch 38] Training Batch [192/391]: Loss 1.8650691345101222e-05\n",
      "[Epoch 38] Training Batch [193/391]: Loss 1.799026722437702e-05\n",
      "[Epoch 38] Training Batch [194/391]: Loss 4.610033647622913e-05\n",
      "[Epoch 38] Training Batch [195/391]: Loss 5.634590706904419e-05\n",
      "[Epoch 38] Training Batch [196/391]: Loss 5.647844591294415e-05\n",
      "[Epoch 38] Training Batch [197/391]: Loss 6.58384378766641e-05\n",
      "[Epoch 38] Training Batch [198/391]: Loss 1.598263406776823e-05\n",
      "[Epoch 38] Training Batch [199/391]: Loss 1.7088441381929442e-05\n",
      "[Epoch 38] Training Batch [200/391]: Loss 1.2141906154283788e-05\n",
      "[Epoch 38] Training Batch [201/391]: Loss 2.9019582143519074e-05\n",
      "[Epoch 38] Training Batch [202/391]: Loss 4.10729153372813e-05\n",
      "[Epoch 38] Training Batch [203/391]: Loss 4.790174716617912e-05\n",
      "[Epoch 38] Training Batch [204/391]: Loss 1.989869269891642e-05\n",
      "[Epoch 38] Training Batch [205/391]: Loss 5.7017245126189664e-05\n",
      "[Epoch 38] Training Batch [206/391]: Loss 3.113139246124774e-05\n",
      "[Epoch 38] Training Batch [207/391]: Loss 4.011317287222482e-05\n",
      "[Epoch 38] Training Batch [208/391]: Loss 1.8984366761287674e-05\n",
      "[Epoch 38] Training Batch [209/391]: Loss 2.0408053387654945e-05\n",
      "[Epoch 38] Training Batch [210/391]: Loss 2.7752277674153447e-05\n",
      "[Epoch 38] Training Batch [211/391]: Loss 3.082148759858683e-05\n",
      "[Epoch 38] Training Batch [212/391]: Loss 6.756098446203396e-05\n",
      "[Epoch 38] Training Batch [213/391]: Loss 3.108816235908307e-05\n",
      "[Epoch 38] Training Batch [214/391]: Loss 3.682225360535085e-05\n",
      "[Epoch 38] Training Batch [215/391]: Loss 2.3026714188745245e-05\n",
      "[Epoch 38] Training Batch [216/391]: Loss 4.0170147258322686e-05\n",
      "[Epoch 38] Training Batch [217/391]: Loss 4.22437115048524e-05\n",
      "[Epoch 38] Training Batch [218/391]: Loss 2.652741750353016e-05\n",
      "[Epoch 38] Training Batch [219/391]: Loss 2.5331599317723885e-05\n",
      "[Epoch 38] Training Batch [220/391]: Loss 2.937516910606064e-05\n",
      "[Epoch 38] Training Batch [221/391]: Loss 2.2513570002047345e-05\n",
      "[Epoch 38] Training Batch [222/391]: Loss 2.8624692276935093e-05\n",
      "[Epoch 38] Training Batch [223/391]: Loss 2.001906614168547e-05\n",
      "[Epoch 38] Training Batch [224/391]: Loss 5.154472455615178e-05\n",
      "[Epoch 38] Training Batch [225/391]: Loss 3.711835597641766e-05\n",
      "[Epoch 38] Training Batch [226/391]: Loss 5.261005208012648e-05\n",
      "[Epoch 38] Training Batch [227/391]: Loss 1.9871382392011583e-05\n",
      "[Epoch 38] Training Batch [228/391]: Loss 3.710912278620526e-05\n",
      "[Epoch 38] Training Batch [229/391]: Loss 4.2647494410630316e-05\n",
      "[Epoch 38] Training Batch [230/391]: Loss 2.6509824238019064e-05\n",
      "[Epoch 38] Training Batch [231/391]: Loss 2.3961978513398208e-05\n",
      "[Epoch 38] Training Batch [232/391]: Loss 4.540139343589544e-05\n",
      "[Epoch 38] Training Batch [233/391]: Loss 2.126522122125607e-05\n",
      "[Epoch 38] Training Batch [234/391]: Loss 2.7937705453950912e-05\n",
      "[Epoch 38] Training Batch [235/391]: Loss 3.58345678250771e-05\n",
      "[Epoch 38] Training Batch [236/391]: Loss 4.662489300244488e-05\n",
      "[Epoch 38] Training Batch [237/391]: Loss 2.7739604774978943e-05\n",
      "[Epoch 38] Training Batch [238/391]: Loss 2.571261575212702e-05\n",
      "[Epoch 38] Training Batch [239/391]: Loss 3.536371514201164e-05\n",
      "[Epoch 38] Training Batch [240/391]: Loss 3.494216070976108e-05\n",
      "[Epoch 38] Training Batch [241/391]: Loss 3.862856101477519e-05\n",
      "[Epoch 38] Training Batch [242/391]: Loss 3.869586726068519e-05\n",
      "[Epoch 38] Training Batch [243/391]: Loss 3.818589175352827e-05\n",
      "[Epoch 38] Training Batch [244/391]: Loss 2.222953480668366e-05\n",
      "[Epoch 38] Training Batch [245/391]: Loss 1.9705359591171145e-05\n",
      "[Epoch 38] Training Batch [246/391]: Loss 7.226849265862256e-05\n",
      "[Epoch 38] Training Batch [247/391]: Loss 5.091672937851399e-05\n",
      "[Epoch 38] Training Batch [248/391]: Loss 3.560443292371929e-05\n",
      "[Epoch 38] Training Batch [249/391]: Loss 1.9137556591886096e-05\n",
      "[Epoch 38] Training Batch [250/391]: Loss 3.993999052909203e-05\n",
      "[Epoch 38] Training Batch [251/391]: Loss 3.7139288906473666e-05\n",
      "[Epoch 38] Training Batch [252/391]: Loss 3.934674532501958e-05\n",
      "[Epoch 38] Training Batch [253/391]: Loss 2.8677244699792936e-05\n",
      "[Epoch 38] Training Batch [254/391]: Loss 3.668061981443316e-05\n",
      "[Epoch 38] Training Batch [255/391]: Loss 3.557025411282666e-05\n",
      "[Epoch 38] Training Batch [256/391]: Loss 1.4144865417620167e-05\n",
      "[Epoch 38] Training Batch [257/391]: Loss 4.2738709453260526e-05\n",
      "[Epoch 38] Training Batch [258/391]: Loss 2.7414231226430275e-05\n",
      "[Epoch 38] Training Batch [259/391]: Loss 3.207977715646848e-05\n",
      "[Epoch 38] Training Batch [260/391]: Loss 5.021928882342763e-05\n",
      "[Epoch 38] Training Batch [261/391]: Loss 3.46959859598428e-05\n",
      "[Epoch 38] Training Batch [262/391]: Loss 2.9506723876693286e-05\n",
      "[Epoch 38] Training Batch [263/391]: Loss 3.148878749925643e-05\n",
      "[Epoch 38] Training Batch [264/391]: Loss 4.719537537312135e-05\n",
      "[Epoch 38] Training Batch [265/391]: Loss 3.428444324526936e-05\n",
      "[Epoch 38] Training Batch [266/391]: Loss 2.5836032364168204e-05\n",
      "[Epoch 38] Training Batch [267/391]: Loss 3.989859760622494e-05\n",
      "[Epoch 38] Training Batch [268/391]: Loss 3.158040635753423e-05\n",
      "[Epoch 38] Training Batch [269/391]: Loss 3.7378289562184364e-05\n",
      "[Epoch 38] Training Batch [270/391]: Loss 3.2600120903225616e-05\n",
      "[Epoch 38] Training Batch [271/391]: Loss 2.3941707695485093e-05\n",
      "[Epoch 38] Training Batch [272/391]: Loss 2.621166640892625e-05\n",
      "[Epoch 38] Training Batch [273/391]: Loss 4.007808456663042e-05\n",
      "[Epoch 38] Training Batch [274/391]: Loss 2.8624186597880907e-05\n",
      "[Epoch 38] Training Batch [275/391]: Loss 3.600546915549785e-05\n",
      "[Epoch 38] Training Batch [276/391]: Loss 2.7843014322570525e-05\n",
      "[Epoch 38] Training Batch [277/391]: Loss 3.732646655407734e-05\n",
      "[Epoch 38] Training Batch [278/391]: Loss 2.1495607143151574e-05\n",
      "[Epoch 38] Training Batch [279/391]: Loss 2.1914140234002843e-05\n",
      "[Epoch 38] Training Batch [280/391]: Loss 2.4310065782628953e-05\n",
      "[Epoch 38] Training Batch [281/391]: Loss 3.4374061215203255e-05\n",
      "[Epoch 38] Training Batch [282/391]: Loss 2.8527272661449388e-05\n",
      "[Epoch 38] Training Batch [283/391]: Loss 1.628274731046986e-05\n",
      "[Epoch 38] Training Batch [284/391]: Loss 2.1161697077332065e-05\n",
      "[Epoch 38] Training Batch [285/391]: Loss 6.253532774280757e-05\n",
      "[Epoch 38] Training Batch [286/391]: Loss 2.2203239495866e-05\n",
      "[Epoch 38] Training Batch [287/391]: Loss 3.668406861834228e-05\n",
      "[Epoch 38] Training Batch [288/391]: Loss 3.447020935709588e-05\n",
      "[Epoch 38] Training Batch [289/391]: Loss 3.762653432204388e-05\n",
      "[Epoch 38] Training Batch [290/391]: Loss 6.741259130649269e-05\n",
      "[Epoch 38] Training Batch [291/391]: Loss 2.6688852813094854e-05\n",
      "[Epoch 38] Training Batch [292/391]: Loss 2.9715260097873397e-05\n",
      "[Epoch 38] Training Batch [293/391]: Loss 4.660038757720031e-05\n",
      "[Epoch 38] Training Batch [294/391]: Loss 4.432468267623335e-05\n",
      "[Epoch 38] Training Batch [295/391]: Loss 4.492574225878343e-05\n",
      "[Epoch 38] Training Batch [296/391]: Loss 4.785998316947371e-05\n",
      "[Epoch 38] Training Batch [297/391]: Loss 4.730004002340138e-05\n",
      "[Epoch 38] Training Batch [298/391]: Loss 3.236529300920665e-05\n",
      "[Epoch 38] Training Batch [299/391]: Loss 2.5774397727218457e-05\n",
      "[Epoch 38] Training Batch [300/391]: Loss 6.535775901284069e-05\n",
      "[Epoch 38] Training Batch [301/391]: Loss 2.6190318749286234e-05\n",
      "[Epoch 38] Training Batch [302/391]: Loss 3.027614002348855e-05\n",
      "[Epoch 38] Training Batch [303/391]: Loss 2.3993479771888815e-05\n",
      "[Epoch 38] Training Batch [304/391]: Loss 2.1151292457943782e-05\n",
      "[Epoch 38] Training Batch [305/391]: Loss 4.711063229478896e-05\n",
      "[Epoch 38] Training Batch [306/391]: Loss 2.936629971372895e-05\n",
      "[Epoch 38] Training Batch [307/391]: Loss 4.664898733608425e-05\n",
      "[Epoch 38] Training Batch [308/391]: Loss 3.669087527669035e-05\n",
      "[Epoch 38] Training Batch [309/391]: Loss 3.8406964449677616e-05\n",
      "[Epoch 38] Training Batch [310/391]: Loss 8.149333734763786e-05\n",
      "[Epoch 38] Training Batch [311/391]: Loss 2.35382958635455e-05\n",
      "[Epoch 38] Training Batch [312/391]: Loss 3.657933120848611e-05\n",
      "[Epoch 38] Training Batch [313/391]: Loss 2.6870524379773997e-05\n",
      "[Epoch 38] Training Batch [314/391]: Loss 1.9381815945962444e-05\n",
      "[Epoch 38] Training Batch [315/391]: Loss 4.34118919656612e-05\n",
      "[Epoch 38] Training Batch [316/391]: Loss 3.095297870459035e-05\n",
      "[Epoch 38] Training Batch [317/391]: Loss 2.611024319776334e-05\n",
      "[Epoch 38] Training Batch [318/391]: Loss 2.4057859263848513e-05\n",
      "[Epoch 38] Training Batch [319/391]: Loss 1.92905317817349e-05\n",
      "[Epoch 38] Training Batch [320/391]: Loss 3.328688762849197e-05\n",
      "[Epoch 38] Training Batch [321/391]: Loss 8.68523566168733e-05\n",
      "[Epoch 38] Training Batch [322/391]: Loss 3.447070048423484e-05\n",
      "[Epoch 38] Training Batch [323/391]: Loss 3.935617132810876e-05\n",
      "[Epoch 38] Training Batch [324/391]: Loss 2.4692966690054163e-05\n",
      "[Epoch 38] Training Batch [325/391]: Loss 2.5896433726302348e-05\n",
      "[Epoch 38] Training Batch [326/391]: Loss 2.7233449145569466e-05\n",
      "[Epoch 38] Training Batch [327/391]: Loss 1.7981639757636003e-05\n",
      "[Epoch 38] Training Batch [328/391]: Loss 7.66293887863867e-05\n",
      "[Epoch 38] Training Batch [329/391]: Loss 2.8348740670480765e-05\n",
      "[Epoch 38] Training Batch [330/391]: Loss 2.8659704184974544e-05\n",
      "[Epoch 38] Training Batch [331/391]: Loss 3.222787563572638e-05\n",
      "[Epoch 38] Training Batch [332/391]: Loss 3.292831024737097e-05\n",
      "[Epoch 38] Training Batch [333/391]: Loss 5.4901618568692356e-05\n",
      "[Epoch 38] Training Batch [334/391]: Loss 2.4219774786615744e-05\n",
      "[Epoch 38] Training Batch [335/391]: Loss 4.444855221663602e-05\n",
      "[Epoch 38] Training Batch [336/391]: Loss 2.7089745344710536e-05\n",
      "[Epoch 38] Training Batch [337/391]: Loss 1.9457122107269242e-05\n",
      "[Epoch 38] Training Batch [338/391]: Loss 2.568273885117378e-05\n",
      "[Epoch 38] Training Batch [339/391]: Loss 3.299091258668341e-05\n",
      "[Epoch 38] Training Batch [340/391]: Loss 3.364219082868658e-05\n",
      "[Epoch 38] Training Batch [341/391]: Loss 2.073257383017335e-05\n",
      "[Epoch 38] Training Batch [342/391]: Loss 3.553435089997947e-05\n",
      "[Epoch 38] Training Batch [343/391]: Loss 2.8543003281811252e-05\n",
      "[Epoch 38] Training Batch [344/391]: Loss 3.4306314773857594e-05\n",
      "[Epoch 38] Training Batch [345/391]: Loss 3.987316449638456e-05\n",
      "[Epoch 38] Training Batch [346/391]: Loss 5.930199404247105e-05\n",
      "[Epoch 38] Training Batch [347/391]: Loss 2.995243266923353e-05\n",
      "[Epoch 38] Training Batch [348/391]: Loss 2.572203266026918e-05\n",
      "[Epoch 38] Training Batch [349/391]: Loss 2.8418495276127942e-05\n",
      "[Epoch 38] Training Batch [350/391]: Loss 2.75268448604038e-05\n",
      "[Epoch 38] Training Batch [351/391]: Loss 4.2263982322765514e-05\n",
      "[Epoch 38] Training Batch [352/391]: Loss 2.1984462364343926e-05\n",
      "[Epoch 38] Training Batch [353/391]: Loss 2.963242332043592e-05\n",
      "[Epoch 38] Training Batch [354/391]: Loss 4.909185372525826e-05\n",
      "[Epoch 38] Training Batch [355/391]: Loss 3.696545900311321e-05\n",
      "[Epoch 38] Training Batch [356/391]: Loss 3.5727360227610916e-05\n",
      "[Epoch 38] Training Batch [357/391]: Loss 5.551471986109391e-05\n",
      "[Epoch 38] Training Batch [358/391]: Loss 3.437121631577611e-05\n",
      "[Epoch 38] Training Batch [359/391]: Loss 3.5621749702841043e-05\n",
      "[Epoch 38] Training Batch [360/391]: Loss 3.9325066609308124e-05\n",
      "[Epoch 38] Training Batch [361/391]: Loss 4.3248488509561867e-05\n",
      "[Epoch 38] Training Batch [362/391]: Loss 4.685912062996067e-05\n",
      "[Epoch 38] Training Batch [363/391]: Loss 2.210239472333342e-05\n",
      "[Epoch 38] Training Batch [364/391]: Loss 2.853791374946013e-05\n",
      "[Epoch 38] Training Batch [365/391]: Loss 4.229226760799065e-05\n",
      "[Epoch 38] Training Batch [366/391]: Loss 3.420421853661537e-05\n",
      "[Epoch 38] Training Batch [367/391]: Loss 1.8403337890049443e-05\n",
      "[Epoch 38] Training Batch [368/391]: Loss 2.3769365725456737e-05\n",
      "[Epoch 38] Training Batch [369/391]: Loss 1.8071654267259873e-05\n",
      "[Epoch 38] Training Batch [370/391]: Loss 7.57786474423483e-05\n",
      "[Epoch 38] Training Batch [371/391]: Loss 3.851548535749316e-05\n",
      "[Epoch 38] Training Batch [372/391]: Loss 2.8672056942014024e-05\n",
      "[Epoch 38] Training Batch [373/391]: Loss 2.7944626708631404e-05\n",
      "[Epoch 38] Training Batch [374/391]: Loss 2.182743264711462e-05\n",
      "[Epoch 38] Training Batch [375/391]: Loss 1.917475492518861e-05\n",
      "[Epoch 38] Training Batch [376/391]: Loss 5.852180765941739e-05\n",
      "[Epoch 38] Training Batch [377/391]: Loss 2.723522084124852e-05\n",
      "[Epoch 38] Training Batch [378/391]: Loss 2.4830700567690656e-05\n",
      "[Epoch 38] Training Batch [379/391]: Loss 2.3485679776058532e-05\n",
      "[Epoch 38] Training Batch [380/391]: Loss 3.9514125091955066e-05\n",
      "[Epoch 38] Training Batch [381/391]: Loss 2.8261722036404535e-05\n",
      "[Epoch 38] Training Batch [382/391]: Loss 2.9520526368287392e-05\n",
      "[Epoch 38] Training Batch [383/391]: Loss 2.2090875063440762e-05\n",
      "[Epoch 38] Training Batch [384/391]: Loss 3.458650826360099e-05\n",
      "[Epoch 38] Training Batch [385/391]: Loss 4.490570427151397e-05\n",
      "[Epoch 38] Training Batch [386/391]: Loss 4.142207762924954e-05\n",
      "[Epoch 38] Training Batch [387/391]: Loss 1.795343450794462e-05\n",
      "[Epoch 38] Training Batch [388/391]: Loss 4.2808936996152624e-05\n",
      "[Epoch 38] Training Batch [389/391]: Loss 3.625072349677794e-05\n",
      "[Epoch 38] Training Batch [390/391]: Loss 4.0331025957129896e-05\n",
      "[Epoch 38] Training Batch [391/391]: Loss 1.684095877863001e-05\n",
      "Epoch 38 - Train Loss: 0.0000\n",
      "*********  Epoch 39/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39] Training Batch [1/391]: Loss 4.3945277866441756e-05\n",
      "[Epoch 39] Training Batch [2/391]: Loss 4.0102611819747835e-05\n",
      "[Epoch 39] Training Batch [3/391]: Loss 1.785981839930173e-05\n",
      "[Epoch 39] Training Batch [4/391]: Loss 4.339970837463625e-05\n",
      "[Epoch 39] Training Batch [5/391]: Loss 3.2585499866399914e-05\n",
      "[Epoch 39] Training Batch [6/391]: Loss 2.8345944883767515e-05\n",
      "[Epoch 39] Training Batch [7/391]: Loss 3.559179458534345e-05\n",
      "[Epoch 39] Training Batch [8/391]: Loss 4.574789272737689e-05\n",
      "[Epoch 39] Training Batch [9/391]: Loss 2.0898398361168802e-05\n",
      "[Epoch 39] Training Batch [10/391]: Loss 2.4064416720648296e-05\n",
      "[Epoch 39] Training Batch [11/391]: Loss 1.2060816516168416e-05\n",
      "[Epoch 39] Training Batch [12/391]: Loss 4.682259532273747e-05\n",
      "[Epoch 39] Training Batch [13/391]: Loss 3.4992030123248696e-05\n",
      "[Epoch 39] Training Batch [14/391]: Loss 3.017676863237284e-05\n",
      "[Epoch 39] Training Batch [15/391]: Loss 3.5636912798509e-05\n",
      "[Epoch 39] Training Batch [16/391]: Loss 3.176033351337537e-05\n",
      "[Epoch 39] Training Batch [17/391]: Loss 4.279174390831031e-05\n",
      "[Epoch 39] Training Batch [18/391]: Loss 4.182668635621667e-05\n",
      "[Epoch 39] Training Batch [19/391]: Loss 2.6836198230739683e-05\n",
      "[Epoch 39] Training Batch [20/391]: Loss 2.3780594347044826e-05\n",
      "[Epoch 39] Training Batch [21/391]: Loss 3.081714385189116e-05\n",
      "[Epoch 39] Training Batch [22/391]: Loss 3.08548696921207e-05\n",
      "[Epoch 39] Training Batch [23/391]: Loss 3.049783299502451e-05\n",
      "[Epoch 39] Training Batch [24/391]: Loss 1.6519212294952013e-05\n",
      "[Epoch 39] Training Batch [25/391]: Loss 2.6360281481174752e-05\n",
      "[Epoch 39] Training Batch [26/391]: Loss 4.518542846199125e-05\n",
      "[Epoch 39] Training Batch [27/391]: Loss 1.759917358867824e-05\n",
      "[Epoch 39] Training Batch [28/391]: Loss 2.2051581254345365e-05\n",
      "[Epoch 39] Training Batch [29/391]: Loss 2.870343450922519e-05\n",
      "[Epoch 39] Training Batch [30/391]: Loss 3.698563523357734e-05\n",
      "[Epoch 39] Training Batch [31/391]: Loss 3.4362714359303936e-05\n",
      "[Epoch 39] Training Batch [32/391]: Loss 4.300523141864687e-05\n",
      "[Epoch 39] Training Batch [33/391]: Loss 2.7710442736861296e-05\n",
      "[Epoch 39] Training Batch [34/391]: Loss 3.257896605646238e-05\n",
      "[Epoch 39] Training Batch [35/391]: Loss 4.003588401246816e-05\n",
      "[Epoch 39] Training Batch [36/391]: Loss 3.7950187106616795e-05\n",
      "[Epoch 39] Training Batch [37/391]: Loss 4.203736170893535e-05\n",
      "[Epoch 39] Training Batch [38/391]: Loss 3.615370951592922e-05\n",
      "[Epoch 39] Training Batch [39/391]: Loss 1.9390648958506063e-05\n",
      "[Epoch 39] Training Batch [40/391]: Loss 2.8353684683679603e-05\n",
      "[Epoch 39] Training Batch [41/391]: Loss 5.330576095730066e-05\n",
      "[Epoch 39] Training Batch [42/391]: Loss 2.5507513782940805e-05\n",
      "[Epoch 39] Training Batch [43/391]: Loss 2.0437866623979062e-05\n",
      "[Epoch 39] Training Batch [44/391]: Loss 2.1369711248553358e-05\n",
      "[Epoch 39] Training Batch [45/391]: Loss 2.949688678199891e-05\n",
      "[Epoch 39] Training Batch [46/391]: Loss 3.902121898136102e-05\n",
      "[Epoch 39] Training Batch [47/391]: Loss 4.448753679753281e-05\n",
      "[Epoch 39] Training Batch [48/391]: Loss 3.2075105991680175e-05\n",
      "[Epoch 39] Training Batch [49/391]: Loss 2.0189057977404445e-05\n",
      "[Epoch 39] Training Batch [50/391]: Loss 1.8399485270492733e-05\n",
      "[Epoch 39] Training Batch [51/391]: Loss 5.466711081680842e-05\n",
      "[Epoch 39] Training Batch [52/391]: Loss 5.04330346302595e-05\n",
      "[Epoch 39] Training Batch [53/391]: Loss 1.4589884813176468e-05\n",
      "[Epoch 39] Training Batch [54/391]: Loss 2.031067015195731e-05\n",
      "[Epoch 39] Training Batch [55/391]: Loss 2.424139529466629e-05\n",
      "[Epoch 39] Training Batch [56/391]: Loss 2.8026419386151247e-05\n",
      "[Epoch 39] Training Batch [57/391]: Loss 4.799624002771452e-05\n",
      "[Epoch 39] Training Batch [58/391]: Loss 4.816129512619227e-05\n",
      "[Epoch 39] Training Batch [59/391]: Loss 3.567278690752573e-05\n",
      "[Epoch 39] Training Batch [60/391]: Loss 2.7062098524766043e-05\n",
      "[Epoch 39] Training Batch [61/391]: Loss 2.1586118236882612e-05\n",
      "[Epoch 39] Training Batch [62/391]: Loss 3.893335451721214e-05\n",
      "[Epoch 39] Training Batch [63/391]: Loss 2.9469716537278146e-05\n",
      "[Epoch 39] Training Batch [64/391]: Loss 2.227079494332429e-05\n",
      "[Epoch 39] Training Batch [65/391]: Loss 3.4178206988144666e-05\n",
      "[Epoch 39] Training Batch [66/391]: Loss 2.377225609961897e-05\n",
      "[Epoch 39] Training Batch [67/391]: Loss 1.4810575521551073e-05\n",
      "[Epoch 39] Training Batch [68/391]: Loss 2.2393869585357606e-05\n",
      "[Epoch 39] Training Batch [69/391]: Loss 2.6294486815459095e-05\n",
      "[Epoch 39] Training Batch [70/391]: Loss 3.0746236006962135e-05\n",
      "[Epoch 39] Training Batch [71/391]: Loss 2.3805325326975435e-05\n",
      "[Epoch 39] Training Batch [72/391]: Loss 1.9272658391855657e-05\n",
      "[Epoch 39] Training Batch [73/391]: Loss 4.9352034693583846e-05\n",
      "[Epoch 39] Training Batch [74/391]: Loss 2.7559601221582852e-05\n",
      "[Epoch 39] Training Batch [75/391]: Loss 2.8424683478078805e-05\n",
      "[Epoch 39] Training Batch [76/391]: Loss 4.880604683421552e-05\n",
      "[Epoch 39] Training Batch [77/391]: Loss 3.2287960493704304e-05\n",
      "[Epoch 39] Training Batch [78/391]: Loss 2.1333918994059786e-05\n",
      "[Epoch 39] Training Batch [79/391]: Loss 2.0411172954482026e-05\n",
      "[Epoch 39] Training Batch [80/391]: Loss 3.546910738805309e-05\n",
      "[Epoch 39] Training Batch [81/391]: Loss 4.5166532800067216e-05\n",
      "[Epoch 39] Training Batch [82/391]: Loss 2.777593363134656e-05\n",
      "[Epoch 39] Training Batch [83/391]: Loss 2.9538725357269868e-05\n",
      "[Epoch 39] Training Batch [84/391]: Loss 4.927872942062095e-05\n",
      "[Epoch 39] Training Batch [85/391]: Loss 2.717896677495446e-05\n",
      "[Epoch 39] Training Batch [86/391]: Loss 1.8650400306796655e-05\n",
      "[Epoch 39] Training Batch [87/391]: Loss 2.1239517081994563e-05\n",
      "[Epoch 39] Training Batch [88/391]: Loss 4.0493570850230753e-05\n",
      "[Epoch 39] Training Batch [89/391]: Loss 2.076227610814385e-05\n",
      "[Epoch 39] Training Batch [90/391]: Loss 4.284829265088774e-05\n",
      "[Epoch 39] Training Batch [91/391]: Loss 3.545672007021494e-05\n",
      "[Epoch 39] Training Batch [92/391]: Loss 3.0139901355141774e-05\n",
      "[Epoch 39] Training Batch [93/391]: Loss 2.927408422692679e-05\n",
      "[Epoch 39] Training Batch [94/391]: Loss 2.8728974939440377e-05\n",
      "[Epoch 39] Training Batch [95/391]: Loss 1.9961920770583674e-05\n",
      "[Epoch 39] Training Batch [96/391]: Loss 3.281136741861701e-05\n",
      "[Epoch 39] Training Batch [97/391]: Loss 3.2246953196590766e-05\n",
      "[Epoch 39] Training Batch [98/391]: Loss 4.1395967855351046e-05\n",
      "[Epoch 39] Training Batch [99/391]: Loss 2.793494422803633e-05\n",
      "[Epoch 39] Training Batch [100/391]: Loss 5.152015364728868e-05\n",
      "[Epoch 39] Training Batch [101/391]: Loss 3.474812911008485e-05\n",
      "[Epoch 39] Training Batch [102/391]: Loss 3.026953709195368e-05\n",
      "[Epoch 39] Training Batch [103/391]: Loss 4.0992814319906756e-05\n",
      "[Epoch 39] Training Batch [104/391]: Loss 2.8514765290310606e-05\n",
      "[Epoch 39] Training Batch [105/391]: Loss 2.7682823201757856e-05\n",
      "[Epoch 39] Training Batch [106/391]: Loss 1.4828054190729745e-05\n",
      "[Epoch 39] Training Batch [107/391]: Loss 2.8065465812687762e-05\n",
      "[Epoch 39] Training Batch [108/391]: Loss 3.6026900488650426e-05\n",
      "[Epoch 39] Training Batch [109/391]: Loss 4.0011571400100365e-05\n",
      "[Epoch 39] Training Batch [110/391]: Loss 1.756269375619013e-05\n",
      "[Epoch 39] Training Batch [111/391]: Loss 3.2730429666116834e-05\n",
      "[Epoch 39] Training Batch [112/391]: Loss 2.0834339011344127e-05\n",
      "[Epoch 39] Training Batch [113/391]: Loss 1.4129760529613122e-05\n",
      "[Epoch 39] Training Batch [114/391]: Loss 1.900498682516627e-05\n",
      "[Epoch 39] Training Batch [115/391]: Loss 2.0652241801144555e-05\n",
      "[Epoch 39] Training Batch [116/391]: Loss 2.5150877263513394e-05\n",
      "[Epoch 39] Training Batch [117/391]: Loss 3.6746583646163344e-05\n",
      "[Epoch 39] Training Batch [118/391]: Loss 2.2291131244855933e-05\n",
      "[Epoch 39] Training Batch [119/391]: Loss 3.0284112654044293e-05\n",
      "[Epoch 39] Training Batch [120/391]: Loss 2.9737244403804652e-05\n",
      "[Epoch 39] Training Batch [121/391]: Loss 1.9133427485940047e-05\n",
      "[Epoch 39] Training Batch [122/391]: Loss 3.5531105822883546e-05\n",
      "[Epoch 39] Training Batch [123/391]: Loss 2.920931001426652e-05\n",
      "[Epoch 39] Training Batch [124/391]: Loss 3.908765938831493e-05\n",
      "[Epoch 39] Training Batch [125/391]: Loss 3.4164273529313505e-05\n",
      "[Epoch 39] Training Batch [126/391]: Loss 3.639301212388091e-05\n",
      "[Epoch 39] Training Batch [127/391]: Loss 3.649328209576197e-05\n",
      "[Epoch 39] Training Batch [128/391]: Loss 3.485432898742147e-05\n",
      "[Epoch 39] Training Batch [129/391]: Loss 3.745180947589688e-05\n",
      "[Epoch 39] Training Batch [130/391]: Loss 3.6076700780540705e-05\n",
      "[Epoch 39] Training Batch [131/391]: Loss 2.013730045291595e-05\n",
      "[Epoch 39] Training Batch [132/391]: Loss 3.860300421365537e-05\n",
      "[Epoch 39] Training Batch [133/391]: Loss 2.2442900444730185e-05\n",
      "[Epoch 39] Training Batch [134/391]: Loss 3.761416155612096e-05\n",
      "[Epoch 39] Training Batch [135/391]: Loss 7.893753354437649e-05\n",
      "[Epoch 39] Training Batch [136/391]: Loss 1.7651966118137352e-05\n",
      "[Epoch 39] Training Batch [137/391]: Loss 2.4624545403639786e-05\n",
      "[Epoch 39] Training Batch [138/391]: Loss 3.427290721447207e-05\n",
      "[Epoch 39] Training Batch [139/391]: Loss 2.6975587388733402e-05\n",
      "[Epoch 39] Training Batch [140/391]: Loss 3.346348603372462e-05\n",
      "[Epoch 39] Training Batch [141/391]: Loss 1.9954133676947095e-05\n",
      "[Epoch 39] Training Batch [142/391]: Loss 2.683115963009186e-05\n",
      "[Epoch 39] Training Batch [143/391]: Loss 4.158104638918303e-05\n",
      "[Epoch 39] Training Batch [144/391]: Loss 2.419910924800206e-05\n",
      "[Epoch 39] Training Batch [145/391]: Loss 1.9004697605851106e-05\n",
      "[Epoch 39] Training Batch [146/391]: Loss 2.8385880796122365e-05\n",
      "[Epoch 39] Training Batch [147/391]: Loss 2.790632970572915e-05\n",
      "[Epoch 39] Training Batch [148/391]: Loss 1.506556600361364e-05\n",
      "[Epoch 39] Training Batch [149/391]: Loss 5.859333759872243e-05\n",
      "[Epoch 39] Training Batch [150/391]: Loss 2.3368220354313962e-05\n",
      "[Epoch 39] Training Batch [151/391]: Loss 3.366532109794207e-05\n",
      "[Epoch 39] Training Batch [152/391]: Loss 3.109314639004879e-05\n",
      "[Epoch 39] Training Batch [153/391]: Loss 2.5147013730020262e-05\n",
      "[Epoch 39] Training Batch [154/391]: Loss 4.041624561068602e-05\n",
      "[Epoch 39] Training Batch [155/391]: Loss 4.6568431571358815e-05\n",
      "[Epoch 39] Training Batch [156/391]: Loss 2.973253322124947e-05\n",
      "[Epoch 39] Training Batch [157/391]: Loss 2.155152105842717e-05\n",
      "[Epoch 39] Training Batch [158/391]: Loss 2.5412722607143223e-05\n",
      "[Epoch 39] Training Batch [159/391]: Loss 2.0477958969422616e-05\n",
      "[Epoch 39] Training Batch [160/391]: Loss 3.227875640732236e-05\n",
      "[Epoch 39] Training Batch [161/391]: Loss 2.994901660713367e-05\n",
      "[Epoch 39] Training Batch [162/391]: Loss 2.6601701392792165e-05\n",
      "[Epoch 39] Training Batch [163/391]: Loss 3.237998316762969e-05\n",
      "[Epoch 39] Training Batch [164/391]: Loss 2.4786271751509048e-05\n",
      "[Epoch 39] Training Batch [165/391]: Loss 1.8393098798696883e-05\n",
      "[Epoch 39] Training Batch [166/391]: Loss 3.578841278795153e-05\n",
      "[Epoch 39] Training Batch [167/391]: Loss 3.1494637369178236e-05\n",
      "[Epoch 39] Training Batch [168/391]: Loss 3.1981013307813555e-05\n",
      "[Epoch 39] Training Batch [169/391]: Loss 3.4174820029875264e-05\n",
      "[Epoch 39] Training Batch [170/391]: Loss 4.683561564888805e-05\n",
      "[Epoch 39] Training Batch [171/391]: Loss 2.761407449725084e-05\n",
      "[Epoch 39] Training Batch [172/391]: Loss 4.193739005131647e-05\n",
      "[Epoch 39] Training Batch [173/391]: Loss 3.767672387766652e-05\n",
      "[Epoch 39] Training Batch [174/391]: Loss 3.862698213197291e-05\n",
      "[Epoch 39] Training Batch [175/391]: Loss 1.803576378733851e-05\n",
      "[Epoch 39] Training Batch [176/391]: Loss 2.4872177164070308e-05\n",
      "[Epoch 39] Training Batch [177/391]: Loss 2.865638816729188e-05\n",
      "[Epoch 39] Training Batch [178/391]: Loss 4.149887536186725e-05\n",
      "[Epoch 39] Training Batch [179/391]: Loss 2.7567706638365053e-05\n",
      "[Epoch 39] Training Batch [180/391]: Loss 3.769927207031287e-05\n",
      "[Epoch 39] Training Batch [181/391]: Loss 2.4279932404169813e-05\n",
      "[Epoch 39] Training Batch [182/391]: Loss 2.1170943000470288e-05\n",
      "[Epoch 39] Training Batch [183/391]: Loss 3.196794932591729e-05\n",
      "[Epoch 39] Training Batch [184/391]: Loss 5.997378320898861e-05\n",
      "[Epoch 39] Training Batch [185/391]: Loss 3.3698950574034825e-05\n",
      "[Epoch 39] Training Batch [186/391]: Loss 1.5050342881295364e-05\n",
      "[Epoch 39] Training Batch [187/391]: Loss 4.019766493001953e-05\n",
      "[Epoch 39] Training Batch [188/391]: Loss 4.516235276241787e-05\n",
      "[Epoch 39] Training Batch [189/391]: Loss 3.927786383428611e-05\n",
      "[Epoch 39] Training Batch [190/391]: Loss 4.250417259754613e-05\n",
      "[Epoch 39] Training Batch [191/391]: Loss 5.0641825509956107e-05\n",
      "[Epoch 39] Training Batch [192/391]: Loss 3.072855179198086e-05\n",
      "[Epoch 39] Training Batch [193/391]: Loss 2.1973133698338643e-05\n",
      "[Epoch 39] Training Batch [194/391]: Loss 3.7312751373974606e-05\n",
      "[Epoch 39] Training Batch [195/391]: Loss 4.071793591720052e-05\n",
      "[Epoch 39] Training Batch [196/391]: Loss 1.178698948933743e-05\n",
      "[Epoch 39] Training Batch [197/391]: Loss 1.7250255041290075e-05\n",
      "[Epoch 39] Training Batch [198/391]: Loss 3.5043718526139855e-05\n",
      "[Epoch 39] Training Batch [199/391]: Loss 1.3136837878846563e-05\n",
      "[Epoch 39] Training Batch [200/391]: Loss 2.5331122742500156e-05\n",
      "[Epoch 39] Training Batch [201/391]: Loss 1.8987571820616722e-05\n",
      "[Epoch 39] Training Batch [202/391]: Loss 1.8016458852798678e-05\n",
      "[Epoch 39] Training Batch [203/391]: Loss 1.8798264136421494e-05\n",
      "[Epoch 39] Training Batch [204/391]: Loss 2.4439910703222267e-05\n",
      "[Epoch 39] Training Batch [205/391]: Loss 2.4605031285318546e-05\n",
      "[Epoch 39] Training Batch [206/391]: Loss 3.0261599022196606e-05\n",
      "[Epoch 39] Training Batch [207/391]: Loss 3.696378553286195e-05\n",
      "[Epoch 39] Training Batch [208/391]: Loss 2.0906007193843834e-05\n",
      "[Epoch 39] Training Batch [209/391]: Loss 3.706116331159137e-05\n",
      "[Epoch 39] Training Batch [210/391]: Loss 2.8209844458615407e-05\n",
      "[Epoch 39] Training Batch [211/391]: Loss 2.222062357759569e-05\n",
      "[Epoch 39] Training Batch [212/391]: Loss 4.445484228199348e-05\n",
      "[Epoch 39] Training Batch [213/391]: Loss 2.6298519514966756e-05\n",
      "[Epoch 39] Training Batch [214/391]: Loss 5.424950722954236e-05\n",
      "[Epoch 39] Training Batch [215/391]: Loss 2.8437252694857307e-05\n",
      "[Epoch 39] Training Batch [216/391]: Loss 3.0306153348647058e-05\n",
      "[Epoch 39] Training Batch [217/391]: Loss 6.127070810180157e-05\n",
      "[Epoch 39] Training Batch [218/391]: Loss 2.692456473596394e-05\n",
      "[Epoch 39] Training Batch [219/391]: Loss 2.5908108000294305e-05\n",
      "[Epoch 39] Training Batch [220/391]: Loss 2.9516679205698892e-05\n",
      "[Epoch 39] Training Batch [221/391]: Loss 3.8245711039053276e-05\n",
      "[Epoch 39] Training Batch [222/391]: Loss 2.2953150619287044e-05\n",
      "[Epoch 39] Training Batch [223/391]: Loss 2.141038203262724e-05\n",
      "[Epoch 39] Training Batch [224/391]: Loss 1.4830900909146294e-05\n",
      "[Epoch 39] Training Batch [225/391]: Loss 3.259931690990925e-05\n",
      "[Epoch 39] Training Batch [226/391]: Loss 3.5096680221613497e-05\n",
      "[Epoch 39] Training Batch [227/391]: Loss 2.871578726626467e-05\n",
      "[Epoch 39] Training Batch [228/391]: Loss 2.509684782125987e-05\n",
      "[Epoch 39] Training Batch [229/391]: Loss 1.7905760614667088e-05\n",
      "[Epoch 39] Training Batch [230/391]: Loss 2.5942836145986803e-05\n",
      "[Epoch 39] Training Batch [231/391]: Loss 2.140503238479141e-05\n",
      "[Epoch 39] Training Batch [232/391]: Loss 1.2720061022264417e-05\n",
      "[Epoch 39] Training Batch [233/391]: Loss 3.90894383599516e-05\n",
      "[Epoch 39] Training Batch [234/391]: Loss 2.8173375540063716e-05\n",
      "[Epoch 39] Training Batch [235/391]: Loss 2.4454177037114277e-05\n",
      "[Epoch 39] Training Batch [236/391]: Loss 2.4135111743817106e-05\n",
      "[Epoch 39] Training Batch [237/391]: Loss 1.4719412320118863e-05\n",
      "[Epoch 39] Training Batch [238/391]: Loss 2.8863201805506833e-05\n",
      "[Epoch 39] Training Batch [239/391]: Loss 2.2460071704699658e-05\n",
      "[Epoch 39] Training Batch [240/391]: Loss 3.6964316677767783e-05\n",
      "[Epoch 39] Training Batch [241/391]: Loss 1.4086295777815394e-05\n",
      "[Epoch 39] Training Batch [242/391]: Loss 3.1944549846230075e-05\n",
      "[Epoch 39] Training Batch [243/391]: Loss 4.518871719483286e-05\n",
      "[Epoch 39] Training Batch [244/391]: Loss 3.32464805978816e-05\n",
      "[Epoch 39] Training Batch [245/391]: Loss 2.604072324174922e-05\n",
      "[Epoch 39] Training Batch [246/391]: Loss 2.582596971478779e-05\n",
      "[Epoch 39] Training Batch [247/391]: Loss 4.419791730470024e-05\n",
      "[Epoch 39] Training Batch [248/391]: Loss 3.1275543733499944e-05\n",
      "[Epoch 39] Training Batch [249/391]: Loss 3.1846979254623875e-05\n",
      "[Epoch 39] Training Batch [250/391]: Loss 1.57381782628363e-05\n",
      "[Epoch 39] Training Batch [251/391]: Loss 2.6359077310189605e-05\n",
      "[Epoch 39] Training Batch [252/391]: Loss 2.9501992685254663e-05\n",
      "[Epoch 39] Training Batch [253/391]: Loss 2.9396893296507187e-05\n",
      "[Epoch 39] Training Batch [254/391]: Loss 3.776953963097185e-05\n",
      "[Epoch 39] Training Batch [255/391]: Loss 2.78595198324183e-05\n",
      "[Epoch 39] Training Batch [256/391]: Loss 2.9199745767982677e-05\n",
      "[Epoch 39] Training Batch [257/391]: Loss 9.102369403990451e-06\n",
      "[Epoch 39] Training Batch [258/391]: Loss 1.0263630429108161e-05\n",
      "[Epoch 39] Training Batch [259/391]: Loss 8.172997695510276e-06\n",
      "[Epoch 39] Training Batch [260/391]: Loss 3.162545181112364e-05\n",
      "[Epoch 39] Training Batch [261/391]: Loss 2.931987000920344e-05\n",
      "[Epoch 39] Training Batch [262/391]: Loss 2.6559349862509407e-05\n",
      "[Epoch 39] Training Batch [263/391]: Loss 2.9809290936100297e-05\n",
      "[Epoch 39] Training Batch [264/391]: Loss 3.6492987419478595e-05\n",
      "[Epoch 39] Training Batch [265/391]: Loss 4.978659490006976e-05\n",
      "[Epoch 39] Training Batch [266/391]: Loss 5.377733032219112e-05\n",
      "[Epoch 39] Training Batch [267/391]: Loss 2.473553831805475e-05\n",
      "[Epoch 39] Training Batch [268/391]: Loss 1.2330820027273148e-05\n",
      "[Epoch 39] Training Batch [269/391]: Loss 2.500120535842143e-05\n",
      "[Epoch 39] Training Batch [270/391]: Loss 2.4616439986857586e-05\n",
      "[Epoch 39] Training Batch [271/391]: Loss 1.7098933312809095e-05\n",
      "[Epoch 39] Training Batch [272/391]: Loss 2.5783203454921022e-05\n",
      "[Epoch 39] Training Batch [273/391]: Loss 4.605577123584226e-05\n",
      "[Epoch 39] Training Batch [274/391]: Loss 2.7556943678064272e-05\n",
      "[Epoch 39] Training Batch [275/391]: Loss 2.480003058735747e-05\n",
      "[Epoch 39] Training Batch [276/391]: Loss 2.3399017663905397e-05\n",
      "[Epoch 39] Training Batch [277/391]: Loss 4.601796536007896e-05\n",
      "[Epoch 39] Training Batch [278/391]: Loss 2.6944691853714176e-05\n",
      "[Epoch 39] Training Batch [279/391]: Loss 3.543528509908356e-05\n",
      "[Epoch 39] Training Batch [280/391]: Loss 1.9714079826371744e-05\n",
      "[Epoch 39] Training Batch [281/391]: Loss 2.819180372171104e-05\n",
      "[Epoch 39] Training Batch [282/391]: Loss 3.940979513572529e-05\n",
      "[Epoch 39] Training Batch [283/391]: Loss 2.8996801120229065e-05\n",
      "[Epoch 39] Training Batch [284/391]: Loss 3.4145421523135155e-05\n",
      "[Epoch 39] Training Batch [285/391]: Loss 1.1052778972953092e-05\n",
      "[Epoch 39] Training Batch [286/391]: Loss 4.742551391245797e-05\n",
      "[Epoch 39] Training Batch [287/391]: Loss 2.8362046577967703e-05\n",
      "[Epoch 39] Training Batch [288/391]: Loss 1.8774981072056107e-05\n",
      "[Epoch 39] Training Batch [289/391]: Loss 3.455433397903107e-05\n",
      "[Epoch 39] Training Batch [290/391]: Loss 4.2566938645904884e-05\n",
      "[Epoch 39] Training Batch [291/391]: Loss 3.476689016679302e-05\n",
      "[Epoch 39] Training Batch [292/391]: Loss 1.9015065845451318e-05\n",
      "[Epoch 39] Training Batch [293/391]: Loss 4.4436044845497236e-05\n",
      "[Epoch 39] Training Batch [294/391]: Loss 2.2440151951741427e-05\n",
      "[Epoch 39] Training Batch [295/391]: Loss 4.202475611236878e-05\n",
      "[Epoch 39] Training Batch [296/391]: Loss 3.217223274987191e-05\n",
      "[Epoch 39] Training Batch [297/391]: Loss 2.4367796868318692e-05\n",
      "[Epoch 39] Training Batch [298/391]: Loss 1.006683123705443e-05\n",
      "[Epoch 39] Training Batch [299/391]: Loss 2.9227114282548428e-05\n",
      "[Epoch 39] Training Batch [300/391]: Loss 4.286445982870646e-05\n",
      "[Epoch 39] Training Batch [301/391]: Loss 3.172220749547705e-05\n",
      "[Epoch 39] Training Batch [302/391]: Loss 2.3053366021486e-05\n",
      "[Epoch 39] Training Batch [303/391]: Loss 1.910717946884688e-05\n",
      "[Epoch 39] Training Batch [304/391]: Loss 2.3330998374149203e-05\n",
      "[Epoch 39] Training Batch [305/391]: Loss 2.8794185709557496e-05\n",
      "[Epoch 39] Training Batch [306/391]: Loss 4.264388553565368e-05\n",
      "[Epoch 39] Training Batch [307/391]: Loss 2.962698636110872e-05\n",
      "[Epoch 39] Training Batch [308/391]: Loss 2.3712987967883237e-05\n",
      "[Epoch 39] Training Batch [309/391]: Loss 2.010020580200944e-05\n",
      "[Epoch 39] Training Batch [310/391]: Loss 2.1338724764063954e-05\n",
      "[Epoch 39] Training Batch [311/391]: Loss 3.338086753501557e-05\n",
      "[Epoch 39] Training Batch [312/391]: Loss 4.6480832679662853e-05\n",
      "[Epoch 39] Training Batch [313/391]: Loss 1.2680853615165688e-05\n",
      "[Epoch 39] Training Batch [314/391]: Loss 4.788660953636281e-05\n",
      "[Epoch 39] Training Batch [315/391]: Loss 1.93489613593556e-05\n",
      "[Epoch 39] Training Batch [316/391]: Loss 2.5332650693599135e-05\n",
      "[Epoch 39] Training Batch [317/391]: Loss 2.241242691525258e-05\n",
      "[Epoch 39] Training Batch [318/391]: Loss 2.853016121662222e-05\n",
      "[Epoch 39] Training Batch [319/391]: Loss 2.1772946638520807e-05\n",
      "[Epoch 39] Training Batch [320/391]: Loss 2.095755553455092e-05\n",
      "[Epoch 39] Training Batch [321/391]: Loss 4.447156243259087e-05\n",
      "[Epoch 39] Training Batch [322/391]: Loss 2.3092767150956206e-05\n",
      "[Epoch 39] Training Batch [323/391]: Loss 2.876860526157543e-05\n",
      "[Epoch 39] Training Batch [324/391]: Loss 3.544912760844454e-05\n",
      "[Epoch 39] Training Batch [325/391]: Loss 1.8202441424364224e-05\n",
      "[Epoch 39] Training Batch [326/391]: Loss 3.0458044420811348e-05\n",
      "[Epoch 39] Training Batch [327/391]: Loss 3.2827785616973415e-05\n",
      "[Epoch 39] Training Batch [328/391]: Loss 1.7476160792284645e-05\n",
      "[Epoch 39] Training Batch [329/391]: Loss 3.350336919538677e-05\n",
      "[Epoch 39] Training Batch [330/391]: Loss 3.0338494980242103e-05\n",
      "[Epoch 39] Training Batch [331/391]: Loss 1.9532157239154913e-05\n",
      "[Epoch 39] Training Batch [332/391]: Loss 5.043430792284198e-05\n",
      "[Epoch 39] Training Batch [333/391]: Loss 3.2096082577481866e-05\n",
      "[Epoch 39] Training Batch [334/391]: Loss 3.643736999947578e-05\n",
      "[Epoch 39] Training Batch [335/391]: Loss 3.3707980037434027e-05\n",
      "[Epoch 39] Training Batch [336/391]: Loss 1.8140261090593413e-05\n",
      "[Epoch 39] Training Batch [337/391]: Loss 3.8811944250483066e-05\n",
      "[Epoch 39] Training Batch [338/391]: Loss 1.8881486539612524e-05\n",
      "[Epoch 39] Training Batch [339/391]: Loss 2.6213299861410633e-05\n",
      "[Epoch 39] Training Batch [340/391]: Loss 1.8459408238413744e-05\n",
      "[Epoch 39] Training Batch [341/391]: Loss 3.922208634321578e-05\n",
      "[Epoch 39] Training Batch [342/391]: Loss 1.2620678717212286e-05\n",
      "[Epoch 39] Training Batch [343/391]: Loss 2.203019721491728e-05\n",
      "[Epoch 39] Training Batch [344/391]: Loss 1.0215143447567243e-05\n",
      "[Epoch 39] Training Batch [345/391]: Loss 3.207976988051087e-05\n",
      "[Epoch 39] Training Batch [346/391]: Loss 5.700804103980772e-05\n",
      "[Epoch 39] Training Batch [347/391]: Loss 1.9472836356726475e-05\n",
      "[Epoch 39] Training Batch [348/391]: Loss 3.757761442102492e-05\n",
      "[Epoch 39] Training Batch [349/391]: Loss 2.8628044674405828e-05\n",
      "[Epoch 39] Training Batch [350/391]: Loss 2.6073372282553464e-05\n",
      "[Epoch 39] Training Batch [351/391]: Loss 2.6887872081715614e-05\n",
      "[Epoch 39] Training Batch [352/391]: Loss 1.1431448911025655e-05\n",
      "[Epoch 39] Training Batch [353/391]: Loss 3.2079620723379776e-05\n",
      "[Epoch 39] Training Batch [354/391]: Loss 2.4675637178006582e-05\n",
      "[Epoch 39] Training Batch [355/391]: Loss 2.1760881281807087e-05\n",
      "[Epoch 39] Training Batch [356/391]: Loss 3.599218689487316e-05\n",
      "[Epoch 39] Training Batch [357/391]: Loss 2.1035746613051742e-05\n",
      "[Epoch 39] Training Batch [358/391]: Loss 3.137466046609916e-05\n",
      "[Epoch 39] Training Batch [359/391]: Loss 5.4216237913351506e-05\n",
      "[Epoch 39] Training Batch [360/391]: Loss 1.9554629034246318e-05\n",
      "[Epoch 39] Training Batch [361/391]: Loss 2.7442847567726858e-05\n",
      "[Epoch 39] Training Batch [362/391]: Loss 2.2683720089844428e-05\n",
      "[Epoch 39] Training Batch [363/391]: Loss 3.963896961067803e-05\n",
      "[Epoch 39] Training Batch [364/391]: Loss 4.0953651478048414e-05\n",
      "[Epoch 39] Training Batch [365/391]: Loss 4.188721868558787e-05\n",
      "[Epoch 39] Training Batch [366/391]: Loss 2.383763057878241e-05\n",
      "[Epoch 39] Training Batch [367/391]: Loss 2.6736206564237364e-05\n",
      "[Epoch 39] Training Batch [368/391]: Loss 4.652775533031672e-05\n",
      "[Epoch 39] Training Batch [369/391]: Loss 1.3888693501939997e-05\n",
      "[Epoch 39] Training Batch [370/391]: Loss 2.623727050377056e-05\n",
      "[Epoch 39] Training Batch [371/391]: Loss 2.7596170184551738e-05\n",
      "[Epoch 39] Training Batch [372/391]: Loss 4.675417221733369e-05\n",
      "[Epoch 39] Training Batch [373/391]: Loss 3.382113573024981e-05\n",
      "[Epoch 39] Training Batch [374/391]: Loss 2.9940483727841638e-05\n",
      "[Epoch 39] Training Batch [375/391]: Loss 1.8322305550100282e-05\n",
      "[Epoch 39] Training Batch [376/391]: Loss 2.121412217093166e-05\n",
      "[Epoch 39] Training Batch [377/391]: Loss 4.204723154543899e-05\n",
      "[Epoch 39] Training Batch [378/391]: Loss 2.061610393866431e-05\n",
      "[Epoch 39] Training Batch [379/391]: Loss 3.853674934362061e-05\n",
      "[Epoch 39] Training Batch [380/391]: Loss 2.6256149794789962e-05\n",
      "[Epoch 39] Training Batch [381/391]: Loss 2.1541269234148785e-05\n",
      "[Epoch 39] Training Batch [382/391]: Loss 3.959377863793634e-05\n",
      "[Epoch 39] Training Batch [383/391]: Loss 1.370732752548065e-05\n",
      "[Epoch 39] Training Batch [384/391]: Loss 2.615404264361132e-05\n",
      "[Epoch 39] Training Batch [385/391]: Loss 2.524756928323768e-05\n",
      "[Epoch 39] Training Batch [386/391]: Loss 8.518587492289953e-06\n",
      "[Epoch 39] Training Batch [387/391]: Loss 3.78676995751448e-05\n",
      "[Epoch 39] Training Batch [388/391]: Loss 2.929961192421615e-05\n",
      "[Epoch 39] Training Batch [389/391]: Loss 4.155177157372236e-05\n",
      "[Epoch 39] Training Batch [390/391]: Loss 1.4840599760646e-05\n",
      "[Epoch 39] Training Batch [391/391]: Loss 1.4445851775235496e-05\n",
      "Epoch 39 - Train Loss: 0.0000\n",
      "*********  Epoch 40/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40] Training Batch [1/391]: Loss 2.828792821674142e-05\n",
      "[Epoch 40] Training Batch [2/391]: Loss 1.9982897356385365e-05\n",
      "[Epoch 40] Training Batch [3/391]: Loss 1.4098305655352306e-05\n",
      "[Epoch 40] Training Batch [4/391]: Loss 2.002459041250404e-05\n",
      "[Epoch 40] Training Batch [5/391]: Loss 1.7436672351323068e-05\n",
      "[Epoch 40] Training Batch [6/391]: Loss 3.1080289772944525e-05\n",
      "[Epoch 40] Training Batch [7/391]: Loss 2.4196378944907337e-05\n",
      "[Epoch 40] Training Batch [8/391]: Loss 1.626959965506103e-05\n",
      "[Epoch 40] Training Batch [9/391]: Loss 2.2381767848855816e-05\n",
      "[Epoch 40] Training Batch [10/391]: Loss 2.340987339266576e-05\n",
      "[Epoch 40] Training Batch [11/391]: Loss 2.673180824785959e-05\n",
      "[Epoch 40] Training Batch [12/391]: Loss 3.576669405447319e-05\n",
      "[Epoch 40] Training Batch [13/391]: Loss 1.5902156519587152e-05\n",
      "[Epoch 40] Training Batch [14/391]: Loss 2.192515785282012e-05\n",
      "[Epoch 40] Training Batch [15/391]: Loss 4.7072655434021726e-05\n",
      "[Epoch 40] Training Batch [16/391]: Loss 1.5973280824255198e-05\n",
      "[Epoch 40] Training Batch [17/391]: Loss 2.919059625128284e-05\n",
      "[Epoch 40] Training Batch [18/391]: Loss 2.5743673177203164e-05\n",
      "[Epoch 40] Training Batch [19/391]: Loss 2.030156429100316e-05\n",
      "[Epoch 40] Training Batch [20/391]: Loss 1.5704037650721148e-05\n",
      "[Epoch 40] Training Batch [21/391]: Loss 2.2509018890559673e-05\n",
      "[Epoch 40] Training Batch [22/391]: Loss 2.897670492529869e-05\n",
      "[Epoch 40] Training Batch [23/391]: Loss 3.4213710023323074e-05\n",
      "[Epoch 40] Training Batch [24/391]: Loss 1.6331796359736472e-05\n",
      "[Epoch 40] Training Batch [25/391]: Loss 2.4765668058535084e-05\n",
      "[Epoch 40] Training Batch [26/391]: Loss 2.5699155230540782e-05\n",
      "[Epoch 40] Training Batch [27/391]: Loss 3.0481523936032318e-05\n",
      "[Epoch 40] Training Batch [28/391]: Loss 1.3475187188305426e-05\n",
      "[Epoch 40] Training Batch [29/391]: Loss 2.2714542865287513e-05\n",
      "[Epoch 40] Training Batch [30/391]: Loss 2.6256428100168705e-05\n",
      "[Epoch 40] Training Batch [31/391]: Loss 4.294472455512732e-05\n",
      "[Epoch 40] Training Batch [32/391]: Loss 3.6892961361445487e-05\n",
      "[Epoch 40] Training Batch [33/391]: Loss 2.1221881979727186e-05\n",
      "[Epoch 40] Training Batch [34/391]: Loss 1.8575048670754768e-05\n",
      "[Epoch 40] Training Batch [35/391]: Loss 1.9735220121219754e-05\n",
      "[Epoch 40] Training Batch [36/391]: Loss 2.9942493711132556e-05\n",
      "[Epoch 40] Training Batch [37/391]: Loss 1.3806939023197629e-05\n",
      "[Epoch 40] Training Batch [38/391]: Loss 2.2254767827689648e-05\n",
      "[Epoch 40] Training Batch [39/391]: Loss 3.141106208204292e-05\n",
      "[Epoch 40] Training Batch [40/391]: Loss 2.6359815819887444e-05\n",
      "[Epoch 40] Training Batch [41/391]: Loss 2.1493642634595744e-05\n",
      "[Epoch 40] Training Batch [42/391]: Loss 1.778102341631893e-05\n",
      "[Epoch 40] Training Batch [43/391]: Loss 2.8966682293685153e-05\n",
      "[Epoch 40] Training Batch [44/391]: Loss 3.193522206856869e-05\n",
      "[Epoch 40] Training Batch [45/391]: Loss 2.1729896616307087e-05\n",
      "[Epoch 40] Training Batch [46/391]: Loss 2.566202965681441e-05\n",
      "[Epoch 40] Training Batch [47/391]: Loss 1.859878102550283e-05\n",
      "[Epoch 40] Training Batch [48/391]: Loss 2.636220960994251e-05\n",
      "[Epoch 40] Training Batch [49/391]: Loss 1.07420137283043e-05\n",
      "[Epoch 40] Training Batch [50/391]: Loss 2.0340185074019246e-05\n",
      "[Epoch 40] Training Batch [51/391]: Loss 2.5104960513999686e-05\n",
      "[Epoch 40] Training Batch [52/391]: Loss 1.9227303710067645e-05\n",
      "[Epoch 40] Training Batch [53/391]: Loss 3.5830737033393234e-05\n",
      "[Epoch 40] Training Batch [54/391]: Loss 1.570271524542477e-05\n",
      "[Epoch 40] Training Batch [55/391]: Loss 1.8920423826784827e-05\n",
      "[Epoch 40] Training Batch [56/391]: Loss 3.2740899769123644e-05\n",
      "[Epoch 40] Training Batch [57/391]: Loss 1.745986992318649e-05\n",
      "[Epoch 40] Training Batch [58/391]: Loss 3.911721796612255e-05\n",
      "[Epoch 40] Training Batch [59/391]: Loss 2.318197766726371e-05\n",
      "[Epoch 40] Training Batch [60/391]: Loss 3.534853385644965e-05\n",
      "[Epoch 40] Training Batch [61/391]: Loss 1.604122189746704e-05\n",
      "[Epoch 40] Training Batch [62/391]: Loss 2.5706909582368098e-05\n",
      "[Epoch 40] Training Batch [63/391]: Loss 2.3525513825006783e-05\n",
      "[Epoch 40] Training Batch [64/391]: Loss 2.0177065380266868e-05\n",
      "[Epoch 40] Training Batch [65/391]: Loss 2.4594381102360785e-05\n",
      "[Epoch 40] Training Batch [66/391]: Loss 2.775877328531351e-05\n",
      "[Epoch 40] Training Batch [67/391]: Loss 4.017480387119576e-05\n",
      "[Epoch 40] Training Batch [68/391]: Loss 2.28290355153149e-05\n",
      "[Epoch 40] Training Batch [69/391]: Loss 1.926343611557968e-05\n",
      "[Epoch 40] Training Batch [70/391]: Loss 3.408069824217819e-05\n",
      "[Epoch 40] Training Batch [71/391]: Loss 3.3370914024999365e-05\n",
      "[Epoch 40] Training Batch [72/391]: Loss 1.508431068941718e-05\n",
      "[Epoch 40] Training Batch [73/391]: Loss 2.129484710167162e-05\n",
      "[Epoch 40] Training Batch [74/391]: Loss 2.4122055037878454e-05\n",
      "[Epoch 40] Training Batch [75/391]: Loss 1.967267962754704e-05\n",
      "[Epoch 40] Training Batch [76/391]: Loss 3.0376811992027797e-05\n",
      "[Epoch 40] Training Batch [77/391]: Loss 2.6279532903572544e-05\n",
      "[Epoch 40] Training Batch [78/391]: Loss 3.9950115024112165e-05\n",
      "[Epoch 40] Training Batch [79/391]: Loss 3.311662658234127e-05\n",
      "[Epoch 40] Training Batch [80/391]: Loss 3.8646852772217244e-05\n",
      "[Epoch 40] Training Batch [81/391]: Loss 9.90021726465784e-06\n",
      "[Epoch 40] Training Batch [82/391]: Loss 1.504154261056101e-05\n",
      "[Epoch 40] Training Batch [83/391]: Loss 2.2181389795150608e-05\n",
      "[Epoch 40] Training Batch [84/391]: Loss 2.3231144950841554e-05\n",
      "[Epoch 40] Training Batch [85/391]: Loss 2.1764959456049837e-05\n",
      "[Epoch 40] Training Batch [86/391]: Loss 4.261514914105646e-05\n",
      "[Epoch 40] Training Batch [87/391]: Loss 2.9087272196193226e-05\n",
      "[Epoch 40] Training Batch [88/391]: Loss 2.3450977096217684e-05\n",
      "[Epoch 40] Training Batch [89/391]: Loss 2.417734867776744e-05\n",
      "[Epoch 40] Training Batch [90/391]: Loss 2.4966626369860023e-05\n",
      "[Epoch 40] Training Batch [91/391]: Loss 2.5244846256100573e-05\n",
      "[Epoch 40] Training Batch [92/391]: Loss 1.7859611034509726e-05\n",
      "[Epoch 40] Training Batch [93/391]: Loss 2.8955213565495797e-05\n",
      "[Epoch 40] Training Batch [94/391]: Loss 4.0733604691922665e-05\n",
      "[Epoch 40] Training Batch [95/391]: Loss 1.4662655303254724e-05\n",
      "[Epoch 40] Training Batch [96/391]: Loss 1.9083543520537205e-05\n",
      "[Epoch 40] Training Batch [97/391]: Loss 3.327827653265558e-05\n",
      "[Epoch 40] Training Batch [98/391]: Loss 1.3183612281864043e-05\n",
      "[Epoch 40] Training Batch [99/391]: Loss 2.2939766495255753e-05\n",
      "[Epoch 40] Training Batch [100/391]: Loss 2.913143180194311e-05\n",
      "[Epoch 40] Training Batch [101/391]: Loss 2.968849912576843e-05\n",
      "[Epoch 40] Training Batch [102/391]: Loss 4.265081952326e-05\n",
      "[Epoch 40] Training Batch [103/391]: Loss 2.4200713596655987e-05\n",
      "[Epoch 40] Training Batch [104/391]: Loss 4.3840591388288885e-05\n",
      "[Epoch 40] Training Batch [105/391]: Loss 1.8849215848604217e-05\n",
      "[Epoch 40] Training Batch [106/391]: Loss 3.631821891758591e-05\n",
      "[Epoch 40] Training Batch [107/391]: Loss 2.9944774723844603e-05\n",
      "[Epoch 40] Training Batch [108/391]: Loss 1.9643657651613466e-05\n",
      "[Epoch 40] Training Batch [109/391]: Loss 3.7581106880679727e-05\n",
      "[Epoch 40] Training Batch [110/391]: Loss 4.6122484491206706e-05\n",
      "[Epoch 40] Training Batch [111/391]: Loss 4.1023100493475795e-05\n",
      "[Epoch 40] Training Batch [112/391]: Loss 4.3656797060975805e-05\n",
      "[Epoch 40] Training Batch [113/391]: Loss 2.6603040168993175e-05\n",
      "[Epoch 40] Training Batch [114/391]: Loss 3.398542685317807e-05\n",
      "[Epoch 40] Training Batch [115/391]: Loss 2.2623815311817452e-05\n",
      "[Epoch 40] Training Batch [116/391]: Loss 2.1034506062278524e-05\n",
      "[Epoch 40] Training Batch [117/391]: Loss 1.5030571375973523e-05\n",
      "[Epoch 40] Training Batch [118/391]: Loss 2.1428779291454703e-05\n",
      "[Epoch 40] Training Batch [119/391]: Loss 2.6717822038335726e-05\n",
      "[Epoch 40] Training Batch [120/391]: Loss 1.2127176887588575e-05\n",
      "[Epoch 40] Training Batch [121/391]: Loss 2.3199791030492634e-05\n",
      "[Epoch 40] Training Batch [122/391]: Loss 2.977442454721313e-05\n",
      "[Epoch 40] Training Batch [123/391]: Loss 2.4311104425578378e-05\n",
      "[Epoch 40] Training Batch [124/391]: Loss 3.122766429441981e-05\n",
      "[Epoch 40] Training Batch [125/391]: Loss 3.430715514696203e-05\n",
      "[Epoch 40] Training Batch [126/391]: Loss 2.1453071894939058e-05\n",
      "[Epoch 40] Training Batch [127/391]: Loss 1.7666161511442624e-05\n",
      "[Epoch 40] Training Batch [128/391]: Loss 3.09373572235927e-05\n",
      "[Epoch 40] Training Batch [129/391]: Loss 1.6568066712352447e-05\n",
      "[Epoch 40] Training Batch [130/391]: Loss 3.2959516829578206e-05\n",
      "[Epoch 40] Training Batch [131/391]: Loss 3.207334884791635e-05\n",
      "[Epoch 40] Training Batch [132/391]: Loss 2.6822153813554905e-05\n",
      "[Epoch 40] Training Batch [133/391]: Loss 1.8524269762565382e-05\n",
      "[Epoch 40] Training Batch [134/391]: Loss 3.751715848920867e-05\n",
      "[Epoch 40] Training Batch [135/391]: Loss 3.219845530111343e-05\n",
      "[Epoch 40] Training Batch [136/391]: Loss 4.5316166506381705e-05\n",
      "[Epoch 40] Training Batch [137/391]: Loss 2.4478738851030357e-05\n",
      "[Epoch 40] Training Batch [138/391]: Loss 2.596437479951419e-05\n",
      "[Epoch 40] Training Batch [139/391]: Loss 2.6185400201939046e-05\n",
      "[Epoch 40] Training Batch [140/391]: Loss 2.7797284928965382e-05\n",
      "[Epoch 40] Training Batch [141/391]: Loss 4.3706193537218496e-05\n",
      "[Epoch 40] Training Batch [142/391]: Loss 2.9749615350738168e-05\n",
      "[Epoch 40] Training Batch [143/391]: Loss 5.020563185098581e-05\n",
      "[Epoch 40] Training Batch [144/391]: Loss 2.0787590983673e-05\n",
      "[Epoch 40] Training Batch [145/391]: Loss 4.1018509364221245e-05\n",
      "[Epoch 40] Training Batch [146/391]: Loss 4.159431409789249e-05\n",
      "[Epoch 40] Training Batch [147/391]: Loss 3.218338679289445e-05\n",
      "[Epoch 40] Training Batch [148/391]: Loss 3.939683301723562e-05\n",
      "[Epoch 40] Training Batch [149/391]: Loss 1.7793368897400796e-05\n",
      "[Epoch 40] Training Batch [150/391]: Loss 1.1353100489941426e-05\n",
      "[Epoch 40] Training Batch [151/391]: Loss 3.9250498957699165e-05\n",
      "[Epoch 40] Training Batch [152/391]: Loss 2.6076600988744758e-05\n",
      "[Epoch 40] Training Batch [153/391]: Loss 2.8697852030745707e-05\n",
      "[Epoch 40] Training Batch [154/391]: Loss 2.042012056335807e-05\n",
      "[Epoch 40] Training Batch [155/391]: Loss 2.670475259947125e-05\n",
      "[Epoch 40] Training Batch [156/391]: Loss 3.365910379216075e-05\n",
      "[Epoch 40] Training Batch [157/391]: Loss 3.3709045965224504e-05\n",
      "[Epoch 40] Training Batch [158/391]: Loss 2.9296079446794465e-05\n",
      "[Epoch 40] Training Batch [159/391]: Loss 3.212089723092504e-05\n",
      "[Epoch 40] Training Batch [160/391]: Loss 2.5685711079859175e-05\n",
      "[Epoch 40] Training Batch [161/391]: Loss 1.3881207451049704e-05\n",
      "[Epoch 40] Training Batch [162/391]: Loss 2.2593656467506662e-05\n",
      "[Epoch 40] Training Batch [163/391]: Loss 3.92057336284779e-05\n",
      "[Epoch 40] Training Batch [164/391]: Loss 2.3146503735915758e-05\n",
      "[Epoch 40] Training Batch [165/391]: Loss 2.296216189279221e-05\n",
      "[Epoch 40] Training Batch [166/391]: Loss 4.2240288166794926e-05\n",
      "[Epoch 40] Training Batch [167/391]: Loss 2.5811197701841593e-05\n",
      "[Epoch 40] Training Batch [168/391]: Loss 3.747070877579972e-05\n",
      "[Epoch 40] Training Batch [169/391]: Loss 4.8139249884116e-06\n",
      "[Epoch 40] Training Batch [170/391]: Loss 3.294455018476583e-05\n",
      "[Epoch 40] Training Batch [171/391]: Loss 3.1872608815319836e-05\n",
      "[Epoch 40] Training Batch [172/391]: Loss 3.6590059607988223e-05\n",
      "[Epoch 40] Training Batch [173/391]: Loss 4.344473927631043e-05\n",
      "[Epoch 40] Training Batch [174/391]: Loss 2.7663563741953112e-05\n",
      "[Epoch 40] Training Batch [175/391]: Loss 2.8401218514773063e-05\n",
      "[Epoch 40] Training Batch [176/391]: Loss 3.236758129787631e-05\n",
      "[Epoch 40] Training Batch [177/391]: Loss 1.7609629139769822e-05\n",
      "[Epoch 40] Training Batch [178/391]: Loss 2.401713209110312e-05\n",
      "[Epoch 40] Training Batch [179/391]: Loss 2.3300990505958907e-05\n",
      "[Epoch 40] Training Batch [180/391]: Loss 2.239846799056977e-05\n",
      "[Epoch 40] Training Batch [181/391]: Loss 3.610794374253601e-05\n",
      "[Epoch 40] Training Batch [182/391]: Loss 2.7487567422213033e-05\n",
      "[Epoch 40] Training Batch [183/391]: Loss 1.74833712662803e-05\n",
      "[Epoch 40] Training Batch [184/391]: Loss 2.4444017981295474e-05\n",
      "[Epoch 40] Training Batch [185/391]: Loss 2.123535523423925e-05\n",
      "[Epoch 40] Training Batch [186/391]: Loss 1.3517944353225175e-05\n",
      "[Epoch 40] Training Batch [187/391]: Loss 6.612556899199262e-05\n",
      "[Epoch 40] Training Batch [188/391]: Loss 2.2087408069637604e-05\n",
      "[Epoch 40] Training Batch [189/391]: Loss 4.2698608012869954e-05\n",
      "[Epoch 40] Training Batch [190/391]: Loss 2.395673618593719e-05\n",
      "[Epoch 40] Training Batch [191/391]: Loss 9.947913895302918e-06\n",
      "[Epoch 40] Training Batch [192/391]: Loss 2.9068405638099648e-05\n",
      "[Epoch 40] Training Batch [193/391]: Loss 2.2859760065330192e-05\n",
      "[Epoch 40] Training Batch [194/391]: Loss 3.863268648274243e-05\n",
      "[Epoch 40] Training Batch [195/391]: Loss 3.829590423265472e-05\n",
      "[Epoch 40] Training Batch [196/391]: Loss 2.069189758913126e-05\n",
      "[Epoch 40] Training Batch [197/391]: Loss 2.303049222973641e-05\n",
      "[Epoch 40] Training Batch [198/391]: Loss 3.3304651879007e-05\n",
      "[Epoch 40] Training Batch [199/391]: Loss 2.932082861661911e-05\n",
      "[Epoch 40] Training Batch [200/391]: Loss 9.726953067001887e-06\n",
      "[Epoch 40] Training Batch [201/391]: Loss 3.491030656732619e-05\n",
      "[Epoch 40] Training Batch [202/391]: Loss 1.762068859534338e-05\n",
      "[Epoch 40] Training Batch [203/391]: Loss 1.72779182321392e-05\n",
      "[Epoch 40] Training Batch [204/391]: Loss 5.163454989087768e-05\n",
      "[Epoch 40] Training Batch [205/391]: Loss 2.667871376615949e-05\n",
      "[Epoch 40] Training Batch [206/391]: Loss 1.112590416596504e-05\n",
      "[Epoch 40] Training Batch [207/391]: Loss 2.2102567527326755e-05\n",
      "[Epoch 40] Training Batch [208/391]: Loss 2.858007610484492e-05\n",
      "[Epoch 40] Training Batch [209/391]: Loss 1.837883610278368e-05\n",
      "[Epoch 40] Training Batch [210/391]: Loss 1.9251461708336137e-05\n",
      "[Epoch 40] Training Batch [211/391]: Loss 4.128430737182498e-05\n",
      "[Epoch 40] Training Batch [212/391]: Loss 2.4527225832571276e-05\n",
      "[Epoch 40] Training Batch [213/391]: Loss 2.8505888622021303e-05\n",
      "[Epoch 40] Training Batch [214/391]: Loss 1.7906158973346464e-05\n",
      "[Epoch 40] Training Batch [215/391]: Loss 1.5491796148126014e-05\n",
      "[Epoch 40] Training Batch [216/391]: Loss 1.841314951889217e-05\n",
      "[Epoch 40] Training Batch [217/391]: Loss 2.791498445731122e-05\n",
      "[Epoch 40] Training Batch [218/391]: Loss 2.551761281210929e-05\n",
      "[Epoch 40] Training Batch [219/391]: Loss 1.7137268514488824e-05\n",
      "[Epoch 40] Training Batch [220/391]: Loss 4.051627547596581e-05\n",
      "[Epoch 40] Training Batch [221/391]: Loss 2.425324601063039e-05\n",
      "[Epoch 40] Training Batch [222/391]: Loss 4.374404306872748e-05\n",
      "[Epoch 40] Training Batch [223/391]: Loss 2.201092866016552e-05\n",
      "[Epoch 40] Training Batch [224/391]: Loss 1.2181168131064624e-05\n",
      "[Epoch 40] Training Batch [225/391]: Loss 3.578153337002732e-05\n",
      "[Epoch 40] Training Batch [226/391]: Loss 1.8652768631000072e-05\n",
      "[Epoch 40] Training Batch [227/391]: Loss 2.4296379706356674e-05\n",
      "[Epoch 40] Training Batch [228/391]: Loss 1.8446782632963732e-05\n",
      "[Epoch 40] Training Batch [229/391]: Loss 1.9936462194891647e-05\n",
      "[Epoch 40] Training Batch [230/391]: Loss 2.483258504071273e-05\n",
      "[Epoch 40] Training Batch [231/391]: Loss 1.5420997442561202e-05\n",
      "[Epoch 40] Training Batch [232/391]: Loss 1.4034887499292381e-05\n",
      "[Epoch 40] Training Batch [233/391]: Loss 3.069594822591171e-05\n",
      "[Epoch 40] Training Batch [234/391]: Loss 3.117637606919743e-05\n",
      "[Epoch 40] Training Batch [235/391]: Loss 2.3499356757383794e-05\n",
      "[Epoch 40] Training Batch [236/391]: Loss 4.4912714656675234e-05\n",
      "[Epoch 40] Training Batch [237/391]: Loss 4.7368583182105795e-05\n",
      "[Epoch 40] Training Batch [238/391]: Loss 2.719970143516548e-05\n",
      "[Epoch 40] Training Batch [239/391]: Loss 2.4473150915582664e-05\n",
      "[Epoch 40] Training Batch [240/391]: Loss 4.403579805511981e-05\n",
      "[Epoch 40] Training Batch [241/391]: Loss 3.470058800303377e-05\n",
      "[Epoch 40] Training Batch [242/391]: Loss 2.2137784981168807e-05\n",
      "[Epoch 40] Training Batch [243/391]: Loss 2.075445809168741e-05\n",
      "[Epoch 40] Training Batch [244/391]: Loss 2.4050334104686044e-05\n",
      "[Epoch 40] Training Batch [245/391]: Loss 2.917553501902148e-05\n",
      "[Epoch 40] Training Batch [246/391]: Loss 2.3553840946988203e-05\n",
      "[Epoch 40] Training Batch [247/391]: Loss 3.490771268843673e-05\n",
      "[Epoch 40] Training Batch [248/391]: Loss 1.8421760614728555e-05\n",
      "[Epoch 40] Training Batch [249/391]: Loss 4.232096398482099e-05\n",
      "[Epoch 40] Training Batch [250/391]: Loss 3.1190342269837856e-05\n",
      "[Epoch 40] Training Batch [251/391]: Loss 2.6468376745469868e-05\n",
      "[Epoch 40] Training Batch [252/391]: Loss 1.8569655367173254e-05\n",
      "[Epoch 40] Training Batch [253/391]: Loss 4.938124766340479e-05\n",
      "[Epoch 40] Training Batch [254/391]: Loss 3.119882239843719e-05\n",
      "[Epoch 40] Training Batch [255/391]: Loss 2.1152736735530198e-05\n",
      "[Epoch 40] Training Batch [256/391]: Loss 2.9194978196755983e-05\n",
      "[Epoch 40] Training Batch [257/391]: Loss 2.2134337996249087e-05\n",
      "[Epoch 40] Training Batch [258/391]: Loss 3.201301660737954e-05\n",
      "[Epoch 40] Training Batch [259/391]: Loss 9.773761121323332e-06\n",
      "[Epoch 40] Training Batch [260/391]: Loss 3.4180586226284504e-05\n",
      "[Epoch 40] Training Batch [261/391]: Loss 3.1785220926394686e-05\n",
      "[Epoch 40] Training Batch [262/391]: Loss 1.5885398170212284e-05\n",
      "[Epoch 40] Training Batch [263/391]: Loss 2.3827618861105293e-05\n",
      "[Epoch 40] Training Batch [264/391]: Loss 1.8966980860568583e-05\n",
      "[Epoch 40] Training Batch [265/391]: Loss 2.9239003197290003e-05\n",
      "[Epoch 40] Training Batch [266/391]: Loss 3.1470346584683284e-05\n",
      "[Epoch 40] Training Batch [267/391]: Loss 2.3354366931016557e-05\n",
      "[Epoch 40] Training Batch [268/391]: Loss 3.9431004552170634e-05\n",
      "[Epoch 40] Training Batch [269/391]: Loss 3.6266010283725336e-05\n",
      "[Epoch 40] Training Batch [270/391]: Loss 3.106459917034954e-05\n",
      "[Epoch 40] Training Batch [271/391]: Loss 2.31137964874506e-05\n",
      "[Epoch 40] Training Batch [272/391]: Loss 1.7290883988607675e-05\n",
      "[Epoch 40] Training Batch [273/391]: Loss 2.0644396499847062e-05\n",
      "[Epoch 40] Training Batch [274/391]: Loss 2.9705372071475722e-05\n",
      "[Epoch 40] Training Batch [275/391]: Loss 2.7495480026118457e-05\n",
      "[Epoch 40] Training Batch [276/391]: Loss 2.283733556396328e-05\n",
      "[Epoch 40] Training Batch [277/391]: Loss 2.351277908019256e-05\n",
      "[Epoch 40] Training Batch [278/391]: Loss 3.838220436591655e-05\n",
      "[Epoch 40] Training Batch [279/391]: Loss 3.06420297420118e-05\n",
      "[Epoch 40] Training Batch [280/391]: Loss 2.891726944653783e-05\n",
      "[Epoch 40] Training Batch [281/391]: Loss 2.6375109882792458e-05\n",
      "[Epoch 40] Training Batch [282/391]: Loss 5.17832268087659e-05\n",
      "[Epoch 40] Training Batch [283/391]: Loss 2.4903745725168847e-05\n",
      "[Epoch 40] Training Batch [284/391]: Loss 2.2787096895626746e-05\n",
      "[Epoch 40] Training Batch [285/391]: Loss 3.18877246172633e-05\n",
      "[Epoch 40] Training Batch [286/391]: Loss 4.276688923710026e-05\n",
      "[Epoch 40] Training Batch [287/391]: Loss 2.29714423767291e-05\n",
      "[Epoch 40] Training Batch [288/391]: Loss 2.343281448702328e-05\n",
      "[Epoch 40] Training Batch [289/391]: Loss 1.681087087490596e-05\n",
      "[Epoch 40] Training Batch [290/391]: Loss 2.4157781808753498e-05\n",
      "[Epoch 40] Training Batch [291/391]: Loss 2.864034649974201e-05\n",
      "[Epoch 40] Training Batch [292/391]: Loss 4.2906020098598674e-05\n",
      "[Epoch 40] Training Batch [293/391]: Loss 3.17728154186625e-05\n",
      "[Epoch 40] Training Batch [294/391]: Loss 2.0406170733622275e-05\n",
      "[Epoch 40] Training Batch [295/391]: Loss 1.942921335285064e-05\n",
      "[Epoch 40] Training Batch [296/391]: Loss 3.8183243304956704e-05\n",
      "[Epoch 40] Training Batch [297/391]: Loss 1.7875830963021144e-05\n",
      "[Epoch 40] Training Batch [298/391]: Loss 2.7513880922924727e-05\n",
      "[Epoch 40] Training Batch [299/391]: Loss 1.594449713593349e-05\n",
      "[Epoch 40] Training Batch [300/391]: Loss 1.0077294064103626e-05\n",
      "[Epoch 40] Training Batch [301/391]: Loss 1.471554787713103e-05\n",
      "[Epoch 40] Training Batch [302/391]: Loss 1.4356529391079675e-05\n",
      "[Epoch 40] Training Batch [303/391]: Loss 3.466862835921347e-05\n",
      "[Epoch 40] Training Batch [304/391]: Loss 2.5237466616090387e-05\n",
      "[Epoch 40] Training Batch [305/391]: Loss 2.834048427757807e-05\n",
      "[Epoch 40] Training Batch [306/391]: Loss 2.3884080292191356e-05\n",
      "[Epoch 40] Training Batch [307/391]: Loss 3.746724905795418e-05\n",
      "[Epoch 40] Training Batch [308/391]: Loss 3.320341056678444e-05\n",
      "[Epoch 40] Training Batch [309/391]: Loss 2.016514554270543e-05\n",
      "[Epoch 40] Training Batch [310/391]: Loss 2.5311501303804107e-05\n",
      "[Epoch 40] Training Batch [311/391]: Loss 2.6195324608124793e-05\n",
      "[Epoch 40] Training Batch [312/391]: Loss 1.612134110473562e-05\n",
      "[Epoch 40] Training Batch [313/391]: Loss 2.8766269679181278e-05\n",
      "[Epoch 40] Training Batch [314/391]: Loss 1.8925511540146545e-05\n",
      "[Epoch 40] Training Batch [315/391]: Loss 2.293010948051233e-05\n",
      "[Epoch 40] Training Batch [316/391]: Loss 2.5358003767905757e-05\n",
      "[Epoch 40] Training Batch [317/391]: Loss 1.8696857296163216e-05\n",
      "[Epoch 40] Training Batch [318/391]: Loss 3.849196218652651e-05\n",
      "[Epoch 40] Training Batch [319/391]: Loss 1.969130607903935e-05\n",
      "[Epoch 40] Training Batch [320/391]: Loss 2.662797851371579e-05\n",
      "[Epoch 40] Training Batch [321/391]: Loss 3.184174420312047e-05\n",
      "[Epoch 40] Training Batch [322/391]: Loss 1.7658210708759725e-05\n",
      "[Epoch 40] Training Batch [323/391]: Loss 1.4745663065696135e-05\n",
      "[Epoch 40] Training Batch [324/391]: Loss 2.6724987037596293e-05\n",
      "[Epoch 40] Training Batch [325/391]: Loss 1.9143491954309866e-05\n",
      "[Epoch 40] Training Batch [326/391]: Loss 1.6912808860070072e-05\n",
      "[Epoch 40] Training Batch [327/391]: Loss 2.46494328166591e-05\n",
      "[Epoch 40] Training Batch [328/391]: Loss 3.339069371577352e-05\n",
      "[Epoch 40] Training Batch [329/391]: Loss 1.9225339201511815e-05\n",
      "[Epoch 40] Training Batch [330/391]: Loss 2.820094596245326e-05\n",
      "[Epoch 40] Training Batch [331/391]: Loss 5.3932148148305714e-05\n",
      "[Epoch 40] Training Batch [332/391]: Loss 2.873194716812577e-05\n",
      "[Epoch 40] Training Batch [333/391]: Loss 2.2219148377189413e-05\n",
      "[Epoch 40] Training Batch [334/391]: Loss 2.5076253223232925e-05\n",
      "[Epoch 40] Training Batch [335/391]: Loss 4.5819291699444875e-05\n",
      "[Epoch 40] Training Batch [336/391]: Loss 2.9720280508627184e-05\n",
      "[Epoch 40] Training Batch [337/391]: Loss 3.1203828257275745e-05\n",
      "[Epoch 40] Training Batch [338/391]: Loss 2.018262784986291e-05\n",
      "[Epoch 40] Training Batch [339/391]: Loss 1.537392927275505e-05\n",
      "[Epoch 40] Training Batch [340/391]: Loss 4.194044231553562e-05\n",
      "[Epoch 40] Training Batch [341/391]: Loss 3.550608744262718e-05\n",
      "[Epoch 40] Training Batch [342/391]: Loss 2.279276304761879e-05\n",
      "[Epoch 40] Training Batch [343/391]: Loss 2.6211977456114255e-05\n",
      "[Epoch 40] Training Batch [344/391]: Loss 2.8114667657064274e-05\n",
      "[Epoch 40] Training Batch [345/391]: Loss 2.9600922061945312e-05\n",
      "[Epoch 40] Training Batch [346/391]: Loss 2.272547317261342e-05\n",
      "[Epoch 40] Training Batch [347/391]: Loss 1.7841039152699523e-05\n",
      "[Epoch 40] Training Batch [348/391]: Loss 1.7895899873110466e-05\n",
      "[Epoch 40] Training Batch [349/391]: Loss 3.157075843773782e-05\n",
      "[Epoch 40] Training Batch [350/391]: Loss 3.548654422047548e-05\n",
      "[Epoch 40] Training Batch [351/391]: Loss 1.0552173080213834e-05\n",
      "[Epoch 40] Training Batch [352/391]: Loss 2.361040787945967e-05\n",
      "[Epoch 40] Training Batch [353/391]: Loss 2.7492462322697975e-05\n",
      "[Epoch 40] Training Batch [354/391]: Loss 1.7866783309727907e-05\n",
      "[Epoch 40] Training Batch [355/391]: Loss 4.014048317912966e-05\n",
      "[Epoch 40] Training Batch [356/391]: Loss 1.124505706684431e-05\n",
      "[Epoch 40] Training Batch [357/391]: Loss 2.9010914659011178e-05\n",
      "[Epoch 40] Training Batch [358/391]: Loss 3.588964682421647e-05\n",
      "[Epoch 40] Training Batch [359/391]: Loss 2.2471962438430637e-05\n",
      "[Epoch 40] Training Batch [360/391]: Loss 2.7514059183886275e-05\n",
      "[Epoch 40] Training Batch [361/391]: Loss 5.218338264967315e-05\n",
      "[Epoch 40] Training Batch [362/391]: Loss 1.3243046851130202e-05\n",
      "[Epoch 40] Training Batch [363/391]: Loss 3.9266949897864833e-05\n",
      "[Epoch 40] Training Batch [364/391]: Loss 3.0423276257351972e-05\n",
      "[Epoch 40] Training Batch [365/391]: Loss 1.5647534382878803e-05\n",
      "[Epoch 40] Training Batch [366/391]: Loss 2.4288925487780944e-05\n",
      "[Epoch 40] Training Batch [367/391]: Loss 5.6000149925239384e-05\n",
      "[Epoch 40] Training Batch [368/391]: Loss 2.4658838810864836e-05\n",
      "[Epoch 40] Training Batch [369/391]: Loss 4.410224937601015e-05\n",
      "[Epoch 40] Training Batch [370/391]: Loss 2.0676736312452704e-05\n",
      "[Epoch 40] Training Batch [371/391]: Loss 2.0159355699433945e-05\n",
      "[Epoch 40] Training Batch [372/391]: Loss 1.4518763236992527e-05\n",
      "[Epoch 40] Training Batch [373/391]: Loss 1.656988388276659e-05\n",
      "[Epoch 40] Training Batch [374/391]: Loss 1.4283570635598153e-05\n",
      "[Epoch 40] Training Batch [375/391]: Loss 4.5433251216309145e-05\n",
      "[Epoch 40] Training Batch [376/391]: Loss 2.2622298274654895e-05\n",
      "[Epoch 40] Training Batch [377/391]: Loss 2.01415314222686e-05\n",
      "[Epoch 40] Training Batch [378/391]: Loss 3.3715550671331584e-05\n",
      "[Epoch 40] Training Batch [379/391]: Loss 2.3633274395251647e-05\n",
      "[Epoch 40] Training Batch [380/391]: Loss 3.015615584445186e-05\n",
      "[Epoch 40] Training Batch [381/391]: Loss 3.379377085366286e-05\n",
      "[Epoch 40] Training Batch [382/391]: Loss 1.4584038581233472e-05\n",
      "[Epoch 40] Training Batch [383/391]: Loss 3.350236511323601e-05\n",
      "[Epoch 40] Training Batch [384/391]: Loss 3.960702815675177e-05\n",
      "[Epoch 40] Training Batch [385/391]: Loss 1.2742492799588945e-05\n",
      "[Epoch 40] Training Batch [386/391]: Loss 1.9334709577378817e-05\n",
      "[Epoch 40] Training Batch [387/391]: Loss 2.0951181795680895e-05\n",
      "[Epoch 40] Training Batch [388/391]: Loss 1.3088650121062528e-05\n",
      "[Epoch 40] Training Batch [389/391]: Loss 2.2387757780961692e-05\n",
      "[Epoch 40] Training Batch [390/391]: Loss 2.7644276997307315e-05\n",
      "[Epoch 40] Training Batch [391/391]: Loss 5.0883238145615906e-05\n",
      "Epoch 40 - Train Loss: 0.0000\n",
      "*********  Epoch 41/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41] Training Batch [1/391]: Loss 2.4903692974476144e-05\n",
      "[Epoch 41] Training Batch [2/391]: Loss 1.5672647350584157e-05\n",
      "[Epoch 41] Training Batch [3/391]: Loss 3.1299998227041215e-05\n",
      "[Epoch 41] Training Batch [4/391]: Loss 2.2381884264177643e-05\n",
      "[Epoch 41] Training Batch [5/391]: Loss 2.656906144693494e-05\n",
      "[Epoch 41] Training Batch [6/391]: Loss 2.64435129793128e-05\n",
      "[Epoch 41] Training Batch [7/391]: Loss 3.098196611972526e-05\n",
      "[Epoch 41] Training Batch [8/391]: Loss 1.7789341654861346e-05\n",
      "[Epoch 41] Training Batch [9/391]: Loss 1.8674818420549855e-05\n",
      "[Epoch 41] Training Batch [10/391]: Loss 2.37703206948936e-05\n",
      "[Epoch 41] Training Batch [11/391]: Loss 3.1832132663112134e-05\n",
      "[Epoch 41] Training Batch [12/391]: Loss 2.3047114154906012e-05\n",
      "[Epoch 41] Training Batch [13/391]: Loss 2.5022942281793803e-05\n",
      "[Epoch 41] Training Batch [14/391]: Loss 1.7250373275601305e-05\n",
      "[Epoch 41] Training Batch [15/391]: Loss 1.5127065125852823e-05\n",
      "[Epoch 41] Training Batch [16/391]: Loss 3.380619455128908e-05\n",
      "[Epoch 41] Training Batch [17/391]: Loss 3.04444765788503e-05\n",
      "[Epoch 41] Training Batch [18/391]: Loss 2.0342622519819997e-05\n",
      "[Epoch 41] Training Batch [19/391]: Loss 1.5962374163791537e-05\n",
      "[Epoch 41] Training Batch [20/391]: Loss 3.7892070395173505e-05\n",
      "[Epoch 41] Training Batch [21/391]: Loss 2.3071594114298932e-05\n",
      "[Epoch 41] Training Batch [22/391]: Loss 2.3329004761762917e-05\n",
      "[Epoch 41] Training Batch [23/391]: Loss 1.666754724283237e-05\n",
      "[Epoch 41] Training Batch [24/391]: Loss 2.7275978936813772e-05\n",
      "[Epoch 41] Training Batch [25/391]: Loss 2.3914819394121878e-05\n",
      "[Epoch 41] Training Batch [26/391]: Loss 1.3920979654358234e-05\n",
      "[Epoch 41] Training Batch [27/391]: Loss 2.3265365598490462e-05\n",
      "[Epoch 41] Training Batch [28/391]: Loss 2.5783472665352747e-05\n",
      "[Epoch 41] Training Batch [29/391]: Loss 2.2076645109336823e-05\n",
      "[Epoch 41] Training Batch [30/391]: Loss 2.6247204004903324e-05\n",
      "[Epoch 41] Training Batch [31/391]: Loss 1.6415775462519377e-05\n",
      "[Epoch 41] Training Batch [32/391]: Loss 1.658054861763958e-05\n",
      "[Epoch 41] Training Batch [33/391]: Loss 1.599738243385218e-05\n",
      "[Epoch 41] Training Batch [34/391]: Loss 2.276919076393824e-05\n",
      "[Epoch 41] Training Batch [35/391]: Loss 2.1871628632652573e-05\n",
      "[Epoch 41] Training Batch [36/391]: Loss 3.1649047741666436e-05\n",
      "[Epoch 41] Training Batch [37/391]: Loss 1.786234679457266e-05\n",
      "[Epoch 41] Training Batch [38/391]: Loss 3.6066630855202675e-05\n",
      "[Epoch 41] Training Batch [39/391]: Loss 2.0602292352123186e-05\n",
      "[Epoch 41] Training Batch [40/391]: Loss 3.4113887522835284e-05\n",
      "[Epoch 41] Training Batch [41/391]: Loss 3.8809266698081046e-05\n",
      "[Epoch 41] Training Batch [42/391]: Loss 2.068715366476681e-05\n",
      "[Epoch 41] Training Batch [43/391]: Loss 1.8243767044623382e-05\n",
      "[Epoch 41] Training Batch [44/391]: Loss 2.2586620616493747e-05\n",
      "[Epoch 41] Training Batch [45/391]: Loss 2.053099888144061e-05\n",
      "[Epoch 41] Training Batch [46/391]: Loss 2.2550664652953856e-05\n",
      "[Epoch 41] Training Batch [47/391]: Loss 4.375215576146729e-05\n",
      "[Epoch 41] Training Batch [48/391]: Loss 2.1929947251919657e-05\n",
      "[Epoch 41] Training Batch [49/391]: Loss 3.0455054002231918e-05\n",
      "[Epoch 41] Training Batch [50/391]: Loss 2.18728728214046e-05\n",
      "[Epoch 41] Training Batch [51/391]: Loss 2.2653832274954766e-05\n",
      "[Epoch 41] Training Batch [52/391]: Loss 3.1904095521895215e-05\n",
      "[Epoch 41] Training Batch [53/391]: Loss 2.469515493430663e-05\n",
      "[Epoch 41] Training Batch [54/391]: Loss 1.3822338587488048e-05\n",
      "[Epoch 41] Training Batch [55/391]: Loss 2.3548260287498124e-05\n",
      "[Epoch 41] Training Batch [56/391]: Loss 1.7460062736063264e-05\n",
      "[Epoch 41] Training Batch [57/391]: Loss 1.7864898836705834e-05\n",
      "[Epoch 41] Training Batch [58/391]: Loss 1.0744933206296992e-05\n",
      "[Epoch 41] Training Batch [59/391]: Loss 2.6652491214917973e-05\n",
      "[Epoch 41] Training Batch [60/391]: Loss 2.1029885829193518e-05\n",
      "[Epoch 41] Training Batch [61/391]: Loss 3.786772504099645e-05\n",
      "[Epoch 41] Training Batch [62/391]: Loss 1.7260172171518207e-05\n",
      "[Epoch 41] Training Batch [63/391]: Loss 1.6317366316798143e-05\n",
      "[Epoch 41] Training Batch [64/391]: Loss 2.108064290951006e-05\n",
      "[Epoch 41] Training Batch [65/391]: Loss 4.648304820875637e-05\n",
      "[Epoch 41] Training Batch [66/391]: Loss 2.6329471438657492e-05\n",
      "[Epoch 41] Training Batch [67/391]: Loss 2.8449692763388157e-05\n",
      "[Epoch 41] Training Batch [68/391]: Loss 2.5309165721409954e-05\n",
      "[Epoch 41] Training Batch [69/391]: Loss 2.7257201509200968e-05\n",
      "[Epoch 41] Training Batch [70/391]: Loss 2.3164158847066574e-05\n",
      "[Epoch 41] Training Batch [71/391]: Loss 2.2549931600224227e-05\n",
      "[Epoch 41] Training Batch [72/391]: Loss 2.004095949814655e-05\n",
      "[Epoch 41] Training Batch [73/391]: Loss 1.0473691872903146e-05\n",
      "[Epoch 41] Training Batch [74/391]: Loss 3.315495632705279e-05\n",
      "[Epoch 41] Training Batch [75/391]: Loss 1.693408194114454e-05\n",
      "[Epoch 41] Training Batch [76/391]: Loss 2.665715328475926e-05\n",
      "[Epoch 41] Training Batch [77/391]: Loss 2.3424476239597425e-05\n",
      "[Epoch 41] Training Batch [78/391]: Loss 5.153187157702632e-05\n",
      "[Epoch 41] Training Batch [79/391]: Loss 2.6098519811057486e-05\n",
      "[Epoch 41] Training Batch [80/391]: Loss 2.5047140297829174e-05\n",
      "[Epoch 41] Training Batch [81/391]: Loss 1.5192089449556079e-05\n",
      "[Epoch 41] Training Batch [82/391]: Loss 3.189647395629436e-05\n",
      "[Epoch 41] Training Batch [83/391]: Loss 3.0620096367783844e-05\n",
      "[Epoch 41] Training Batch [84/391]: Loss 2.8696038498310372e-05\n",
      "[Epoch 41] Training Batch [85/391]: Loss 1.565234924782999e-05\n",
      "[Epoch 41] Training Batch [86/391]: Loss 1.0908146578003652e-05\n",
      "[Epoch 41] Training Batch [87/391]: Loss 1.858572068158537e-05\n",
      "[Epoch 41] Training Batch [88/391]: Loss 7.888927029853221e-06\n",
      "[Epoch 41] Training Batch [89/391]: Loss 4.0240149246528745e-05\n",
      "[Epoch 41] Training Batch [90/391]: Loss 1.8106646166415885e-05\n",
      "[Epoch 41] Training Batch [91/391]: Loss 2.3257303837453946e-05\n",
      "[Epoch 41] Training Batch [92/391]: Loss 1.6780459191068076e-05\n",
      "[Epoch 41] Training Batch [93/391]: Loss 2.4557321012252942e-05\n",
      "[Epoch 41] Training Batch [94/391]: Loss 2.0703300833702087e-05\n",
      "[Epoch 41] Training Batch [95/391]: Loss 2.0342877178336494e-05\n",
      "[Epoch 41] Training Batch [96/391]: Loss 2.841017885657493e-05\n",
      "[Epoch 41] Training Batch [97/391]: Loss 3.8440990465460345e-05\n",
      "[Epoch 41] Training Batch [98/391]: Loss 3.088229277636856e-05\n",
      "[Epoch 41] Training Batch [99/391]: Loss 2.0802754079340957e-05\n",
      "[Epoch 41] Training Batch [100/391]: Loss 1.1700362847477663e-05\n",
      "[Epoch 41] Training Batch [101/391]: Loss 2.4879438569769263e-05\n",
      "[Epoch 41] Training Batch [102/391]: Loss 2.6550249458523467e-05\n",
      "[Epoch 41] Training Batch [103/391]: Loss 2.709161526581738e-05\n",
      "[Epoch 41] Training Batch [104/391]: Loss 2.9619704946526326e-05\n",
      "[Epoch 41] Training Batch [105/391]: Loss 2.4189517716877162e-05\n",
      "[Epoch 41] Training Batch [106/391]: Loss 2.0082448827452026e-05\n",
      "[Epoch 41] Training Batch [107/391]: Loss 2.2972373699303716e-05\n",
      "[Epoch 41] Training Batch [108/391]: Loss 2.5764926249394193e-05\n",
      "[Epoch 41] Training Batch [109/391]: Loss 3.013353307323996e-05\n",
      "[Epoch 41] Training Batch [110/391]: Loss 2.5261344489990734e-05\n",
      "[Epoch 41] Training Batch [111/391]: Loss 2.4922155716922134e-05\n",
      "[Epoch 41] Training Batch [112/391]: Loss 1.6625144780846313e-05\n",
      "[Epoch 41] Training Batch [113/391]: Loss 2.1936933990218677e-05\n",
      "[Epoch 41] Training Batch [114/391]: Loss 1.8046119294012897e-05\n",
      "[Epoch 41] Training Batch [115/391]: Loss 3.807733810390346e-05\n",
      "[Epoch 41] Training Batch [116/391]: Loss 2.0594146917574108e-05\n",
      "[Epoch 41] Training Batch [117/391]: Loss 2.5658435333753005e-05\n",
      "[Epoch 41] Training Batch [118/391]: Loss 3.0286684705060907e-05\n",
      "[Epoch 41] Training Batch [119/391]: Loss 2.92338718281826e-05\n",
      "[Epoch 41] Training Batch [120/391]: Loss 3.177994949510321e-05\n",
      "[Epoch 41] Training Batch [121/391]: Loss 1.7067457520170137e-05\n",
      "[Epoch 41] Training Batch [122/391]: Loss 2.8729209589073434e-05\n",
      "[Epoch 41] Training Batch [123/391]: Loss 1.7168935301015154e-05\n",
      "[Epoch 41] Training Batch [124/391]: Loss 1.7326839952147566e-05\n",
      "[Epoch 41] Training Batch [125/391]: Loss 2.417404903098941e-05\n",
      "[Epoch 41] Training Batch [126/391]: Loss 2.566136754467152e-05\n",
      "[Epoch 41] Training Batch [127/391]: Loss 2.607665919640567e-05\n",
      "[Epoch 41] Training Batch [128/391]: Loss 1.6954307284322567e-05\n",
      "[Epoch 41] Training Batch [129/391]: Loss 2.1738072973676026e-05\n",
      "[Epoch 41] Training Batch [130/391]: Loss 2.2851456378703006e-05\n",
      "[Epoch 41] Training Batch [131/391]: Loss 1.1589490895858034e-05\n",
      "[Epoch 41] Training Batch [132/391]: Loss 1.83249303518096e-05\n",
      "[Epoch 41] Training Batch [133/391]: Loss 1.7540573026053607e-05\n",
      "[Epoch 41] Training Batch [134/391]: Loss 1.783259904186707e-05\n",
      "[Epoch 41] Training Batch [135/391]: Loss 2.067719105980359e-05\n",
      "[Epoch 41] Training Batch [136/391]: Loss 2.2050677216611803e-05\n",
      "[Epoch 41] Training Batch [137/391]: Loss 2.8144648240413517e-05\n",
      "[Epoch 41] Training Batch [138/391]: Loss 3.4093609428964555e-05\n",
      "[Epoch 41] Training Batch [139/391]: Loss 2.4715205654501915e-05\n",
      "[Epoch 41] Training Batch [140/391]: Loss 2.3567066818941385e-05\n",
      "[Epoch 41] Training Batch [141/391]: Loss 2.5074677978409454e-05\n",
      "[Epoch 41] Training Batch [142/391]: Loss 2.303881774423644e-05\n",
      "[Epoch 41] Training Batch [143/391]: Loss 2.408788350294344e-05\n",
      "[Epoch 41] Training Batch [144/391]: Loss 3.995726001448929e-05\n",
      "[Epoch 41] Training Batch [145/391]: Loss 4.26875521952752e-05\n",
      "[Epoch 41] Training Batch [146/391]: Loss 1.819626231736038e-05\n",
      "[Epoch 41] Training Batch [147/391]: Loss 2.481274896126706e-05\n",
      "[Epoch 41] Training Batch [148/391]: Loss 1.0618519809213467e-05\n",
      "[Epoch 41] Training Batch [149/391]: Loss 3.105512587353587e-05\n",
      "[Epoch 41] Training Batch [150/391]: Loss 2.803523966576904e-05\n",
      "[Epoch 41] Training Batch [151/391]: Loss 5.288388274493627e-05\n",
      "[Epoch 41] Training Batch [152/391]: Loss 2.5758528863661923e-05\n",
      "[Epoch 41] Training Batch [153/391]: Loss 2.6152752980124205e-05\n",
      "[Epoch 41] Training Batch [154/391]: Loss 1.0028547876572702e-05\n",
      "[Epoch 41] Training Batch [155/391]: Loss 2.1634152290062048e-05\n",
      "[Epoch 41] Training Batch [156/391]: Loss 1.993622208829038e-05\n",
      "[Epoch 41] Training Batch [157/391]: Loss 2.9454393370542675e-05\n",
      "[Epoch 41] Training Batch [158/391]: Loss 1.350821548840031e-05\n",
      "[Epoch 41] Training Batch [159/391]: Loss 2.2738448024028912e-05\n",
      "[Epoch 41] Training Batch [160/391]: Loss 4.259355409885757e-05\n",
      "[Epoch 41] Training Batch [161/391]: Loss 2.924576801888179e-05\n",
      "[Epoch 41] Training Batch [162/391]: Loss 2.199868868046906e-05\n",
      "[Epoch 41] Training Batch [163/391]: Loss 1.9622588297352195e-05\n",
      "[Epoch 41] Training Batch [164/391]: Loss 3.054169792449102e-05\n",
      "[Epoch 41] Training Batch [165/391]: Loss 3.3450181945227087e-05\n",
      "[Epoch 41] Training Batch [166/391]: Loss 3.150142219965346e-05\n",
      "[Epoch 41] Training Batch [167/391]: Loss 1.9821998648694716e-05\n",
      "[Epoch 41] Training Batch [168/391]: Loss 1.0408853995613754e-05\n",
      "[Epoch 41] Training Batch [169/391]: Loss 4.03855592594482e-05\n",
      "[Epoch 41] Training Batch [170/391]: Loss 1.813028211472556e-05\n",
      "[Epoch 41] Training Batch [171/391]: Loss 2.9563218049588613e-05\n",
      "[Epoch 41] Training Batch [172/391]: Loss 6.947255315026268e-06\n",
      "[Epoch 41] Training Batch [173/391]: Loss 3.268930231570266e-05\n",
      "[Epoch 41] Training Batch [174/391]: Loss 2.7260457500233315e-05\n",
      "[Epoch 41] Training Batch [175/391]: Loss 1.9807033822871745e-05\n",
      "[Epoch 41] Training Batch [176/391]: Loss 2.052462696155999e-05\n",
      "[Epoch 41] Training Batch [177/391]: Loss 4.4229811464902014e-05\n",
      "[Epoch 41] Training Batch [178/391]: Loss 2.5739338525454514e-05\n",
      "[Epoch 41] Training Batch [179/391]: Loss 1.5039699974295218e-05\n",
      "[Epoch 41] Training Batch [180/391]: Loss 1.1967717000516132e-05\n",
      "[Epoch 41] Training Batch [181/391]: Loss 1.9328204871271737e-05\n",
      "[Epoch 41] Training Batch [182/391]: Loss 2.3205700927064754e-05\n",
      "[Epoch 41] Training Batch [183/391]: Loss 2.8432323233573698e-05\n",
      "[Epoch 41] Training Batch [184/391]: Loss 1.7482856492279097e-05\n",
      "[Epoch 41] Training Batch [185/391]: Loss 2.1341162209864706e-05\n",
      "[Epoch 41] Training Batch [186/391]: Loss 1.446195619791979e-05\n",
      "[Epoch 41] Training Batch [187/391]: Loss 2.9262124371598475e-05\n",
      "[Epoch 41] Training Batch [188/391]: Loss 3.010788896062877e-05\n",
      "[Epoch 41] Training Batch [189/391]: Loss 2.2532492948812433e-05\n",
      "[Epoch 41] Training Batch [190/391]: Loss 1.9420976968831383e-05\n",
      "[Epoch 41] Training Batch [191/391]: Loss 2.7611209588940255e-05\n",
      "[Epoch 41] Training Batch [192/391]: Loss 2.7253392545389943e-05\n",
      "[Epoch 41] Training Batch [193/391]: Loss 1.4592896150134038e-05\n",
      "[Epoch 41] Training Batch [194/391]: Loss 3.8651491195196286e-05\n",
      "[Epoch 41] Training Batch [195/391]: Loss 1.580776006449014e-05\n",
      "[Epoch 41] Training Batch [196/391]: Loss 3.1622941605746746e-05\n",
      "[Epoch 41] Training Batch [197/391]: Loss 3.933320112992078e-05\n",
      "[Epoch 41] Training Batch [198/391]: Loss 3.679054498206824e-05\n",
      "[Epoch 41] Training Batch [199/391]: Loss 1.3428707461571321e-05\n",
      "[Epoch 41] Training Batch [200/391]: Loss 2.8470443794503808e-05\n",
      "[Epoch 41] Training Batch [201/391]: Loss 2.263226269860752e-05\n",
      "[Epoch 41] Training Batch [202/391]: Loss 2.8407343052094802e-05\n",
      "[Epoch 41] Training Batch [203/391]: Loss 9.780819709703792e-06\n",
      "[Epoch 41] Training Batch [204/391]: Loss 3.0529368814313784e-05\n",
      "[Epoch 41] Training Batch [205/391]: Loss 2.82410819636425e-05\n",
      "[Epoch 41] Training Batch [206/391]: Loss 9.797493476071395e-06\n",
      "[Epoch 41] Training Batch [207/391]: Loss 2.4974275220301934e-05\n",
      "[Epoch 41] Training Batch [208/391]: Loss 3.3354128390783444e-05\n",
      "[Epoch 41] Training Batch [209/391]: Loss 3.487130379653536e-05\n",
      "[Epoch 41] Training Batch [210/391]: Loss 2.011131073231809e-05\n",
      "[Epoch 41] Training Batch [211/391]: Loss 2.2061485651647672e-05\n",
      "[Epoch 41] Training Batch [212/391]: Loss 3.942547118640505e-05\n",
      "[Epoch 41] Training Batch [213/391]: Loss 1.953602441062685e-05\n",
      "[Epoch 41] Training Batch [214/391]: Loss 3.441195440245792e-05\n",
      "[Epoch 41] Training Batch [215/391]: Loss 2.4142482288880274e-05\n",
      "[Epoch 41] Training Batch [216/391]: Loss 1.4004199329065159e-05\n",
      "[Epoch 41] Training Batch [217/391]: Loss 4.018539402750321e-05\n",
      "[Epoch 41] Training Batch [218/391]: Loss 2.418085205135867e-05\n",
      "[Epoch 41] Training Batch [219/391]: Loss 2.1661307982867584e-05\n",
      "[Epoch 41] Training Batch [220/391]: Loss 2.393049726379104e-05\n",
      "[Epoch 41] Training Batch [221/391]: Loss 2.418844997009728e-05\n",
      "[Epoch 41] Training Batch [222/391]: Loss 2.6822472136700526e-05\n",
      "[Epoch 41] Training Batch [223/391]: Loss 3.135788574581966e-05\n",
      "[Epoch 41] Training Batch [224/391]: Loss 1.877756221801974e-05\n",
      "[Epoch 41] Training Batch [225/391]: Loss 2.308821967744734e-05\n",
      "[Epoch 41] Training Batch [226/391]: Loss 2.000515269173775e-05\n",
      "[Epoch 41] Training Batch [227/391]: Loss 2.323578519281e-05\n",
      "[Epoch 41] Training Batch [228/391]: Loss 3.485001434455626e-05\n",
      "[Epoch 41] Training Batch [229/391]: Loss 1.9008375602425076e-05\n",
      "[Epoch 41] Training Batch [230/391]: Loss 3.643188756541349e-05\n",
      "[Epoch 41] Training Batch [231/391]: Loss 1.7495702195446938e-05\n",
      "[Epoch 41] Training Batch [232/391]: Loss 3.0047533073229715e-05\n",
      "[Epoch 41] Training Batch [233/391]: Loss 2.093206421704963e-05\n",
      "[Epoch 41] Training Batch [234/391]: Loss 1.852999412221834e-05\n",
      "[Epoch 41] Training Batch [235/391]: Loss 2.8237114747753367e-05\n",
      "[Epoch 41] Training Batch [236/391]: Loss 3.2960142561933026e-05\n",
      "[Epoch 41] Training Batch [237/391]: Loss 2.5001721951412037e-05\n",
      "[Epoch 41] Training Batch [238/391]: Loss 1.5372324924101122e-05\n",
      "[Epoch 41] Training Batch [239/391]: Loss 2.3664832042413764e-05\n",
      "[Epoch 41] Training Batch [240/391]: Loss 2.1415731680463068e-05\n",
      "[Epoch 41] Training Batch [241/391]: Loss 2.2113146769697778e-05\n",
      "[Epoch 41] Training Batch [242/391]: Loss 1.3042968021181878e-05\n",
      "[Epoch 41] Training Batch [243/391]: Loss 1.386011081194738e-05\n",
      "[Epoch 41] Training Batch [244/391]: Loss 2.4839342586346902e-05\n",
      "[Epoch 41] Training Batch [245/391]: Loss 2.86069971480174e-05\n",
      "[Epoch 41] Training Batch [246/391]: Loss 3.2683496101526543e-05\n",
      "[Epoch 41] Training Batch [247/391]: Loss 2.1758491129730828e-05\n",
      "[Epoch 41] Training Batch [248/391]: Loss 2.0137533283559605e-05\n",
      "[Epoch 41] Training Batch [249/391]: Loss 2.2435078790294938e-05\n",
      "[Epoch 41] Training Batch [250/391]: Loss 2.7046218747273088e-05\n",
      "[Epoch 41] Training Batch [251/391]: Loss 2.040379331447184e-05\n",
      "[Epoch 41] Training Batch [252/391]: Loss 2.4341805328731425e-05\n",
      "[Epoch 41] Training Batch [253/391]: Loss 1.7959224351216108e-05\n",
      "[Epoch 41] Training Batch [254/391]: Loss 2.4993174520204775e-05\n",
      "[Epoch 41] Training Batch [255/391]: Loss 2.35771294683218e-05\n",
      "[Epoch 41] Training Batch [256/391]: Loss 2.1006610040785745e-05\n",
      "[Epoch 41] Training Batch [257/391]: Loss 2.1933958123554476e-05\n",
      "[Epoch 41] Training Batch [258/391]: Loss 2.3276028514374048e-05\n",
      "[Epoch 41] Training Batch [259/391]: Loss 2.1445579477585852e-05\n",
      "[Epoch 41] Training Batch [260/391]: Loss 1.9343340682098642e-05\n",
      "[Epoch 41] Training Batch [261/391]: Loss 2.9339709726627916e-05\n",
      "[Epoch 41] Training Batch [262/391]: Loss 1.9710361812030897e-05\n",
      "[Epoch 41] Training Batch [263/391]: Loss 3.6649311368819326e-05\n",
      "[Epoch 41] Training Batch [264/391]: Loss 2.4646804376970977e-05\n",
      "[Epoch 41] Training Batch [265/391]: Loss 2.635405326145701e-05\n",
      "[Epoch 41] Training Batch [266/391]: Loss 2.185724770242814e-05\n",
      "[Epoch 41] Training Batch [267/391]: Loss 3.882804230670445e-05\n",
      "[Epoch 41] Training Batch [268/391]: Loss 2.286420567543246e-05\n",
      "[Epoch 41] Training Batch [269/391]: Loss 2.034112549154088e-05\n",
      "[Epoch 41] Training Batch [270/391]: Loss 1.592822081875056e-05\n",
      "[Epoch 41] Training Batch [271/391]: Loss 1.9006847651326098e-05\n",
      "[Epoch 41] Training Batch [272/391]: Loss 2.5997798729804344e-05\n",
      "[Epoch 41] Training Batch [273/391]: Loss 2.380073601671029e-05\n",
      "[Epoch 41] Training Batch [274/391]: Loss 2.4728949938435107e-05\n",
      "[Epoch 41] Training Batch [275/391]: Loss 1.670704659773037e-05\n",
      "[Epoch 41] Training Batch [276/391]: Loss 2.040384424617514e-05\n",
      "[Epoch 41] Training Batch [277/391]: Loss 1.5620367776136845e-05\n",
      "[Epoch 41] Training Batch [278/391]: Loss 2.3781631171004847e-05\n",
      "[Epoch 41] Training Batch [279/391]: Loss 1.4356059182318859e-05\n",
      "[Epoch 41] Training Batch [280/391]: Loss 2.2927906684344634e-05\n",
      "[Epoch 41] Training Batch [281/391]: Loss 3.2145795557880774e-05\n",
      "[Epoch 41] Training Batch [282/391]: Loss 1.7201700757141225e-05\n",
      "[Epoch 41] Training Batch [283/391]: Loss 3.3967982744798064e-05\n",
      "[Epoch 41] Training Batch [284/391]: Loss 1.9868803065037355e-05\n",
      "[Epoch 41] Training Batch [285/391]: Loss 1.9620109014795162e-05\n",
      "[Epoch 41] Training Batch [286/391]: Loss 1.9618422811618075e-05\n",
      "[Epoch 41] Training Batch [287/391]: Loss 2.662561928445939e-05\n",
      "[Epoch 41] Training Batch [288/391]: Loss 3.0887113098287955e-05\n",
      "[Epoch 41] Training Batch [289/391]: Loss 2.0291703549446538e-05\n",
      "[Epoch 41] Training Batch [290/391]: Loss 2.2841715690447018e-05\n",
      "[Epoch 41] Training Batch [291/391]: Loss 4.245948730385862e-05\n",
      "[Epoch 41] Training Batch [292/391]: Loss 2.8203467081766576e-05\n",
      "[Epoch 41] Training Batch [293/391]: Loss 4.398393502924591e-05\n",
      "[Epoch 41] Training Batch [294/391]: Loss 3.579867188818753e-05\n",
      "[Epoch 41] Training Batch [295/391]: Loss 2.6868265194934793e-05\n",
      "[Epoch 41] Training Batch [296/391]: Loss 1.739186336635612e-05\n",
      "[Epoch 41] Training Batch [297/391]: Loss 2.8138692869106308e-05\n",
      "[Epoch 41] Training Batch [298/391]: Loss 2.7126250643050298e-05\n",
      "[Epoch 41] Training Batch [299/391]: Loss 2.0735078578582034e-05\n",
      "[Epoch 41] Training Batch [300/391]: Loss 1.5074971088324673e-05\n",
      "[Epoch 41] Training Batch [301/391]: Loss 3.216415279894136e-05\n",
      "[Epoch 41] Training Batch [302/391]: Loss 2.295047124789562e-05\n",
      "[Epoch 41] Training Batch [303/391]: Loss 1.8363971321377903e-05\n",
      "[Epoch 41] Training Batch [304/391]: Loss 1.752174830471631e-05\n",
      "[Epoch 41] Training Batch [305/391]: Loss 1.0437762284709606e-05\n",
      "[Epoch 41] Training Batch [306/391]: Loss 1.9934856027248316e-05\n",
      "[Epoch 41] Training Batch [307/391]: Loss 2.9527718652389012e-05\n",
      "[Epoch 41] Training Batch [308/391]: Loss 2.1463472876348533e-05\n",
      "[Epoch 41] Training Batch [309/391]: Loss 1.8011778593063354e-05\n",
      "[Epoch 41] Training Batch [310/391]: Loss 2.723317447816953e-05\n",
      "[Epoch 41] Training Batch [311/391]: Loss 1.869171319412999e-05\n",
      "[Epoch 41] Training Batch [312/391]: Loss 3.134637154289521e-05\n",
      "[Epoch 41] Training Batch [313/391]: Loss 4.25430916948244e-05\n",
      "[Epoch 41] Training Batch [314/391]: Loss 2.571857658040244e-05\n",
      "[Epoch 41] Training Batch [315/391]: Loss 9.634877642383799e-06\n",
      "[Epoch 41] Training Batch [316/391]: Loss 2.5606152121326886e-05\n",
      "[Epoch 41] Training Batch [317/391]: Loss 3.4764816518872976e-05\n",
      "[Epoch 41] Training Batch [318/391]: Loss 3.330647450638935e-05\n",
      "[Epoch 41] Training Batch [319/391]: Loss 3.432087760302238e-05\n",
      "[Epoch 41] Training Batch [320/391]: Loss 3.111200931016356e-05\n",
      "[Epoch 41] Training Batch [321/391]: Loss 1.1164926036144607e-05\n",
      "[Epoch 41] Training Batch [322/391]: Loss 2.2825966880191118e-05\n",
      "[Epoch 41] Training Batch [323/391]: Loss 2.537930413382128e-05\n",
      "[Epoch 41] Training Batch [324/391]: Loss 1.89072270586621e-05\n",
      "[Epoch 41] Training Batch [325/391]: Loss 1.4971005839470308e-05\n",
      "[Epoch 41] Training Batch [326/391]: Loss 1.9464350771158934e-05\n",
      "[Epoch 41] Training Batch [327/391]: Loss 2.9856380933779292e-05\n",
      "[Epoch 41] Training Batch [328/391]: Loss 3.0552746466128156e-05\n",
      "[Epoch 41] Training Batch [329/391]: Loss 1.5860230632824823e-05\n",
      "[Epoch 41] Training Batch [330/391]: Loss 8.215780326281674e-06\n",
      "[Epoch 41] Training Batch [331/391]: Loss 1.191556566482177e-05\n",
      "[Epoch 41] Training Batch [332/391]: Loss 2.4499096980434842e-05\n",
      "[Epoch 41] Training Batch [333/391]: Loss 3.670915248221718e-05\n",
      "[Epoch 41] Training Batch [334/391]: Loss 2.1089723304612562e-05\n",
      "[Epoch 41] Training Batch [335/391]: Loss 1.8371689293417148e-05\n",
      "[Epoch 41] Training Batch [336/391]: Loss 1.6166881323442794e-05\n",
      "[Epoch 41] Training Batch [337/391]: Loss 2.6891873858403414e-05\n",
      "[Epoch 41] Training Batch [338/391]: Loss 2.488634163455572e-05\n",
      "[Epoch 41] Training Batch [339/391]: Loss 2.8259824830456637e-05\n",
      "[Epoch 41] Training Batch [340/391]: Loss 2.462043266859837e-05\n",
      "[Epoch 41] Training Batch [341/391]: Loss 2.0675324776675552e-05\n",
      "[Epoch 41] Training Batch [342/391]: Loss 3.2065512641565874e-05\n",
      "[Epoch 41] Training Batch [343/391]: Loss 2.0998222680645995e-05\n",
      "[Epoch 41] Training Batch [344/391]: Loss 1.8542694306233898e-05\n",
      "[Epoch 41] Training Batch [345/391]: Loss 2.2928850739845075e-05\n",
      "[Epoch 41] Training Batch [346/391]: Loss 3.476995334494859e-05\n",
      "[Epoch 41] Training Batch [347/391]: Loss 1.768505353538785e-05\n",
      "[Epoch 41] Training Batch [348/391]: Loss 2.4596365619800054e-05\n",
      "[Epoch 41] Training Batch [349/391]: Loss 1.545386840007268e-05\n",
      "[Epoch 41] Training Batch [350/391]: Loss 1.0914473932643887e-05\n",
      "[Epoch 41] Training Batch [351/391]: Loss 1.5754441847093403e-05\n",
      "[Epoch 41] Training Batch [352/391]: Loss 2.066731030936353e-05\n",
      "[Epoch 41] Training Batch [353/391]: Loss 4.079088830621913e-05\n",
      "[Epoch 41] Training Batch [354/391]: Loss 3.273665424785577e-05\n",
      "[Epoch 41] Training Batch [355/391]: Loss 2.0788293113582768e-05\n",
      "[Epoch 41] Training Batch [356/391]: Loss 2.1319579900591634e-05\n",
      "[Epoch 41] Training Batch [357/391]: Loss 1.862067983893212e-05\n",
      "[Epoch 41] Training Batch [358/391]: Loss 3.081912291236222e-05\n",
      "[Epoch 41] Training Batch [359/391]: Loss 1.1857880963361822e-05\n",
      "[Epoch 41] Training Batch [360/391]: Loss 2.342494008189533e-05\n",
      "[Epoch 41] Training Batch [361/391]: Loss 2.116065843438264e-05\n",
      "[Epoch 41] Training Batch [362/391]: Loss 3.503503467072733e-05\n",
      "[Epoch 41] Training Batch [363/391]: Loss 1.4773107068322133e-05\n",
      "[Epoch 41] Training Batch [364/391]: Loss 2.881889304262586e-05\n",
      "[Epoch 41] Training Batch [365/391]: Loss 2.525872150727082e-05\n",
      "[Epoch 41] Training Batch [366/391]: Loss 1.9817156498902477e-05\n",
      "[Epoch 41] Training Batch [367/391]: Loss 1.5630936104571447e-05\n",
      "[Epoch 41] Training Batch [368/391]: Loss 1.9398517906665802e-05\n",
      "[Epoch 41] Training Batch [369/391]: Loss 3.0012053684913553e-05\n",
      "[Epoch 41] Training Batch [370/391]: Loss 1.350516777165467e-05\n",
      "[Epoch 41] Training Batch [371/391]: Loss 3.215403194190003e-05\n",
      "[Epoch 41] Training Batch [372/391]: Loss 9.960347597370856e-06\n",
      "[Epoch 41] Training Batch [373/391]: Loss 4.094711403013207e-05\n",
      "[Epoch 41] Training Batch [374/391]: Loss 1.7053256669896655e-05\n",
      "[Epoch 41] Training Batch [375/391]: Loss 1.4286373698269017e-05\n",
      "[Epoch 41] Training Batch [376/391]: Loss 1.2742939361487515e-05\n",
      "[Epoch 41] Training Batch [377/391]: Loss 2.8169599318061955e-05\n",
      "[Epoch 41] Training Batch [378/391]: Loss 1.9923385480069555e-05\n",
      "[Epoch 41] Training Batch [379/391]: Loss 2.1258714696159586e-05\n",
      "[Epoch 41] Training Batch [380/391]: Loss 2.952454269689042e-05\n",
      "[Epoch 41] Training Batch [381/391]: Loss 2.1145486243767664e-05\n",
      "[Epoch 41] Training Batch [382/391]: Loss 2.509397381800227e-05\n",
      "[Epoch 41] Training Batch [383/391]: Loss 1.9249948309152387e-05\n",
      "[Epoch 41] Training Batch [384/391]: Loss 2.328293521713931e-05\n",
      "[Epoch 41] Training Batch [385/391]: Loss 3.103612471022643e-05\n",
      "[Epoch 41] Training Batch [386/391]: Loss 4.2670049879234284e-05\n",
      "[Epoch 41] Training Batch [387/391]: Loss 3.7870417145313695e-05\n",
      "[Epoch 41] Training Batch [388/391]: Loss 2.2120189896668307e-05\n",
      "[Epoch 41] Training Batch [389/391]: Loss 3.04032328131143e-05\n",
      "[Epoch 41] Training Batch [390/391]: Loss 2.3065495042828843e-05\n",
      "[Epoch 41] Training Batch [391/391]: Loss 2.4418975954176858e-05\n",
      "Epoch 41 - Train Loss: 0.0000\n",
      "*********  Epoch 42/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42] Training Batch [1/391]: Loss 2.1382596969488077e-05\n",
      "[Epoch 42] Training Batch [2/391]: Loss 1.7711196051095612e-05\n",
      "[Epoch 42] Training Batch [3/391]: Loss 2.4929429855546914e-05\n",
      "[Epoch 42] Training Batch [4/391]: Loss 2.0692357793450356e-05\n",
      "[Epoch 42] Training Batch [5/391]: Loss 7.926878424768802e-06\n",
      "[Epoch 42] Training Batch [6/391]: Loss 1.9535020328476094e-05\n",
      "[Epoch 42] Training Batch [7/391]: Loss 1.6839792806422338e-05\n",
      "[Epoch 42] Training Batch [8/391]: Loss 3.496440695016645e-05\n",
      "[Epoch 42] Training Batch [9/391]: Loss 6.7071196099277586e-06\n",
      "[Epoch 42] Training Batch [10/391]: Loss 1.8837878087651916e-05\n",
      "[Epoch 42] Training Batch [11/391]: Loss 2.8855123673565686e-05\n",
      "[Epoch 42] Training Batch [12/391]: Loss 3.5890829167328775e-05\n",
      "[Epoch 42] Training Batch [13/391]: Loss 1.863299257820472e-05\n",
      "[Epoch 42] Training Batch [14/391]: Loss 1.032401360134827e-05\n",
      "[Epoch 42] Training Batch [15/391]: Loss 2.0931009203195572e-05\n",
      "[Epoch 42] Training Batch [16/391]: Loss 2.985337414429523e-05\n",
      "[Epoch 42] Training Batch [17/391]: Loss 3.2447085686726496e-05\n",
      "[Epoch 42] Training Batch [18/391]: Loss 2.7317762942402624e-05\n",
      "[Epoch 42] Training Batch [19/391]: Loss 2.9348737371037714e-05\n",
      "[Epoch 42] Training Batch [20/391]: Loss 3.3263262594118714e-05\n",
      "[Epoch 42] Training Batch [21/391]: Loss 2.6815521778189577e-05\n",
      "[Epoch 42] Training Batch [22/391]: Loss 1.767287903930992e-05\n",
      "[Epoch 42] Training Batch [23/391]: Loss 1.9712146240635775e-05\n",
      "[Epoch 42] Training Batch [24/391]: Loss 2.0258938093320467e-05\n",
      "[Epoch 42] Training Batch [25/391]: Loss 2.617177779029589e-05\n",
      "[Epoch 42] Training Batch [26/391]: Loss 1.5053568859002553e-05\n",
      "[Epoch 42] Training Batch [27/391]: Loss 2.0315026631578803e-05\n",
      "[Epoch 42] Training Batch [28/391]: Loss 2.169289109588135e-05\n",
      "[Epoch 42] Training Batch [29/391]: Loss 3.542170088621788e-05\n",
      "[Epoch 42] Training Batch [30/391]: Loss 1.968339529412333e-05\n",
      "[Epoch 42] Training Batch [31/391]: Loss 2.9168848413974047e-05\n",
      "[Epoch 42] Training Batch [32/391]: Loss 2.167199090763461e-05\n",
      "[Epoch 42] Training Batch [33/391]: Loss 2.0969642719137482e-05\n",
      "[Epoch 42] Training Batch [34/391]: Loss 2.7875055820913985e-05\n",
      "[Epoch 42] Training Batch [35/391]: Loss 1.620732291485183e-05\n",
      "[Epoch 42] Training Batch [36/391]: Loss 1.619837166799698e-05\n",
      "[Epoch 42] Training Batch [37/391]: Loss 1.972602512978483e-05\n",
      "[Epoch 42] Training Batch [38/391]: Loss 2.225799107691273e-05\n",
      "[Epoch 42] Training Batch [39/391]: Loss 1.4456104509008583e-05\n",
      "[Epoch 42] Training Batch [40/391]: Loss 2.3553684513899498e-05\n",
      "[Epoch 42] Training Batch [41/391]: Loss 2.365812542848289e-05\n",
      "[Epoch 42] Training Batch [42/391]: Loss 1.561501994729042e-05\n",
      "[Epoch 42] Training Batch [43/391]: Loss 1.7494248822913505e-05\n",
      "[Epoch 42] Training Batch [44/391]: Loss 2.614409095258452e-05\n",
      "[Epoch 42] Training Batch [45/391]: Loss 1.8308446669834666e-05\n",
      "[Epoch 42] Training Batch [46/391]: Loss 1.1069423635490239e-05\n",
      "[Epoch 42] Training Batch [47/391]: Loss 1.0686379027902149e-05\n",
      "[Epoch 42] Training Batch [48/391]: Loss 2.8634711270569824e-05\n",
      "[Epoch 42] Training Batch [49/391]: Loss 2.7592090191319585e-05\n",
      "[Epoch 42] Training Batch [50/391]: Loss 1.1071856533817481e-05\n",
      "[Epoch 42] Training Batch [51/391]: Loss 1.957850508915726e-05\n",
      "[Epoch 42] Training Batch [52/391]: Loss 2.8682197807938792e-05\n",
      "[Epoch 42] Training Batch [53/391]: Loss 7.216553967737127e-06\n",
      "[Epoch 42] Training Batch [54/391]: Loss 1.8403545254841447e-05\n",
      "[Epoch 42] Training Batch [55/391]: Loss 1.4862559510220308e-05\n",
      "[Epoch 42] Training Batch [56/391]: Loss 1.3531016520573758e-05\n",
      "[Epoch 42] Training Batch [57/391]: Loss 1.440690266463207e-05\n",
      "[Epoch 42] Training Batch [58/391]: Loss 3.269994704169221e-05\n",
      "[Epoch 42] Training Batch [59/391]: Loss 2.1949223082629032e-05\n",
      "[Epoch 42] Training Batch [60/391]: Loss 3.054885019082576e-05\n",
      "[Epoch 42] Training Batch [61/391]: Loss 2.755415698629804e-05\n",
      "[Epoch 42] Training Batch [62/391]: Loss 1.8371036276221275e-05\n",
      "[Epoch 42] Training Batch [63/391]: Loss 1.8613545762491412e-05\n",
      "[Epoch 42] Training Batch [64/391]: Loss 1.4244799785956275e-05\n",
      "[Epoch 42] Training Batch [65/391]: Loss 2.882259650505148e-05\n",
      "[Epoch 42] Training Batch [66/391]: Loss 1.398500353388954e-05\n",
      "[Epoch 42] Training Batch [67/391]: Loss 1.7746926459949464e-05\n",
      "[Epoch 42] Training Batch [68/391]: Loss 9.49071727518458e-06\n",
      "[Epoch 42] Training Batch [69/391]: Loss 2.2830776288174093e-05\n",
      "[Epoch 42] Training Batch [70/391]: Loss 2.611758463899605e-05\n",
      "[Epoch 42] Training Batch [71/391]: Loss 1.905801400425844e-05\n",
      "[Epoch 42] Training Batch [72/391]: Loss 3.130135883111507e-05\n",
      "[Epoch 42] Training Batch [73/391]: Loss 1.042348776536528e-05\n",
      "[Epoch 42] Training Batch [74/391]: Loss 2.297978790011257e-05\n",
      "[Epoch 42] Training Batch [75/391]: Loss 1.6850523024913855e-05\n",
      "[Epoch 42] Training Batch [76/391]: Loss 2.6737852749647573e-05\n",
      "[Epoch 42] Training Batch [77/391]: Loss 2.7026730094803497e-05\n",
      "[Epoch 42] Training Batch [78/391]: Loss 2.3733669877401553e-05\n",
      "[Epoch 42] Training Batch [79/391]: Loss 1.8311668100068346e-05\n",
      "[Epoch 42] Training Batch [80/391]: Loss 2.346171640965622e-05\n",
      "[Epoch 42] Training Batch [81/391]: Loss 1.2747274922730867e-05\n",
      "[Epoch 42] Training Batch [82/391]: Loss 4.291659206501208e-05\n",
      "[Epoch 42] Training Batch [83/391]: Loss 2.077945100609213e-05\n",
      "[Epoch 42] Training Batch [84/391]: Loss 1.6420948668383062e-05\n",
      "[Epoch 42] Training Batch [85/391]: Loss 1.1920378710783552e-05\n",
      "[Epoch 42] Training Batch [86/391]: Loss 2.5176337658194825e-05\n",
      "[Epoch 42] Training Batch [87/391]: Loss 1.1400517905713059e-05\n",
      "[Epoch 42] Training Batch [88/391]: Loss 2.9879416615585797e-05\n",
      "[Epoch 42] Training Batch [89/391]: Loss 2.183986180170905e-05\n",
      "[Epoch 42] Training Batch [90/391]: Loss 2.1064819520688616e-05\n",
      "[Epoch 42] Training Batch [91/391]: Loss 2.6894198526861146e-05\n",
      "[Epoch 42] Training Batch [92/391]: Loss 2.5577499400242232e-05\n",
      "[Epoch 42] Training Batch [93/391]: Loss 1.3385531019594055e-05\n",
      "[Epoch 42] Training Batch [94/391]: Loss 2.470088111294899e-05\n",
      "[Epoch 42] Training Batch [95/391]: Loss 2.8169009965495206e-05\n",
      "[Epoch 42] Training Batch [96/391]: Loss 2.655800199136138e-05\n",
      "[Epoch 42] Training Batch [97/391]: Loss 7.079660008457722e-06\n",
      "[Epoch 42] Training Batch [98/391]: Loss 2.2086402168497443e-05\n",
      "[Epoch 42] Training Batch [99/391]: Loss 2.686028710741084e-05\n",
      "[Epoch 42] Training Batch [100/391]: Loss 2.5530798666295595e-05\n",
      "[Epoch 42] Training Batch [101/391]: Loss 4.001722845714539e-05\n",
      "[Epoch 42] Training Batch [102/391]: Loss 2.3611881260876544e-05\n",
      "[Epoch 42] Training Batch [103/391]: Loss 1.536727540951688e-05\n",
      "[Epoch 42] Training Batch [104/391]: Loss 3.314973946544342e-05\n",
      "[Epoch 42] Training Batch [105/391]: Loss 5.811306891700951e-06\n",
      "[Epoch 42] Training Batch [106/391]: Loss 1.732939745124895e-05\n",
      "[Epoch 42] Training Batch [107/391]: Loss 2.2077287212596275e-05\n",
      "[Epoch 42] Training Batch [108/391]: Loss 1.6520531062269583e-05\n",
      "[Epoch 42] Training Batch [109/391]: Loss 1.5685713151469827e-05\n",
      "[Epoch 42] Training Batch [110/391]: Loss 1.596658694325015e-05\n",
      "[Epoch 42] Training Batch [111/391]: Loss 2.2740152417100035e-05\n",
      "[Epoch 42] Training Batch [112/391]: Loss 1.366927335766377e-05\n",
      "[Epoch 42] Training Batch [113/391]: Loss 3.6007098970003426e-05\n",
      "[Epoch 42] Training Batch [114/391]: Loss 1.863137549662497e-05\n",
      "[Epoch 42] Training Batch [115/391]: Loss 2.909395880124066e-05\n",
      "[Epoch 42] Training Batch [116/391]: Loss 2.7591326215770096e-05\n",
      "[Epoch 42] Training Batch [117/391]: Loss 2.4363193006138317e-05\n",
      "[Epoch 42] Training Batch [118/391]: Loss 1.0645326256053522e-05\n",
      "[Epoch 42] Training Batch [119/391]: Loss 2.1287343770381995e-05\n",
      "[Epoch 42] Training Batch [120/391]: Loss 2.0679666704381816e-05\n",
      "[Epoch 42] Training Batch [121/391]: Loss 2.6210505893686786e-05\n",
      "[Epoch 42] Training Batch [122/391]: Loss 2.254084756714292e-05\n",
      "[Epoch 42] Training Batch [123/391]: Loss 3.637763802544214e-05\n",
      "[Epoch 42] Training Batch [124/391]: Loss 1.2248913662915584e-05\n",
      "[Epoch 42] Training Batch [125/391]: Loss 1.3131970263202675e-05\n",
      "[Epoch 42] Training Batch [126/391]: Loss 1.5803057976881973e-05\n",
      "[Epoch 42] Training Batch [127/391]: Loss 3.7196609810052905e-06\n",
      "[Epoch 42] Training Batch [128/391]: Loss 3.837544136331417e-05\n",
      "[Epoch 42] Training Batch [129/391]: Loss 2.3404620151268318e-05\n",
      "[Epoch 42] Training Batch [130/391]: Loss 7.300393008335959e-06\n",
      "[Epoch 42] Training Batch [131/391]: Loss 1.9107339539914392e-05\n",
      "[Epoch 42] Training Batch [132/391]: Loss 3.871431545121595e-05\n",
      "[Epoch 42] Training Batch [133/391]: Loss 2.3906055503175594e-05\n",
      "[Epoch 42] Training Batch [134/391]: Loss 3.0231938580982387e-05\n",
      "[Epoch 42] Training Batch [135/391]: Loss 2.5220029783667997e-05\n",
      "[Epoch 42] Training Batch [136/391]: Loss 1.482395327911945e-05\n",
      "[Epoch 42] Training Batch [137/391]: Loss 3.491030656732619e-05\n",
      "[Epoch 42] Training Batch [138/391]: Loss 1.8693464880925603e-05\n",
      "[Epoch 42] Training Batch [139/391]: Loss 4.952613380737603e-05\n",
      "[Epoch 42] Training Batch [140/391]: Loss 1.8434902813169174e-05\n",
      "[Epoch 42] Training Batch [141/391]: Loss 2.848763506335672e-05\n",
      "[Epoch 42] Training Batch [142/391]: Loss 2.8969065169803798e-05\n",
      "[Epoch 42] Training Batch [143/391]: Loss 3.568584725144319e-05\n",
      "[Epoch 42] Training Batch [144/391]: Loss 2.9404965971480124e-05\n",
      "[Epoch 42] Training Batch [145/391]: Loss 2.3537273591500707e-05\n",
      "[Epoch 42] Training Batch [146/391]: Loss 1.9702449208125472e-05\n",
      "[Epoch 42] Training Batch [147/391]: Loss 1.7019598089973442e-05\n",
      "[Epoch 42] Training Batch [148/391]: Loss 2.5636203645262867e-05\n",
      "[Epoch 42] Training Batch [149/391]: Loss 1.2070976481481921e-05\n",
      "[Epoch 42] Training Batch [150/391]: Loss 1.848040301410947e-05\n",
      "[Epoch 42] Training Batch [151/391]: Loss 1.536516356281936e-05\n",
      "[Epoch 42] Training Batch [152/391]: Loss 2.8490421755122952e-05\n",
      "[Epoch 42] Training Batch [153/391]: Loss 2.2268401153269224e-05\n",
      "[Epoch 42] Training Batch [154/391]: Loss 1.580987373017706e-05\n",
      "[Epoch 42] Training Batch [155/391]: Loss 1.1213710422453005e-05\n",
      "[Epoch 42] Training Batch [156/391]: Loss 2.821349880832713e-05\n",
      "[Epoch 42] Training Batch [157/391]: Loss 2.4444321752525866e-05\n",
      "[Epoch 42] Training Batch [158/391]: Loss 2.5881565306917764e-05\n",
      "[Epoch 42] Training Batch [159/391]: Loss 1.511307891632896e-05\n",
      "[Epoch 42] Training Batch [160/391]: Loss 1.4495842151518445e-05\n",
      "[Epoch 42] Training Batch [161/391]: Loss 1.261549459741218e-05\n",
      "[Epoch 42] Training Batch [162/391]: Loss 2.3815935492166318e-05\n",
      "[Epoch 42] Training Batch [163/391]: Loss 1.6204026906052604e-05\n",
      "[Epoch 42] Training Batch [164/391]: Loss 2.318096267117653e-05\n",
      "[Epoch 42] Training Batch [165/391]: Loss 3.093898340011947e-05\n",
      "[Epoch 42] Training Batch [166/391]: Loss 2.3247877834364772e-05\n",
      "[Epoch 42] Training Batch [167/391]: Loss 2.3056358259054832e-05\n",
      "[Epoch 42] Training Batch [168/391]: Loss 2.2149049982544966e-05\n",
      "[Epoch 42] Training Batch [169/391]: Loss 2.2815793272457086e-05\n",
      "[Epoch 42] Training Batch [170/391]: Loss 1.4811421351623721e-05\n",
      "[Epoch 42] Training Batch [171/391]: Loss 1.717110899335239e-05\n",
      "[Epoch 42] Training Batch [172/391]: Loss 1.911762956297025e-05\n",
      "[Epoch 42] Training Batch [173/391]: Loss 1.371460166410543e-05\n",
      "[Epoch 42] Training Batch [174/391]: Loss 1.751031231833622e-05\n",
      "[Epoch 42] Training Batch [175/391]: Loss 2.1850701159564778e-05\n",
      "[Epoch 42] Training Batch [176/391]: Loss 1.1303733117529191e-05\n",
      "[Epoch 42] Training Batch [177/391]: Loss 2.0978111933800392e-05\n",
      "[Epoch 42] Training Batch [178/391]: Loss 2.1475349058164284e-05\n",
      "[Epoch 42] Training Batch [179/391]: Loss 3.052769898204133e-05\n",
      "[Epoch 42] Training Batch [180/391]: Loss 2.1444840967888013e-05\n",
      "[Epoch 42] Training Batch [181/391]: Loss 2.0820378267671913e-05\n",
      "[Epoch 42] Training Batch [182/391]: Loss 3.20703984471038e-05\n",
      "[Epoch 42] Training Batch [183/391]: Loss 1.6181516912183724e-05\n",
      "[Epoch 42] Training Batch [184/391]: Loss 1.5409219486173242e-05\n",
      "[Epoch 42] Training Batch [185/391]: Loss 3.378445762791671e-05\n",
      "[Epoch 42] Training Batch [186/391]: Loss 2.3666449123993516e-05\n",
      "[Epoch 42] Training Batch [187/391]: Loss 2.3289432647288777e-05\n",
      "[Epoch 42] Training Batch [188/391]: Loss 1.724832327454351e-05\n",
      "[Epoch 42] Training Batch [189/391]: Loss 1.8067426935886033e-05\n",
      "[Epoch 42] Training Batch [190/391]: Loss 2.2189895389601588e-05\n",
      "[Epoch 42] Training Batch [191/391]: Loss 2.5974237360060215e-05\n",
      "[Epoch 42] Training Batch [192/391]: Loss 2.2481643100036308e-05\n",
      "[Epoch 42] Training Batch [193/391]: Loss 2.009407035075128e-05\n",
      "[Epoch 42] Training Batch [194/391]: Loss 1.534628427179996e-05\n",
      "[Epoch 42] Training Batch [195/391]: Loss 3.047880090889521e-05\n",
      "[Epoch 42] Training Batch [196/391]: Loss 3.228187051718123e-05\n",
      "[Epoch 42] Training Batch [197/391]: Loss 1.5860605344641954e-05\n",
      "[Epoch 42] Training Batch [198/391]: Loss 2.6021421945188195e-05\n",
      "[Epoch 42] Training Batch [199/391]: Loss 2.2939222617424093e-05\n",
      "[Epoch 42] Training Batch [200/391]: Loss 1.881588832475245e-05\n",
      "[Epoch 42] Training Batch [201/391]: Loss 9.83045083557954e-06\n",
      "[Epoch 42] Training Batch [202/391]: Loss 2.0739023966598324e-05\n",
      "[Epoch 42] Training Batch [203/391]: Loss 3.271504465374164e-05\n",
      "[Epoch 42] Training Batch [204/391]: Loss 1.774374868546147e-05\n",
      "[Epoch 42] Training Batch [205/391]: Loss 1.769281516317278e-05\n",
      "[Epoch 42] Training Batch [206/391]: Loss 2.1195912268012762e-05\n",
      "[Epoch 42] Training Batch [207/391]: Loss 3.0042370781302452e-05\n",
      "[Epoch 42] Training Batch [208/391]: Loss 1.5703788449172862e-05\n",
      "[Epoch 42] Training Batch [209/391]: Loss 2.3851382138673216e-05\n",
      "[Epoch 42] Training Batch [210/391]: Loss 3.3020773116732016e-05\n",
      "[Epoch 42] Training Batch [211/391]: Loss 2.4085760742309503e-05\n",
      "[Epoch 42] Training Batch [212/391]: Loss 2.4555236450396478e-05\n",
      "[Epoch 42] Training Batch [213/391]: Loss 3.640724753495306e-05\n",
      "[Epoch 42] Training Batch [214/391]: Loss 8.295947736769449e-06\n",
      "[Epoch 42] Training Batch [215/391]: Loss 2.164734178222716e-05\n",
      "[Epoch 42] Training Batch [216/391]: Loss 2.873162884498015e-05\n",
      "[Epoch 42] Training Batch [217/391]: Loss 1.7552381905261427e-05\n",
      "[Epoch 42] Training Batch [218/391]: Loss 2.9998609534231946e-05\n",
      "[Epoch 42] Training Batch [219/391]: Loss 2.6681162125896662e-05\n",
      "[Epoch 42] Training Batch [220/391]: Loss 2.0360505004646257e-05\n",
      "[Epoch 42] Training Batch [221/391]: Loss 2.2997872292762622e-05\n",
      "[Epoch 42] Training Batch [222/391]: Loss 1.2459940990083851e-05\n",
      "[Epoch 42] Training Batch [223/391]: Loss 2.2612750399275683e-05\n",
      "[Epoch 42] Training Batch [224/391]: Loss 2.4355147616006434e-05\n",
      "[Epoch 42] Training Batch [225/391]: Loss 1.6815301933092996e-05\n",
      "[Epoch 42] Training Batch [226/391]: Loss 2.0913472326355986e-05\n",
      "[Epoch 42] Training Batch [227/391]: Loss 2.2148935386212543e-05\n",
      "[Epoch 42] Training Batch [228/391]: Loss 2.7327703719493002e-05\n",
      "[Epoch 42] Training Batch [229/391]: Loss 1.6435995348729193e-05\n",
      "[Epoch 42] Training Batch [230/391]: Loss 2.3431304725818336e-05\n",
      "[Epoch 42] Training Batch [231/391]: Loss 1.9309896742925048e-05\n",
      "[Epoch 42] Training Batch [232/391]: Loss 2.4101085728034377e-05\n",
      "[Epoch 42] Training Batch [233/391]: Loss 1.5573392374790274e-05\n",
      "[Epoch 42] Training Batch [234/391]: Loss 2.1197856767685153e-05\n",
      "[Epoch 42] Training Batch [235/391]: Loss 1.9393173715798184e-05\n",
      "[Epoch 42] Training Batch [236/391]: Loss 2.2351761799654923e-05\n",
      "[Epoch 42] Training Batch [237/391]: Loss 1.859443545981776e-05\n",
      "[Epoch 42] Training Batch [238/391]: Loss 2.829483310051728e-05\n",
      "[Epoch 42] Training Batch [239/391]: Loss 2.756304274953436e-05\n",
      "[Epoch 42] Training Batch [240/391]: Loss 2.0215051335981116e-05\n",
      "[Epoch 42] Training Batch [241/391]: Loss 2.6010889996541664e-05\n",
      "[Epoch 42] Training Batch [242/391]: Loss 3.674828985822387e-05\n",
      "[Epoch 42] Training Batch [243/391]: Loss 1.9113538655801676e-05\n",
      "[Epoch 42] Training Batch [244/391]: Loss 1.3385386409936473e-05\n",
      "[Epoch 42] Training Batch [245/391]: Loss 2.041656080109533e-05\n",
      "[Epoch 42] Training Batch [246/391]: Loss 2.0622996089514345e-05\n",
      "[Epoch 42] Training Batch [247/391]: Loss 1.986493771255482e-05\n",
      "[Epoch 42] Training Batch [248/391]: Loss 2.1100086087244563e-05\n",
      "[Epoch 42] Training Batch [249/391]: Loss 2.961255631817039e-05\n",
      "[Epoch 42] Training Batch [250/391]: Loss 2.068309004243929e-05\n",
      "[Epoch 42] Training Batch [251/391]: Loss 2.4741228116909042e-05\n",
      "[Epoch 42] Training Batch [252/391]: Loss 4.302886736695655e-05\n",
      "[Epoch 42] Training Batch [253/391]: Loss 2.4562559701735154e-05\n",
      "[Epoch 42] Training Batch [254/391]: Loss 1.3766575648332946e-05\n",
      "[Epoch 42] Training Batch [255/391]: Loss 3.812911018030718e-05\n",
      "[Epoch 42] Training Batch [256/391]: Loss 1.0806505997607019e-05\n",
      "[Epoch 42] Training Batch [257/391]: Loss 2.179388320655562e-05\n",
      "[Epoch 42] Training Batch [258/391]: Loss 2.0407656847964972e-05\n",
      "[Epoch 42] Training Batch [259/391]: Loss 2.4526423658244312e-05\n",
      "[Epoch 42] Training Batch [260/391]: Loss 9.723570656205993e-06\n",
      "[Epoch 42] Training Batch [261/391]: Loss 2.181197669415269e-05\n",
      "[Epoch 42] Training Batch [262/391]: Loss 1.7661341189523228e-05\n",
      "[Epoch 42] Training Batch [263/391]: Loss 1.0544422366365325e-05\n",
      "[Epoch 42] Training Batch [264/391]: Loss 3.013692912645638e-05\n",
      "[Epoch 42] Training Batch [265/391]: Loss 2.2455258658737876e-05\n",
      "[Epoch 42] Training Batch [266/391]: Loss 2.0547890017041937e-05\n",
      "[Epoch 42] Training Batch [267/391]: Loss 2.029567440331448e-05\n",
      "[Epoch 42] Training Batch [268/391]: Loss 2.349743044760544e-05\n",
      "[Epoch 42] Training Batch [269/391]: Loss 1.5680823707953095e-05\n",
      "[Epoch 42] Training Batch [270/391]: Loss 1.7002834283630364e-05\n",
      "[Epoch 42] Training Batch [271/391]: Loss 1.8434697267366573e-05\n",
      "[Epoch 42] Training Batch [272/391]: Loss 1.672071266511921e-05\n",
      "[Epoch 42] Training Batch [273/391]: Loss 3.494185148156248e-05\n",
      "[Epoch 42] Training Batch [274/391]: Loss 1.620056718820706e-05\n",
      "[Epoch 42] Training Batch [275/391]: Loss 3.066858698730357e-05\n",
      "[Epoch 42] Training Batch [276/391]: Loss 1.9152243112330325e-05\n",
      "[Epoch 42] Training Batch [277/391]: Loss 9.25786116567906e-06\n",
      "[Epoch 42] Training Batch [278/391]: Loss 2.7843752832268365e-05\n",
      "[Epoch 42] Training Batch [279/391]: Loss 2.9293169063748792e-05\n",
      "[Epoch 42] Training Batch [280/391]: Loss 4.1511262679705396e-05\n",
      "[Epoch 42] Training Batch [281/391]: Loss 3.0299494028440677e-05\n",
      "[Epoch 42] Training Batch [282/391]: Loss 1.4617276065109763e-05\n",
      "[Epoch 42] Training Batch [283/391]: Loss 3.496867066132836e-05\n",
      "[Epoch 42] Training Batch [284/391]: Loss 2.5364461180288345e-05\n",
      "[Epoch 42] Training Batch [285/391]: Loss 2.1297413695720024e-05\n",
      "[Epoch 42] Training Batch [286/391]: Loss 3.577602910809219e-05\n",
      "[Epoch 42] Training Batch [287/391]: Loss 3.0469796911347657e-05\n",
      "[Epoch 42] Training Batch [288/391]: Loss 2.3344136934611015e-05\n",
      "[Epoch 42] Training Batch [289/391]: Loss 4.3557189201237634e-05\n",
      "[Epoch 42] Training Batch [290/391]: Loss 2.189730184909422e-05\n",
      "[Epoch 42] Training Batch [291/391]: Loss 2.5032617486431263e-05\n",
      "[Epoch 42] Training Batch [292/391]: Loss 3.920647213817574e-05\n",
      "[Epoch 42] Training Batch [293/391]: Loss 1.4129122064332478e-05\n",
      "[Epoch 42] Training Batch [294/391]: Loss 1.5239543245115783e-05\n",
      "[Epoch 42] Training Batch [295/391]: Loss 1.733992394292727e-05\n",
      "[Epoch 42] Training Batch [296/391]: Loss 2.025803769356571e-05\n",
      "[Epoch 42] Training Batch [297/391]: Loss 2.2914600776857696e-05\n",
      "[Epoch 42] Training Batch [298/391]: Loss 1.942326343851164e-05\n",
      "[Epoch 42] Training Batch [299/391]: Loss 2.2423992049880326e-05\n",
      "[Epoch 42] Training Batch [300/391]: Loss 1.9807486751233228e-05\n",
      "[Epoch 42] Training Batch [301/391]: Loss 3.4339635021751747e-05\n",
      "[Epoch 42] Training Batch [302/391]: Loss 2.6583506041788496e-05\n",
      "[Epoch 42] Training Batch [303/391]: Loss 1.0775789633044042e-05\n",
      "[Epoch 42] Training Batch [304/391]: Loss 1.390132456435822e-05\n",
      "[Epoch 42] Training Batch [305/391]: Loss 2.270920595037751e-05\n",
      "[Epoch 42] Training Batch [306/391]: Loss 2.2861648176331073e-05\n",
      "[Epoch 42] Training Batch [307/391]: Loss 2.5027497031260282e-05\n",
      "[Epoch 42] Training Batch [308/391]: Loss 2.870388561859727e-05\n",
      "[Epoch 42] Training Batch [309/391]: Loss 1.550336673972197e-05\n",
      "[Epoch 42] Training Batch [310/391]: Loss 2.547889198467601e-05\n",
      "[Epoch 42] Training Batch [311/391]: Loss 3.22451232932508e-05\n",
      "[Epoch 42] Training Batch [312/391]: Loss 1.940283073054161e-05\n",
      "[Epoch 42] Training Batch [313/391]: Loss 1.4735096556250937e-05\n",
      "[Epoch 42] Training Batch [314/391]: Loss 2.9041015295661055e-05\n",
      "[Epoch 42] Training Batch [315/391]: Loss 2.7080992367700674e-05\n",
      "[Epoch 42] Training Batch [316/391]: Loss 2.4985931304399855e-05\n",
      "[Epoch 42] Training Batch [317/391]: Loss 1.1726645425369497e-05\n",
      "[Epoch 42] Training Batch [318/391]: Loss 1.2877790140919387e-05\n",
      "[Epoch 42] Training Batch [319/391]: Loss 1.2842814612668008e-05\n",
      "[Epoch 42] Training Batch [320/391]: Loss 1.793386400095187e-05\n",
      "[Epoch 42] Training Batch [321/391]: Loss 1.8458156773704104e-05\n",
      "[Epoch 42] Training Batch [322/391]: Loss 2.3368556867353618e-05\n",
      "[Epoch 42] Training Batch [323/391]: Loss 1.7785218005883507e-05\n",
      "[Epoch 42] Training Batch [324/391]: Loss 1.2474124559958e-05\n",
      "[Epoch 42] Training Batch [325/391]: Loss 1.9873665223713033e-05\n",
      "[Epoch 42] Training Batch [326/391]: Loss 1.810699723137077e-05\n",
      "[Epoch 42] Training Batch [327/391]: Loss 2.6794303266797215e-05\n",
      "[Epoch 42] Training Batch [328/391]: Loss 1.2396947568049654e-05\n",
      "[Epoch 42] Training Batch [329/391]: Loss 2.284106085426174e-05\n",
      "[Epoch 42] Training Batch [330/391]: Loss 1.766233253874816e-05\n",
      "[Epoch 42] Training Batch [331/391]: Loss 3.720657332451083e-05\n",
      "[Epoch 42] Training Batch [332/391]: Loss 1.459661143599078e-05\n",
      "[Epoch 42] Training Batch [333/391]: Loss 2.408956606814172e-05\n",
      "[Epoch 42] Training Batch [334/391]: Loss 2.7638674509944394e-05\n",
      "[Epoch 42] Training Batch [335/391]: Loss 1.0001029295381159e-05\n",
      "[Epoch 42] Training Batch [336/391]: Loss 1.1532606549735647e-05\n",
      "[Epoch 42] Training Batch [337/391]: Loss 2.5064640794880688e-05\n",
      "[Epoch 42] Training Batch [338/391]: Loss 2.705164297367446e-05\n",
      "[Epoch 42] Training Batch [339/391]: Loss 1.9663710190798156e-05\n",
      "[Epoch 42] Training Batch [340/391]: Loss 2.4026669052545913e-05\n",
      "[Epoch 42] Training Batch [341/391]: Loss 2.6175615857937373e-05\n",
      "[Epoch 42] Training Batch [342/391]: Loss 3.198349440935999e-05\n",
      "[Epoch 42] Training Batch [343/391]: Loss 1.7562439097673632e-05\n",
      "[Epoch 42] Training Batch [344/391]: Loss 2.747623329923954e-05\n",
      "[Epoch 42] Training Batch [345/391]: Loss 1.2074383448634762e-05\n",
      "[Epoch 42] Training Batch [346/391]: Loss 2.993450289068278e-05\n",
      "[Epoch 42] Training Batch [347/391]: Loss 1.073192288458813e-05\n",
      "[Epoch 42] Training Batch [348/391]: Loss 2.6482694011065178e-05\n",
      "[Epoch 42] Training Batch [349/391]: Loss 1.8745340639725327e-05\n",
      "[Epoch 42] Training Batch [350/391]: Loss 2.1614114302792586e-05\n",
      "[Epoch 42] Training Batch [351/391]: Loss 1.7497399312560447e-05\n",
      "[Epoch 42] Training Batch [352/391]: Loss 2.1071160517749377e-05\n",
      "[Epoch 42] Training Batch [353/391]: Loss 1.6057338143582456e-05\n",
      "[Epoch 42] Training Batch [354/391]: Loss 2.036056866927538e-05\n",
      "[Epoch 42] Training Batch [355/391]: Loss 2.2869166059535928e-05\n",
      "[Epoch 42] Training Batch [356/391]: Loss 1.5088915461092256e-05\n",
      "[Epoch 42] Training Batch [357/391]: Loss 2.6894702386925928e-05\n",
      "[Epoch 42] Training Batch [358/391]: Loss 1.5117977454792708e-05\n",
      "[Epoch 42] Training Batch [359/391]: Loss 2.6417295885039493e-05\n",
      "[Epoch 42] Training Batch [360/391]: Loss 3.7432713725138456e-05\n",
      "[Epoch 42] Training Batch [361/391]: Loss 2.4140186724253e-05\n",
      "[Epoch 42] Training Batch [362/391]: Loss 2.011866308748722e-05\n",
      "[Epoch 42] Training Batch [363/391]: Loss 2.491616214683745e-05\n",
      "[Epoch 42] Training Batch [364/391]: Loss 2.899307946790941e-05\n",
      "[Epoch 42] Training Batch [365/391]: Loss 2.3413724193233065e-05\n",
      "[Epoch 42] Training Batch [366/391]: Loss 1.9504821466398425e-05\n",
      "[Epoch 42] Training Batch [367/391]: Loss 2.203954863944091e-05\n",
      "[Epoch 42] Training Batch [368/391]: Loss 1.82625008164905e-05\n",
      "[Epoch 42] Training Batch [369/391]: Loss 3.2435127650387585e-05\n",
      "[Epoch 42] Training Batch [370/391]: Loss 2.3226681150845252e-05\n",
      "[Epoch 42] Training Batch [371/391]: Loss 2.2712743884767406e-05\n",
      "[Epoch 42] Training Batch [372/391]: Loss 1.7100550394388847e-05\n",
      "[Epoch 42] Training Batch [373/391]: Loss 1.6992082237266004e-05\n",
      "[Epoch 42] Training Batch [374/391]: Loss 1.4784060113015585e-05\n",
      "[Epoch 42] Training Batch [375/391]: Loss 2.635090277181007e-05\n",
      "[Epoch 42] Training Batch [376/391]: Loss 2.410190791124478e-05\n",
      "[Epoch 42] Training Batch [377/391]: Loss 1.3639088138006628e-05\n",
      "[Epoch 42] Training Batch [378/391]: Loss 2.4655022571096197e-05\n",
      "[Epoch 42] Training Batch [379/391]: Loss 2.6488607545616105e-05\n",
      "[Epoch 42] Training Batch [380/391]: Loss 3.41384147759527e-05\n",
      "[Epoch 42] Training Batch [381/391]: Loss 3.4488693927414715e-05\n",
      "[Epoch 42] Training Batch [382/391]: Loss 2.0522646082099527e-05\n",
      "[Epoch 42] Training Batch [383/391]: Loss 1.4893306797603145e-05\n",
      "[Epoch 42] Training Batch [384/391]: Loss 1.656308268138673e-05\n",
      "[Epoch 42] Training Batch [385/391]: Loss 2.1769092199974693e-05\n",
      "[Epoch 42] Training Batch [386/391]: Loss 3.179474515491165e-05\n",
      "[Epoch 42] Training Batch [387/391]: Loss 2.080988087982405e-05\n",
      "[Epoch 42] Training Batch [388/391]: Loss 1.8515092961024493e-05\n",
      "[Epoch 42] Training Batch [389/391]: Loss 3.363977521075867e-05\n",
      "[Epoch 42] Training Batch [390/391]: Loss 1.8506148990127258e-05\n",
      "[Epoch 42] Training Batch [391/391]: Loss 3.067357465624809e-05\n",
      "Epoch 42 - Train Loss: 0.0000\n",
      "*********  Epoch 43/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43] Training Batch [1/391]: Loss 2.1810039470437914e-05\n",
      "[Epoch 43] Training Batch [2/391]: Loss 2.6901368983089924e-05\n",
      "[Epoch 43] Training Batch [3/391]: Loss 1.726499613141641e-05\n",
      "[Epoch 43] Training Batch [4/391]: Loss 2.6277957658749074e-05\n",
      "[Epoch 43] Training Batch [5/391]: Loss 2.6000365323852748e-05\n",
      "[Epoch 43] Training Batch [6/391]: Loss 2.7925478207180277e-05\n",
      "[Epoch 43] Training Batch [7/391]: Loss 2.8262700652703643e-05\n",
      "[Epoch 43] Training Batch [8/391]: Loss 9.49901823332766e-06\n",
      "[Epoch 43] Training Batch [9/391]: Loss 3.526370710460469e-05\n",
      "[Epoch 43] Training Batch [10/391]: Loss 1.3509711607184727e-05\n",
      "[Epoch 43] Training Batch [11/391]: Loss 2.0385708921821788e-05\n",
      "[Epoch 43] Training Batch [12/391]: Loss 1.1904856364708394e-05\n",
      "[Epoch 43] Training Batch [13/391]: Loss 2.5637173166614957e-05\n",
      "[Epoch 43] Training Batch [14/391]: Loss 1.5128431186894886e-05\n",
      "[Epoch 43] Training Batch [15/391]: Loss 1.3444438991427887e-05\n",
      "[Epoch 43] Training Batch [16/391]: Loss 1.9212257029721513e-05\n",
      "[Epoch 43] Training Batch [17/391]: Loss 1.865606100182049e-05\n",
      "[Epoch 43] Training Batch [18/391]: Loss 2.4827935703797266e-05\n",
      "[Epoch 43] Training Batch [19/391]: Loss 2.9955785066704266e-05\n",
      "[Epoch 43] Training Batch [20/391]: Loss 1.0943053894152399e-05\n",
      "[Epoch 43] Training Batch [21/391]: Loss 1.701925248198677e-05\n",
      "[Epoch 43] Training Batch [22/391]: Loss 1.3136149391357321e-05\n",
      "[Epoch 43] Training Batch [23/391]: Loss 2.0662979295593686e-05\n",
      "[Epoch 43] Training Batch [24/391]: Loss 2.6615172828314826e-05\n",
      "[Epoch 43] Training Batch [25/391]: Loss 8.313558282679878e-06\n",
      "[Epoch 43] Training Batch [26/391]: Loss 2.4501810912624933e-05\n",
      "[Epoch 43] Training Batch [27/391]: Loss 2.5691693736007437e-05\n",
      "[Epoch 43] Training Batch [28/391]: Loss 2.625119850563351e-05\n",
      "[Epoch 43] Training Batch [29/391]: Loss 1.7996311726165004e-05\n",
      "[Epoch 43] Training Batch [30/391]: Loss 1.8561295291874558e-05\n",
      "[Epoch 43] Training Batch [31/391]: Loss 2.2319401978165843e-05\n",
      "[Epoch 43] Training Batch [32/391]: Loss 1.7980170014197938e-05\n",
      "[Epoch 43] Training Batch [33/391]: Loss 2.3406748368870467e-05\n",
      "[Epoch 43] Training Batch [34/391]: Loss 1.2224617421452422e-05\n",
      "[Epoch 43] Training Batch [35/391]: Loss 2.548577140260022e-05\n",
      "[Epoch 43] Training Batch [36/391]: Loss 1.4643012036685832e-05\n",
      "[Epoch 43] Training Batch [37/391]: Loss 1.3727617442782503e-05\n",
      "[Epoch 43] Training Batch [38/391]: Loss 3.007142004207708e-05\n",
      "[Epoch 43] Training Batch [39/391]: Loss 2.3330730982706882e-05\n",
      "[Epoch 43] Training Batch [40/391]: Loss 1.4791135981795378e-05\n",
      "[Epoch 43] Training Batch [41/391]: Loss 2.0961399059160613e-05\n",
      "[Epoch 43] Training Batch [42/391]: Loss 1.0622895388223697e-05\n",
      "[Epoch 43] Training Batch [43/391]: Loss 3.191320865880698e-05\n",
      "[Epoch 43] Training Batch [44/391]: Loss 1.771850475051906e-05\n",
      "[Epoch 43] Training Batch [45/391]: Loss 2.0345354641904123e-05\n",
      "[Epoch 43] Training Batch [46/391]: Loss 1.788295412552543e-05\n",
      "[Epoch 43] Training Batch [47/391]: Loss 2.149081410607323e-05\n",
      "[Epoch 43] Training Batch [48/391]: Loss 1.2798187526641414e-05\n",
      "[Epoch 43] Training Batch [49/391]: Loss 2.8091712010791525e-05\n",
      "[Epoch 43] Training Batch [50/391]: Loss 2.3609629351994954e-05\n",
      "[Epoch 43] Training Batch [51/391]: Loss 1.3852326446794905e-05\n",
      "[Epoch 43] Training Batch [52/391]: Loss 2.1425539671326987e-05\n",
      "[Epoch 43] Training Batch [53/391]: Loss 1.4030189049663022e-05\n",
      "[Epoch 43] Training Batch [54/391]: Loss 2.095309719152283e-05\n",
      "[Epoch 43] Training Batch [55/391]: Loss 2.2170826923684217e-05\n",
      "[Epoch 43] Training Batch [56/391]: Loss 2.6478661311557516e-05\n",
      "[Epoch 43] Training Batch [57/391]: Loss 1.5420049749081954e-05\n",
      "[Epoch 43] Training Batch [58/391]: Loss 1.299621544603724e-05\n",
      "[Epoch 43] Training Batch [59/391]: Loss 2.22803409997141e-05\n",
      "[Epoch 43] Training Batch [60/391]: Loss 1.0276670764142182e-05\n",
      "[Epoch 43] Training Batch [61/391]: Loss 1.516249540145509e-05\n",
      "[Epoch 43] Training Batch [62/391]: Loss 2.6957732188748196e-05\n",
      "[Epoch 43] Training Batch [63/391]: Loss 1.8701099179452285e-05\n",
      "[Epoch 43] Training Batch [64/391]: Loss 9.932808097801171e-06\n",
      "[Epoch 43] Training Batch [65/391]: Loss 1.493439958721865e-05\n",
      "[Epoch 43] Training Batch [66/391]: Loss 3.096507498412393e-05\n",
      "[Epoch 43] Training Batch [67/391]: Loss 1.977255305973813e-05\n",
      "[Epoch 43] Training Batch [68/391]: Loss 2.295209196745418e-05\n",
      "[Epoch 43] Training Batch [69/391]: Loss 2.4600294636911713e-05\n",
      "[Epoch 43] Training Batch [70/391]: Loss 9.543996384309139e-06\n",
      "[Epoch 43] Training Batch [71/391]: Loss 2.725989543250762e-05\n",
      "[Epoch 43] Training Batch [72/391]: Loss 1.8183360225521028e-05\n",
      "[Epoch 43] Training Batch [73/391]: Loss 2.103728911606595e-05\n",
      "[Epoch 43] Training Batch [74/391]: Loss 2.251922524010297e-05\n",
      "[Epoch 43] Training Batch [75/391]: Loss 2.586494156275876e-05\n",
      "[Epoch 43] Training Batch [76/391]: Loss 2.3957745725056157e-05\n",
      "[Epoch 43] Training Batch [77/391]: Loss 1.4657948668173049e-05\n",
      "[Epoch 43] Training Batch [78/391]: Loss 1.7120470147347078e-05\n",
      "[Epoch 43] Training Batch [79/391]: Loss 7.663576070626732e-06\n",
      "[Epoch 43] Training Batch [80/391]: Loss 1.3188246157369576e-05\n",
      "[Epoch 43] Training Batch [81/391]: Loss 1.5058108147059102e-05\n",
      "[Epoch 43] Training Batch [82/391]: Loss 1.048884769261349e-05\n",
      "[Epoch 43] Training Batch [83/391]: Loss 2.0558893083943985e-05\n",
      "[Epoch 43] Training Batch [84/391]: Loss 1.6786148989922367e-05\n",
      "[Epoch 43] Training Batch [85/391]: Loss 2.59022563113831e-05\n",
      "[Epoch 43] Training Batch [86/391]: Loss 7.548991106887115e-06\n",
      "[Epoch 43] Training Batch [87/391]: Loss 2.5817600544542074e-05\n",
      "[Epoch 43] Training Batch [88/391]: Loss 1.4091373486735392e-05\n",
      "[Epoch 43] Training Batch [89/391]: Loss 2.697277523111552e-05\n",
      "[Epoch 43] Training Batch [90/391]: Loss 2.8754455343005247e-05\n",
      "[Epoch 43] Training Batch [91/391]: Loss 2.4717322958167642e-05\n",
      "[Epoch 43] Training Batch [92/391]: Loss 1.722040178719908e-05\n",
      "[Epoch 43] Training Batch [93/391]: Loss 1.616427289263811e-05\n",
      "[Epoch 43] Training Batch [94/391]: Loss 2.5254497813875787e-05\n",
      "[Epoch 43] Training Batch [95/391]: Loss 1.7719677998684347e-05\n",
      "[Epoch 43] Training Batch [96/391]: Loss 2.338246486033313e-05\n",
      "[Epoch 43] Training Batch [97/391]: Loss 1.0142131031898316e-05\n",
      "[Epoch 43] Training Batch [98/391]: Loss 4.466588143259287e-05\n",
      "[Epoch 43] Training Batch [99/391]: Loss 1.9082403014181182e-05\n",
      "[Epoch 43] Training Batch [100/391]: Loss 1.0531695807003416e-05\n",
      "[Epoch 43] Training Batch [101/391]: Loss 1.616984445718117e-05\n",
      "[Epoch 43] Training Batch [102/391]: Loss 3.886692502419464e-05\n",
      "[Epoch 43] Training Batch [103/391]: Loss 1.779849117156118e-05\n",
      "[Epoch 43] Training Batch [104/391]: Loss 2.321979081898462e-05\n",
      "[Epoch 43] Training Batch [105/391]: Loss 3.082136026932858e-05\n",
      "[Epoch 43] Training Batch [106/391]: Loss 1.4788326552661601e-05\n",
      "[Epoch 43] Training Batch [107/391]: Loss 4.244616866344586e-05\n",
      "[Epoch 43] Training Batch [108/391]: Loss 2.410079468972981e-05\n",
      "[Epoch 43] Training Batch [109/391]: Loss 2.3696078642387874e-05\n",
      "[Epoch 43] Training Batch [110/391]: Loss 1.2071735909557901e-05\n",
      "[Epoch 43] Training Batch [111/391]: Loss 1.6308118574670516e-05\n",
      "[Epoch 43] Training Batch [112/391]: Loss 2.587385824881494e-05\n",
      "[Epoch 43] Training Batch [113/391]: Loss 3.256274794694036e-05\n",
      "[Epoch 43] Training Batch [114/391]: Loss 1.585381687618792e-05\n",
      "[Epoch 43] Training Batch [115/391]: Loss 1.603614327905234e-05\n",
      "[Epoch 43] Training Batch [116/391]: Loss 2.6276213247911073e-05\n",
      "[Epoch 43] Training Batch [117/391]: Loss 1.733402314130217e-05\n",
      "[Epoch 43] Training Batch [118/391]: Loss 1.9254928702139296e-05\n",
      "[Epoch 43] Training Batch [119/391]: Loss 1.4493683920591138e-05\n",
      "[Epoch 43] Training Batch [120/391]: Loss 2.5524435841361992e-05\n",
      "[Epoch 43] Training Batch [121/391]: Loss 2.7584621420828626e-05\n",
      "[Epoch 43] Training Batch [122/391]: Loss 2.049826980510261e-05\n",
      "[Epoch 43] Training Batch [123/391]: Loss 2.0011892047477886e-05\n",
      "[Epoch 43] Training Batch [124/391]: Loss 1.728148345137015e-05\n",
      "[Epoch 43] Training Batch [125/391]: Loss 2.3559545297757722e-05\n",
      "[Epoch 43] Training Batch [126/391]: Loss 2.6906807761406526e-05\n",
      "[Epoch 43] Training Batch [127/391]: Loss 3.3073010854423046e-05\n",
      "[Epoch 43] Training Batch [128/391]: Loss 2.0332316125859506e-05\n",
      "[Epoch 43] Training Batch [129/391]: Loss 1.7738464521244168e-05\n",
      "[Epoch 43] Training Batch [130/391]: Loss 1.6249359759967774e-05\n",
      "[Epoch 43] Training Batch [131/391]: Loss 3.283597470726818e-05\n",
      "[Epoch 43] Training Batch [132/391]: Loss 1.9012501070392318e-05\n",
      "[Epoch 43] Training Batch [133/391]: Loss 2.1517169443541206e-05\n",
      "[Epoch 43] Training Batch [134/391]: Loss 1.419966429239139e-05\n",
      "[Epoch 43] Training Batch [135/391]: Loss 2.165692603739444e-05\n",
      "[Epoch 43] Training Batch [136/391]: Loss 9.124643838731572e-06\n",
      "[Epoch 43] Training Batch [137/391]: Loss 2.3829008569009602e-05\n",
      "[Epoch 43] Training Batch [138/391]: Loss 2.1176670998102054e-05\n",
      "[Epoch 43] Training Batch [139/391]: Loss 1.8710390577325597e-05\n",
      "[Epoch 43] Training Batch [140/391]: Loss 3.332427149871364e-05\n",
      "[Epoch 43] Training Batch [141/391]: Loss 2.6042347599286586e-05\n",
      "[Epoch 43] Training Batch [142/391]: Loss 1.6835865608300082e-05\n",
      "[Epoch 43] Training Batch [143/391]: Loss 2.9815088055329397e-05\n",
      "[Epoch 43] Training Batch [144/391]: Loss 2.1522040697163902e-05\n",
      "[Epoch 43] Training Batch [145/391]: Loss 2.856403443729505e-05\n",
      "[Epoch 43] Training Batch [146/391]: Loss 2.1875528545933776e-05\n",
      "[Epoch 43] Training Batch [147/391]: Loss 1.7516762454761192e-05\n",
      "[Epoch 43] Training Batch [148/391]: Loss 2.4561837562941946e-05\n",
      "[Epoch 43] Training Batch [149/391]: Loss 2.9913251637481153e-05\n",
      "[Epoch 43] Training Batch [150/391]: Loss 2.015790050791111e-05\n",
      "[Epoch 43] Training Batch [151/391]: Loss 1.8778186131385155e-05\n",
      "[Epoch 43] Training Batch [152/391]: Loss 2.3426411644322798e-05\n",
      "[Epoch 43] Training Batch [153/391]: Loss 1.662846807448659e-05\n",
      "[Epoch 43] Training Batch [154/391]: Loss 1.9524324670783244e-05\n",
      "[Epoch 43] Training Batch [155/391]: Loss 3.1636638595955446e-05\n",
      "[Epoch 43] Training Batch [156/391]: Loss 2.6072155378642492e-05\n",
      "[Epoch 43] Training Batch [157/391]: Loss 3.787624154938385e-05\n",
      "[Epoch 43] Training Batch [158/391]: Loss 1.4095570804784074e-05\n",
      "[Epoch 43] Training Batch [159/391]: Loss 2.672757182153873e-05\n",
      "[Epoch 43] Training Batch [160/391]: Loss 1.5033123418106697e-05\n",
      "[Epoch 43] Training Batch [161/391]: Loss 1.8801152691594325e-05\n",
      "[Epoch 43] Training Batch [162/391]: Loss 1.6928437617025338e-05\n",
      "[Epoch 43] Training Batch [163/391]: Loss 3.244171603000723e-05\n",
      "[Epoch 43] Training Batch [164/391]: Loss 1.5816875020391308e-05\n",
      "[Epoch 43] Training Batch [165/391]: Loss 2.0766021407325752e-05\n",
      "[Epoch 43] Training Batch [166/391]: Loss 1.0797100003401283e-05\n",
      "[Epoch 43] Training Batch [167/391]: Loss 1.835200600908138e-05\n",
      "[Epoch 43] Training Batch [168/391]: Loss 1.2325287571002264e-05\n",
      "[Epoch 43] Training Batch [169/391]: Loss 2.7092262826045044e-05\n",
      "[Epoch 43] Training Batch [170/391]: Loss 1.6320551367243752e-05\n",
      "[Epoch 43] Training Batch [171/391]: Loss 1.4344094779517036e-05\n",
      "[Epoch 43] Training Batch [172/391]: Loss 2.6728419470600784e-05\n",
      "[Epoch 43] Training Batch [173/391]: Loss 3.020193071279209e-05\n",
      "[Epoch 43] Training Batch [174/391]: Loss 2.1420191842480563e-05\n",
      "[Epoch 43] Training Batch [175/391]: Loss 2.021829641307704e-05\n",
      "[Epoch 43] Training Batch [176/391]: Loss 2.7292549930280074e-05\n",
      "[Epoch 43] Training Batch [177/391]: Loss 1.6006111763999797e-05\n",
      "[Epoch 43] Training Batch [178/391]: Loss 1.913855885504745e-05\n",
      "[Epoch 43] Training Batch [179/391]: Loss 2.1841702618985437e-05\n",
      "[Epoch 43] Training Batch [180/391]: Loss 2.464175850036554e-05\n",
      "[Epoch 43] Training Batch [181/391]: Loss 1.1854050171677954e-05\n",
      "[Epoch 43] Training Batch [182/391]: Loss 2.7099224098492414e-05\n",
      "[Epoch 43] Training Batch [183/391]: Loss 1.6079242413979955e-05\n",
      "[Epoch 43] Training Batch [184/391]: Loss 2.3894956029835157e-05\n",
      "[Epoch 43] Training Batch [185/391]: Loss 1.037509719026275e-05\n",
      "[Epoch 43] Training Batch [186/391]: Loss 1.9419343516347e-05\n",
      "[Epoch 43] Training Batch [187/391]: Loss 2.1022644432378e-05\n",
      "[Epoch 43] Training Batch [188/391]: Loss 1.8581105905468576e-05\n",
      "[Epoch 43] Training Batch [189/391]: Loss 1.888907718239352e-05\n",
      "[Epoch 43] Training Batch [190/391]: Loss 2.4573688278906047e-05\n",
      "[Epoch 43] Training Batch [191/391]: Loss 1.7070027752197348e-05\n",
      "[Epoch 43] Training Batch [192/391]: Loss 1.4284345525084063e-05\n",
      "[Epoch 43] Training Batch [193/391]: Loss 1.071509723260533e-05\n",
      "[Epoch 43] Training Batch [194/391]: Loss 1.5522507965215482e-05\n",
      "[Epoch 43] Training Batch [195/391]: Loss 1.883579170680605e-05\n",
      "[Epoch 43] Training Batch [196/391]: Loss 7.523871317971498e-06\n",
      "[Epoch 43] Training Batch [197/391]: Loss 2.568032141425647e-05\n",
      "[Epoch 43] Training Batch [198/391]: Loss 2.686246625671629e-05\n",
      "[Epoch 43] Training Batch [199/391]: Loss 8.955982593761291e-06\n",
      "[Epoch 43] Training Batch [200/391]: Loss 1.1215464837732725e-05\n",
      "[Epoch 43] Training Batch [201/391]: Loss 1.729611904011108e-05\n",
      "[Epoch 43] Training Batch [202/391]: Loss 2.1369032765505835e-05\n",
      "[Epoch 43] Training Batch [203/391]: Loss 1.561596400279086e-05\n",
      "[Epoch 43] Training Batch [204/391]: Loss 2.4351189495064318e-05\n",
      "[Epoch 43] Training Batch [205/391]: Loss 2.2061816707719117e-05\n",
      "[Epoch 43] Training Batch [206/391]: Loss 2.3497690563090146e-05\n",
      "[Epoch 43] Training Batch [207/391]: Loss 1.6057761968113482e-05\n",
      "[Epoch 43] Training Batch [208/391]: Loss 1.5921563317533582e-05\n",
      "[Epoch 43] Training Batch [209/391]: Loss 1.8592254491522908e-05\n",
      "[Epoch 43] Training Batch [210/391]: Loss 2.807136479532346e-05\n",
      "[Epoch 43] Training Batch [211/391]: Loss 1.9142238670610823e-05\n",
      "[Epoch 43] Training Batch [212/391]: Loss 1.7258786101592705e-05\n",
      "[Epoch 43] Training Batch [213/391]: Loss 2.0543897335301153e-05\n",
      "[Epoch 43] Training Batch [214/391]: Loss 4.011589044239372e-05\n",
      "[Epoch 43] Training Batch [215/391]: Loss 1.8608776372275315e-05\n",
      "[Epoch 43] Training Batch [216/391]: Loss 1.980313209060114e-05\n",
      "[Epoch 43] Training Batch [217/391]: Loss 2.682442936929874e-05\n",
      "[Epoch 43] Training Batch [218/391]: Loss 2.531207246647682e-05\n",
      "[Epoch 43] Training Batch [219/391]: Loss 1.2897460692329332e-05\n",
      "[Epoch 43] Training Batch [220/391]: Loss 1.3086557373753749e-05\n",
      "[Epoch 43] Training Batch [221/391]: Loss 1.497238190495409e-05\n",
      "[Epoch 43] Training Batch [222/391]: Loss 1.216003147419542e-05\n",
      "[Epoch 43] Training Batch [223/391]: Loss 3.271892637712881e-05\n",
      "[Epoch 43] Training Batch [224/391]: Loss 2.0434668840607628e-05\n",
      "[Epoch 43] Training Batch [225/391]: Loss 2.639123340486549e-05\n",
      "[Epoch 43] Training Batch [226/391]: Loss 2.1825911971973255e-05\n",
      "[Epoch 43] Training Batch [227/391]: Loss 1.9496110326144844e-05\n",
      "[Epoch 43] Training Batch [228/391]: Loss 2.2439027816290036e-05\n",
      "[Epoch 43] Training Batch [229/391]: Loss 1.552389221615158e-05\n",
      "[Epoch 43] Training Batch [230/391]: Loss 2.6710660677053966e-05\n",
      "[Epoch 43] Training Batch [231/391]: Loss 1.0971240953949746e-05\n",
      "[Epoch 43] Training Batch [232/391]: Loss 3.9879152609501034e-05\n",
      "[Epoch 43] Training Batch [233/391]: Loss 3.6206805816618726e-05\n",
      "[Epoch 43] Training Batch [234/391]: Loss 1.2515894013631623e-05\n",
      "[Epoch 43] Training Batch [235/391]: Loss 1.9946637621615082e-05\n",
      "[Epoch 43] Training Batch [236/391]: Loss 1.2122277439630125e-05\n",
      "[Epoch 43] Training Batch [237/391]: Loss 1.5585083019686863e-05\n",
      "[Epoch 43] Training Batch [238/391]: Loss 1.1733501196431462e-05\n",
      "[Epoch 43] Training Batch [239/391]: Loss 1.6118423445732333e-05\n",
      "[Epoch 43] Training Batch [240/391]: Loss 1.620333205210045e-05\n",
      "[Epoch 43] Training Batch [241/391]: Loss 1.4759525583940558e-05\n",
      "[Epoch 43] Training Batch [242/391]: Loss 2.47182251769118e-05\n",
      "[Epoch 43] Training Batch [243/391]: Loss 2.879822022805456e-05\n",
      "[Epoch 43] Training Batch [244/391]: Loss 1.0849862519535236e-05\n",
      "[Epoch 43] Training Batch [245/391]: Loss 1.3666950508195441e-05\n",
      "[Epoch 43] Training Batch [246/391]: Loss 2.311096068297047e-05\n",
      "[Epoch 43] Training Batch [247/391]: Loss 2.0557314201141708e-05\n",
      "[Epoch 43] Training Batch [248/391]: Loss 3.023664976353757e-05\n",
      "[Epoch 43] Training Batch [249/391]: Loss 1.1009185982402414e-05\n",
      "[Epoch 43] Training Batch [250/391]: Loss 2.1589863536064513e-05\n",
      "[Epoch 43] Training Batch [251/391]: Loss 3.6180401366436854e-05\n",
      "[Epoch 43] Training Batch [252/391]: Loss 1.885364508780185e-05\n",
      "[Epoch 43] Training Batch [253/391]: Loss 1.9238887034589425e-05\n",
      "[Epoch 43] Training Batch [254/391]: Loss 2.2121219444670714e-05\n",
      "[Epoch 43] Training Batch [255/391]: Loss 1.6731233699829318e-05\n",
      "[Epoch 43] Training Batch [256/391]: Loss 1.212895404023584e-05\n",
      "[Epoch 43] Training Batch [257/391]: Loss 2.999429671035614e-05\n",
      "[Epoch 43] Training Batch [258/391]: Loss 2.3526175937149674e-05\n",
      "[Epoch 43] Training Batch [259/391]: Loss 2.883362139982637e-05\n",
      "[Epoch 43] Training Batch [260/391]: Loss 3.057123103644699e-05\n",
      "[Epoch 43] Training Batch [261/391]: Loss 9.976844012271613e-06\n",
      "[Epoch 43] Training Batch [262/391]: Loss 9.332267836725805e-06\n",
      "[Epoch 43] Training Batch [263/391]: Loss 2.862465953512583e-05\n",
      "[Epoch 43] Training Batch [264/391]: Loss 2.5666678993729874e-05\n",
      "[Epoch 43] Training Batch [265/391]: Loss 1.195350159832742e-05\n",
      "[Epoch 43] Training Batch [266/391]: Loss 1.2748168046528008e-05\n",
      "[Epoch 43] Training Batch [267/391]: Loss 1.2539198905869853e-05\n",
      "[Epoch 43] Training Batch [268/391]: Loss 1.8667235053726472e-05\n",
      "[Epoch 43] Training Batch [269/391]: Loss 3.565519364201464e-05\n",
      "[Epoch 43] Training Batch [270/391]: Loss 2.1220475900918245e-05\n",
      "[Epoch 43] Training Batch [271/391]: Loss 1.6500236597494222e-05\n",
      "[Epoch 43] Training Batch [272/391]: Loss 2.137617593689356e-05\n",
      "[Epoch 43] Training Batch [273/391]: Loss 1.569562664371915e-05\n",
      "[Epoch 43] Training Batch [274/391]: Loss 1.4808100786467548e-05\n",
      "[Epoch 43] Training Batch [275/391]: Loss 3.165989983244799e-05\n",
      "[Epoch 43] Training Batch [276/391]: Loss 2.7934049285249785e-05\n",
      "[Epoch 43] Training Batch [277/391]: Loss 3.107759403064847e-05\n",
      "[Epoch 43] Training Batch [278/391]: Loss 1.738310129439924e-05\n",
      "[Epoch 43] Training Batch [279/391]: Loss 1.4168298548611347e-05\n",
      "[Epoch 43] Training Batch [280/391]: Loss 1.587159204063937e-05\n",
      "[Epoch 43] Training Batch [281/391]: Loss 2.5299472326878458e-05\n",
      "[Epoch 43] Training Batch [282/391]: Loss 2.171942287532147e-05\n",
      "[Epoch 43] Training Batch [283/391]: Loss 1.701498513284605e-05\n",
      "[Epoch 43] Training Batch [284/391]: Loss 2.6431425794726238e-05\n",
      "[Epoch 43] Training Batch [285/391]: Loss 1.0778431715152692e-05\n",
      "[Epoch 43] Training Batch [286/391]: Loss 2.2308795450953767e-05\n",
      "[Epoch 43] Training Batch [287/391]: Loss 1.8520278899814002e-05\n",
      "[Epoch 43] Training Batch [288/391]: Loss 1.2867159966845065e-05\n",
      "[Epoch 43] Training Batch [289/391]: Loss 2.0833957023569383e-05\n",
      "[Epoch 43] Training Batch [290/391]: Loss 1.7615082470001653e-05\n",
      "[Epoch 43] Training Batch [291/391]: Loss 2.4565049898228608e-05\n",
      "[Epoch 43] Training Batch [292/391]: Loss 3.98440970457159e-05\n",
      "[Epoch 43] Training Batch [293/391]: Loss 3.496644058031961e-05\n",
      "[Epoch 43] Training Batch [294/391]: Loss 2.177357964683324e-05\n",
      "[Epoch 43] Training Batch [295/391]: Loss 1.414667258359259e-05\n",
      "[Epoch 43] Training Batch [296/391]: Loss 1.4001339877722785e-05\n",
      "[Epoch 43] Training Batch [297/391]: Loss 8.141982107190415e-06\n",
      "[Epoch 43] Training Batch [298/391]: Loss 2.1187988750170916e-05\n",
      "[Epoch 43] Training Batch [299/391]: Loss 1.782658728188835e-05\n",
      "[Epoch 43] Training Batch [300/391]: Loss 1.1505720976856537e-05\n",
      "[Epoch 43] Training Batch [301/391]: Loss 1.5257298400683794e-05\n",
      "[Epoch 43] Training Batch [302/391]: Loss 2.135959584848024e-05\n",
      "[Epoch 43] Training Batch [303/391]: Loss 2.447629412927199e-05\n",
      "[Epoch 43] Training Batch [304/391]: Loss 1.5899511708994396e-05\n",
      "[Epoch 43] Training Batch [305/391]: Loss 1.897503352665808e-05\n",
      "[Epoch 43] Training Batch [306/391]: Loss 1.2968721421202645e-05\n",
      "[Epoch 43] Training Batch [307/391]: Loss 2.9459049983415753e-05\n",
      "[Epoch 43] Training Batch [308/391]: Loss 2.0250055968062952e-05\n",
      "[Epoch 43] Training Batch [309/391]: Loss 1.7391097571817227e-05\n",
      "[Epoch 43] Training Batch [310/391]: Loss 1.427817733201664e-05\n",
      "[Epoch 43] Training Batch [311/391]: Loss 2.325595596630592e-05\n",
      "[Epoch 43] Training Batch [312/391]: Loss 2.1239877241896465e-05\n",
      "[Epoch 43] Training Batch [313/391]: Loss 1.1968375474680215e-05\n",
      "[Epoch 43] Training Batch [314/391]: Loss 2.0179140847176313e-05\n",
      "[Epoch 43] Training Batch [315/391]: Loss 3.258515425841324e-05\n",
      "[Epoch 43] Training Batch [316/391]: Loss 2.4737269995966926e-05\n",
      "[Epoch 43] Training Batch [317/391]: Loss 1.5856014215387404e-05\n",
      "[Epoch 43] Training Batch [318/391]: Loss 1.4062129594094586e-05\n",
      "[Epoch 43] Training Batch [319/391]: Loss 2.9712182367802598e-05\n",
      "[Epoch 43] Training Batch [320/391]: Loss 1.5710258594481274e-05\n",
      "[Epoch 43] Training Batch [321/391]: Loss 2.6284780688001774e-05\n",
      "[Epoch 43] Training Batch [322/391]: Loss 2.0019411749672145e-05\n",
      "[Epoch 43] Training Batch [323/391]: Loss 2.1210185877862386e-05\n",
      "[Epoch 43] Training Batch [324/391]: Loss 8.4820867414237e-06\n",
      "[Epoch 43] Training Batch [325/391]: Loss 1.7142501747002825e-05\n",
      "[Epoch 43] Training Batch [326/391]: Loss 1.004932801151881e-05\n",
      "[Epoch 43] Training Batch [327/391]: Loss 2.2663876734441146e-05\n",
      "[Epoch 43] Training Batch [328/391]: Loss 1.841562334448099e-05\n",
      "[Epoch 43] Training Batch [329/391]: Loss 2.856086939573288e-05\n",
      "[Epoch 43] Training Batch [330/391]: Loss 2.3557047825306654e-05\n",
      "[Epoch 43] Training Batch [331/391]: Loss 1.2629850061784964e-05\n",
      "[Epoch 43] Training Batch [332/391]: Loss 8.237032488978002e-06\n",
      "[Epoch 43] Training Batch [333/391]: Loss 1.1007401553797536e-05\n",
      "[Epoch 43] Training Batch [334/391]: Loss 1.7183698219014332e-05\n",
      "[Epoch 43] Training Batch [335/391]: Loss 2.533489532652311e-05\n",
      "[Epoch 43] Training Batch [336/391]: Loss 2.0702762412838638e-05\n",
      "[Epoch 43] Training Batch [337/391]: Loss 2.0711198885692284e-05\n",
      "[Epoch 43] Training Batch [338/391]: Loss 2.6343277568230405e-05\n",
      "[Epoch 43] Training Batch [339/391]: Loss 1.945786425494589e-05\n",
      "[Epoch 43] Training Batch [340/391]: Loss 2.6207677365164272e-05\n",
      "[Epoch 43] Training Batch [341/391]: Loss 1.7120495613198727e-05\n",
      "[Epoch 43] Training Batch [342/391]: Loss 8.809686733002309e-06\n",
      "[Epoch 43] Training Batch [343/391]: Loss 2.1095440388307907e-05\n",
      "[Epoch 43] Training Batch [344/391]: Loss 2.70302716671722e-05\n",
      "[Epoch 43] Training Batch [345/391]: Loss 2.7901100111193955e-05\n",
      "[Epoch 43] Training Batch [346/391]: Loss 2.4726368792471476e-05\n",
      "[Epoch 43] Training Batch [347/391]: Loss 2.9048202122794464e-05\n",
      "[Epoch 43] Training Batch [348/391]: Loss 1.3562005733547267e-05\n",
      "[Epoch 43] Training Batch [349/391]: Loss 2.402147038083058e-05\n",
      "[Epoch 43] Training Batch [350/391]: Loss 1.776963472366333e-05\n",
      "[Epoch 43] Training Batch [351/391]: Loss 2.1349298549466766e-05\n",
      "[Epoch 43] Training Batch [352/391]: Loss 2.4290497094625607e-05\n",
      "[Epoch 43] Training Batch [353/391]: Loss 1.493276886321837e-05\n",
      "[Epoch 43] Training Batch [354/391]: Loss 1.0390837815066334e-05\n",
      "[Epoch 43] Training Batch [355/391]: Loss 2.1107773136463948e-05\n",
      "[Epoch 43] Training Batch [356/391]: Loss 3.948240919271484e-05\n",
      "[Epoch 43] Training Batch [357/391]: Loss 2.9990456823725253e-05\n",
      "[Epoch 43] Training Batch [358/391]: Loss 3.5105062124785036e-05\n",
      "[Epoch 43] Training Batch [359/391]: Loss 1.9127830455545336e-05\n",
      "[Epoch 43] Training Batch [360/391]: Loss 1.4393524907063693e-05\n",
      "[Epoch 43] Training Batch [361/391]: Loss 2.1189542167121544e-05\n",
      "[Epoch 43] Training Batch [362/391]: Loss 2.275095357617829e-05\n",
      "[Epoch 43] Training Batch [363/391]: Loss 1.195178811030928e-05\n",
      "[Epoch 43] Training Batch [364/391]: Loss 2.708881947910413e-05\n",
      "[Epoch 43] Training Batch [365/391]: Loss 1.4469545021711383e-05\n",
      "[Epoch 43] Training Batch [366/391]: Loss 1.966381933016237e-05\n",
      "[Epoch 43] Training Batch [367/391]: Loss 1.9545472241588868e-05\n",
      "[Epoch 43] Training Batch [368/391]: Loss 2.006434806389734e-05\n",
      "[Epoch 43] Training Batch [369/391]: Loss 5.82045777264284e-06\n",
      "[Epoch 43] Training Batch [370/391]: Loss 1.619056456547696e-05\n",
      "[Epoch 43] Training Batch [371/391]: Loss 2.178771137550939e-05\n",
      "[Epoch 43] Training Batch [372/391]: Loss 1.4986248970672023e-05\n",
      "[Epoch 43] Training Batch [373/391]: Loss 2.9166352760512382e-05\n",
      "[Epoch 43] Training Batch [374/391]: Loss 8.303963113576174e-06\n",
      "[Epoch 43] Training Batch [375/391]: Loss 1.5476314729312435e-05\n",
      "[Epoch 43] Training Batch [376/391]: Loss 2.4381222829106264e-05\n",
      "[Epoch 43] Training Batch [377/391]: Loss 1.30726812130888e-05\n",
      "[Epoch 43] Training Batch [378/391]: Loss 1.4848707905912306e-05\n",
      "[Epoch 43] Training Batch [379/391]: Loss 2.7048530682804994e-05\n",
      "[Epoch 43] Training Batch [380/391]: Loss 2.572005905676633e-05\n",
      "[Epoch 43] Training Batch [381/391]: Loss 1.6486914319102652e-05\n",
      "[Epoch 43] Training Batch [382/391]: Loss 1.1111111234640703e-05\n",
      "[Epoch 43] Training Batch [383/391]: Loss 2.2921591153135523e-05\n",
      "[Epoch 43] Training Batch [384/391]: Loss 4.151534812990576e-05\n",
      "[Epoch 43] Training Batch [385/391]: Loss 3.158360777888447e-05\n",
      "[Epoch 43] Training Batch [386/391]: Loss 1.3554390534409322e-05\n",
      "[Epoch 43] Training Batch [387/391]: Loss 1.3452930943458341e-05\n",
      "[Epoch 43] Training Batch [388/391]: Loss 7.220022780529689e-06\n",
      "[Epoch 43] Training Batch [389/391]: Loss 1.736236663418822e-05\n",
      "[Epoch 43] Training Batch [390/391]: Loss 1.443083328922512e-05\n",
      "[Epoch 43] Training Batch [391/391]: Loss 1.6659474567859434e-05\n",
      "Epoch 43 - Train Loss: 0.0000\n",
      "*********  Epoch 44/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44] Training Batch [1/391]: Loss 1.5009033631940838e-05\n",
      "[Epoch 44] Training Batch [2/391]: Loss 1.1506505870784167e-05\n",
      "[Epoch 44] Training Batch [3/391]: Loss 1.113851340051042e-05\n",
      "[Epoch 44] Training Batch [4/391]: Loss 2.2441958208219148e-05\n",
      "[Epoch 44] Training Batch [5/391]: Loss 1.4398154235095717e-05\n",
      "[Epoch 44] Training Batch [6/391]: Loss 3.431820368859917e-05\n",
      "[Epoch 44] Training Batch [7/391]: Loss 3.029885920113884e-05\n",
      "[Epoch 44] Training Batch [8/391]: Loss 1.9181643438059837e-05\n",
      "[Epoch 44] Training Batch [9/391]: Loss 2.296167986060027e-05\n",
      "[Epoch 44] Training Batch [10/391]: Loss 1.6961806977633387e-05\n",
      "[Epoch 44] Training Batch [11/391]: Loss 1.611771949683316e-05\n",
      "[Epoch 44] Training Batch [12/391]: Loss 1.4047781405679416e-05\n",
      "[Epoch 44] Training Batch [13/391]: Loss 1.319737930316478e-05\n",
      "[Epoch 44] Training Batch [14/391]: Loss 1.889488885353785e-05\n",
      "[Epoch 44] Training Batch [15/391]: Loss 1.1724269825208467e-05\n",
      "[Epoch 44] Training Batch [16/391]: Loss 1.3386058526521083e-05\n",
      "[Epoch 44] Training Batch [17/391]: Loss 1.7553082216181792e-05\n",
      "[Epoch 44] Training Batch [18/391]: Loss 2.9321883630473167e-05\n",
      "[Epoch 44] Training Batch [19/391]: Loss 1.0080627362185623e-05\n",
      "[Epoch 44] Training Batch [20/391]: Loss 2.9239654395496473e-05\n",
      "[Epoch 44] Training Batch [21/391]: Loss 2.4551491151214577e-05\n",
      "[Epoch 44] Training Batch [22/391]: Loss 2.048992973868735e-05\n",
      "[Epoch 44] Training Batch [23/391]: Loss 1.451447587896837e-05\n",
      "[Epoch 44] Training Batch [24/391]: Loss 1.805631836759858e-05\n",
      "[Epoch 44] Training Batch [25/391]: Loss 2.10775851883227e-05\n",
      "[Epoch 44] Training Batch [26/391]: Loss 1.5449371858267114e-05\n",
      "[Epoch 44] Training Batch [27/391]: Loss 2.564738861110527e-05\n",
      "[Epoch 44] Training Batch [28/391]: Loss 2.0528494133031927e-05\n",
      "[Epoch 44] Training Batch [29/391]: Loss 1.3622789992950857e-05\n",
      "[Epoch 44] Training Batch [30/391]: Loss 2.8051103072357364e-05\n",
      "[Epoch 44] Training Batch [31/391]: Loss 1.7692300389171578e-05\n",
      "[Epoch 44] Training Batch [32/391]: Loss 3.0234568839659914e-05\n",
      "[Epoch 44] Training Batch [33/391]: Loss 2.18945733649889e-05\n",
      "[Epoch 44] Training Batch [34/391]: Loss 2.2160968001117e-05\n",
      "[Epoch 44] Training Batch [35/391]: Loss 1.23845056805294e-05\n",
      "[Epoch 44] Training Batch [36/391]: Loss 3.583481156965718e-05\n",
      "[Epoch 44] Training Batch [37/391]: Loss 2.394144030404277e-05\n",
      "[Epoch 44] Training Batch [38/391]: Loss 2.5178696887451224e-05\n",
      "[Epoch 44] Training Batch [39/391]: Loss 2.3003169189905748e-05\n",
      "[Epoch 44] Training Batch [40/391]: Loss 1.679403430898674e-05\n",
      "[Epoch 44] Training Batch [41/391]: Loss 3.4376625990262255e-05\n",
      "[Epoch 44] Training Batch [42/391]: Loss 2.590664917079266e-05\n",
      "[Epoch 44] Training Batch [43/391]: Loss 2.17905380850425e-05\n",
      "[Epoch 44] Training Batch [44/391]: Loss 1.0890997145907022e-05\n",
      "[Epoch 44] Training Batch [45/391]: Loss 1.7700040189083666e-05\n",
      "[Epoch 44] Training Batch [46/391]: Loss 1.5753286788822152e-05\n",
      "[Epoch 44] Training Batch [47/391]: Loss 1.1774531230912544e-05\n",
      "[Epoch 44] Training Batch [48/391]: Loss 1.373984559904784e-05\n",
      "[Epoch 44] Training Batch [49/391]: Loss 2.1781635950901546e-05\n",
      "[Epoch 44] Training Batch [50/391]: Loss 1.9451706975814886e-05\n",
      "[Epoch 44] Training Batch [51/391]: Loss 2.7060263164457865e-05\n",
      "[Epoch 44] Training Batch [52/391]: Loss 1.798868652258534e-05\n",
      "[Epoch 44] Training Batch [53/391]: Loss 5.958178007858805e-06\n",
      "[Epoch 44] Training Batch [54/391]: Loss 1.2249746760062408e-05\n",
      "[Epoch 44] Training Batch [55/391]: Loss 1.2664930181927048e-05\n",
      "[Epoch 44] Training Batch [56/391]: Loss 2.2404985429602675e-05\n",
      "[Epoch 44] Training Batch [57/391]: Loss 1.1337304385961033e-05\n",
      "[Epoch 44] Training Batch [58/391]: Loss 2.8506357921287417e-05\n",
      "[Epoch 44] Training Batch [59/391]: Loss 1.2267611054994632e-05\n",
      "[Epoch 44] Training Batch [60/391]: Loss 2.025383037107531e-05\n",
      "[Epoch 44] Training Batch [61/391]: Loss 1.585656536917668e-05\n",
      "[Epoch 44] Training Batch [62/391]: Loss 1.0804727025970351e-05\n",
      "[Epoch 44] Training Batch [63/391]: Loss 1.1757621905417182e-05\n",
      "[Epoch 44] Training Batch [64/391]: Loss 1.091720969270682e-05\n",
      "[Epoch 44] Training Batch [65/391]: Loss 2.2051321138860658e-05\n",
      "[Epoch 44] Training Batch [66/391]: Loss 1.8701555745792575e-05\n",
      "[Epoch 44] Training Batch [67/391]: Loss 2.7082798624178395e-05\n",
      "[Epoch 44] Training Batch [68/391]: Loss 2.284835500176996e-05\n",
      "[Epoch 44] Training Batch [69/391]: Loss 1.9294013327453285e-05\n",
      "[Epoch 44] Training Batch [70/391]: Loss 1.6792882888694294e-05\n",
      "[Epoch 44] Training Batch [71/391]: Loss 3.723936970345676e-05\n",
      "[Epoch 44] Training Batch [72/391]: Loss 1.0968289643642493e-05\n",
      "[Epoch 44] Training Batch [73/391]: Loss 2.0565623344737105e-05\n",
      "[Epoch 44] Training Batch [74/391]: Loss 1.5026544133434072e-05\n",
      "[Epoch 44] Training Batch [75/391]: Loss 1.5063435057527386e-05\n",
      "[Epoch 44] Training Batch [76/391]: Loss 1.136954415414948e-05\n",
      "[Epoch 44] Training Batch [77/391]: Loss 1.623739444767125e-05\n",
      "[Epoch 44] Training Batch [78/391]: Loss 3.0162982511683367e-05\n",
      "[Epoch 44] Training Batch [79/391]: Loss 9.852937182586174e-06\n",
      "[Epoch 44] Training Batch [80/391]: Loss 1.8575898138806224e-05\n",
      "[Epoch 44] Training Batch [81/391]: Loss 1.4664366062788758e-05\n",
      "[Epoch 44] Training Batch [82/391]: Loss 2.1133948393980972e-05\n",
      "[Epoch 44] Training Batch [83/391]: Loss 1.6665611838106997e-05\n",
      "[Epoch 44] Training Batch [84/391]: Loss 2.105156636389438e-05\n",
      "[Epoch 44] Training Batch [85/391]: Loss 1.5938876458676532e-05\n",
      "[Epoch 44] Training Batch [86/391]: Loss 2.8056776500307024e-05\n",
      "[Epoch 44] Training Batch [87/391]: Loss 2.4305201804963872e-05\n",
      "[Epoch 44] Training Batch [88/391]: Loss 1.0860195288842078e-05\n",
      "[Epoch 44] Training Batch [89/391]: Loss 9.239210157829802e-06\n",
      "[Epoch 44] Training Batch [90/391]: Loss 1.5634705050615594e-05\n",
      "[Epoch 44] Training Batch [91/391]: Loss 1.4940538676455617e-05\n",
      "[Epoch 44] Training Batch [92/391]: Loss 2.6610750865074806e-05\n",
      "[Epoch 44] Training Batch [93/391]: Loss 2.4476048565702513e-05\n",
      "[Epoch 44] Training Batch [94/391]: Loss 1.4351041500049178e-05\n",
      "[Epoch 44] Training Batch [95/391]: Loss 2.2146392439026386e-05\n",
      "[Epoch 44] Training Batch [96/391]: Loss 2.69436786766164e-05\n",
      "[Epoch 44] Training Batch [97/391]: Loss 1.588943268870935e-05\n",
      "[Epoch 44] Training Batch [98/391]: Loss 1.977309148060158e-05\n",
      "[Epoch 44] Training Batch [99/391]: Loss 1.6013138520065695e-05\n",
      "[Epoch 44] Training Batch [100/391]: Loss 8.105034794425592e-06\n",
      "[Epoch 44] Training Batch [101/391]: Loss 1.1141325558128301e-05\n",
      "[Epoch 44] Training Batch [102/391]: Loss 1.9133010937366635e-05\n",
      "[Epoch 44] Training Batch [103/391]: Loss 1.895643254101742e-05\n",
      "[Epoch 44] Training Batch [104/391]: Loss 1.1381005606381223e-05\n",
      "[Epoch 44] Training Batch [105/391]: Loss 2.889850475185085e-05\n",
      "[Epoch 44] Training Batch [106/391]: Loss 2.7272211809759028e-05\n",
      "[Epoch 44] Training Batch [107/391]: Loss 1.4783536244067363e-05\n",
      "[Epoch 44] Training Batch [108/391]: Loss 1.6390138625865802e-05\n",
      "[Epoch 44] Training Batch [109/391]: Loss 2.6098110538441688e-05\n",
      "[Epoch 44] Training Batch [110/391]: Loss 1.1106304555141833e-05\n",
      "[Epoch 44] Training Batch [111/391]: Loss 1.7026724890456535e-05\n",
      "[Epoch 44] Training Batch [112/391]: Loss 1.7554924852447584e-05\n",
      "[Epoch 44] Training Batch [113/391]: Loss 3.505869608488865e-05\n",
      "[Epoch 44] Training Batch [114/391]: Loss 3.079972884734161e-05\n",
      "[Epoch 44] Training Batch [115/391]: Loss 2.0447547285584733e-05\n",
      "[Epoch 44] Training Batch [116/391]: Loss 1.5009113667474594e-05\n",
      "[Epoch 44] Training Batch [117/391]: Loss 1.747637907101307e-05\n",
      "[Epoch 44] Training Batch [118/391]: Loss 2.0578274416038767e-05\n",
      "[Epoch 44] Training Batch [119/391]: Loss 2.3990149202290922e-05\n",
      "[Epoch 44] Training Batch [120/391]: Loss 9.240136932930909e-06\n",
      "[Epoch 44] Training Batch [121/391]: Loss 2.283361209265422e-05\n",
      "[Epoch 44] Training Batch [122/391]: Loss 1.4981005733716302e-05\n",
      "[Epoch 44] Training Batch [123/391]: Loss 2.244541610707529e-05\n",
      "[Epoch 44] Training Batch [124/391]: Loss 2.1986586943967268e-05\n",
      "[Epoch 44] Training Batch [125/391]: Loss 1.9376784621272236e-05\n",
      "[Epoch 44] Training Batch [126/391]: Loss 1.983936454053037e-05\n",
      "[Epoch 44] Training Batch [127/391]: Loss 2.8723057766910642e-05\n",
      "[Epoch 44] Training Batch [128/391]: Loss 2.4527789719286375e-05\n",
      "[Epoch 44] Training Batch [129/391]: Loss 3.098179149674252e-05\n",
      "[Epoch 44] Training Batch [130/391]: Loss 1.632400199014228e-05\n",
      "[Epoch 44] Training Batch [131/391]: Loss 2.5426470529055223e-05\n",
      "[Epoch 44] Training Batch [132/391]: Loss 1.361035174340941e-05\n",
      "[Epoch 44] Training Batch [133/391]: Loss 1.7704374840832315e-05\n",
      "[Epoch 44] Training Batch [134/391]: Loss 1.2300264643272385e-05\n",
      "[Epoch 44] Training Batch [135/391]: Loss 2.746440986811649e-05\n",
      "[Epoch 44] Training Batch [136/391]: Loss 1.997083381866105e-05\n",
      "[Epoch 44] Training Batch [137/391]: Loss 1.6641843103570864e-05\n",
      "[Epoch 44] Training Batch [138/391]: Loss 2.0532685084617697e-05\n",
      "[Epoch 44] Training Batch [139/391]: Loss 2.278805368405301e-05\n",
      "[Epoch 44] Training Batch [140/391]: Loss 2.226966171292588e-05\n",
      "[Epoch 44] Training Batch [141/391]: Loss 1.4058354281587526e-05\n",
      "[Epoch 44] Training Batch [142/391]: Loss 2.1100369849591516e-05\n",
      "[Epoch 44] Training Batch [143/391]: Loss 1.84652853931766e-05\n",
      "[Epoch 44] Training Batch [144/391]: Loss 1.8496342818252742e-05\n",
      "[Epoch 44] Training Batch [145/391]: Loss 2.416556526441127e-05\n",
      "[Epoch 44] Training Batch [146/391]: Loss 1.964140756172128e-05\n",
      "[Epoch 44] Training Batch [147/391]: Loss 1.4865974662825465e-05\n",
      "[Epoch 44] Training Batch [148/391]: Loss 2.237736407550983e-05\n",
      "[Epoch 44] Training Batch [149/391]: Loss 1.0738532182585914e-05\n",
      "[Epoch 44] Training Batch [150/391]: Loss 1.5535417333012447e-05\n",
      "[Epoch 44] Training Batch [151/391]: Loss 1.5685931430198252e-05\n",
      "[Epoch 44] Training Batch [152/391]: Loss 1.878357943496667e-05\n",
      "[Epoch 44] Training Batch [153/391]: Loss 1.3806197785015684e-05\n",
      "[Epoch 44] Training Batch [154/391]: Loss 1.8002612705458887e-05\n",
      "[Epoch 44] Training Batch [155/391]: Loss 1.2585534932441078e-05\n",
      "[Epoch 44] Training Batch [156/391]: Loss 1.579938907525502e-05\n",
      "[Epoch 44] Training Batch [157/391]: Loss 2.2496815290651284e-05\n",
      "[Epoch 44] Training Batch [158/391]: Loss 3.255539195379242e-05\n",
      "[Epoch 44] Training Batch [159/391]: Loss 1.0901675523200538e-05\n",
      "[Epoch 44] Training Batch [160/391]: Loss 1.668468030402437e-05\n",
      "[Epoch 44] Training Batch [161/391]: Loss 1.5574936696793884e-05\n",
      "[Epoch 44] Training Batch [162/391]: Loss 2.395537194388453e-05\n",
      "[Epoch 44] Training Batch [163/391]: Loss 1.7749487597029656e-05\n",
      "[Epoch 44] Training Batch [164/391]: Loss 2.835939812939614e-05\n",
      "[Epoch 44] Training Batch [165/391]: Loss 1.607373997103423e-05\n",
      "[Epoch 44] Training Batch [166/391]: Loss 2.0242543541826308e-05\n",
      "[Epoch 44] Training Batch [167/391]: Loss 1.848610190791078e-05\n",
      "[Epoch 44] Training Batch [168/391]: Loss 2.1214063963270746e-05\n",
      "[Epoch 44] Training Batch [169/391]: Loss 1.7060581740224734e-05\n",
      "[Epoch 44] Training Batch [170/391]: Loss 1.2793952919309959e-05\n",
      "[Epoch 44] Training Batch [171/391]: Loss 1.717189297778532e-05\n",
      "[Epoch 44] Training Batch [172/391]: Loss 3.267693682573736e-05\n",
      "[Epoch 44] Training Batch [173/391]: Loss 2.0311455955379643e-05\n",
      "[Epoch 44] Training Batch [174/391]: Loss 1.4182885934133083e-05\n",
      "[Epoch 44] Training Batch [175/391]: Loss 2.4639823095640168e-05\n",
      "[Epoch 44] Training Batch [176/391]: Loss 2.636103999975603e-05\n",
      "[Epoch 44] Training Batch [177/391]: Loss 2.97580954793375e-05\n",
      "[Epoch 44] Training Batch [178/391]: Loss 1.9520368368830532e-05\n",
      "[Epoch 44] Training Batch [179/391]: Loss 2.121726174664218e-05\n",
      "[Epoch 44] Training Batch [180/391]: Loss 2.17146152863279e-05\n",
      "[Epoch 44] Training Batch [181/391]: Loss 2.2085734599386342e-05\n",
      "[Epoch 44] Training Batch [182/391]: Loss 2.411411151115317e-05\n",
      "[Epoch 44] Training Batch [183/391]: Loss 2.5872572223306634e-05\n",
      "[Epoch 44] Training Batch [184/391]: Loss 1.8297574570169672e-05\n",
      "[Epoch 44] Training Batch [185/391]: Loss 1.4302881936600897e-05\n",
      "[Epoch 44] Training Batch [186/391]: Loss 2.1169844330870546e-05\n",
      "[Epoch 44] Training Batch [187/391]: Loss 2.3574792066938244e-05\n",
      "[Epoch 44] Training Batch [188/391]: Loss 1.6415307982242666e-05\n",
      "[Epoch 44] Training Batch [189/391]: Loss 1.5224200978991576e-05\n",
      "[Epoch 44] Training Batch [190/391]: Loss 1.9151555534335785e-05\n",
      "[Epoch 44] Training Batch [191/391]: Loss 1.558791882416699e-05\n",
      "[Epoch 44] Training Batch [192/391]: Loss 1.8970156816067174e-05\n",
      "[Epoch 44] Training Batch [193/391]: Loss 3.440380169195123e-05\n",
      "[Epoch 44] Training Batch [194/391]: Loss 1.4204174476617482e-05\n",
      "[Epoch 44] Training Batch [195/391]: Loss 1.5782467016833834e-05\n",
      "[Epoch 44] Training Batch [196/391]: Loss 1.1342811376380268e-05\n",
      "[Epoch 44] Training Batch [197/391]: Loss 1.7396612747688778e-05\n",
      "[Epoch 44] Training Batch [198/391]: Loss 2.1847230527782813e-05\n",
      "[Epoch 44] Training Batch [199/391]: Loss 1.6816346033010632e-05\n",
      "[Epoch 44] Training Batch [200/391]: Loss 2.6921417884295806e-05\n",
      "[Epoch 44] Training Batch [201/391]: Loss 2.2861844627186656e-05\n",
      "[Epoch 44] Training Batch [202/391]: Loss 1.583459925313946e-05\n",
      "[Epoch 44] Training Batch [203/391]: Loss 1.860457268776372e-05\n",
      "[Epoch 44] Training Batch [204/391]: Loss 1.7851787561085075e-05\n",
      "[Epoch 44] Training Batch [205/391]: Loss 2.6369900297140703e-05\n",
      "[Epoch 44] Training Batch [206/391]: Loss 1.4344698683999013e-05\n",
      "[Epoch 44] Training Batch [207/391]: Loss 8.661825631861575e-06\n",
      "[Epoch 44] Training Batch [208/391]: Loss 1.355126005364582e-05\n",
      "[Epoch 44] Training Batch [209/391]: Loss 1.830879227782134e-05\n",
      "[Epoch 44] Training Batch [210/391]: Loss 2.5385657863807864e-05\n",
      "[Epoch 44] Training Batch [211/391]: Loss 2.2135880499263294e-05\n",
      "[Epoch 44] Training Batch [212/391]: Loss 2.072252391371876e-05\n",
      "[Epoch 44] Training Batch [213/391]: Loss 2.546988798712846e-05\n",
      "[Epoch 44] Training Batch [214/391]: Loss 1.251792855327949e-05\n",
      "[Epoch 44] Training Batch [215/391]: Loss 8.246492143371142e-06\n",
      "[Epoch 44] Training Batch [216/391]: Loss 1.9077075194218196e-05\n",
      "[Epoch 44] Training Batch [217/391]: Loss 1.0338976608181838e-05\n",
      "[Epoch 44] Training Batch [218/391]: Loss 3.1867730285739526e-05\n",
      "[Epoch 44] Training Batch [219/391]: Loss 3.381714486749843e-05\n",
      "[Epoch 44] Training Batch [220/391]: Loss 2.356428922212217e-05\n",
      "[Epoch 44] Training Batch [221/391]: Loss 1.7125630620284937e-05\n",
      "[Epoch 44] Training Batch [222/391]: Loss 1.783595871529542e-05\n",
      "[Epoch 44] Training Batch [223/391]: Loss 1.440730738977436e-05\n",
      "[Epoch 44] Training Batch [224/391]: Loss 1.0669074072211515e-05\n",
      "[Epoch 44] Training Batch [225/391]: Loss 2.169901017623488e-05\n",
      "[Epoch 44] Training Batch [226/391]: Loss 1.0073797056975309e-05\n",
      "[Epoch 44] Training Batch [227/391]: Loss 1.4644752809545025e-05\n",
      "[Epoch 44] Training Batch [228/391]: Loss 1.7266074792132713e-05\n",
      "[Epoch 44] Training Batch [229/391]: Loss 2.0458921426325105e-05\n",
      "[Epoch 44] Training Batch [230/391]: Loss 2.3147262254497036e-05\n",
      "[Epoch 44] Training Batch [231/391]: Loss 9.576303455105517e-06\n",
      "[Epoch 44] Training Batch [232/391]: Loss 2.7765912818722427e-05\n",
      "[Epoch 44] Training Batch [233/391]: Loss 1.9012006305274554e-05\n",
      "[Epoch 44] Training Batch [234/391]: Loss 1.6843530829646625e-05\n",
      "[Epoch 44] Training Batch [235/391]: Loss 2.0534818759188056e-05\n",
      "[Epoch 44] Training Batch [236/391]: Loss 3.2167772587854415e-05\n",
      "[Epoch 44] Training Batch [237/391]: Loss 1.501876613474451e-05\n",
      "[Epoch 44] Training Batch [238/391]: Loss 2.4925448087742552e-05\n",
      "[Epoch 44] Training Batch [239/391]: Loss 1.2288811376492959e-05\n",
      "[Epoch 44] Training Batch [240/391]: Loss 1.8279846699442714e-05\n",
      "[Epoch 44] Training Batch [241/391]: Loss 2.2573352907784283e-05\n",
      "[Epoch 44] Training Batch [242/391]: Loss 2.4004346414585598e-05\n",
      "[Epoch 44] Training Batch [243/391]: Loss 2.6736735890153795e-05\n",
      "[Epoch 44] Training Batch [244/391]: Loss 1.4979166735429317e-05\n",
      "[Epoch 44] Training Batch [245/391]: Loss 1.660989255469758e-05\n",
      "[Epoch 44] Training Batch [246/391]: Loss 1.0118860700458754e-05\n",
      "[Epoch 44] Training Batch [247/391]: Loss 1.9233757484471425e-05\n",
      "[Epoch 44] Training Batch [248/391]: Loss 1.0620041393849533e-05\n",
      "[Epoch 44] Training Batch [249/391]: Loss 1.63187287398614e-05\n",
      "[Epoch 44] Training Batch [250/391]: Loss 1.786738539522048e-05\n",
      "[Epoch 44] Training Batch [251/391]: Loss 2.2035123038222082e-05\n",
      "[Epoch 44] Training Batch [252/391]: Loss 1.8221186110167764e-05\n",
      "[Epoch 44] Training Batch [253/391]: Loss 2.9961254767840728e-05\n",
      "[Epoch 44] Training Batch [254/391]: Loss 1.2779315511579625e-05\n",
      "[Epoch 44] Training Batch [255/391]: Loss 2.154651883756742e-05\n",
      "[Epoch 44] Training Batch [256/391]: Loss 1.8447943148203194e-05\n",
      "[Epoch 44] Training Batch [257/391]: Loss 1.6465790395159274e-05\n",
      "[Epoch 44] Training Batch [258/391]: Loss 1.9134724425384775e-05\n",
      "[Epoch 44] Training Batch [259/391]: Loss 2.4645956727908924e-05\n",
      "[Epoch 44] Training Batch [260/391]: Loss 1.566100399941206e-05\n",
      "[Epoch 44] Training Batch [261/391]: Loss 1.747375245031435e-05\n",
      "[Epoch 44] Training Batch [262/391]: Loss 2.9146429369575344e-05\n",
      "[Epoch 44] Training Batch [263/391]: Loss 1.5008965419838205e-05\n",
      "[Epoch 44] Training Batch [264/391]: Loss 3.53772884409409e-05\n",
      "[Epoch 44] Training Batch [265/391]: Loss 2.7752912501455285e-05\n",
      "[Epoch 44] Training Batch [266/391]: Loss 1.8366512449574657e-05\n",
      "[Epoch 44] Training Batch [267/391]: Loss 2.0415773178683594e-05\n",
      "[Epoch 44] Training Batch [268/391]: Loss 1.8015107343671843e-05\n",
      "[Epoch 44] Training Batch [269/391]: Loss 2.2894430003361776e-05\n",
      "[Epoch 44] Training Batch [270/391]: Loss 1.8899354472523555e-05\n",
      "[Epoch 44] Training Batch [271/391]: Loss 2.2816071577835828e-05\n",
      "[Epoch 44] Training Batch [272/391]: Loss 1.8779463061946444e-05\n",
      "[Epoch 44] Training Batch [273/391]: Loss 9.325547580374405e-06\n",
      "[Epoch 44] Training Batch [274/391]: Loss 2.171644155168906e-05\n",
      "[Epoch 44] Training Batch [275/391]: Loss 1.1651882232399657e-05\n",
      "[Epoch 44] Training Batch [276/391]: Loss 2.3129316105041653e-05\n",
      "[Epoch 44] Training Batch [277/391]: Loss 7.101133633113932e-06\n",
      "[Epoch 44] Training Batch [278/391]: Loss 2.3076971046975814e-05\n",
      "[Epoch 44] Training Batch [279/391]: Loss 1.629222788324114e-05\n",
      "[Epoch 44] Training Batch [280/391]: Loss 8.992928087536711e-06\n",
      "[Epoch 44] Training Batch [281/391]: Loss 1.2173972208984196e-05\n",
      "[Epoch 44] Training Batch [282/391]: Loss 1.865673766587861e-05\n",
      "[Epoch 44] Training Batch [283/391]: Loss 1.2757398508256301e-05\n",
      "[Epoch 44] Training Batch [284/391]: Loss 1.4419732906389982e-05\n",
      "[Epoch 44] Training Batch [285/391]: Loss 1.395120307279285e-05\n",
      "[Epoch 44] Training Batch [286/391]: Loss 2.0454946934478357e-05\n",
      "[Epoch 44] Training Batch [287/391]: Loss 1.5863841326790862e-05\n",
      "[Epoch 44] Training Batch [288/391]: Loss 1.778304431354627e-05\n",
      "[Epoch 44] Training Batch [289/391]: Loss 1.9417062503634952e-05\n",
      "[Epoch 44] Training Batch [290/391]: Loss 9.634071830078028e-06\n",
      "[Epoch 44] Training Batch [291/391]: Loss 1.1676880603772588e-05\n",
      "[Epoch 44] Training Batch [292/391]: Loss 1.5932106180116534e-05\n",
      "[Epoch 44] Training Batch [293/391]: Loss 1.6954654711298645e-05\n",
      "[Epoch 44] Training Batch [294/391]: Loss 2.1679510609828867e-05\n",
      "[Epoch 44] Training Batch [295/391]: Loss 1.2419990525813773e-05\n",
      "[Epoch 44] Training Batch [296/391]: Loss 2.7363721528672613e-05\n",
      "[Epoch 44] Training Batch [297/391]: Loss 1.785127278708387e-05\n",
      "[Epoch 44] Training Batch [298/391]: Loss 2.117648364219349e-05\n",
      "[Epoch 44] Training Batch [299/391]: Loss 1.9736005924642086e-05\n",
      "[Epoch 44] Training Batch [300/391]: Loss 1.963537943083793e-05\n",
      "[Epoch 44] Training Batch [301/391]: Loss 1.3333315109775867e-05\n",
      "[Epoch 44] Training Batch [302/391]: Loss 1.334752596449107e-05\n",
      "[Epoch 44] Training Batch [303/391]: Loss 1.9850509488605894e-05\n",
      "[Epoch 44] Training Batch [304/391]: Loss 1.7868731447379105e-05\n",
      "[Epoch 44] Training Batch [305/391]: Loss 1.7659980585449375e-05\n",
      "[Epoch 44] Training Batch [306/391]: Loss 1.578744013386313e-05\n",
      "[Epoch 44] Training Batch [307/391]: Loss 1.788388726708945e-05\n",
      "[Epoch 44] Training Batch [308/391]: Loss 1.3383136320044287e-05\n",
      "[Epoch 44] Training Batch [309/391]: Loss 2.347763984289486e-05\n",
      "[Epoch 44] Training Batch [310/391]: Loss 1.6023166608647443e-05\n",
      "[Epoch 44] Training Batch [311/391]: Loss 1.7206957636517473e-05\n",
      "[Epoch 44] Training Batch [312/391]: Loss 1.958907159860246e-05\n",
      "[Epoch 44] Training Batch [313/391]: Loss 2.8506603484856896e-05\n",
      "[Epoch 44] Training Batch [314/391]: Loss 1.5778749002492987e-05\n",
      "[Epoch 44] Training Batch [315/391]: Loss 2.2352438463713042e-05\n",
      "[Epoch 44] Training Batch [316/391]: Loss 1.3251738891995046e-05\n",
      "[Epoch 44] Training Batch [317/391]: Loss 1.4820078831689898e-05\n",
      "[Epoch 44] Training Batch [318/391]: Loss 1.32469440359273e-05\n",
      "[Epoch 44] Training Batch [319/391]: Loss 2.038826460193377e-05\n",
      "[Epoch 44] Training Batch [320/391]: Loss 1.2269087164895609e-05\n",
      "[Epoch 44] Training Batch [321/391]: Loss 2.085714731947519e-05\n",
      "[Epoch 44] Training Batch [322/391]: Loss 9.69998654909432e-06\n",
      "[Epoch 44] Training Batch [323/391]: Loss 1.0163726983591914e-05\n",
      "[Epoch 44] Training Batch [324/391]: Loss 2.1429437765618786e-05\n",
      "[Epoch 44] Training Batch [325/391]: Loss 2.2550331777893007e-05\n",
      "[Epoch 44] Training Batch [326/391]: Loss 2.452979242661968e-05\n",
      "[Epoch 44] Training Batch [327/391]: Loss 1.7071766706067137e-05\n",
      "[Epoch 44] Training Batch [328/391]: Loss 8.21907178760739e-06\n",
      "[Epoch 44] Training Batch [329/391]: Loss 4.0667830035090446e-05\n",
      "[Epoch 44] Training Batch [330/391]: Loss 2.6376930691185407e-05\n",
      "[Epoch 44] Training Batch [331/391]: Loss 1.9387336578802206e-05\n",
      "[Epoch 44] Training Batch [332/391]: Loss 1.7493972336524166e-05\n",
      "[Epoch 44] Training Batch [333/391]: Loss 1.0941456821456086e-05\n",
      "[Epoch 44] Training Batch [334/391]: Loss 2.7644378860713914e-05\n",
      "[Epoch 44] Training Batch [335/391]: Loss 1.4035518688615412e-05\n",
      "[Epoch 44] Training Batch [336/391]: Loss 1.7539668988320045e-05\n",
      "[Epoch 44] Training Batch [337/391]: Loss 1.1869595255120657e-05\n",
      "[Epoch 44] Training Batch [338/391]: Loss 2.1124506019987166e-05\n",
      "[Epoch 44] Training Batch [339/391]: Loss 1.598556264070794e-05\n",
      "[Epoch 44] Training Batch [340/391]: Loss 2.493554347893223e-05\n",
      "[Epoch 44] Training Batch [341/391]: Loss 1.4877655303280335e-05\n",
      "[Epoch 44] Training Batch [342/391]: Loss 1.5736108252895065e-05\n",
      "[Epoch 44] Training Batch [343/391]: Loss 1.3254696568765212e-05\n",
      "[Epoch 44] Training Batch [344/391]: Loss 3.5687444324139506e-05\n",
      "[Epoch 44] Training Batch [345/391]: Loss 1.3984448742121458e-05\n",
      "[Epoch 44] Training Batch [346/391]: Loss 2.1870331693207845e-05\n",
      "[Epoch 44] Training Batch [347/391]: Loss 2.829716686392203e-05\n",
      "[Epoch 44] Training Batch [348/391]: Loss 3.414847742533311e-05\n",
      "[Epoch 44] Training Batch [349/391]: Loss 2.478287206031382e-05\n",
      "[Epoch 44] Training Batch [350/391]: Loss 4.079793143318966e-05\n",
      "[Epoch 44] Training Batch [351/391]: Loss 1.4603822819481138e-05\n",
      "[Epoch 44] Training Batch [352/391]: Loss 2.8157066481071524e-05\n",
      "[Epoch 44] Training Batch [353/391]: Loss 2.6259227524860762e-05\n",
      "[Epoch 44] Training Batch [354/391]: Loss 1.2497323041316122e-05\n",
      "[Epoch 44] Training Batch [355/391]: Loss 1.5419353076140396e-05\n",
      "[Epoch 44] Training Batch [356/391]: Loss 1.756547499098815e-05\n",
      "[Epoch 44] Training Batch [357/391]: Loss 1.6313508240273222e-05\n",
      "[Epoch 44] Training Batch [358/391]: Loss 1.7974161892198026e-05\n",
      "[Epoch 44] Training Batch [359/391]: Loss 1.4480038771580439e-05\n",
      "[Epoch 44] Training Batch [360/391]: Loss 1.4793631635257043e-05\n",
      "[Epoch 44] Training Batch [361/391]: Loss 9.699154361442197e-06\n",
      "[Epoch 44] Training Batch [362/391]: Loss 1.911436083901208e-05\n",
      "[Epoch 44] Training Batch [363/391]: Loss 1.957834501808975e-05\n",
      "[Epoch 44] Training Batch [364/391]: Loss 3.1721501727588475e-05\n",
      "[Epoch 44] Training Batch [365/391]: Loss 8.54629251989536e-06\n",
      "[Epoch 44] Training Batch [366/391]: Loss 1.2019472706015222e-05\n",
      "[Epoch 44] Training Batch [367/391]: Loss 1.5130066458368674e-05\n",
      "[Epoch 44] Training Batch [368/391]: Loss 1.4065095456317067e-05\n",
      "[Epoch 44] Training Batch [369/391]: Loss 1.4284009012044407e-05\n",
      "[Epoch 44] Training Batch [370/391]: Loss 2.44166894844966e-05\n",
      "[Epoch 44] Training Batch [371/391]: Loss 1.7122014469350688e-05\n",
      "[Epoch 44] Training Batch [372/391]: Loss 3.2955089409369975e-05\n",
      "[Epoch 44] Training Batch [373/391]: Loss 1.1348361113050487e-05\n",
      "[Epoch 44] Training Batch [374/391]: Loss 1.749207149259746e-05\n",
      "[Epoch 44] Training Batch [375/391]: Loss 2.9155962693039328e-05\n",
      "[Epoch 44] Training Batch [376/391]: Loss 2.0739056708407588e-05\n",
      "[Epoch 44] Training Batch [377/391]: Loss 1.5586396330036223e-05\n",
      "[Epoch 44] Training Batch [378/391]: Loss 1.915564098453615e-05\n",
      "[Epoch 44] Training Batch [379/391]: Loss 1.3579540791397449e-05\n",
      "[Epoch 44] Training Batch [380/391]: Loss 2.5184212063322775e-05\n",
      "[Epoch 44] Training Batch [381/391]: Loss 1.3152900464774575e-05\n",
      "[Epoch 44] Training Batch [382/391]: Loss 2.6812531359610148e-05\n",
      "[Epoch 44] Training Batch [383/391]: Loss 9.240248800779227e-06\n",
      "[Epoch 44] Training Batch [384/391]: Loss 9.245351975550875e-06\n",
      "[Epoch 44] Training Batch [385/391]: Loss 1.1139642992930021e-05\n",
      "[Epoch 44] Training Batch [386/391]: Loss 1.7132411812781356e-05\n",
      "[Epoch 44] Training Batch [387/391]: Loss 1.442493885406293e-05\n",
      "[Epoch 44] Training Batch [388/391]: Loss 1.7427573766326532e-05\n",
      "[Epoch 44] Training Batch [389/391]: Loss 1.7308888345723972e-05\n",
      "[Epoch 44] Training Batch [390/391]: Loss 1.9413997506489977e-05\n",
      "[Epoch 44] Training Batch [391/391]: Loss 8.846557648212183e-06\n",
      "Epoch 44 - Train Loss: 0.0000\n",
      "*********  Epoch 45/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45] Training Batch [1/391]: Loss 2.8372896849759854e-05\n",
      "[Epoch 45] Training Batch [2/391]: Loss 1.433844408893492e-05\n",
      "[Epoch 45] Training Batch [3/391]: Loss 2.2576457922696136e-05\n",
      "[Epoch 45] Training Batch [4/391]: Loss 2.3720254830550402e-05\n",
      "[Epoch 45] Training Batch [5/391]: Loss 2.5281797206844203e-05\n",
      "[Epoch 45] Training Batch [6/391]: Loss 1.0523482160351705e-05\n",
      "[Epoch 45] Training Batch [7/391]: Loss 1.953332503035199e-05\n",
      "[Epoch 45] Training Batch [8/391]: Loss 3.2154563086805865e-05\n",
      "[Epoch 45] Training Batch [9/391]: Loss 1.5124600395211019e-05\n",
      "[Epoch 45] Training Batch [10/391]: Loss 2.4631977794342674e-05\n",
      "[Epoch 45] Training Batch [11/391]: Loss 1.1374883797543589e-05\n",
      "[Epoch 45] Training Batch [12/391]: Loss 2.628970833029598e-05\n",
      "[Epoch 45] Training Batch [13/391]: Loss 1.2081036402378231e-05\n",
      "[Epoch 45] Training Batch [14/391]: Loss 2.2175892809173092e-05\n",
      "[Epoch 45] Training Batch [15/391]: Loss 1.311288451688597e-05\n",
      "[Epoch 45] Training Batch [16/391]: Loss 2.0983694412279874e-05\n",
      "[Epoch 45] Training Batch [17/391]: Loss 1.0752158232207876e-05\n",
      "[Epoch 45] Training Batch [18/391]: Loss 2.5038958483492024e-05\n",
      "[Epoch 45] Training Batch [19/391]: Loss 1.4311539416667074e-05\n",
      "[Epoch 45] Training Batch [20/391]: Loss 1.6110312571981922e-05\n",
      "[Epoch 45] Training Batch [21/391]: Loss 1.7047177607310005e-05\n",
      "[Epoch 45] Training Batch [22/391]: Loss 2.0813416995224543e-05\n",
      "[Epoch 45] Training Batch [23/391]: Loss 2.3169508494902402e-05\n",
      "[Epoch 45] Training Batch [24/391]: Loss 2.280940861965064e-05\n",
      "[Epoch 45] Training Batch [25/391]: Loss 1.566999162605498e-05\n",
      "[Epoch 45] Training Batch [26/391]: Loss 1.5393196008517407e-05\n",
      "[Epoch 45] Training Batch [27/391]: Loss 2.0383282389957458e-05\n",
      "[Epoch 45] Training Batch [28/391]: Loss 1.3812395991408266e-05\n",
      "[Epoch 45] Training Batch [29/391]: Loss 1.594033165019937e-05\n",
      "[Epoch 45] Training Batch [30/391]: Loss 1.092844922823133e-05\n",
      "[Epoch 45] Training Batch [31/391]: Loss 1.5028462257760111e-05\n",
      "[Epoch 45] Training Batch [32/391]: Loss 1.7194073734572157e-05\n",
      "[Epoch 45] Training Batch [33/391]: Loss 8.867543328960892e-06\n",
      "[Epoch 45] Training Batch [34/391]: Loss 1.9671297195600346e-05\n",
      "[Epoch 45] Training Batch [35/391]: Loss 1.274333681067219e-05\n",
      "[Epoch 45] Training Batch [36/391]: Loss 1.1211907803954091e-05\n",
      "[Epoch 45] Training Batch [37/391]: Loss 1.290428190259263e-05\n",
      "[Epoch 45] Training Batch [38/391]: Loss 8.829241778585128e-06\n",
      "[Epoch 45] Training Batch [39/391]: Loss 1.8285878468304873e-05\n",
      "[Epoch 45] Training Batch [40/391]: Loss 1.2817088645533659e-05\n",
      "[Epoch 45] Training Batch [41/391]: Loss 1.2935319318785332e-05\n",
      "[Epoch 45] Training Batch [42/391]: Loss 2.0343966752989218e-05\n",
      "[Epoch 45] Training Batch [43/391]: Loss 1.8069305951939896e-05\n",
      "[Epoch 45] Training Batch [44/391]: Loss 1.6234320355579257e-05\n",
      "[Epoch 45] Training Batch [45/391]: Loss 1.5540121239610016e-05\n",
      "[Epoch 45] Training Batch [46/391]: Loss 2.4437338652205653e-05\n",
      "[Epoch 45] Training Batch [47/391]: Loss 2.4681408831384033e-05\n",
      "[Epoch 45] Training Batch [48/391]: Loss 1.4715649740537629e-05\n",
      "[Epoch 45] Training Batch [49/391]: Loss 2.5806915800785646e-05\n",
      "[Epoch 45] Training Batch [50/391]: Loss 1.1823341992567293e-05\n",
      "[Epoch 45] Training Batch [51/391]: Loss 1.1139680282212794e-05\n",
      "[Epoch 45] Training Batch [52/391]: Loss 2.1294719772413373e-05\n",
      "[Epoch 45] Training Batch [53/391]: Loss 1.3084980309940875e-05\n",
      "[Epoch 45] Training Batch [54/391]: Loss 2.2150525182951242e-05\n",
      "[Epoch 45] Training Batch [55/391]: Loss 2.4637456590426154e-05\n",
      "[Epoch 45] Training Batch [56/391]: Loss 2.1580190150416456e-05\n",
      "[Epoch 45] Training Batch [57/391]: Loss 1.9179036826244555e-05\n",
      "[Epoch 45] Training Batch [58/391]: Loss 1.7566380847711116e-05\n",
      "[Epoch 45] Training Batch [59/391]: Loss 2.444765777909197e-05\n",
      "[Epoch 45] Training Batch [60/391]: Loss 2.0046965801157057e-05\n",
      "[Epoch 45] Training Batch [61/391]: Loss 7.446562449331395e-06\n",
      "[Epoch 45] Training Batch [62/391]: Loss 1.798339690139983e-05\n",
      "[Epoch 45] Training Batch [63/391]: Loss 1.6368343494832516e-05\n",
      "[Epoch 45] Training Batch [64/391]: Loss 2.6063655241159722e-05\n",
      "[Epoch 45] Training Batch [65/391]: Loss 2.6625946702552028e-05\n",
      "[Epoch 45] Training Batch [66/391]: Loss 1.0390237548563164e-05\n",
      "[Epoch 45] Training Batch [67/391]: Loss 1.6414205674664117e-05\n",
      "[Epoch 45] Training Batch [68/391]: Loss 1.8098300643032417e-05\n",
      "[Epoch 45] Training Batch [69/391]: Loss 1.3403015145740937e-05\n",
      "[Epoch 45] Training Batch [70/391]: Loss 1.6510117347934283e-05\n",
      "[Epoch 45] Training Batch [71/391]: Loss 1.7228749129571952e-05\n",
      "[Epoch 45] Training Batch [72/391]: Loss 1.7912074326886795e-05\n",
      "[Epoch 45] Training Batch [73/391]: Loss 1.9064889784203842e-05\n",
      "[Epoch 45] Training Batch [74/391]: Loss 2.1103571270941757e-05\n",
      "[Epoch 45] Training Batch [75/391]: Loss 1.4994724551797844e-05\n",
      "[Epoch 45] Training Batch [76/391]: Loss 2.0657540517277084e-05\n",
      "[Epoch 45] Training Batch [77/391]: Loss 1.9666946172947064e-05\n",
      "[Epoch 45] Training Batch [78/391]: Loss 1.883067852759268e-05\n",
      "[Epoch 45] Training Batch [79/391]: Loss 3.100730464211665e-05\n",
      "[Epoch 45] Training Batch [80/391]: Loss 2.7543797841644846e-05\n",
      "[Epoch 45] Training Batch [81/391]: Loss 1.734738179948181e-05\n",
      "[Epoch 45] Training Batch [82/391]: Loss 1.4615979125665035e-05\n",
      "[Epoch 45] Training Batch [83/391]: Loss 1.236796651937766e-05\n",
      "[Epoch 45] Training Batch [84/391]: Loss 9.285774467571173e-06\n",
      "[Epoch 45] Training Batch [85/391]: Loss 2.06874556170078e-05\n",
      "[Epoch 45] Training Batch [86/391]: Loss 2.2733745936420746e-05\n",
      "[Epoch 45] Training Batch [87/391]: Loss 1.0881814887397923e-05\n",
      "[Epoch 45] Training Batch [88/391]: Loss 1.2404607332427986e-05\n",
      "[Epoch 45] Training Batch [89/391]: Loss 2.1222718714852817e-05\n",
      "[Epoch 45] Training Batch [90/391]: Loss 2.0657702407334e-05\n",
      "[Epoch 45] Training Batch [91/391]: Loss 2.152951310563367e-05\n",
      "[Epoch 45] Training Batch [92/391]: Loss 2.1162722987355664e-05\n",
      "[Epoch 45] Training Batch [93/391]: Loss 1.8757893485599197e-05\n",
      "[Epoch 45] Training Batch [94/391]: Loss 3.0267776310211048e-05\n",
      "[Epoch 45] Training Batch [95/391]: Loss 2.6953239284921438e-05\n",
      "[Epoch 45] Training Batch [96/391]: Loss 1.671836980676744e-05\n",
      "[Epoch 45] Training Batch [97/391]: Loss 2.650285568961408e-05\n",
      "[Epoch 45] Training Batch [98/391]: Loss 9.573686838848516e-06\n",
      "[Epoch 45] Training Batch [99/391]: Loss 1.9420956959947944e-05\n",
      "[Epoch 45] Training Batch [100/391]: Loss 1.276250168302795e-05\n",
      "[Epoch 45] Training Batch [101/391]: Loss 2.077723365800921e-05\n",
      "[Epoch 45] Training Batch [102/391]: Loss 1.251870889973361e-05\n",
      "[Epoch 45] Training Batch [103/391]: Loss 2.4626335289212875e-05\n",
      "[Epoch 45] Training Batch [104/391]: Loss 2.186100755352527e-05\n",
      "[Epoch 45] Training Batch [105/391]: Loss 1.9014330973732285e-05\n",
      "[Epoch 45] Training Batch [106/391]: Loss 1.9537930711521767e-05\n",
      "[Epoch 45] Training Batch [107/391]: Loss 2.5083938453462906e-05\n",
      "[Epoch 45] Training Batch [108/391]: Loss 1.772376890585292e-05\n",
      "[Epoch 45] Training Batch [109/391]: Loss 1.998459265450947e-05\n",
      "[Epoch 45] Training Batch [110/391]: Loss 1.3040677913522813e-05\n",
      "[Epoch 45] Training Batch [111/391]: Loss 7.116856522770831e-06\n",
      "[Epoch 45] Training Batch [112/391]: Loss 2.1553581973421387e-05\n",
      "[Epoch 45] Training Batch [113/391]: Loss 1.2332027836237103e-05\n",
      "[Epoch 45] Training Batch [114/391]: Loss 9.272802344639786e-06\n",
      "[Epoch 45] Training Batch [115/391]: Loss 1.8138292944058776e-05\n",
      "[Epoch 45] Training Batch [116/391]: Loss 2.2822932805866003e-05\n",
      "[Epoch 45] Training Batch [117/391]: Loss 1.3098098861519247e-05\n",
      "[Epoch 45] Training Batch [118/391]: Loss 1.6873429558472708e-05\n",
      "[Epoch 45] Training Batch [119/391]: Loss 2.1848212782060727e-05\n",
      "[Epoch 45] Training Batch [120/391]: Loss 3.240995283704251e-05\n",
      "[Epoch 45] Training Batch [121/391]: Loss 1.6474506992381066e-05\n",
      "[Epoch 45] Training Batch [122/391]: Loss 2.283807043568231e-05\n",
      "[Epoch 45] Training Batch [123/391]: Loss 1.4963436115067452e-05\n",
      "[Epoch 45] Training Batch [124/391]: Loss 2.0093595594516955e-05\n",
      "[Epoch 45] Training Batch [125/391]: Loss 1.713508572720457e-05\n",
      "[Epoch 45] Training Batch [126/391]: Loss 3.2101837859954685e-05\n",
      "[Epoch 45] Training Batch [127/391]: Loss 1.8470062059350312e-05\n",
      "[Epoch 45] Training Batch [128/391]: Loss 1.641280687181279e-05\n",
      "[Epoch 45] Training Batch [129/391]: Loss 1.4663528418168426e-05\n",
      "[Epoch 45] Training Batch [130/391]: Loss 7.87467070040293e-06\n",
      "[Epoch 45] Training Batch [131/391]: Loss 1.4957427083572838e-05\n",
      "[Epoch 45] Training Batch [132/391]: Loss 1.3735144420934375e-05\n",
      "[Epoch 45] Training Batch [133/391]: Loss 1.7685244529275224e-05\n",
      "[Epoch 45] Training Batch [134/391]: Loss 8.725093721295707e-06\n",
      "[Epoch 45] Training Batch [135/391]: Loss 2.7517819034983404e-05\n",
      "[Epoch 45] Training Batch [136/391]: Loss 1.673442602623254e-05\n",
      "[Epoch 45] Training Batch [137/391]: Loss 1.2982619409740437e-05\n",
      "[Epoch 45] Training Batch [138/391]: Loss 1.3088212654110976e-05\n",
      "[Epoch 45] Training Batch [139/391]: Loss 1.9743878510780632e-05\n",
      "[Epoch 45] Training Batch [140/391]: Loss 2.265255716338288e-05\n",
      "[Epoch 45] Training Batch [141/391]: Loss 2.5755682145245373e-05\n",
      "[Epoch 45] Training Batch [142/391]: Loss 1.9503537259879522e-05\n",
      "[Epoch 45] Training Batch [143/391]: Loss 1.4149466551316436e-05\n",
      "[Epoch 45] Training Batch [144/391]: Loss 1.4117565115157049e-05\n",
      "[Epoch 45] Training Batch [145/391]: Loss 2.2868231098982506e-05\n",
      "[Epoch 45] Training Batch [146/391]: Loss 9.552524716127664e-06\n",
      "[Epoch 45] Training Batch [147/391]: Loss 1.667239484959282e-05\n",
      "[Epoch 45] Training Batch [148/391]: Loss 1.4215262126526795e-05\n",
      "[Epoch 45] Training Batch [149/391]: Loss 1.609948230907321e-05\n",
      "[Epoch 45] Training Batch [150/391]: Loss 1.3334415598365013e-05\n",
      "[Epoch 45] Training Batch [151/391]: Loss 2.0846566258114763e-05\n",
      "[Epoch 45] Training Batch [152/391]: Loss 1.5800864275661297e-05\n",
      "[Epoch 45] Training Batch [153/391]: Loss 2.4758384824963287e-05\n",
      "[Epoch 45] Training Batch [154/391]: Loss 2.1708297936129384e-05\n",
      "[Epoch 45] Training Batch [155/391]: Loss 3.348444079165347e-05\n",
      "[Epoch 45] Training Batch [156/391]: Loss 1.3454902727971785e-05\n",
      "[Epoch 45] Training Batch [157/391]: Loss 2.2929472834221087e-05\n",
      "[Epoch 45] Training Batch [158/391]: Loss 1.7161237337859347e-05\n",
      "[Epoch 45] Training Batch [159/391]: Loss 6.936080808372935e-06\n",
      "[Epoch 45] Training Batch [160/391]: Loss 1.5488374629057944e-05\n",
      "[Epoch 45] Training Batch [161/391]: Loss 2.1685957108275034e-05\n",
      "[Epoch 45] Training Batch [162/391]: Loss 1.7808537450036965e-05\n",
      "[Epoch 45] Training Batch [163/391]: Loss 1.0446550732012838e-05\n",
      "[Epoch 45] Training Batch [164/391]: Loss 1.7951164409168996e-05\n",
      "[Epoch 45] Training Batch [165/391]: Loss 2.250747274956666e-05\n",
      "[Epoch 45] Training Batch [166/391]: Loss 2.0168814444332384e-05\n",
      "[Epoch 45] Training Batch [167/391]: Loss 1.5389146938105114e-05\n",
      "[Epoch 45] Training Batch [168/391]: Loss 1.8251967048854567e-05\n",
      "[Epoch 45] Training Batch [169/391]: Loss 2.35219322348712e-05\n",
      "[Epoch 45] Training Batch [170/391]: Loss 1.0502683835511561e-05\n",
      "[Epoch 45] Training Batch [171/391]: Loss 2.4405886506428942e-05\n",
      "[Epoch 45] Training Batch [172/391]: Loss 1.825176877900958e-05\n",
      "[Epoch 45] Training Batch [173/391]: Loss 1.9314093151479028e-05\n",
      "[Epoch 45] Training Batch [174/391]: Loss 1.1612523849180434e-05\n",
      "[Epoch 45] Training Batch [175/391]: Loss 1.4843075405224226e-05\n",
      "[Epoch 45] Training Batch [176/391]: Loss 2.102847611240577e-05\n",
      "[Epoch 45] Training Batch [177/391]: Loss 1.1992551662842743e-05\n",
      "[Epoch 45] Training Batch [178/391]: Loss 2.0027773643960245e-05\n",
      "[Epoch 45] Training Batch [179/391]: Loss 1.8060527509078383e-05\n",
      "[Epoch 45] Training Batch [180/391]: Loss 1.7864334949990734e-05\n",
      "[Epoch 45] Training Batch [181/391]: Loss 1.1102666576334741e-05\n",
      "[Epoch 45] Training Batch [182/391]: Loss 1.4223951438907534e-05\n",
      "[Epoch 45] Training Batch [183/391]: Loss 2.097489414154552e-05\n",
      "[Epoch 45] Training Batch [184/391]: Loss 2.4754017431405373e-05\n",
      "[Epoch 45] Training Batch [185/391]: Loss 1.9027662347070873e-05\n",
      "[Epoch 45] Training Batch [186/391]: Loss 2.657881850609556e-05\n",
      "[Epoch 45] Training Batch [187/391]: Loss 1.4689305317006074e-05\n",
      "[Epoch 45] Training Batch [188/391]: Loss 2.07240827876376e-05\n",
      "[Epoch 45] Training Batch [189/391]: Loss 1.4345612726174295e-05\n",
      "[Epoch 45] Training Batch [190/391]: Loss 2.482971285644453e-05\n",
      "[Epoch 45] Training Batch [191/391]: Loss 8.436580174020492e-06\n",
      "[Epoch 45] Training Batch [192/391]: Loss 1.426157723471988e-05\n",
      "[Epoch 45] Training Batch [193/391]: Loss 1.7939184544957243e-05\n",
      "[Epoch 45] Training Batch [194/391]: Loss 2.4522865714970976e-05\n",
      "[Epoch 45] Training Batch [195/391]: Loss 8.074038305494469e-06\n",
      "[Epoch 45] Training Batch [196/391]: Loss 1.2610386875167023e-05\n",
      "[Epoch 45] Training Batch [197/391]: Loss 1.881364187283907e-05\n",
      "[Epoch 45] Training Batch [198/391]: Loss 1.906292345665861e-05\n",
      "[Epoch 45] Training Batch [199/391]: Loss 9.48308934312081e-06\n",
      "[Epoch 45] Training Batch [200/391]: Loss 1.325703487964347e-05\n",
      "[Epoch 45] Training Batch [201/391]: Loss 1.1920375072804745e-05\n",
      "[Epoch 45] Training Batch [202/391]: Loss 2.4121813112287782e-05\n",
      "[Epoch 45] Training Batch [203/391]: Loss 1.3557982128986623e-05\n",
      "[Epoch 45] Training Batch [204/391]: Loss 1.3452963685267605e-05\n",
      "[Epoch 45] Training Batch [205/391]: Loss 1.7230540834134445e-05\n",
      "[Epoch 45] Training Batch [206/391]: Loss 1.3728379599342588e-05\n",
      "[Epoch 45] Training Batch [207/391]: Loss 2.152066554117482e-05\n",
      "[Epoch 45] Training Batch [208/391]: Loss 1.515393887530081e-05\n",
      "[Epoch 45] Training Batch [209/391]: Loss 1.6987818526104093e-05\n",
      "[Epoch 45] Training Batch [210/391]: Loss 1.6638094166410156e-05\n",
      "[Epoch 45] Training Batch [211/391]: Loss 3.772465788642876e-05\n",
      "[Epoch 45] Training Batch [212/391]: Loss 1.9341116058058105e-05\n",
      "[Epoch 45] Training Batch [213/391]: Loss 3.524126441334374e-05\n",
      "[Epoch 45] Training Batch [214/391]: Loss 1.1667123544611968e-05\n",
      "[Epoch 45] Training Batch [215/391]: Loss 2.4911558284657076e-05\n",
      "[Epoch 45] Training Batch [216/391]: Loss 1.2654663805733435e-05\n",
      "[Epoch 45] Training Batch [217/391]: Loss 1.5524787158938125e-05\n",
      "[Epoch 45] Training Batch [218/391]: Loss 3.254662806284614e-05\n",
      "[Epoch 45] Training Batch [219/391]: Loss 2.0090887119295076e-05\n",
      "[Epoch 45] Training Batch [220/391]: Loss 2.3184180463431403e-05\n",
      "[Epoch 45] Training Batch [221/391]: Loss 1.2780445103999227e-05\n",
      "[Epoch 45] Training Batch [222/391]: Loss 7.694749911024701e-06\n",
      "[Epoch 45] Training Batch [223/391]: Loss 1.8367471056990325e-05\n",
      "[Epoch 45] Training Batch [224/391]: Loss 1.5025114407762885e-05\n",
      "[Epoch 45] Training Batch [225/391]: Loss 1.056165092450101e-05\n",
      "[Epoch 45] Training Batch [226/391]: Loss 3.395762178115547e-05\n",
      "[Epoch 45] Training Batch [227/391]: Loss 1.775769305822905e-05\n",
      "[Epoch 45] Training Batch [228/391]: Loss 1.9072253053309396e-05\n",
      "[Epoch 45] Training Batch [229/391]: Loss 1.1783668924181256e-05\n",
      "[Epoch 45] Training Batch [230/391]: Loss 2.6105188226210885e-05\n",
      "[Epoch 45] Training Batch [231/391]: Loss 2.348555608477909e-05\n",
      "[Epoch 45] Training Batch [232/391]: Loss 1.0668965842342004e-05\n",
      "[Epoch 45] Training Batch [233/391]: Loss 2.447060978738591e-05\n",
      "[Epoch 45] Training Batch [234/391]: Loss 1.3034985386184417e-05\n",
      "[Epoch 45] Training Batch [235/391]: Loss 1.7748881873558275e-05\n",
      "[Epoch 45] Training Batch [236/391]: Loss 2.4754865080467425e-05\n",
      "[Epoch 45] Training Batch [237/391]: Loss 2.6920668460661545e-05\n",
      "[Epoch 45] Training Batch [238/391]: Loss 1.5662930309190415e-05\n",
      "[Epoch 45] Training Batch [239/391]: Loss 2.3244861949933693e-05\n",
      "[Epoch 45] Training Batch [240/391]: Loss 1.3933495210949332e-05\n",
      "[Epoch 45] Training Batch [241/391]: Loss 5.446194791147718e-06\n",
      "[Epoch 45] Training Batch [242/391]: Loss 1.7545809896546416e-05\n",
      "[Epoch 45] Training Batch [243/391]: Loss 2.2811453163740225e-05\n",
      "[Epoch 45] Training Batch [244/391]: Loss 1.9582103050197475e-05\n",
      "[Epoch 45] Training Batch [245/391]: Loss 2.5108940462814644e-05\n",
      "[Epoch 45] Training Batch [246/391]: Loss 1.0680409104679711e-05\n",
      "[Epoch 45] Training Batch [247/391]: Loss 1.5907831766526215e-05\n",
      "[Epoch 45] Training Batch [248/391]: Loss 1.2869133570347913e-05\n",
      "[Epoch 45] Training Batch [249/391]: Loss 2.28667922783643e-05\n",
      "[Epoch 45] Training Batch [250/391]: Loss 1.9229581084800884e-05\n",
      "[Epoch 45] Training Batch [251/391]: Loss 1.6637606677250005e-05\n",
      "[Epoch 45] Training Batch [252/391]: Loss 1.0542878044361714e-05\n",
      "[Epoch 45] Training Batch [253/391]: Loss 1.0795876733027399e-05\n",
      "[Epoch 45] Training Batch [254/391]: Loss 1.3459222827805206e-05\n",
      "[Epoch 45] Training Batch [255/391]: Loss 1.6725298337405547e-05\n",
      "[Epoch 45] Training Batch [256/391]: Loss 1.5416659152833745e-05\n",
      "[Epoch 45] Training Batch [257/391]: Loss 1.7843476598500274e-05\n",
      "[Epoch 45] Training Batch [258/391]: Loss 1.197209167003166e-05\n",
      "[Epoch 45] Training Batch [259/391]: Loss 1.8645865566213615e-05\n",
      "[Epoch 45] Training Batch [260/391]: Loss 1.5154264474404044e-05\n",
      "[Epoch 45] Training Batch [261/391]: Loss 2.7102785679744557e-05\n",
      "[Epoch 45] Training Batch [262/391]: Loss 1.1973144864896312e-05\n",
      "[Epoch 45] Training Batch [263/391]: Loss 1.1271205949014984e-05\n",
      "[Epoch 45] Training Batch [264/391]: Loss 1.5143892596825026e-05\n",
      "[Epoch 45] Training Batch [265/391]: Loss 1.9273355064797215e-05\n",
      "[Epoch 45] Training Batch [266/391]: Loss 8.066401278483681e-06\n",
      "[Epoch 45] Training Batch [267/391]: Loss 1.0708150512073189e-05\n",
      "[Epoch 45] Training Batch [268/391]: Loss 1.9629575035651214e-05\n",
      "[Epoch 45] Training Batch [269/391]: Loss 1.88845515367575e-05\n",
      "[Epoch 45] Training Batch [270/391]: Loss 1.871700442279689e-05\n",
      "[Epoch 45] Training Batch [271/391]: Loss 1.8336364519200288e-05\n",
      "[Epoch 45] Training Batch [272/391]: Loss 2.7657652026391588e-05\n",
      "[Epoch 45] Training Batch [273/391]: Loss 1.0781137461890467e-05\n",
      "[Epoch 45] Training Batch [274/391]: Loss 2.7361449610907584e-05\n",
      "[Epoch 45] Training Batch [275/391]: Loss 2.2673453713650815e-05\n",
      "[Epoch 45] Training Batch [276/391]: Loss 1.0768117135739885e-05\n",
      "[Epoch 45] Training Batch [277/391]: Loss 2.131053952325601e-05\n",
      "[Epoch 45] Training Batch [278/391]: Loss 1.4310273400042206e-05\n",
      "[Epoch 45] Training Batch [279/391]: Loss 1.462726777390344e-05\n",
      "[Epoch 45] Training Batch [280/391]: Loss 2.686454899958335e-05\n",
      "[Epoch 45] Training Batch [281/391]: Loss 1.6225101717282087e-05\n",
      "[Epoch 45] Training Batch [282/391]: Loss 2.0602188669727184e-05\n",
      "[Epoch 45] Training Batch [283/391]: Loss 1.2199730008433107e-05\n",
      "[Epoch 45] Training Batch [284/391]: Loss 2.325116474821698e-05\n",
      "[Epoch 45] Training Batch [285/391]: Loss 2.0948697056155652e-05\n",
      "[Epoch 45] Training Batch [286/391]: Loss 1.2812229215342086e-05\n",
      "[Epoch 45] Training Batch [287/391]: Loss 2.110229070240166e-05\n",
      "[Epoch 45] Training Batch [288/391]: Loss 1.7333108189632185e-05\n",
      "[Epoch 45] Training Batch [289/391]: Loss 1.0174087947234511e-05\n",
      "[Epoch 45] Training Batch [290/391]: Loss 3.226033732062206e-05\n",
      "[Epoch 45] Training Batch [291/391]: Loss 1.7982851204578765e-05\n",
      "[Epoch 45] Training Batch [292/391]: Loss 2.0786654204130173e-05\n",
      "[Epoch 45] Training Batch [293/391]: Loss 1.6803567632450722e-05\n",
      "[Epoch 45] Training Batch [294/391]: Loss 2.8157102860859595e-05\n",
      "[Epoch 45] Training Batch [295/391]: Loss 7.453063517459668e-06\n",
      "[Epoch 45] Training Batch [296/391]: Loss 1.945049370988272e-05\n",
      "[Epoch 45] Training Batch [297/391]: Loss 1.2462057384254877e-05\n",
      "[Epoch 45] Training Batch [298/391]: Loss 8.876872016116977e-06\n",
      "[Epoch 45] Training Batch [299/391]: Loss 7.677424946450628e-06\n",
      "[Epoch 45] Training Batch [300/391]: Loss 1.7156608009827323e-05\n",
      "[Epoch 45] Training Batch [301/391]: Loss 1.932812483573798e-05\n",
      "[Epoch 45] Training Batch [302/391]: Loss 2.7852436687680893e-05\n",
      "[Epoch 45] Training Batch [303/391]: Loss 2.3010303266346455e-05\n",
      "[Epoch 45] Training Batch [304/391]: Loss 2.6662330128601752e-05\n",
      "[Epoch 45] Training Batch [305/391]: Loss 9.96938342723297e-06\n",
      "[Epoch 45] Training Batch [306/391]: Loss 1.0601442227198277e-05\n",
      "[Epoch 45] Training Batch [307/391]: Loss 2.519166810088791e-05\n",
      "[Epoch 45] Training Batch [308/391]: Loss 7.593474038003478e-06\n",
      "[Epoch 45] Training Batch [309/391]: Loss 1.4275344256020617e-05\n",
      "[Epoch 45] Training Batch [310/391]: Loss 2.2014030037098564e-05\n",
      "[Epoch 45] Training Batch [311/391]: Loss 1.444533063477138e-05\n",
      "[Epoch 45] Training Batch [312/391]: Loss 1.3678967661689967e-05\n",
      "[Epoch 45] Training Batch [313/391]: Loss 7.856323463784065e-06\n",
      "[Epoch 45] Training Batch [314/391]: Loss 1.9883473214576952e-05\n",
      "[Epoch 45] Training Batch [315/391]: Loss 2.3759341274853796e-05\n",
      "[Epoch 45] Training Batch [316/391]: Loss 8.574176717957016e-06\n",
      "[Epoch 45] Training Batch [317/391]: Loss 2.2544831153936684e-05\n",
      "[Epoch 45] Training Batch [318/391]: Loss 2.0890613086521626e-05\n",
      "[Epoch 45] Training Batch [319/391]: Loss 1.2727577086479869e-05\n",
      "[Epoch 45] Training Batch [320/391]: Loss 2.5119643396465108e-05\n",
      "[Epoch 45] Training Batch [321/391]: Loss 1.0801523785630707e-05\n",
      "[Epoch 45] Training Batch [322/391]: Loss 2.0293118723202497e-05\n",
      "[Epoch 45] Training Batch [323/391]: Loss 1.2108941518818028e-05\n",
      "[Epoch 45] Training Batch [324/391]: Loss 1.8899427232099697e-05\n",
      "[Epoch 45] Training Batch [325/391]: Loss 1.6145681001944467e-05\n",
      "[Epoch 45] Training Batch [326/391]: Loss 7.707253644184675e-06\n",
      "[Epoch 45] Training Batch [327/391]: Loss 1.5487594282603823e-05\n",
      "[Epoch 45] Training Batch [328/391]: Loss 1.7982645658776164e-05\n",
      "[Epoch 45] Training Batch [329/391]: Loss 1.415875340171624e-05\n",
      "[Epoch 45] Training Batch [330/391]: Loss 1.2974826859135646e-05\n",
      "[Epoch 45] Training Batch [331/391]: Loss 2.6751011318992823e-05\n",
      "[Epoch 45] Training Batch [332/391]: Loss 2.332169606233947e-05\n",
      "[Epoch 45] Training Batch [333/391]: Loss 1.575684655108489e-05\n",
      "[Epoch 45] Training Batch [334/391]: Loss 9.80627828539582e-06\n",
      "[Epoch 45] Training Batch [335/391]: Loss 8.857345164869912e-06\n",
      "[Epoch 45] Training Batch [336/391]: Loss 2.2525044187204912e-05\n",
      "[Epoch 45] Training Batch [337/391]: Loss 1.279660591535503e-05\n",
      "[Epoch 45] Training Batch [338/391]: Loss 1.0308363016520161e-05\n",
      "[Epoch 45] Training Batch [339/391]: Loss 1.7455189663451165e-05\n",
      "[Epoch 45] Training Batch [340/391]: Loss 1.9059243641095236e-05\n",
      "[Epoch 45] Training Batch [341/391]: Loss 1.8595163055579178e-05\n",
      "[Epoch 45] Training Batch [342/391]: Loss 1.3236912309366744e-05\n",
      "[Epoch 45] Training Batch [343/391]: Loss 1.290846284973668e-05\n",
      "[Epoch 45] Training Batch [344/391]: Loss 2.3247477656695992e-05\n",
      "[Epoch 45] Training Batch [345/391]: Loss 3.654119063867256e-05\n",
      "[Epoch 45] Training Batch [346/391]: Loss 1.742650056257844e-05\n",
      "[Epoch 45] Training Batch [347/391]: Loss 1.4822069715592079e-05\n",
      "[Epoch 45] Training Batch [348/391]: Loss 1.163725846708985e-05\n",
      "[Epoch 45] Training Batch [349/391]: Loss 2.1485300749191083e-05\n",
      "[Epoch 45] Training Batch [350/391]: Loss 2.8933265639352612e-05\n",
      "[Epoch 45] Training Batch [351/391]: Loss 2.1130113964318298e-05\n",
      "[Epoch 45] Training Batch [352/391]: Loss 1.6268644685624167e-05\n",
      "[Epoch 45] Training Batch [353/391]: Loss 1.0827671758306678e-05\n",
      "[Epoch 45] Training Batch [354/391]: Loss 1.5557192455162294e-05\n",
      "[Epoch 45] Training Batch [355/391]: Loss 2.4926679543568753e-05\n",
      "[Epoch 45] Training Batch [356/391]: Loss 1.8955208361148834e-05\n",
      "[Epoch 45] Training Batch [357/391]: Loss 1.2607244570972398e-05\n",
      "[Epoch 45] Training Batch [358/391]: Loss 2.4022367142606527e-05\n",
      "[Epoch 45] Training Batch [359/391]: Loss 1.5132468433876056e-05\n",
      "[Epoch 45] Training Batch [360/391]: Loss 2.043510539806448e-05\n",
      "[Epoch 45] Training Batch [361/391]: Loss 1.6195881471503526e-05\n",
      "[Epoch 45] Training Batch [362/391]: Loss 2.1230494894552976e-05\n",
      "[Epoch 45] Training Batch [363/391]: Loss 1.6391782992286608e-05\n",
      "[Epoch 45] Training Batch [364/391]: Loss 2.1670202841050923e-05\n",
      "[Epoch 45] Training Batch [365/391]: Loss 2.5853503757389262e-05\n",
      "[Epoch 45] Training Batch [366/391]: Loss 1.1712394552887417e-05\n",
      "[Epoch 45] Training Batch [367/391]: Loss 1.4173644558468368e-05\n",
      "[Epoch 45] Training Batch [368/391]: Loss 1.0627568372001406e-05\n",
      "[Epoch 45] Training Batch [369/391]: Loss 1.5798719687154517e-05\n",
      "[Epoch 45] Training Batch [370/391]: Loss 1.6312333173118532e-05\n",
      "[Epoch 45] Training Batch [371/391]: Loss 1.4196680240274873e-05\n",
      "[Epoch 45] Training Batch [372/391]: Loss 1.7413309251423925e-05\n",
      "[Epoch 45] Training Batch [373/391]: Loss 2.6438881832291372e-05\n",
      "[Epoch 45] Training Batch [374/391]: Loss 1.8502340026316233e-05\n",
      "[Epoch 45] Training Batch [375/391]: Loss 2.0049785234732553e-05\n",
      "[Epoch 45] Training Batch [376/391]: Loss 1.4819481293670833e-05\n",
      "[Epoch 45] Training Batch [377/391]: Loss 2.3597811377840117e-05\n",
      "[Epoch 45] Training Batch [378/391]: Loss 1.8682994777918793e-05\n",
      "[Epoch 45] Training Batch [379/391]: Loss 1.5607682144036517e-05\n",
      "[Epoch 45] Training Batch [380/391]: Loss 2.997184310515877e-05\n",
      "[Epoch 45] Training Batch [381/391]: Loss 2.1311037926352583e-05\n",
      "[Epoch 45] Training Batch [382/391]: Loss 8.742642421566416e-06\n",
      "[Epoch 45] Training Batch [383/391]: Loss 1.012931352306623e-05\n",
      "[Epoch 45] Training Batch [384/391]: Loss 8.123493898892775e-06\n",
      "[Epoch 45] Training Batch [385/391]: Loss 2.309803676325828e-05\n",
      "[Epoch 45] Training Batch [386/391]: Loss 1.580167190695647e-05\n",
      "[Epoch 45] Training Batch [387/391]: Loss 1.2604817129613366e-05\n",
      "[Epoch 45] Training Batch [388/391]: Loss 1.9994213289464824e-05\n",
      "[Epoch 45] Training Batch [389/391]: Loss 2.243517155875452e-05\n",
      "[Epoch 45] Training Batch [390/391]: Loss 2.3766384401824325e-05\n",
      "[Epoch 45] Training Batch [391/391]: Loss 1.2972343029105105e-05\n",
      "Epoch 45 - Train Loss: 0.0000\n",
      "*********  Epoch 46/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46] Training Batch [1/391]: Loss 2.663196573848836e-05\n",
      "[Epoch 46] Training Batch [2/391]: Loss 1.3286092325870413e-05\n",
      "[Epoch 46] Training Batch [3/391]: Loss 1.0295746506017167e-05\n",
      "[Epoch 46] Training Batch [4/391]: Loss 1.6980842701741494e-05\n",
      "[Epoch 46] Training Batch [5/391]: Loss 1.3026087799516972e-05\n",
      "[Epoch 46] Training Batch [6/391]: Loss 2.1310408556018956e-05\n",
      "[Epoch 46] Training Batch [7/391]: Loss 1.1978387192357332e-05\n",
      "[Epoch 46] Training Batch [8/391]: Loss 1.707652336335741e-05\n",
      "[Epoch 46] Training Batch [9/391]: Loss 1.546686871733982e-05\n",
      "[Epoch 46] Training Batch [10/391]: Loss 2.1255729734548368e-05\n",
      "[Epoch 46] Training Batch [11/391]: Loss 2.3839465939090587e-05\n",
      "[Epoch 46] Training Batch [12/391]: Loss 2.3642747692065313e-05\n",
      "[Epoch 46] Training Batch [13/391]: Loss 2.569199205026962e-05\n",
      "[Epoch 46] Training Batch [14/391]: Loss 2.694232352951076e-05\n",
      "[Epoch 46] Training Batch [15/391]: Loss 1.387933843943756e-05\n",
      "[Epoch 46] Training Batch [16/391]: Loss 1.4182928680384066e-05\n",
      "[Epoch 46] Training Batch [17/391]: Loss 1.4460159945883788e-05\n",
      "[Epoch 46] Training Batch [18/391]: Loss 1.3139855582267046e-05\n",
      "[Epoch 46] Training Batch [19/391]: Loss 1.5019621059764177e-05\n",
      "[Epoch 46] Training Batch [20/391]: Loss 1.131806038756622e-05\n",
      "[Epoch 46] Training Batch [21/391]: Loss 1.2564704775286373e-05\n",
      "[Epoch 46] Training Batch [22/391]: Loss 2.2231657567317598e-05\n",
      "[Epoch 46] Training Batch [23/391]: Loss 1.825843719416298e-05\n",
      "[Epoch 46] Training Batch [24/391]: Loss 1.4596086657547858e-05\n",
      "[Epoch 46] Training Batch [25/391]: Loss 6.712614776915871e-06\n",
      "[Epoch 46] Training Batch [26/391]: Loss 2.078981196973473e-05\n",
      "[Epoch 46] Training Batch [27/391]: Loss 1.7110829503508285e-05\n",
      "[Epoch 46] Training Batch [28/391]: Loss 1.2283418072911445e-05\n",
      "[Epoch 46] Training Batch [29/391]: Loss 1.166234869742766e-05\n",
      "[Epoch 46] Training Batch [30/391]: Loss 1.23402342069312e-05\n",
      "[Epoch 46] Training Batch [31/391]: Loss 1.7171854779007845e-05\n",
      "[Epoch 46] Training Batch [32/391]: Loss 1.290265572606586e-05\n",
      "[Epoch 46] Training Batch [33/391]: Loss 2.3666232664254494e-05\n",
      "[Epoch 46] Training Batch [34/391]: Loss 2.8490238037193194e-05\n",
      "[Epoch 46] Training Batch [35/391]: Loss 1.9612958567449823e-05\n",
      "[Epoch 46] Training Batch [36/391]: Loss 1.2656362741836347e-05\n",
      "[Epoch 46] Training Batch [37/391]: Loss 1.5177565728663467e-05\n",
      "[Epoch 46] Training Batch [38/391]: Loss 1.6734755263314582e-05\n",
      "[Epoch 46] Training Batch [39/391]: Loss 2.7061900254921056e-05\n",
      "[Epoch 46] Training Batch [40/391]: Loss 2.459967799950391e-05\n",
      "[Epoch 46] Training Batch [41/391]: Loss 2.1998726879246533e-05\n",
      "[Epoch 46] Training Batch [42/391]: Loss 1.499909285485046e-05\n",
      "[Epoch 46] Training Batch [43/391]: Loss 1.6450556358904578e-05\n",
      "[Epoch 46] Training Batch [44/391]: Loss 8.37373318063328e-06\n",
      "[Epoch 46] Training Batch [45/391]: Loss 1.8674156308406964e-05\n",
      "[Epoch 46] Training Batch [46/391]: Loss 1.0919887245108839e-05\n",
      "[Epoch 46] Training Batch [47/391]: Loss 2.1054589524283074e-05\n",
      "[Epoch 46] Training Batch [48/391]: Loss 1.3612874681712128e-05\n",
      "[Epoch 46] Training Batch [49/391]: Loss 1.8653148799785413e-05\n",
      "[Epoch 46] Training Batch [50/391]: Loss 2.0922758267261088e-05\n",
      "[Epoch 46] Training Batch [51/391]: Loss 1.156645521405153e-05\n",
      "[Epoch 46] Training Batch [52/391]: Loss 2.3906368369353004e-05\n",
      "[Epoch 46] Training Batch [53/391]: Loss 2.1083436877233908e-05\n",
      "[Epoch 46] Training Batch [54/391]: Loss 1.7559892512508668e-05\n",
      "[Epoch 46] Training Batch [55/391]: Loss 1.432443878002232e-05\n",
      "[Epoch 46] Training Batch [56/391]: Loss 1.0944330824713688e-05\n",
      "[Epoch 46] Training Batch [57/391]: Loss 2.4371029212488793e-05\n",
      "[Epoch 46] Training Batch [58/391]: Loss 1.4482879123534076e-05\n",
      "[Epoch 46] Training Batch [59/391]: Loss 1.7117739844252355e-05\n",
      "[Epoch 46] Training Batch [60/391]: Loss 1.775612327037379e-05\n",
      "[Epoch 46] Training Batch [61/391]: Loss 1.4262512195273302e-05\n",
      "[Epoch 46] Training Batch [62/391]: Loss 1.4476111573458184e-05\n",
      "[Epoch 46] Training Batch [63/391]: Loss 1.1963177712459583e-05\n",
      "[Epoch 46] Training Batch [64/391]: Loss 1.6437104932265356e-05\n",
      "[Epoch 46] Training Batch [65/391]: Loss 2.189548649766948e-05\n",
      "[Epoch 46] Training Batch [66/391]: Loss 1.005927970254561e-05\n",
      "[Epoch 46] Training Batch [67/391]: Loss 2.6898203941527754e-05\n",
      "[Epoch 46] Training Batch [68/391]: Loss 1.7157872207462788e-05\n",
      "[Epoch 46] Training Batch [69/391]: Loss 1.5847852409933694e-05\n",
      "[Epoch 46] Training Batch [70/391]: Loss 1.4188398381520528e-05\n",
      "[Epoch 46] Training Batch [71/391]: Loss 2.24375726247672e-05\n",
      "[Epoch 46] Training Batch [72/391]: Loss 1.684321614447981e-05\n",
      "[Epoch 46] Training Batch [73/391]: Loss 2.0341076378826983e-05\n",
      "[Epoch 46] Training Batch [74/391]: Loss 1.691843499429524e-05\n",
      "[Epoch 46] Training Batch [75/391]: Loss 2.661301004991401e-05\n",
      "[Epoch 46] Training Batch [76/391]: Loss 2.328528353245929e-05\n",
      "[Epoch 46] Training Batch [77/391]: Loss 2.1006184397265315e-05\n",
      "[Epoch 46] Training Batch [78/391]: Loss 1.7106705854530446e-05\n",
      "[Epoch 46] Training Batch [79/391]: Loss 1.3036928976362105e-05\n",
      "[Epoch 46] Training Batch [80/391]: Loss 1.705477734503802e-05\n",
      "[Epoch 46] Training Batch [81/391]: Loss 1.3429647879092954e-05\n",
      "[Epoch 46] Training Batch [82/391]: Loss 1.400570272380719e-05\n",
      "[Epoch 46] Training Batch [83/391]: Loss 2.745721758401487e-05\n",
      "[Epoch 46] Training Batch [84/391]: Loss 1.4064715287531726e-05\n",
      "[Epoch 46] Training Batch [85/391]: Loss 2.796456465148367e-05\n",
      "[Epoch 46] Training Batch [86/391]: Loss 1.3980464245832991e-05\n",
      "[Epoch 46] Training Batch [87/391]: Loss 1.1216988241358195e-05\n",
      "[Epoch 46] Training Batch [88/391]: Loss 2.370989568589721e-05\n",
      "[Epoch 46] Training Batch [89/391]: Loss 1.7734635548549704e-05\n",
      "[Epoch 46] Training Batch [90/391]: Loss 1.4946665942261461e-05\n",
      "[Epoch 46] Training Batch [91/391]: Loss 1.600718496774789e-05\n",
      "[Epoch 46] Training Batch [92/391]: Loss 1.1886626452906057e-05\n",
      "[Epoch 46] Training Batch [93/391]: Loss 8.04234605311649e-06\n",
      "[Epoch 46] Training Batch [94/391]: Loss 2.0853978639934212e-05\n",
      "[Epoch 46] Training Batch [95/391]: Loss 1.4968131836212706e-05\n",
      "[Epoch 46] Training Batch [96/391]: Loss 1.0415229553473182e-05\n",
      "[Epoch 46] Training Batch [97/391]: Loss 1.2631041499844287e-05\n",
      "[Epoch 46] Training Batch [98/391]: Loss 1.5609421097906306e-05\n",
      "[Epoch 46] Training Batch [99/391]: Loss 1.763108593877405e-05\n",
      "[Epoch 46] Training Batch [100/391]: Loss 1.344761585642118e-05\n",
      "[Epoch 46] Training Batch [101/391]: Loss 1.5937130228849128e-05\n",
      "[Epoch 46] Training Batch [102/391]: Loss 2.1011534045101143e-05\n",
      "[Epoch 46] Training Batch [103/391]: Loss 2.111183130182326e-05\n",
      "[Epoch 46] Training Batch [104/391]: Loss 1.0732703231042251e-05\n",
      "[Epoch 46] Training Batch [105/391]: Loss 1.2881386282970197e-05\n",
      "[Epoch 46] Training Batch [106/391]: Loss 1.2028083801851608e-05\n",
      "[Epoch 46] Training Batch [107/391]: Loss 2.510105332476087e-05\n",
      "[Epoch 46] Training Batch [108/391]: Loss 1.3735250831814483e-05\n",
      "[Epoch 46] Training Batch [109/391]: Loss 1.0218586794508155e-05\n",
      "[Epoch 46] Training Batch [110/391]: Loss 1.2446849723346531e-05\n",
      "[Epoch 46] Training Batch [111/391]: Loss 2.4246572138508782e-05\n",
      "[Epoch 46] Training Batch [112/391]: Loss 2.2034180801711045e-05\n",
      "[Epoch 46] Training Batch [113/391]: Loss 2.0857380150118843e-05\n",
      "[Epoch 46] Training Batch [114/391]: Loss 1.1909236491192132e-05\n",
      "[Epoch 46] Training Batch [115/391]: Loss 2.023581510002259e-05\n",
      "[Epoch 46] Training Batch [116/391]: Loss 1.7670876331976615e-05\n",
      "[Epoch 46] Training Batch [117/391]: Loss 2.099368066410534e-05\n",
      "[Epoch 46] Training Batch [118/391]: Loss 1.9931889255531132e-05\n",
      "[Epoch 46] Training Batch [119/391]: Loss 1.6666830561007373e-05\n",
      "[Epoch 46] Training Batch [120/391]: Loss 2.5065312001970597e-05\n",
      "[Epoch 46] Training Batch [121/391]: Loss 7.075940629874822e-06\n",
      "[Epoch 46] Training Batch [122/391]: Loss 2.590398071333766e-05\n",
      "[Epoch 46] Training Batch [123/391]: Loss 1.024215089273639e-05\n",
      "[Epoch 46] Training Batch [124/391]: Loss 8.662735126563348e-06\n",
      "[Epoch 46] Training Batch [125/391]: Loss 2.0922290786984377e-05\n",
      "[Epoch 46] Training Batch [126/391]: Loss 2.5765337340999395e-05\n",
      "[Epoch 46] Training Batch [127/391]: Loss 3.174040830344893e-05\n",
      "[Epoch 46] Training Batch [128/391]: Loss 1.3201471119828057e-05\n",
      "[Epoch 46] Training Batch [129/391]: Loss 1.364438867312856e-05\n",
      "[Epoch 46] Training Batch [130/391]: Loss 1.321710897173034e-05\n",
      "[Epoch 46] Training Batch [131/391]: Loss 8.55390044307569e-06\n",
      "[Epoch 46] Training Batch [132/391]: Loss 1.4810156244493555e-05\n",
      "[Epoch 46] Training Batch [133/391]: Loss 1.7548116375110112e-05\n",
      "[Epoch 46] Training Batch [134/391]: Loss 1.5113467270566616e-05\n",
      "[Epoch 46] Training Batch [135/391]: Loss 1.8965021808980964e-05\n",
      "[Epoch 46] Training Batch [136/391]: Loss 2.0117740859859623e-05\n",
      "[Epoch 46] Training Batch [137/391]: Loss 2.2836236894363537e-05\n",
      "[Epoch 46] Training Batch [138/391]: Loss 1.675617204455193e-05\n",
      "[Epoch 46] Training Batch [139/391]: Loss 2.323943226656411e-05\n",
      "[Epoch 46] Training Batch [140/391]: Loss 2.0645393306040205e-05\n",
      "[Epoch 46] Training Batch [141/391]: Loss 2.022169610427227e-05\n",
      "[Epoch 46] Training Batch [142/391]: Loss 9.911445886245929e-06\n",
      "[Epoch 46] Training Batch [143/391]: Loss 1.485771281295456e-05\n",
      "[Epoch 46] Training Batch [144/391]: Loss 1.4449273294303566e-05\n",
      "[Epoch 46] Training Batch [145/391]: Loss 1.9685410734382458e-05\n",
      "[Epoch 46] Training Batch [146/391]: Loss 1.2220590178912971e-05\n",
      "[Epoch 46] Training Batch [147/391]: Loss 1.901022005768027e-05\n",
      "[Epoch 46] Training Batch [148/391]: Loss 2.3380605853162706e-05\n",
      "[Epoch 46] Training Batch [149/391]: Loss 1.3933675290900283e-05\n",
      "[Epoch 46] Training Batch [150/391]: Loss 1.3318110177351627e-05\n",
      "[Epoch 46] Training Batch [151/391]: Loss 1.671379322942812e-05\n",
      "[Epoch 46] Training Batch [152/391]: Loss 3.0279408747446723e-05\n",
      "[Epoch 46] Training Batch [153/391]: Loss 2.8952397769899108e-05\n",
      "[Epoch 46] Training Batch [154/391]: Loss 1.4172605915518943e-05\n",
      "[Epoch 46] Training Batch [155/391]: Loss 1.1158318557136226e-05\n",
      "[Epoch 46] Training Batch [156/391]: Loss 8.217354661610443e-06\n",
      "[Epoch 46] Training Batch [157/391]: Loss 1.061731563822832e-05\n",
      "[Epoch 46] Training Batch [158/391]: Loss 2.5803052267292514e-05\n",
      "[Epoch 46] Training Batch [159/391]: Loss 2.110104469466023e-05\n",
      "[Epoch 46] Training Batch [160/391]: Loss 2.391797534073703e-05\n",
      "[Epoch 46] Training Batch [161/391]: Loss 3.781722625717521e-05\n",
      "[Epoch 46] Training Batch [162/391]: Loss 1.3637610209116247e-05\n",
      "[Epoch 46] Training Batch [163/391]: Loss 1.4278699381975457e-05\n",
      "[Epoch 46] Training Batch [164/391]: Loss 6.1324849411903415e-06\n",
      "[Epoch 46] Training Batch [165/391]: Loss 1.745445842971094e-05\n",
      "[Epoch 46] Training Batch [166/391]: Loss 3.216362529201433e-05\n",
      "[Epoch 46] Training Batch [167/391]: Loss 1.8664870367501862e-05\n",
      "[Epoch 46] Training Batch [168/391]: Loss 1.7050537280738354e-05\n",
      "[Epoch 46] Training Batch [169/391]: Loss 1.0753578862932045e-05\n",
      "[Epoch 46] Training Batch [170/391]: Loss 2.092057366098743e-05\n",
      "[Epoch 46] Training Batch [171/391]: Loss 1.817509655666072e-05\n",
      "[Epoch 46] Training Batch [172/391]: Loss 1.8410230040899478e-05\n",
      "[Epoch 46] Training Batch [173/391]: Loss 2.2105654352344573e-05\n",
      "[Epoch 46] Training Batch [174/391]: Loss 1.586398320796434e-05\n",
      "[Epoch 46] Training Batch [175/391]: Loss 1.3497681720764376e-05\n",
      "[Epoch 46] Training Batch [176/391]: Loss 1.8245707906316966e-05\n",
      "[Epoch 46] Training Batch [177/391]: Loss 1.2095231795683503e-05\n",
      "[Epoch 46] Training Batch [178/391]: Loss 1.5411364074680023e-05\n",
      "[Epoch 46] Training Batch [179/391]: Loss 1.850030275818426e-05\n",
      "[Epoch 46] Training Batch [180/391]: Loss 1.5539961168542504e-05\n",
      "[Epoch 46] Training Batch [181/391]: Loss 1.393975253449753e-05\n",
      "[Epoch 46] Training Batch [182/391]: Loss 1.614588290976826e-05\n",
      "[Epoch 46] Training Batch [183/391]: Loss 9.686968951427843e-06\n",
      "[Epoch 46] Training Batch [184/391]: Loss 1.811663605622016e-05\n",
      "[Epoch 46] Training Batch [185/391]: Loss 1.9895174773409963e-05\n",
      "[Epoch 46] Training Batch [186/391]: Loss 1.9283763322164305e-05\n",
      "[Epoch 46] Training Batch [187/391]: Loss 2.177147143811453e-05\n",
      "[Epoch 46] Training Batch [188/391]: Loss 1.6193351257243194e-05\n",
      "[Epoch 46] Training Batch [189/391]: Loss 1.8358381566940807e-05\n",
      "[Epoch 46] Training Batch [190/391]: Loss 1.4222751815395895e-05\n",
      "[Epoch 46] Training Batch [191/391]: Loss 2.42260102822911e-05\n",
      "[Epoch 46] Training Batch [192/391]: Loss 2.0056746507179923e-05\n",
      "[Epoch 46] Training Batch [193/391]: Loss 1.2373697245493531e-05\n",
      "[Epoch 46] Training Batch [194/391]: Loss 1.642681127123069e-05\n",
      "[Epoch 46] Training Batch [195/391]: Loss 1.5546866052318364e-05\n",
      "[Epoch 46] Training Batch [196/391]: Loss 1.3432616469799541e-05\n",
      "[Epoch 46] Training Batch [197/391]: Loss 2.00636604859028e-05\n",
      "[Epoch 46] Training Batch [198/391]: Loss 1.3359471267904155e-05\n",
      "[Epoch 46] Training Batch [199/391]: Loss 2.0157433027634397e-05\n",
      "[Epoch 46] Training Batch [200/391]: Loss 1.4623237802879885e-05\n",
      "[Epoch 46] Training Batch [201/391]: Loss 1.1327133506711107e-05\n",
      "[Epoch 46] Training Batch [202/391]: Loss 2.189832775911782e-05\n",
      "[Epoch 46] Training Batch [203/391]: Loss 2.8064314392395318e-05\n",
      "[Epoch 46] Training Batch [204/391]: Loss 1.5715746485511772e-05\n",
      "[Epoch 46] Training Batch [205/391]: Loss 2.564082205935847e-05\n",
      "[Epoch 46] Training Batch [206/391]: Loss 1.625789809622802e-05\n",
      "[Epoch 46] Training Batch [207/391]: Loss 1.9926428649341688e-05\n",
      "[Epoch 46] Training Batch [208/391]: Loss 2.4945031327661127e-05\n",
      "[Epoch 46] Training Batch [209/391]: Loss 8.71378870215267e-06\n",
      "[Epoch 46] Training Batch [210/391]: Loss 3.6286950489738956e-05\n",
      "[Epoch 46] Training Batch [211/391]: Loss 1.310160405409988e-05\n",
      "[Epoch 46] Training Batch [212/391]: Loss 1.6501109712407924e-05\n",
      "[Epoch 46] Training Batch [213/391]: Loss 1.4072440535528585e-05\n",
      "[Epoch 46] Training Batch [214/391]: Loss 1.6795571355032735e-05\n",
      "[Epoch 46] Training Batch [215/391]: Loss 1.3099693205731455e-05\n",
      "[Epoch 46] Training Batch [216/391]: Loss 1.0214276699116454e-05\n",
      "[Epoch 46] Training Batch [217/391]: Loss 1.108578362618573e-05\n",
      "[Epoch 46] Training Batch [218/391]: Loss 1.357254950562492e-05\n",
      "[Epoch 46] Training Batch [219/391]: Loss 2.5844672563835047e-05\n",
      "[Epoch 46] Training Batch [220/391]: Loss 2.4095390472211875e-05\n",
      "[Epoch 46] Training Batch [221/391]: Loss 2.395601950411219e-05\n",
      "[Epoch 46] Training Batch [222/391]: Loss 2.1319276129361242e-05\n",
      "[Epoch 46] Training Batch [223/391]: Loss 1.3796766324958298e-05\n",
      "[Epoch 46] Training Batch [224/391]: Loss 1.7061875041690655e-05\n",
      "[Epoch 46] Training Batch [225/391]: Loss 1.498231267760275e-05\n",
      "[Epoch 46] Training Batch [226/391]: Loss 8.937425263866317e-06\n",
      "[Epoch 46] Training Batch [227/391]: Loss 1.80054921656847e-05\n",
      "[Epoch 46] Training Batch [228/391]: Loss 1.6099567801575176e-05\n",
      "[Epoch 46] Training Batch [229/391]: Loss 2.0766434317920357e-05\n",
      "[Epoch 46] Training Batch [230/391]: Loss 1.0802587894431781e-05\n",
      "[Epoch 46] Training Batch [231/391]: Loss 6.208967533893883e-06\n",
      "[Epoch 46] Training Batch [232/391]: Loss 1.4086363080423325e-05\n",
      "[Epoch 46] Training Batch [233/391]: Loss 2.600093648652546e-05\n",
      "[Epoch 46] Training Batch [234/391]: Loss 1.350321872450877e-05\n",
      "[Epoch 46] Training Batch [235/391]: Loss 8.39169297250919e-06\n",
      "[Epoch 46] Training Batch [236/391]: Loss 8.829249054542743e-06\n",
      "[Epoch 46] Training Batch [237/391]: Loss 1.4593284504371695e-05\n",
      "[Epoch 46] Training Batch [238/391]: Loss 1.1800912943726871e-05\n",
      "[Epoch 46] Training Batch [239/391]: Loss 2.138246054528281e-05\n",
      "[Epoch 46] Training Batch [240/391]: Loss 1.3128609680279624e-05\n",
      "[Epoch 46] Training Batch [241/391]: Loss 1.3874965588911436e-05\n",
      "[Epoch 46] Training Batch [242/391]: Loss 1.0550130355113652e-05\n",
      "[Epoch 46] Training Batch [243/391]: Loss 1.812223854358308e-05\n",
      "[Epoch 46] Training Batch [244/391]: Loss 2.0548914108076133e-05\n",
      "[Epoch 46] Training Batch [245/391]: Loss 1.5938616343191825e-05\n",
      "[Epoch 46] Training Batch [246/391]: Loss 2.241612492070999e-05\n",
      "[Epoch 46] Training Batch [247/391]: Loss 1.7120457414421253e-05\n",
      "[Epoch 46] Training Batch [248/391]: Loss 8.440246347163338e-06\n",
      "[Epoch 46] Training Batch [249/391]: Loss 3.5586246667662635e-05\n",
      "[Epoch 46] Training Batch [250/391]: Loss 1.5535057173110545e-05\n",
      "[Epoch 46] Training Batch [251/391]: Loss 2.8061804187018424e-05\n",
      "[Epoch 46] Training Batch [252/391]: Loss 1.6929814592003822e-05\n",
      "[Epoch 46] Training Batch [253/391]: Loss 1.525943480373826e-05\n",
      "[Epoch 46] Training Batch [254/391]: Loss 7.923333214421291e-06\n",
      "[Epoch 46] Training Batch [255/391]: Loss 2.0022032913402654e-05\n",
      "[Epoch 46] Training Batch [256/391]: Loss 1.196168341266457e-05\n",
      "[Epoch 46] Training Batch [257/391]: Loss 1.3689334991795477e-05\n",
      "[Epoch 46] Training Batch [258/391]: Loss 1.5190667909337208e-05\n",
      "[Epoch 46] Training Batch [259/391]: Loss 3.10388786601834e-05\n",
      "[Epoch 46] Training Batch [260/391]: Loss 2.3788499674992636e-05\n",
      "[Epoch 46] Training Batch [261/391]: Loss 1.574384623381775e-05\n",
      "[Epoch 46] Training Batch [262/391]: Loss 8.259355126938317e-06\n",
      "[Epoch 46] Training Batch [263/391]: Loss 1.4722798368893564e-05\n",
      "[Epoch 46] Training Batch [264/391]: Loss 1.7806423784350045e-05\n",
      "[Epoch 46] Training Batch [265/391]: Loss 1.9309134586364962e-05\n",
      "[Epoch 46] Training Batch [266/391]: Loss 1.3564361324824858e-05\n",
      "[Epoch 46] Training Batch [267/391]: Loss 1.1509640899021178e-05\n",
      "[Epoch 46] Training Batch [268/391]: Loss 2.0205221517244354e-05\n",
      "[Epoch 46] Training Batch [269/391]: Loss 1.988301482924726e-05\n",
      "[Epoch 46] Training Batch [270/391]: Loss 1.8655935491551645e-05\n",
      "[Epoch 46] Training Batch [271/391]: Loss 2.512939681764692e-05\n",
      "[Epoch 46] Training Batch [272/391]: Loss 1.5731164239696227e-05\n",
      "[Epoch 46] Training Batch [273/391]: Loss 1.6518095435458235e-05\n",
      "[Epoch 46] Training Batch [274/391]: Loss 2.653336923685856e-05\n",
      "[Epoch 46] Training Batch [275/391]: Loss 1.1627894309640396e-05\n",
      "[Epoch 46] Training Batch [276/391]: Loss 2.0803716324735433e-05\n",
      "[Epoch 46] Training Batch [277/391]: Loss 1.8195603843196295e-05\n",
      "[Epoch 46] Training Batch [278/391]: Loss 1.605711440788582e-05\n",
      "[Epoch 46] Training Batch [279/391]: Loss 2.0150351701886393e-05\n",
      "[Epoch 46] Training Batch [280/391]: Loss 2.0228511857567355e-05\n",
      "[Epoch 46] Training Batch [281/391]: Loss 1.1373300367267802e-05\n",
      "[Epoch 46] Training Batch [282/391]: Loss 1.0239196853945032e-05\n",
      "[Epoch 46] Training Batch [283/391]: Loss 3.062588075408712e-05\n",
      "[Epoch 46] Training Batch [284/391]: Loss 9.227901500707958e-06\n",
      "[Epoch 46] Training Batch [285/391]: Loss 1.866865022748243e-05\n",
      "[Epoch 46] Training Batch [286/391]: Loss 1.5493760656681843e-05\n",
      "[Epoch 46] Training Batch [287/391]: Loss 1.8051838196697645e-05\n",
      "[Epoch 46] Training Batch [288/391]: Loss 1.1085771802754607e-05\n",
      "[Epoch 46] Training Batch [289/391]: Loss 1.4151377399684861e-05\n",
      "[Epoch 46] Training Batch [290/391]: Loss 2.2993646780378185e-05\n",
      "[Epoch 46] Training Batch [291/391]: Loss 1.3473501894623041e-05\n",
      "[Epoch 46] Training Batch [292/391]: Loss 1.3652812413056381e-05\n",
      "[Epoch 46] Training Batch [293/391]: Loss 1.6250918633886613e-05\n",
      "[Epoch 46] Training Batch [294/391]: Loss 1.6815381968626752e-05\n",
      "[Epoch 46] Training Batch [295/391]: Loss 2.475849942129571e-05\n",
      "[Epoch 46] Training Batch [296/391]: Loss 1.6338573914254084e-05\n",
      "[Epoch 46] Training Batch [297/391]: Loss 1.4794070921198e-05\n",
      "[Epoch 46] Training Batch [298/391]: Loss 1.1807619557657745e-05\n",
      "[Epoch 46] Training Batch [299/391]: Loss 1.0110838047694415e-05\n",
      "[Epoch 46] Training Batch [300/391]: Loss 1.6903175492188893e-05\n",
      "[Epoch 46] Training Batch [301/391]: Loss 1.8264503523823805e-05\n",
      "[Epoch 46] Training Batch [302/391]: Loss 1.682851871009916e-05\n",
      "[Epoch 46] Training Batch [303/391]: Loss 1.9692221030709334e-05\n",
      "[Epoch 46] Training Batch [304/391]: Loss 1.4531115084537305e-05\n",
      "[Epoch 46] Training Batch [305/391]: Loss 1.3326343832886778e-05\n",
      "[Epoch 46] Training Batch [306/391]: Loss 1.983947367989458e-05\n",
      "[Epoch 46] Training Batch [307/391]: Loss 1.122233607020462e-05\n",
      "[Epoch 46] Training Batch [308/391]: Loss 1.3464194125845097e-05\n",
      "[Epoch 46] Training Batch [309/391]: Loss 1.4862410353089217e-05\n",
      "[Epoch 46] Training Batch [310/391]: Loss 1.3911814676248468e-05\n",
      "[Epoch 46] Training Batch [311/391]: Loss 1.8258295312989503e-05\n",
      "[Epoch 46] Training Batch [312/391]: Loss 1.4786794963583816e-05\n",
      "[Epoch 46] Training Batch [313/391]: Loss 1.7871174350148067e-05\n",
      "[Epoch 46] Training Batch [314/391]: Loss 2.039713945123367e-05\n",
      "[Epoch 46] Training Batch [315/391]: Loss 1.8182372514274903e-05\n",
      "[Epoch 46] Training Batch [316/391]: Loss 1.7109803593484685e-05\n",
      "[Epoch 46] Training Batch [317/391]: Loss 1.3653891983267386e-05\n",
      "[Epoch 46] Training Batch [318/391]: Loss 1.529857581772376e-05\n",
      "[Epoch 46] Training Batch [319/391]: Loss 7.816273864591494e-06\n",
      "[Epoch 46] Training Batch [320/391]: Loss 1.770621929608751e-05\n",
      "[Epoch 46] Training Batch [321/391]: Loss 2.1791494873468764e-05\n",
      "[Epoch 46] Training Batch [322/391]: Loss 1.795755815692246e-05\n",
      "[Epoch 46] Training Batch [323/391]: Loss 1.5540987078566104e-05\n",
      "[Epoch 46] Training Batch [324/391]: Loss 2.1153413399588317e-05\n",
      "[Epoch 46] Training Batch [325/391]: Loss 1.4427188034460414e-05\n",
      "[Epoch 46] Training Batch [326/391]: Loss 6.892397323099431e-06\n",
      "[Epoch 46] Training Batch [327/391]: Loss 2.4275721443700604e-05\n",
      "[Epoch 46] Training Batch [328/391]: Loss 1.3635841241921298e-05\n",
      "[Epoch 46] Training Batch [329/391]: Loss 1.0662006388884038e-05\n",
      "[Epoch 46] Training Batch [330/391]: Loss 1.4991965144872665e-05\n",
      "[Epoch 46] Training Batch [331/391]: Loss 1.423086450813571e-05\n",
      "[Epoch 46] Training Batch [332/391]: Loss 2.0928488083882257e-05\n",
      "[Epoch 46] Training Batch [333/391]: Loss 1.0101214684254956e-05\n",
      "[Epoch 46] Training Batch [334/391]: Loss 1.1785511560447048e-05\n",
      "[Epoch 46] Training Batch [335/391]: Loss 1.9917049939977005e-05\n",
      "[Epoch 46] Training Batch [336/391]: Loss 2.3369964765151963e-05\n",
      "[Epoch 46] Training Batch [337/391]: Loss 1.2712355783150997e-05\n",
      "[Epoch 46] Training Batch [338/391]: Loss 1.9856368453474715e-05\n",
      "[Epoch 46] Training Batch [339/391]: Loss 9.606442290532868e-06\n",
      "[Epoch 46] Training Batch [340/391]: Loss 1.184631219075527e-05\n",
      "[Epoch 46] Training Batch [341/391]: Loss 1.013205292110797e-05\n",
      "[Epoch 46] Training Batch [342/391]: Loss 1.8432952856528573e-05\n",
      "[Epoch 46] Training Batch [343/391]: Loss 1.1493459169287235e-05\n",
      "[Epoch 46] Training Batch [344/391]: Loss 2.1984984414302744e-05\n",
      "[Epoch 46] Training Batch [345/391]: Loss 1.826125844672788e-05\n",
      "[Epoch 46] Training Batch [346/391]: Loss 1.87985751836095e-05\n",
      "[Epoch 46] Training Batch [347/391]: Loss 1.1508334864629433e-05\n",
      "[Epoch 46] Training Batch [348/391]: Loss 1.1645576705632266e-05\n",
      "[Epoch 46] Training Batch [349/391]: Loss 1.542158679512795e-05\n",
      "[Epoch 46] Training Batch [350/391]: Loss 1.800256359274499e-05\n",
      "[Epoch 46] Training Batch [351/391]: Loss 1.1485943105071783e-05\n",
      "[Epoch 46] Training Batch [352/391]: Loss 9.438463166588917e-06\n",
      "[Epoch 46] Training Batch [353/391]: Loss 2.8137030312791467e-05\n",
      "[Epoch 46] Training Batch [354/391]: Loss 2.709611908358056e-05\n",
      "[Epoch 46] Training Batch [355/391]: Loss 2.7162177502759732e-05\n",
      "[Epoch 46] Training Batch [356/391]: Loss 1.3458477951644454e-05\n",
      "[Epoch 46] Training Batch [357/391]: Loss 1.3986424164613709e-05\n",
      "[Epoch 46] Training Batch [358/391]: Loss 1.6322041119565256e-05\n",
      "[Epoch 46] Training Batch [359/391]: Loss 2.2251355403568596e-05\n",
      "[Epoch 46] Training Batch [360/391]: Loss 7.58436408432317e-06\n",
      "[Epoch 46] Training Batch [361/391]: Loss 7.698823537793942e-06\n",
      "[Epoch 46] Training Batch [362/391]: Loss 1.279734988202108e-05\n",
      "[Epoch 46] Training Batch [363/391]: Loss 2.137145565939136e-05\n",
      "[Epoch 46] Training Batch [364/391]: Loss 1.4587166333512869e-05\n",
      "[Epoch 46] Training Batch [365/391]: Loss 1.3192846381571144e-05\n",
      "[Epoch 46] Training Batch [366/391]: Loss 1.1516880476847291e-05\n",
      "[Epoch 46] Training Batch [367/391]: Loss 1.0321594345441554e-05\n",
      "[Epoch 46] Training Batch [368/391]: Loss 1.1463445844128728e-05\n",
      "[Epoch 46] Training Batch [369/391]: Loss 2.3004311515251175e-05\n",
      "[Epoch 46] Training Batch [370/391]: Loss 1.0255753295496106e-05\n",
      "[Epoch 46] Training Batch [371/391]: Loss 1.619838621991221e-05\n",
      "[Epoch 46] Training Batch [372/391]: Loss 1.5825780792511068e-05\n",
      "[Epoch 46] Training Batch [373/391]: Loss 1.6823427358758636e-05\n",
      "[Epoch 46] Training Batch [374/391]: Loss 1.0966622539854143e-05\n",
      "[Epoch 46] Training Batch [375/391]: Loss 3.5373082937439904e-05\n",
      "[Epoch 46] Training Batch [376/391]: Loss 1.717813574941829e-05\n",
      "[Epoch 46] Training Batch [377/391]: Loss 1.1351843568263575e-05\n",
      "[Epoch 46] Training Batch [378/391]: Loss 1.3424963071884122e-05\n",
      "[Epoch 46] Training Batch [379/391]: Loss 2.2140287910588086e-05\n",
      "[Epoch 46] Training Batch [380/391]: Loss 1.8072310922434554e-05\n",
      "[Epoch 46] Training Batch [381/391]: Loss 2.000662061618641e-05\n",
      "[Epoch 46] Training Batch [382/391]: Loss 2.158312054234557e-05\n",
      "[Epoch 46] Training Batch [383/391]: Loss 1.629950202186592e-05\n",
      "[Epoch 46] Training Batch [384/391]: Loss 1.751932177285198e-05\n",
      "[Epoch 46] Training Batch [385/391]: Loss 1.6367383068427444e-05\n",
      "[Epoch 46] Training Batch [386/391]: Loss 1.0830692190211266e-05\n",
      "[Epoch 46] Training Batch [387/391]: Loss 1.4556300811818801e-05\n",
      "[Epoch 46] Training Batch [388/391]: Loss 1.6784528270363808e-05\n",
      "[Epoch 46] Training Batch [389/391]: Loss 9.83686823019525e-06\n",
      "[Epoch 46] Training Batch [390/391]: Loss 9.24368214327842e-06\n",
      "[Epoch 46] Training Batch [391/391]: Loss 1.2764858183800243e-05\n",
      "Epoch 46 - Train Loss: 0.0000\n",
      "*********  Epoch 47/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47] Training Batch [1/391]: Loss 1.2353048077784479e-05\n",
      "[Epoch 47] Training Batch [2/391]: Loss 2.704954385990277e-05\n",
      "[Epoch 47] Training Batch [3/391]: Loss 1.0131266208190937e-05\n",
      "[Epoch 47] Training Batch [4/391]: Loss 1.8132555851479992e-05\n",
      "[Epoch 47] Training Batch [5/391]: Loss 1.6773983588791452e-05\n",
      "[Epoch 47] Training Batch [6/391]: Loss 9.913254871207755e-06\n",
      "[Epoch 47] Training Batch [7/391]: Loss 1.1748123142751865e-05\n",
      "[Epoch 47] Training Batch [8/391]: Loss 1.8577746232040226e-05\n",
      "[Epoch 47] Training Batch [9/391]: Loss 1.870504638645798e-05\n",
      "[Epoch 47] Training Batch [10/391]: Loss 1.583634366397746e-05\n",
      "[Epoch 47] Training Batch [11/391]: Loss 1.684914604993537e-05\n",
      "[Epoch 47] Training Batch [12/391]: Loss 1.5377414456452243e-05\n",
      "[Epoch 47] Training Batch [13/391]: Loss 1.4511313565890305e-05\n",
      "[Epoch 47] Training Batch [14/391]: Loss 9.259190846933052e-06\n",
      "[Epoch 47] Training Batch [15/391]: Loss 1.4885153177601751e-05\n",
      "[Epoch 47] Training Batch [16/391]: Loss 1.6125513866427355e-05\n",
      "[Epoch 47] Training Batch [17/391]: Loss 1.0195600225415546e-05\n",
      "[Epoch 47] Training Batch [18/391]: Loss 1.9013665223610587e-05\n",
      "[Epoch 47] Training Batch [19/391]: Loss 9.941944881575182e-06\n",
      "[Epoch 47] Training Batch [20/391]: Loss 2.3755590518703684e-05\n",
      "[Epoch 47] Training Batch [21/391]: Loss 1.017666272673523e-05\n",
      "[Epoch 47] Training Batch [22/391]: Loss 1.1356627510394901e-05\n",
      "[Epoch 47] Training Batch [23/391]: Loss 1.067469383997377e-05\n",
      "[Epoch 47] Training Batch [24/391]: Loss 1.3324321116670035e-05\n",
      "[Epoch 47] Training Batch [25/391]: Loss 1.675560270086862e-05\n",
      "[Epoch 47] Training Batch [26/391]: Loss 1.415883070876589e-05\n",
      "[Epoch 47] Training Batch [27/391]: Loss 1.9858809537254274e-05\n",
      "[Epoch 47] Training Batch [28/391]: Loss 1.2844299817516003e-05\n",
      "[Epoch 47] Training Batch [29/391]: Loss 1.2919763321406208e-05\n",
      "[Epoch 47] Training Batch [30/391]: Loss 1.6688161849742755e-05\n",
      "[Epoch 47] Training Batch [31/391]: Loss 1.7477617802796885e-05\n",
      "[Epoch 47] Training Batch [32/391]: Loss 1.4976431884861086e-05\n",
      "[Epoch 47] Training Batch [33/391]: Loss 1.5753144907648675e-05\n",
      "[Epoch 47] Training Batch [34/391]: Loss 2.557137304393109e-05\n",
      "[Epoch 47] Training Batch [35/391]: Loss 1.2439591955626383e-05\n",
      "[Epoch 47] Training Batch [36/391]: Loss 1.3276508980197832e-05\n",
      "[Epoch 47] Training Batch [37/391]: Loss 1.7960679542738944e-05\n",
      "[Epoch 47] Training Batch [38/391]: Loss 3.03949727822328e-05\n",
      "[Epoch 47] Training Batch [39/391]: Loss 9.138387213170063e-06\n",
      "[Epoch 47] Training Batch [40/391]: Loss 1.7075013602152467e-05\n",
      "[Epoch 47] Training Batch [41/391]: Loss 2.1987394575262442e-05\n",
      "[Epoch 47] Training Batch [42/391]: Loss 8.67197013576515e-06\n",
      "[Epoch 47] Training Batch [43/391]: Loss 9.364030120195821e-06\n",
      "[Epoch 47] Training Batch [44/391]: Loss 1.3575559023593087e-05\n",
      "[Epoch 47] Training Batch [45/391]: Loss 1.5685964172007516e-05\n",
      "[Epoch 47] Training Batch [46/391]: Loss 1.710823198664002e-05\n",
      "[Epoch 47] Training Batch [47/391]: Loss 8.961731509771198e-06\n",
      "[Epoch 47] Training Batch [48/391]: Loss 1.4381629625859205e-05\n",
      "[Epoch 47] Training Batch [49/391]: Loss 1.3147247955203056e-05\n",
      "[Epoch 47] Training Batch [50/391]: Loss 2.2686755983158946e-05\n",
      "[Epoch 47] Training Batch [51/391]: Loss 8.868638360581826e-06\n",
      "[Epoch 47] Training Batch [52/391]: Loss 1.5435414752573706e-05\n",
      "[Epoch 47] Training Batch [53/391]: Loss 1.0380420462752227e-05\n",
      "[Epoch 47] Training Batch [54/391]: Loss 2.379189209023025e-05\n",
      "[Epoch 47] Training Batch [55/391]: Loss 2.793938256218098e-05\n",
      "[Epoch 47] Training Batch [56/391]: Loss 2.3413831513607875e-05\n",
      "[Epoch 47] Training Batch [57/391]: Loss 2.1968662622384727e-05\n",
      "[Epoch 47] Training Batch [58/391]: Loss 1.636184060771484e-05\n",
      "[Epoch 47] Training Batch [59/391]: Loss 3.022966302523855e-05\n",
      "[Epoch 47] Training Batch [60/391]: Loss 1.56574333232129e-05\n",
      "[Epoch 47] Training Batch [61/391]: Loss 2.7681546271196567e-05\n",
      "[Epoch 47] Training Batch [62/391]: Loss 1.2809348845621571e-05\n",
      "[Epoch 47] Training Batch [63/391]: Loss 1.624532524147071e-05\n",
      "[Epoch 47] Training Batch [64/391]: Loss 1.5780158719280735e-05\n",
      "[Epoch 47] Training Batch [65/391]: Loss 1.968400647456292e-05\n",
      "[Epoch 47] Training Batch [66/391]: Loss 1.787427936505992e-05\n",
      "[Epoch 47] Training Batch [67/391]: Loss 2.6374960725661367e-05\n",
      "[Epoch 47] Training Batch [68/391]: Loss 1.1379647730791476e-05\n",
      "[Epoch 47] Training Batch [69/391]: Loss 1.3540735380956903e-05\n",
      "[Epoch 47] Training Batch [70/391]: Loss 9.545306056679692e-06\n",
      "[Epoch 47] Training Batch [71/391]: Loss 8.03795046522282e-06\n",
      "[Epoch 47] Training Batch [72/391]: Loss 2.0789118934771977e-05\n",
      "[Epoch 47] Training Batch [73/391]: Loss 9.769950338522904e-06\n",
      "[Epoch 47] Training Batch [74/391]: Loss 1.1988905498583335e-05\n",
      "[Epoch 47] Training Batch [75/391]: Loss 1.4007823665451724e-05\n",
      "[Epoch 47] Training Batch [76/391]: Loss 1.4987542272137944e-05\n",
      "[Epoch 47] Training Batch [77/391]: Loss 1.528388384031132e-05\n",
      "[Epoch 47] Training Batch [78/391]: Loss 1.1916025869140867e-05\n",
      "[Epoch 47] Training Batch [79/391]: Loss 1.5744994016131386e-05\n",
      "[Epoch 47] Training Batch [80/391]: Loss 1.7739934264682233e-05\n",
      "[Epoch 47] Training Batch [81/391]: Loss 8.078765858954284e-06\n",
      "[Epoch 47] Training Batch [82/391]: Loss 1.882792821561452e-05\n",
      "[Epoch 47] Training Batch [83/391]: Loss 2.604500696179457e-05\n",
      "[Epoch 47] Training Batch [84/391]: Loss 1.1147742043249309e-05\n",
      "[Epoch 47] Training Batch [85/391]: Loss 7.518347956647631e-06\n",
      "[Epoch 47] Training Batch [86/391]: Loss 7.279985766217578e-06\n",
      "[Epoch 47] Training Batch [87/391]: Loss 1.4068412383494433e-05\n",
      "[Epoch 47] Training Batch [88/391]: Loss 1.2865648386650719e-05\n",
      "[Epoch 47] Training Batch [89/391]: Loss 2.0238367142155766e-05\n",
      "[Epoch 47] Training Batch [90/391]: Loss 1.2409097507770639e-05\n",
      "[Epoch 47] Training Batch [91/391]: Loss 2.3087626686901785e-05\n",
      "[Epoch 47] Training Batch [92/391]: Loss 2.1415955416159704e-05\n",
      "[Epoch 47] Training Batch [93/391]: Loss 5.512403276952682e-06\n",
      "[Epoch 47] Training Batch [94/391]: Loss 1.7829937860369682e-05\n",
      "[Epoch 47] Training Batch [95/391]: Loss 1.9874472855008207e-05\n",
      "[Epoch 47] Training Batch [96/391]: Loss 2.0155661331955343e-05\n",
      "[Epoch 47] Training Batch [97/391]: Loss 1.3605373169411905e-05\n",
      "[Epoch 47] Training Batch [98/391]: Loss 2.244959796371404e-05\n",
      "[Epoch 47] Training Batch [99/391]: Loss 1.3945595128461719e-05\n",
      "[Epoch 47] Training Batch [100/391]: Loss 1.3712755389860831e-05\n",
      "[Epoch 47] Training Batch [101/391]: Loss 1.3724516975344159e-05\n",
      "[Epoch 47] Training Batch [102/391]: Loss 2.3663207684876397e-05\n",
      "[Epoch 47] Training Batch [103/391]: Loss 1.5189212717814371e-05\n",
      "[Epoch 47] Training Batch [104/391]: Loss 6.3196571318258066e-06\n",
      "[Epoch 47] Training Batch [105/391]: Loss 1.2074796359229367e-05\n",
      "[Epoch 47] Training Batch [106/391]: Loss 1.270044049306307e-05\n",
      "[Epoch 47] Training Batch [107/391]: Loss 1.8373359125689603e-05\n",
      "[Epoch 47] Training Batch [108/391]: Loss 9.492262506682891e-06\n",
      "[Epoch 47] Training Batch [109/391]: Loss 8.89714192453539e-06\n",
      "[Epoch 47] Training Batch [110/391]: Loss 1.2790642358595505e-05\n",
      "[Epoch 47] Training Batch [111/391]: Loss 2.667987246240955e-05\n",
      "[Epoch 47] Training Batch [112/391]: Loss 1.6924204828683287e-05\n",
      "[Epoch 47] Training Batch [113/391]: Loss 1.1127322068205103e-05\n",
      "[Epoch 47] Training Batch [114/391]: Loss 1.147123930422822e-05\n",
      "[Epoch 47] Training Batch [115/391]: Loss 1.3131684681866318e-05\n",
      "[Epoch 47] Training Batch [116/391]: Loss 1.0568821380729787e-05\n",
      "[Epoch 47] Training Batch [117/391]: Loss 1.4406645277631469e-05\n",
      "[Epoch 47] Training Batch [118/391]: Loss 5.392190359998494e-06\n",
      "[Epoch 47] Training Batch [119/391]: Loss 1.748631075315643e-05\n",
      "[Epoch 47] Training Batch [120/391]: Loss 1.2870930731878616e-05\n",
      "[Epoch 47] Training Batch [121/391]: Loss 1.2784867976733949e-05\n",
      "[Epoch 47] Training Batch [122/391]: Loss 2.3983195205801167e-05\n",
      "[Epoch 47] Training Batch [123/391]: Loss 2.6819825507118367e-05\n",
      "[Epoch 47] Training Batch [124/391]: Loss 3.231154187233187e-05\n",
      "[Epoch 47] Training Batch [125/391]: Loss 2.5321191060356796e-05\n",
      "[Epoch 47] Training Batch [126/391]: Loss 1.7961634512175806e-05\n",
      "[Epoch 47] Training Batch [127/391]: Loss 1.0780377124319784e-05\n",
      "[Epoch 47] Training Batch [128/391]: Loss 1.949900615727529e-05\n",
      "[Epoch 47] Training Batch [129/391]: Loss 1.4090740478422958e-05\n",
      "[Epoch 47] Training Batch [130/391]: Loss 1.0206599654338788e-05\n",
      "[Epoch 47] Training Batch [131/391]: Loss 2.1150113752810284e-05\n",
      "[Epoch 47] Training Batch [132/391]: Loss 1.2020696885883808e-05\n",
      "[Epoch 47] Training Batch [133/391]: Loss 2.4159298845916055e-05\n",
      "[Epoch 47] Training Batch [134/391]: Loss 1.4851657397230156e-05\n",
      "[Epoch 47] Training Batch [135/391]: Loss 1.4649414879386313e-05\n",
      "[Epoch 47] Training Batch [136/391]: Loss 1.6658699678373523e-05\n",
      "[Epoch 47] Training Batch [137/391]: Loss 1.7082071281038225e-05\n",
      "[Epoch 47] Training Batch [138/391]: Loss 1.490290742367506e-05\n",
      "[Epoch 47] Training Batch [139/391]: Loss 1.3914872397435829e-05\n",
      "[Epoch 47] Training Batch [140/391]: Loss 2.219397538283374e-05\n",
      "[Epoch 47] Training Batch [141/391]: Loss 1.6910404156078584e-05\n",
      "[Epoch 47] Training Batch [142/391]: Loss 6.1250989347172435e-06\n",
      "[Epoch 47] Training Batch [143/391]: Loss 1.2154589967394713e-05\n",
      "[Epoch 47] Training Batch [144/391]: Loss 2.4856908567016944e-05\n",
      "[Epoch 47] Training Batch [145/391]: Loss 1.6753956515458412e-05\n",
      "[Epoch 47] Training Batch [146/391]: Loss 1.5555981008219533e-05\n",
      "[Epoch 47] Training Batch [147/391]: Loss 9.383288670505863e-06\n",
      "[Epoch 47] Training Batch [148/391]: Loss 1.440857977286214e-05\n",
      "[Epoch 47] Training Batch [149/391]: Loss 1.5747824363643304e-05\n",
      "[Epoch 47] Training Batch [150/391]: Loss 1.544083352200687e-05\n",
      "[Epoch 47] Training Batch [151/391]: Loss 1.686413088464178e-05\n",
      "[Epoch 47] Training Batch [152/391]: Loss 9.673187378211878e-06\n",
      "[Epoch 47] Training Batch [153/391]: Loss 1.4674616977572441e-05\n",
      "[Epoch 47] Training Batch [154/391]: Loss 9.175637387670577e-06\n",
      "[Epoch 47] Training Batch [155/391]: Loss 2.56196508416906e-05\n",
      "[Epoch 47] Training Batch [156/391]: Loss 1.4750120499229524e-05\n",
      "[Epoch 47] Training Batch [157/391]: Loss 1.7565786038176157e-05\n",
      "[Epoch 47] Training Batch [158/391]: Loss 1.6274661902571097e-05\n",
      "[Epoch 47] Training Batch [159/391]: Loss 1.9325852917972952e-05\n",
      "[Epoch 47] Training Batch [160/391]: Loss 1.2294339285290334e-05\n",
      "[Epoch 47] Training Batch [161/391]: Loss 2.116271025442984e-05\n",
      "[Epoch 47] Training Batch [162/391]: Loss 2.2768566850572824e-05\n",
      "[Epoch 47] Training Batch [163/391]: Loss 1.954925937752705e-05\n",
      "[Epoch 47] Training Batch [164/391]: Loss 2.4237544494098984e-05\n",
      "[Epoch 47] Training Batch [165/391]: Loss 2.0699555534520186e-05\n",
      "[Epoch 47] Training Batch [166/391]: Loss 1.2029981917294208e-05\n",
      "[Epoch 47] Training Batch [167/391]: Loss 1.7415301044820808e-05\n",
      "[Epoch 47] Training Batch [168/391]: Loss 1.6845886420924217e-05\n",
      "[Epoch 47] Training Batch [169/391]: Loss 2.391425186942797e-05\n",
      "[Epoch 47] Training Batch [170/391]: Loss 1.4127303984423634e-05\n",
      "[Epoch 47] Training Batch [171/391]: Loss 1.7794112864066847e-05\n",
      "[Epoch 47] Training Batch [172/391]: Loss 1.0244100849376991e-05\n",
      "[Epoch 47] Training Batch [173/391]: Loss 2.0149225747445598e-05\n",
      "[Epoch 47] Training Batch [174/391]: Loss 1.6389838492614217e-05\n",
      "[Epoch 47] Training Batch [175/391]: Loss 1.4230803571990691e-05\n",
      "[Epoch 47] Training Batch [176/391]: Loss 1.998366860789247e-05\n",
      "[Epoch 47] Training Batch [177/391]: Loss 1.599656570760999e-05\n",
      "[Epoch 47] Training Batch [178/391]: Loss 1.3285918612382375e-05\n",
      "[Epoch 47] Training Batch [179/391]: Loss 2.4100703740259632e-05\n",
      "[Epoch 47] Training Batch [180/391]: Loss 1.1072011147916783e-05\n",
      "[Epoch 47] Training Batch [181/391]: Loss 2.146243969036732e-05\n",
      "[Epoch 47] Training Batch [182/391]: Loss 1.668104960117489e-05\n",
      "[Epoch 47] Training Batch [183/391]: Loss 1.1548152542673051e-05\n",
      "[Epoch 47] Training Batch [184/391]: Loss 1.532904389023315e-05\n",
      "[Epoch 47] Training Batch [185/391]: Loss 1.2683454770012759e-05\n",
      "[Epoch 47] Training Batch [186/391]: Loss 1.3641078112414107e-05\n",
      "[Epoch 47] Training Batch [187/391]: Loss 1.6487527318531647e-05\n",
      "[Epoch 47] Training Batch [188/391]: Loss 2.8294174626353197e-05\n",
      "[Epoch 47] Training Batch [189/391]: Loss 1.3795410268357955e-05\n",
      "[Epoch 47] Training Batch [190/391]: Loss 1.2145452274126e-05\n",
      "[Epoch 47] Training Batch [191/391]: Loss 2.549547207308933e-05\n",
      "[Epoch 47] Training Batch [192/391]: Loss 1.59987775987247e-05\n",
      "[Epoch 47] Training Batch [193/391]: Loss 2.1031710275565274e-05\n",
      "[Epoch 47] Training Batch [194/391]: Loss 1.946740121638868e-05\n",
      "[Epoch 47] Training Batch [195/391]: Loss 9.689666512713302e-06\n",
      "[Epoch 47] Training Batch [196/391]: Loss 1.9669203538796864e-05\n",
      "[Epoch 47] Training Batch [197/391]: Loss 1.2915606930619106e-05\n",
      "[Epoch 47] Training Batch [198/391]: Loss 6.9306165642046835e-06\n",
      "[Epoch 47] Training Batch [199/391]: Loss 2.2832497052149847e-05\n",
      "[Epoch 47] Training Batch [200/391]: Loss 1.0703833140723873e-05\n",
      "[Epoch 47] Training Batch [201/391]: Loss 1.717456143524032e-05\n",
      "[Epoch 47] Training Batch [202/391]: Loss 3.492864561849274e-05\n",
      "[Epoch 47] Training Batch [203/391]: Loss 8.331304343300872e-06\n",
      "[Epoch 47] Training Batch [204/391]: Loss 1.1527223250595853e-05\n",
      "[Epoch 47] Training Batch [205/391]: Loss 2.330365896341391e-05\n",
      "[Epoch 47] Training Batch [206/391]: Loss 1.1029754205083009e-05\n",
      "[Epoch 47] Training Batch [207/391]: Loss 1.3732765182794537e-05\n",
      "[Epoch 47] Training Batch [208/391]: Loss 1.160314968728926e-05\n",
      "[Epoch 47] Training Batch [209/391]: Loss 1.3450977348838933e-05\n",
      "[Epoch 47] Training Batch [210/391]: Loss 2.819733344949782e-05\n",
      "[Epoch 47] Training Batch [211/391]: Loss 2.820041663653683e-05\n",
      "[Epoch 47] Training Batch [212/391]: Loss 1.381955462420592e-05\n",
      "[Epoch 47] Training Batch [213/391]: Loss 2.130206485162489e-05\n",
      "[Epoch 47] Training Batch [214/391]: Loss 2.359293466724921e-05\n",
      "[Epoch 47] Training Batch [215/391]: Loss 2.536301690270193e-05\n",
      "[Epoch 47] Training Batch [216/391]: Loss 5.682820756192086e-06\n",
      "[Epoch 47] Training Batch [217/391]: Loss 2.7210124244447798e-05\n",
      "[Epoch 47] Training Batch [218/391]: Loss 2.1551548343268223e-05\n",
      "[Epoch 47] Training Batch [219/391]: Loss 1.8061135051539168e-05\n",
      "[Epoch 47] Training Batch [220/391]: Loss 1.3300001228344627e-05\n",
      "[Epoch 47] Training Batch [221/391]: Loss 2.1698422642657533e-05\n",
      "[Epoch 47] Training Batch [222/391]: Loss 1.9107401385554112e-05\n",
      "[Epoch 47] Training Batch [223/391]: Loss 2.314860284968745e-05\n",
      "[Epoch 47] Training Batch [224/391]: Loss 2.2220849132281728e-05\n",
      "[Epoch 47] Training Batch [225/391]: Loss 1.7623671737965196e-05\n",
      "[Epoch 47] Training Batch [226/391]: Loss 1.7383288650307804e-05\n",
      "[Epoch 47] Training Batch [227/391]: Loss 9.45903047977481e-06\n",
      "[Epoch 47] Training Batch [228/391]: Loss 1.4903319424774963e-05\n",
      "[Epoch 47] Training Batch [229/391]: Loss 2.0553352442220785e-05\n",
      "[Epoch 47] Training Batch [230/391]: Loss 8.619690561317839e-06\n",
      "[Epoch 47] Training Batch [231/391]: Loss 1.3011744158575311e-05\n",
      "[Epoch 47] Training Batch [232/391]: Loss 2.106380088662263e-05\n",
      "[Epoch 47] Training Batch [233/391]: Loss 2.1410603949334472e-05\n",
      "[Epoch 47] Training Batch [234/391]: Loss 2.3793896616552956e-05\n",
      "[Epoch 47] Training Batch [235/391]: Loss 9.903249519993551e-06\n",
      "[Epoch 47] Training Batch [236/391]: Loss 9.40405789151555e-06\n",
      "[Epoch 47] Training Batch [237/391]: Loss 2.625868728500791e-05\n",
      "[Epoch 47] Training Batch [238/391]: Loss 7.408251349261263e-06\n",
      "[Epoch 47] Training Batch [239/391]: Loss 2.718045470828656e-05\n",
      "[Epoch 47] Training Batch [240/391]: Loss 2.872225559258368e-05\n",
      "[Epoch 47] Training Batch [241/391]: Loss 1.174571480078157e-05\n",
      "[Epoch 47] Training Batch [242/391]: Loss 1.9432034605415538e-05\n",
      "[Epoch 47] Training Batch [243/391]: Loss 2.0191713701933622e-05\n",
      "[Epoch 47] Training Batch [244/391]: Loss 1.4791012290515937e-05\n",
      "[Epoch 47] Training Batch [245/391]: Loss 1.4958242900320329e-05\n",
      "[Epoch 47] Training Batch [246/391]: Loss 1.3183733244659379e-05\n",
      "[Epoch 47] Training Batch [247/391]: Loss 1.2914523722429294e-05\n",
      "[Epoch 47] Training Batch [248/391]: Loss 2.028759990935214e-05\n",
      "[Epoch 47] Training Batch [249/391]: Loss 2.0826864783884957e-05\n",
      "[Epoch 47] Training Batch [250/391]: Loss 1.6390797100029886e-05\n",
      "[Epoch 47] Training Batch [251/391]: Loss 1.6032130588428117e-05\n",
      "[Epoch 47] Training Batch [252/391]: Loss 2.9643426387337968e-05\n",
      "[Epoch 47] Training Batch [253/391]: Loss 2.0098745153518394e-05\n",
      "[Epoch 47] Training Batch [254/391]: Loss 1.668094591877889e-05\n",
      "[Epoch 47] Training Batch [255/391]: Loss 1.3235497135610785e-05\n",
      "[Epoch 47] Training Batch [256/391]: Loss 1.1726822776836343e-05\n",
      "[Epoch 47] Training Batch [257/391]: Loss 9.407739526068326e-06\n",
      "[Epoch 47] Training Batch [258/391]: Loss 1.5882760635577142e-05\n",
      "[Epoch 47] Training Batch [259/391]: Loss 2.4838642275426537e-05\n",
      "[Epoch 47] Training Batch [260/391]: Loss 1.1112740139651578e-05\n",
      "[Epoch 47] Training Batch [261/391]: Loss 1.7440046576666646e-05\n",
      "[Epoch 47] Training Batch [262/391]: Loss 9.126378245127853e-06\n",
      "[Epoch 47] Training Batch [263/391]: Loss 1.395663002767833e-05\n",
      "[Epoch 47] Training Batch [264/391]: Loss 9.370486623083707e-06\n",
      "[Epoch 47] Training Batch [265/391]: Loss 1.3801934983348474e-05\n",
      "[Epoch 47] Training Batch [266/391]: Loss 2.068349022010807e-05\n",
      "[Epoch 47] Training Batch [267/391]: Loss 8.25212373456452e-06\n",
      "[Epoch 47] Training Batch [268/391]: Loss 1.216868531628279e-05\n",
      "[Epoch 47] Training Batch [269/391]: Loss 2.6291654648957774e-05\n",
      "[Epoch 47] Training Batch [270/391]: Loss 1.240353230969049e-05\n",
      "[Epoch 47] Training Batch [271/391]: Loss 1.4393901437870227e-05\n",
      "[Epoch 47] Training Batch [272/391]: Loss 3.1637253414373845e-05\n",
      "[Epoch 47] Training Batch [273/391]: Loss 7.95400956121739e-06\n",
      "[Epoch 47] Training Batch [274/391]: Loss 2.1697091142414138e-05\n",
      "[Epoch 47] Training Batch [275/391]: Loss 1.7002690583467484e-05\n",
      "[Epoch 47] Training Batch [276/391]: Loss 2.12520008062711e-05\n",
      "[Epoch 47] Training Batch [277/391]: Loss 8.727914064365905e-06\n",
      "[Epoch 47] Training Batch [278/391]: Loss 2.02493629331002e-05\n",
      "[Epoch 47] Training Batch [279/391]: Loss 1.0964822649839334e-05\n",
      "[Epoch 47] Training Batch [280/391]: Loss 9.08459878701251e-06\n",
      "[Epoch 47] Training Batch [281/391]: Loss 1.7308406313532032e-05\n",
      "[Epoch 47] Training Batch [282/391]: Loss 2.2670375983580016e-05\n",
      "[Epoch 47] Training Batch [283/391]: Loss 1.055781376635423e-05\n",
      "[Epoch 47] Training Batch [284/391]: Loss 2.398344804532826e-05\n",
      "[Epoch 47] Training Batch [285/391]: Loss 1.4434484000958037e-05\n",
      "[Epoch 47] Training Batch [286/391]: Loss 2.0386842152220197e-05\n",
      "[Epoch 47] Training Batch [287/391]: Loss 1.7947435480891727e-05\n",
      "[Epoch 47] Training Batch [288/391]: Loss 1.9028924725716934e-05\n",
      "[Epoch 47] Training Batch [289/391]: Loss 1.5540001186309382e-05\n",
      "[Epoch 47] Training Batch [290/391]: Loss 1.4287837984738871e-05\n",
      "[Epoch 47] Training Batch [291/391]: Loss 1.5916870324872434e-05\n",
      "[Epoch 47] Training Batch [292/391]: Loss 2.1388275854405947e-05\n",
      "[Epoch 47] Training Batch [293/391]: Loss 1.2039967259624973e-05\n",
      "[Epoch 47] Training Batch [294/391]: Loss 5.501876785274362e-06\n",
      "[Epoch 47] Training Batch [295/391]: Loss 1.2044743925798684e-05\n",
      "[Epoch 47] Training Batch [296/391]: Loss 1.7549820768181235e-05\n",
      "[Epoch 47] Training Batch [297/391]: Loss 2.3656928533455357e-05\n",
      "[Epoch 47] Training Batch [298/391]: Loss 2.820102236000821e-05\n",
      "[Epoch 47] Training Batch [299/391]: Loss 8.29577857075492e-06\n",
      "[Epoch 47] Training Batch [300/391]: Loss 1.2423724911059253e-05\n",
      "[Epoch 47] Training Batch [301/391]: Loss 1.772956056811381e-05\n",
      "[Epoch 47] Training Batch [302/391]: Loss 1.672946018516086e-05\n",
      "[Epoch 47] Training Batch [303/391]: Loss 1.1271069524809718e-05\n",
      "[Epoch 47] Training Batch [304/391]: Loss 1.7224439943674952e-05\n",
      "[Epoch 47] Training Batch [305/391]: Loss 1.73516855284106e-05\n",
      "[Epoch 47] Training Batch [306/391]: Loss 6.670818947895896e-06\n",
      "[Epoch 47] Training Batch [307/391]: Loss 1.3876561752113048e-05\n",
      "[Epoch 47] Training Batch [308/391]: Loss 2.124282946169842e-05\n",
      "[Epoch 47] Training Batch [309/391]: Loss 8.290317055070773e-06\n",
      "[Epoch 47] Training Batch [310/391]: Loss 2.8530610507004894e-05\n",
      "[Epoch 47] Training Batch [311/391]: Loss 9.563302228343673e-06\n",
      "[Epoch 47] Training Batch [312/391]: Loss 1.4409522009373177e-05\n",
      "[Epoch 47] Training Batch [313/391]: Loss 1.9451681509963237e-05\n",
      "[Epoch 47] Training Batch [314/391]: Loss 1.5147341400734149e-05\n",
      "[Epoch 47] Training Batch [315/391]: Loss 1.3492754987964872e-05\n",
      "[Epoch 47] Training Batch [316/391]: Loss 1.3952605513622984e-05\n",
      "[Epoch 47] Training Batch [317/391]: Loss 1.1291450391581748e-05\n",
      "[Epoch 47] Training Batch [318/391]: Loss 1.2428444279066753e-05\n",
      "[Epoch 47] Training Batch [319/391]: Loss 1.0845434189832304e-05\n",
      "[Epoch 47] Training Batch [320/391]: Loss 1.092746424546931e-05\n",
      "[Epoch 47] Training Batch [321/391]: Loss 3.323711280245334e-05\n",
      "[Epoch 47] Training Batch [322/391]: Loss 1.881311254692264e-05\n",
      "[Epoch 47] Training Batch [323/391]: Loss 1.8514969269745052e-05\n",
      "[Epoch 47] Training Batch [324/391]: Loss 1.4945715520298108e-05\n",
      "[Epoch 47] Training Batch [325/391]: Loss 1.092438651539851e-05\n",
      "[Epoch 47] Training Batch [326/391]: Loss 1.1679824638122227e-05\n",
      "[Epoch 47] Training Batch [327/391]: Loss 1.3692402717424557e-05\n",
      "[Epoch 47] Training Batch [328/391]: Loss 1.649742989684455e-05\n",
      "[Epoch 47] Training Batch [329/391]: Loss 8.270609214378055e-06\n",
      "[Epoch 47] Training Batch [330/391]: Loss 2.2045378500479273e-05\n",
      "[Epoch 47] Training Batch [331/391]: Loss 2.7432422939455137e-05\n",
      "[Epoch 47] Training Batch [332/391]: Loss 2.1906225811108015e-05\n",
      "[Epoch 47] Training Batch [333/391]: Loss 1.3277451216708869e-05\n",
      "[Epoch 47] Training Batch [334/391]: Loss 1.6698739273124374e-05\n",
      "[Epoch 47] Training Batch [335/391]: Loss 1.475439694331726e-05\n",
      "[Epoch 47] Training Batch [336/391]: Loss 1.636711931496393e-05\n",
      "[Epoch 47] Training Batch [337/391]: Loss 1.6647198208374903e-05\n",
      "[Epoch 47] Training Batch [338/391]: Loss 2.2643920601694845e-05\n",
      "[Epoch 47] Training Batch [339/391]: Loss 1.7402760931872763e-05\n",
      "[Epoch 47] Training Batch [340/391]: Loss 1.2304557458264753e-05\n",
      "[Epoch 47] Training Batch [341/391]: Loss 2.2035395886632614e-05\n",
      "[Epoch 47] Training Batch [342/391]: Loss 1.0196234143222682e-05\n",
      "[Epoch 47] Training Batch [343/391]: Loss 2.1649946575053036e-05\n",
      "[Epoch 47] Training Batch [344/391]: Loss 1.5528505173278973e-05\n",
      "[Epoch 47] Training Batch [345/391]: Loss 1.2833628716180101e-05\n",
      "[Epoch 47] Training Batch [346/391]: Loss 2.9345572329475544e-05\n",
      "[Epoch 47] Training Batch [347/391]: Loss 1.8238080883747898e-05\n",
      "[Epoch 47] Training Batch [348/391]: Loss 1.342585437669186e-05\n",
      "[Epoch 47] Training Batch [349/391]: Loss 1.4502079466183204e-05\n",
      "[Epoch 47] Training Batch [350/391]: Loss 1.4930566976545379e-05\n",
      "[Epoch 47] Training Batch [351/391]: Loss 1.2729378795484081e-05\n",
      "[Epoch 47] Training Batch [352/391]: Loss 1.6259444237221032e-05\n",
      "[Epoch 47] Training Batch [353/391]: Loss 1.472644362365827e-05\n",
      "[Epoch 47] Training Batch [354/391]: Loss 1.3282195141073316e-05\n",
      "[Epoch 47] Training Batch [355/391]: Loss 8.37330208014464e-06\n",
      "[Epoch 47] Training Batch [356/391]: Loss 1.2615709238161799e-05\n",
      "[Epoch 47] Training Batch [357/391]: Loss 9.382616553921252e-06\n",
      "[Epoch 47] Training Batch [358/391]: Loss 1.6599449736531824e-05\n",
      "[Epoch 47] Training Batch [359/391]: Loss 9.882768608804327e-06\n",
      "[Epoch 47] Training Batch [360/391]: Loss 9.900206350721419e-06\n",
      "[Epoch 47] Training Batch [361/391]: Loss 1.0400442079117056e-05\n",
      "[Epoch 47] Training Batch [362/391]: Loss 1.1885602361871861e-05\n",
      "[Epoch 47] Training Batch [363/391]: Loss 1.2520637028501369e-05\n",
      "[Epoch 47] Training Batch [364/391]: Loss 1.646285818424076e-05\n",
      "[Epoch 47] Training Batch [365/391]: Loss 1.728606002870947e-05\n",
      "[Epoch 47] Training Batch [366/391]: Loss 1.4961837223381735e-05\n",
      "[Epoch 47] Training Batch [367/391]: Loss 1.7589280105312355e-05\n",
      "[Epoch 47] Training Batch [368/391]: Loss 1.6372370737371966e-05\n",
      "[Epoch 47] Training Batch [369/391]: Loss 7.267757609952241e-06\n",
      "[Epoch 47] Training Batch [370/391]: Loss 1.9832124962704256e-05\n",
      "[Epoch 47] Training Batch [371/391]: Loss 1.535768387839198e-05\n",
      "[Epoch 47] Training Batch [372/391]: Loss 1.4619718058384024e-05\n",
      "[Epoch 47] Training Batch [373/391]: Loss 1.5332456314354204e-05\n",
      "[Epoch 47] Training Batch [374/391]: Loss 1.9761551811825484e-05\n",
      "[Epoch 47] Training Batch [375/391]: Loss 1.97040535567794e-05\n",
      "[Epoch 47] Training Batch [376/391]: Loss 1.540440644021146e-05\n",
      "[Epoch 47] Training Batch [377/391]: Loss 1.8499591533327475e-05\n",
      "[Epoch 47] Training Batch [378/391]: Loss 1.6858326489455067e-05\n",
      "[Epoch 47] Training Batch [379/391]: Loss 1.4178650417306926e-05\n",
      "[Epoch 47] Training Batch [380/391]: Loss 1.5297506251954474e-05\n",
      "[Epoch 47] Training Batch [381/391]: Loss 1.5478712157346308e-05\n",
      "[Epoch 47] Training Batch [382/391]: Loss 1.445260932086967e-05\n",
      "[Epoch 47] Training Batch [383/391]: Loss 1.367689128528582e-05\n",
      "[Epoch 47] Training Batch [384/391]: Loss 2.1178499082452618e-05\n",
      "[Epoch 47] Training Batch [385/391]: Loss 9.826058885664679e-06\n",
      "[Epoch 47] Training Batch [386/391]: Loss 1.909936690935865e-05\n",
      "[Epoch 47] Training Batch [387/391]: Loss 1.4333191757032182e-05\n",
      "[Epoch 47] Training Batch [388/391]: Loss 8.227901162172202e-06\n",
      "[Epoch 47] Training Batch [389/391]: Loss 9.528745067655109e-06\n",
      "[Epoch 47] Training Batch [390/391]: Loss 1.4175300748320296e-05\n",
      "[Epoch 47] Training Batch [391/391]: Loss 9.724224582896568e-06\n",
      "Epoch 47 - Train Loss: 0.0000\n",
      "*********  Epoch 48/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48] Training Batch [1/391]: Loss 2.062878957076464e-05\n",
      "[Epoch 48] Training Batch [2/391]: Loss 2.3768463506712578e-05\n",
      "[Epoch 48] Training Batch [3/391]: Loss 2.075521115330048e-05\n",
      "[Epoch 48] Training Batch [4/391]: Loss 1.4816871043876745e-05\n",
      "[Epoch 48] Training Batch [5/391]: Loss 1.0180569915974047e-05\n",
      "[Epoch 48] Training Batch [6/391]: Loss 1.4388748240889981e-05\n",
      "[Epoch 48] Training Batch [7/391]: Loss 9.779267202247865e-06\n",
      "[Epoch 48] Training Batch [8/391]: Loss 1.4141174688120373e-05\n",
      "[Epoch 48] Training Batch [9/391]: Loss 1.158663599198917e-05\n",
      "[Epoch 48] Training Batch [10/391]: Loss 7.301804998860462e-06\n",
      "[Epoch 48] Training Batch [11/391]: Loss 2.035932629951276e-05\n",
      "[Epoch 48] Training Batch [12/391]: Loss 1.0221612683380954e-05\n",
      "[Epoch 48] Training Batch [13/391]: Loss 1.9522745787980966e-05\n",
      "[Epoch 48] Training Batch [14/391]: Loss 2.196447348978836e-05\n",
      "[Epoch 48] Training Batch [15/391]: Loss 1.87650766747538e-05\n",
      "[Epoch 48] Training Batch [16/391]: Loss 1.4754475159861613e-05\n",
      "[Epoch 48] Training Batch [17/391]: Loss 1.195849745272426e-05\n",
      "[Epoch 48] Training Batch [18/391]: Loss 1.594674904481508e-05\n",
      "[Epoch 48] Training Batch [19/391]: Loss 1.3936234608991072e-05\n",
      "[Epoch 48] Training Batch [20/391]: Loss 2.27294640353648e-05\n",
      "[Epoch 48] Training Batch [21/391]: Loss 1.6543124729651026e-05\n",
      "[Epoch 48] Training Batch [22/391]: Loss 1.6008583770599216e-05\n",
      "[Epoch 48] Training Batch [23/391]: Loss 2.5662780899438076e-05\n",
      "[Epoch 48] Training Batch [24/391]: Loss 1.7851116353995167e-05\n",
      "[Epoch 48] Training Batch [25/391]: Loss 1.4751425624126568e-05\n",
      "[Epoch 48] Training Batch [26/391]: Loss 1.8089132936438546e-05\n",
      "[Epoch 48] Training Batch [27/391]: Loss 1.9434442947385833e-05\n",
      "[Epoch 48] Training Batch [28/391]: Loss 1.650886952120345e-05\n",
      "[Epoch 48] Training Batch [29/391]: Loss 2.3226688426802866e-05\n",
      "[Epoch 48] Training Batch [30/391]: Loss 1.6385216440539807e-05\n",
      "[Epoch 48] Training Batch [31/391]: Loss 2.133467569365166e-05\n",
      "[Epoch 48] Training Batch [32/391]: Loss 1.7588516129762866e-05\n",
      "[Epoch 48] Training Batch [33/391]: Loss 2.2073338186601177e-05\n",
      "[Epoch 48] Training Batch [34/391]: Loss 1.3195008250477258e-05\n",
      "[Epoch 48] Training Batch [35/391]: Loss 1.3563268112193327e-05\n",
      "[Epoch 48] Training Batch [36/391]: Loss 1.3069361557427328e-05\n",
      "[Epoch 48] Training Batch [37/391]: Loss 1.8176342564402148e-05\n",
      "[Epoch 48] Training Batch [38/391]: Loss 1.4767218999622855e-05\n",
      "[Epoch 48] Training Batch [39/391]: Loss 2.2093876395956613e-05\n",
      "[Epoch 48] Training Batch [40/391]: Loss 1.778591104084626e-05\n",
      "[Epoch 48] Training Batch [41/391]: Loss 2.115726238116622e-05\n",
      "[Epoch 48] Training Batch [42/391]: Loss 2.1154808564460836e-05\n",
      "[Epoch 48] Training Batch [43/391]: Loss 1.8731145246420056e-05\n",
      "[Epoch 48] Training Batch [44/391]: Loss 2.181354648200795e-05\n",
      "[Epoch 48] Training Batch [45/391]: Loss 1.3660437616636045e-05\n",
      "[Epoch 48] Training Batch [46/391]: Loss 1.3926316569268238e-05\n",
      "[Epoch 48] Training Batch [47/391]: Loss 1.6576113921473734e-05\n",
      "[Epoch 48] Training Batch [48/391]: Loss 1.9062352293985896e-05\n",
      "[Epoch 48] Training Batch [49/391]: Loss 1.0967211892420892e-05\n",
      "[Epoch 48] Training Batch [50/391]: Loss 9.880330253508873e-06\n",
      "[Epoch 48] Training Batch [51/391]: Loss 2.3875496481196024e-05\n",
      "[Epoch 48] Training Batch [52/391]: Loss 1.987409450521227e-05\n",
      "[Epoch 48] Training Batch [53/391]: Loss 1.7805034076445736e-05\n",
      "[Epoch 48] Training Batch [54/391]: Loss 1.3583788131654728e-05\n",
      "[Epoch 48] Training Batch [55/391]: Loss 1.0754336472018622e-05\n",
      "[Epoch 48] Training Batch [56/391]: Loss 2.1048055714345537e-05\n",
      "[Epoch 48] Training Batch [57/391]: Loss 1.3860830222256482e-05\n",
      "[Epoch 48] Training Batch [58/391]: Loss 1.032870750350412e-05\n",
      "[Epoch 48] Training Batch [59/391]: Loss 1.0634314094204456e-05\n",
      "[Epoch 48] Training Batch [60/391]: Loss 1.2355271792330313e-05\n",
      "[Epoch 48] Training Batch [61/391]: Loss 1.932329723786097e-05\n",
      "[Epoch 48] Training Batch [62/391]: Loss 1.8316986825084314e-05\n",
      "[Epoch 48] Training Batch [63/391]: Loss 2.2964935851632617e-05\n",
      "[Epoch 48] Training Batch [64/391]: Loss 2.3521555704064667e-05\n",
      "[Epoch 48] Training Batch [65/391]: Loss 1.1907057341886684e-05\n",
      "[Epoch 48] Training Batch [66/391]: Loss 1.8945347619592212e-05\n",
      "[Epoch 48] Training Batch [67/391]: Loss 1.8761991668725386e-05\n",
      "[Epoch 48] Training Batch [68/391]: Loss 1.0405054126749747e-05\n",
      "[Epoch 48] Training Batch [69/391]: Loss 1.9018913008039817e-05\n",
      "[Epoch 48] Training Batch [70/391]: Loss 2.2664307834929787e-05\n",
      "[Epoch 48] Training Batch [71/391]: Loss 2.3305125068873167e-05\n",
      "[Epoch 48] Training Batch [72/391]: Loss 9.994482752517797e-06\n",
      "[Epoch 48] Training Batch [73/391]: Loss 1.0020598892879207e-05\n",
      "[Epoch 48] Training Batch [74/391]: Loss 1.781469654815737e-05\n",
      "[Epoch 48] Training Batch [75/391]: Loss 1.7953601854969747e-05\n",
      "[Epoch 48] Training Batch [76/391]: Loss 1.7865526388050057e-05\n",
      "[Epoch 48] Training Batch [77/391]: Loss 1.2711706403933931e-05\n",
      "[Epoch 48] Training Batch [78/391]: Loss 1.0689615010051057e-05\n",
      "[Epoch 48] Training Batch [79/391]: Loss 1.0608944648993202e-05\n",
      "[Epoch 48] Training Batch [80/391]: Loss 1.1314121366012841e-05\n",
      "[Epoch 48] Training Batch [81/391]: Loss 9.264210348192137e-06\n",
      "[Epoch 48] Training Batch [82/391]: Loss 1.1577217264857609e-05\n",
      "[Epoch 48] Training Batch [83/391]: Loss 2.1422643840196542e-05\n",
      "[Epoch 48] Training Batch [84/391]: Loss 1.921899092849344e-05\n",
      "[Epoch 48] Training Batch [85/391]: Loss 1.7227195712621324e-05\n",
      "[Epoch 48] Training Batch [86/391]: Loss 1.522910224593943e-05\n",
      "[Epoch 48] Training Batch [87/391]: Loss 2.0971034246031195e-05\n",
      "[Epoch 48] Training Batch [88/391]: Loss 1.5211133359116502e-05\n",
      "[Epoch 48] Training Batch [89/391]: Loss 9.471628800383769e-06\n",
      "[Epoch 48] Training Batch [90/391]: Loss 1.3143563592166174e-05\n",
      "[Epoch 48] Training Batch [91/391]: Loss 6.367034984577913e-06\n",
      "[Epoch 48] Training Batch [92/391]: Loss 9.04373882804066e-06\n",
      "[Epoch 48] Training Batch [93/391]: Loss 2.0936622604494914e-05\n",
      "[Epoch 48] Training Batch [94/391]: Loss 1.2662660083151422e-05\n",
      "[Epoch 48] Training Batch [95/391]: Loss 8.410395821556449e-06\n",
      "[Epoch 48] Training Batch [96/391]: Loss 2.4307402782142162e-05\n",
      "[Epoch 48] Training Batch [97/391]: Loss 1.568510379001964e-05\n",
      "[Epoch 48] Training Batch [98/391]: Loss 2.1550267774728127e-05\n",
      "[Epoch 48] Training Batch [99/391]: Loss 1.5824431102373637e-05\n",
      "[Epoch 48] Training Batch [100/391]: Loss 1.4833312889095396e-05\n",
      "[Epoch 48] Training Batch [101/391]: Loss 1.762922875059303e-05\n",
      "[Epoch 48] Training Batch [102/391]: Loss 1.700751090538688e-05\n",
      "[Epoch 48] Training Batch [103/391]: Loss 1.6549125575693324e-05\n",
      "[Epoch 48] Training Batch [104/391]: Loss 1.501888436905574e-05\n",
      "[Epoch 48] Training Batch [105/391]: Loss 1.566332866786979e-05\n",
      "[Epoch 48] Training Batch [106/391]: Loss 1.282193716178881e-05\n",
      "[Epoch 48] Training Batch [107/391]: Loss 9.437377229915e-06\n",
      "[Epoch 48] Training Batch [108/391]: Loss 1.7604834283702075e-05\n",
      "[Epoch 48] Training Batch [109/391]: Loss 1.8170385374105535e-05\n",
      "[Epoch 48] Training Batch [110/391]: Loss 1.5393225112347864e-05\n",
      "[Epoch 48] Training Batch [111/391]: Loss 1.9570867152651772e-05\n",
      "[Epoch 48] Training Batch [112/391]: Loss 1.612551204743795e-05\n",
      "[Epoch 48] Training Batch [113/391]: Loss 8.685033208166715e-06\n",
      "[Epoch 48] Training Batch [114/391]: Loss 2.0095812942599878e-05\n",
      "[Epoch 48] Training Batch [115/391]: Loss 1.1697877198457718e-05\n",
      "[Epoch 48] Training Batch [116/391]: Loss 1.5175170119618997e-05\n",
      "[Epoch 48] Training Batch [117/391]: Loss 1.692080513748806e-05\n",
      "[Epoch 48] Training Batch [118/391]: Loss 1.541228630230762e-05\n",
      "[Epoch 48] Training Batch [119/391]: Loss 1.4332921637105756e-05\n",
      "[Epoch 48] Training Batch [120/391]: Loss 4.457227987586521e-06\n",
      "[Epoch 48] Training Batch [121/391]: Loss 1.3869886061002035e-05\n",
      "[Epoch 48] Training Batch [122/391]: Loss 2.1680791178368963e-05\n",
      "[Epoch 48] Training Batch [123/391]: Loss 4.0763384276942816e-06\n",
      "[Epoch 48] Training Batch [124/391]: Loss 1.832594716688618e-05\n",
      "[Epoch 48] Training Batch [125/391]: Loss 1.0185261089645792e-05\n",
      "[Epoch 48] Training Batch [126/391]: Loss 1.0513062079553492e-05\n",
      "[Epoch 48] Training Batch [127/391]: Loss 1.5485524272662587e-05\n",
      "[Epoch 48] Training Batch [128/391]: Loss 1.8322611140320078e-05\n",
      "[Epoch 48] Training Batch [129/391]: Loss 1.1072233064624015e-05\n",
      "[Epoch 48] Training Batch [130/391]: Loss 2.3864005925133824e-05\n",
      "[Epoch 48] Training Batch [131/391]: Loss 2.531894097046461e-05\n",
      "[Epoch 48] Training Batch [132/391]: Loss 1.3868077076040208e-05\n",
      "[Epoch 48] Training Batch [133/391]: Loss 1.3021651284361724e-05\n",
      "[Epoch 48] Training Batch [134/391]: Loss 2.0607616534107365e-05\n",
      "[Epoch 48] Training Batch [135/391]: Loss 2.1120011297171004e-05\n",
      "[Epoch 48] Training Batch [136/391]: Loss 1.4143650332698599e-05\n",
      "[Epoch 48] Training Batch [137/391]: Loss 1.0309760000382084e-05\n",
      "[Epoch 48] Training Batch [138/391]: Loss 7.574246865260648e-06\n",
      "[Epoch 48] Training Batch [139/391]: Loss 2.001793109229766e-05\n",
      "[Epoch 48] Training Batch [140/391]: Loss 1.2799663636542391e-05\n",
      "[Epoch 48] Training Batch [141/391]: Loss 1.1512163837323897e-05\n",
      "[Epoch 48] Training Batch [142/391]: Loss 1.6951202269410715e-05\n",
      "[Epoch 48] Training Batch [143/391]: Loss 1.3448839126795065e-05\n",
      "[Epoch 48] Training Batch [144/391]: Loss 8.650240488350391e-06\n",
      "[Epoch 48] Training Batch [145/391]: Loss 1.3213017155067064e-05\n",
      "[Epoch 48] Training Batch [146/391]: Loss 2.7937876438954845e-05\n",
      "[Epoch 48] Training Batch [147/391]: Loss 7.440920398948947e-06\n",
      "[Epoch 48] Training Batch [148/391]: Loss 1.0462750651640818e-05\n",
      "[Epoch 48] Training Batch [149/391]: Loss 1.4749992260476574e-05\n",
      "[Epoch 48] Training Batch [150/391]: Loss 1.622012678126339e-05\n",
      "[Epoch 48] Training Batch [151/391]: Loss 3.373972504050471e-05\n",
      "[Epoch 48] Training Batch [152/391]: Loss 1.4591413673770148e-05\n",
      "[Epoch 48] Training Batch [153/391]: Loss 2.2326965336105786e-05\n",
      "[Epoch 48] Training Batch [154/391]: Loss 2.4817014491418377e-05\n",
      "[Epoch 48] Training Batch [155/391]: Loss 2.237553235318046e-05\n",
      "[Epoch 48] Training Batch [156/391]: Loss 1.3450898222799879e-05\n",
      "[Epoch 48] Training Batch [157/391]: Loss 7.529452432208927e-06\n",
      "[Epoch 48] Training Batch [158/391]: Loss 1.9665916624944657e-05\n",
      "[Epoch 48] Training Batch [159/391]: Loss 1.2872818842879497e-05\n",
      "[Epoch 48] Training Batch [160/391]: Loss 1.5390280168503523e-05\n",
      "[Epoch 48] Training Batch [161/391]: Loss 1.03479342214996e-05\n",
      "[Epoch 48] Training Batch [162/391]: Loss 1.8420092601445504e-05\n",
      "[Epoch 48] Training Batch [163/391]: Loss 1.3811416465614457e-05\n",
      "[Epoch 48] Training Batch [164/391]: Loss 1.1389390238036867e-05\n",
      "[Epoch 48] Training Batch [165/391]: Loss 1.4960287444409914e-05\n",
      "[Epoch 48] Training Batch [166/391]: Loss 1.2242466254974715e-05\n",
      "[Epoch 48] Training Batch [167/391]: Loss 2.5271754566347227e-05\n",
      "[Epoch 48] Training Batch [168/391]: Loss 1.3342696547624655e-05\n",
      "[Epoch 48] Training Batch [169/391]: Loss 1.2333398444752675e-05\n",
      "[Epoch 48] Training Batch [170/391]: Loss 1.6666252122377045e-05\n",
      "[Epoch 48] Training Batch [171/391]: Loss 1.2878997949883342e-05\n",
      "[Epoch 48] Training Batch [172/391]: Loss 1.3586863133241422e-05\n",
      "[Epoch 48] Training Batch [173/391]: Loss 1.2563719792524353e-05\n",
      "[Epoch 48] Training Batch [174/391]: Loss 8.802442607702687e-06\n",
      "[Epoch 48] Training Batch [175/391]: Loss 1.880563831946347e-05\n",
      "[Epoch 48] Training Batch [176/391]: Loss 1.9428551240707748e-05\n",
      "[Epoch 48] Training Batch [177/391]: Loss 1.2777683878084645e-05\n",
      "[Epoch 48] Training Batch [178/391]: Loss 1.1222721695958171e-05\n",
      "[Epoch 48] Training Batch [179/391]: Loss 1.1864599400723819e-05\n",
      "[Epoch 48] Training Batch [180/391]: Loss 1.5075442206580192e-05\n",
      "[Epoch 48] Training Batch [181/391]: Loss 1.2371315278869588e-05\n",
      "[Epoch 48] Training Batch [182/391]: Loss 3.496754652587697e-05\n",
      "[Epoch 48] Training Batch [183/391]: Loss 1.1290917427686509e-05\n",
      "[Epoch 48] Training Batch [184/391]: Loss 1.3881008271710016e-05\n",
      "[Epoch 48] Training Batch [185/391]: Loss 1.77796227944782e-05\n",
      "[Epoch 48] Training Batch [186/391]: Loss 1.0726466825872194e-05\n",
      "[Epoch 48] Training Batch [187/391]: Loss 2.1651556380675174e-05\n",
      "[Epoch 48] Training Batch [188/391]: Loss 1.2613348189915996e-05\n",
      "[Epoch 48] Training Batch [189/391]: Loss 2.0632722225855105e-05\n",
      "[Epoch 48] Training Batch [190/391]: Loss 1.5575646102661267e-05\n",
      "[Epoch 48] Training Batch [191/391]: Loss 1.202265229949262e-05\n",
      "[Epoch 48] Training Batch [192/391]: Loss 1.8843935322365724e-05\n",
      "[Epoch 48] Training Batch [193/391]: Loss 8.65415859152563e-06\n",
      "[Epoch 48] Training Batch [194/391]: Loss 1.4893275874783285e-05\n",
      "[Epoch 48] Training Batch [195/391]: Loss 1.717726445349399e-05\n",
      "[Epoch 48] Training Batch [196/391]: Loss 1.204290038003819e-05\n",
      "[Epoch 48] Training Batch [197/391]: Loss 9.78763091552537e-06\n",
      "[Epoch 48] Training Batch [198/391]: Loss 1.219779187522363e-05\n",
      "[Epoch 48] Training Batch [199/391]: Loss 2.70570999418851e-05\n",
      "[Epoch 48] Training Batch [200/391]: Loss 1.6050975318648852e-05\n",
      "[Epoch 48] Training Batch [201/391]: Loss 1.6250038243015297e-05\n",
      "[Epoch 48] Training Batch [202/391]: Loss 8.672858712088782e-06\n",
      "[Epoch 48] Training Batch [203/391]: Loss 1.3544060493586585e-05\n",
      "[Epoch 48] Training Batch [204/391]: Loss 1.4892708350089379e-05\n",
      "[Epoch 48] Training Batch [205/391]: Loss 1.6486394088133238e-05\n",
      "[Epoch 48] Training Batch [206/391]: Loss 1.8086044292431325e-05\n",
      "[Epoch 48] Training Batch [207/391]: Loss 1.3313354429556057e-05\n",
      "[Epoch 48] Training Batch [208/391]: Loss 2.5462730263825506e-05\n",
      "[Epoch 48] Training Batch [209/391]: Loss 1.5139476090553217e-05\n",
      "[Epoch 48] Training Batch [210/391]: Loss 1.7166159523185343e-05\n",
      "[Epoch 48] Training Batch [211/391]: Loss 1.5933701433823444e-05\n",
      "[Epoch 48] Training Batch [212/391]: Loss 1.868498293333687e-05\n",
      "[Epoch 48] Training Batch [213/391]: Loss 1.010522373690037e-05\n",
      "[Epoch 48] Training Batch [214/391]: Loss 1.651293678150978e-05\n",
      "[Epoch 48] Training Batch [215/391]: Loss 1.1404988981666975e-05\n",
      "[Epoch 48] Training Batch [216/391]: Loss 2.817684980982449e-05\n",
      "[Epoch 48] Training Batch [217/391]: Loss 1.3068411135463975e-05\n",
      "[Epoch 48] Training Batch [218/391]: Loss 1.5320279999286868e-05\n",
      "[Epoch 48] Training Batch [219/391]: Loss 1.7887188732856885e-05\n",
      "[Epoch 48] Training Batch [220/391]: Loss 1.8285283658769913e-05\n",
      "[Epoch 48] Training Batch [221/391]: Loss 9.671191037341487e-06\n",
      "[Epoch 48] Training Batch [222/391]: Loss 1.4243505574995652e-05\n",
      "[Epoch 48] Training Batch [223/391]: Loss 7.536790235462831e-06\n",
      "[Epoch 48] Training Batch [224/391]: Loss 8.185803380911238e-06\n",
      "[Epoch 48] Training Batch [225/391]: Loss 1.8606020603328943e-05\n",
      "[Epoch 48] Training Batch [226/391]: Loss 1.781481842044741e-05\n",
      "[Epoch 48] Training Batch [227/391]: Loss 1.565659295010846e-05\n",
      "[Epoch 48] Training Batch [228/391]: Loss 1.9606563000706956e-05\n",
      "[Epoch 48] Training Batch [229/391]: Loss 1.45534786497592e-05\n",
      "[Epoch 48] Training Batch [230/391]: Loss 1.390964553138474e-05\n",
      "[Epoch 48] Training Batch [231/391]: Loss 1.534202601760626e-05\n",
      "[Epoch 48] Training Batch [232/391]: Loss 1.1138192348880693e-05\n",
      "[Epoch 48] Training Batch [233/391]: Loss 9.550100003252737e-06\n",
      "[Epoch 48] Training Batch [234/391]: Loss 1.053816140483832e-05\n",
      "[Epoch 48] Training Batch [235/391]: Loss 6.244283667911077e-06\n",
      "[Epoch 48] Training Batch [236/391]: Loss 2.0458310245885514e-05\n",
      "[Epoch 48] Training Batch [237/391]: Loss 2.1642757928930223e-05\n",
      "[Epoch 48] Training Batch [238/391]: Loss 1.4510750588669907e-05\n",
      "[Epoch 48] Training Batch [239/391]: Loss 1.569434789416846e-05\n",
      "[Epoch 48] Training Batch [240/391]: Loss 2.0541872800095007e-05\n",
      "[Epoch 48] Training Batch [241/391]: Loss 1.3643756574310828e-05\n",
      "[Epoch 48] Training Batch [242/391]: Loss 2.193628461100161e-05\n",
      "[Epoch 48] Training Batch [243/391]: Loss 1.4426905181608163e-05\n",
      "[Epoch 48] Training Batch [244/391]: Loss 8.63573768583592e-06\n",
      "[Epoch 48] Training Batch [245/391]: Loss 1.0622930858517066e-05\n",
      "[Epoch 48] Training Batch [246/391]: Loss 1.1910908142453991e-05\n",
      "[Epoch 48] Training Batch [247/391]: Loss 1.2531226275314111e-05\n",
      "[Epoch 48] Training Batch [248/391]: Loss 9.268938811146654e-06\n",
      "[Epoch 48] Training Batch [249/391]: Loss 2.4164437490981072e-05\n",
      "[Epoch 48] Training Batch [250/391]: Loss 1.6506168321939185e-05\n",
      "[Epoch 48] Training Batch [251/391]: Loss 1.7106280211010017e-05\n",
      "[Epoch 48] Training Batch [252/391]: Loss 1.5291127056116238e-05\n",
      "[Epoch 48] Training Batch [253/391]: Loss 1.6496986063430086e-05\n",
      "[Epoch 48] Training Batch [254/391]: Loss 1.4271166946855374e-05\n",
      "[Epoch 48] Training Batch [255/391]: Loss 1.2016734217468183e-05\n",
      "[Epoch 48] Training Batch [256/391]: Loss 1.766997047525365e-05\n",
      "[Epoch 48] Training Batch [257/391]: Loss 1.897634138003923e-05\n",
      "[Epoch 48] Training Batch [258/391]: Loss 1.2351274563116021e-05\n",
      "[Epoch 48] Training Batch [259/391]: Loss 1.1937601811951026e-05\n",
      "[Epoch 48] Training Batch [260/391]: Loss 6.910176125529688e-06\n",
      "[Epoch 48] Training Batch [261/391]: Loss 1.3964210666017607e-05\n",
      "[Epoch 48] Training Batch [262/391]: Loss 2.2803122192271985e-05\n",
      "[Epoch 48] Training Batch [263/391]: Loss 1.313174198003253e-05\n",
      "[Epoch 48] Training Batch [264/391]: Loss 1.3721962204726879e-05\n",
      "[Epoch 48] Training Batch [265/391]: Loss 9.196302926284261e-06\n",
      "[Epoch 48] Training Batch [266/391]: Loss 1.152997629105812e-05\n",
      "[Epoch 48] Training Batch [267/391]: Loss 1.3264490007713903e-05\n",
      "[Epoch 48] Training Batch [268/391]: Loss 1.1843966603919398e-05\n",
      "[Epoch 48] Training Batch [269/391]: Loss 1.5050373804115225e-05\n",
      "[Epoch 48] Training Batch [270/391]: Loss 1.1284942956990562e-05\n",
      "[Epoch 48] Training Batch [271/391]: Loss 1.4639171240560245e-05\n",
      "[Epoch 48] Training Batch [272/391]: Loss 1.2059974324074574e-05\n",
      "[Epoch 48] Training Batch [273/391]: Loss 1.818415694287978e-05\n",
      "[Epoch 48] Training Batch [274/391]: Loss 9.672221494838595e-06\n",
      "[Epoch 48] Training Batch [275/391]: Loss 1.628241443540901e-05\n",
      "[Epoch 48] Training Batch [276/391]: Loss 1.5653224181733094e-05\n",
      "[Epoch 48] Training Batch [277/391]: Loss 1.8999777239514515e-05\n",
      "[Epoch 48] Training Batch [278/391]: Loss 9.724320989334956e-06\n",
      "[Epoch 48] Training Batch [279/391]: Loss 1.869645348051563e-05\n",
      "[Epoch 48] Training Batch [280/391]: Loss 1.4250664207793307e-05\n",
      "[Epoch 48] Training Batch [281/391]: Loss 1.3709782251680735e-05\n",
      "[Epoch 48] Training Batch [282/391]: Loss 1.5758572772028856e-05\n",
      "[Epoch 48] Training Batch [283/391]: Loss 9.279821824748069e-06\n",
      "[Epoch 48] Training Batch [284/391]: Loss 1.6181824321392924e-05\n",
      "[Epoch 48] Training Batch [285/391]: Loss 1.6913490981096402e-05\n",
      "[Epoch 48] Training Batch [286/391]: Loss 1.670529309194535e-05\n",
      "[Epoch 48] Training Batch [287/391]: Loss 1.5955190974636935e-05\n",
      "[Epoch 48] Training Batch [288/391]: Loss 2.5559238565620035e-05\n",
      "[Epoch 48] Training Batch [289/391]: Loss 1.1036615433113184e-05\n",
      "[Epoch 48] Training Batch [290/391]: Loss 1.7064323401427828e-05\n",
      "[Epoch 48] Training Batch [291/391]: Loss 1.4483901395578869e-05\n",
      "[Epoch 48] Training Batch [292/391]: Loss 2.1900177671341226e-05\n",
      "[Epoch 48] Training Batch [293/391]: Loss 1.3330467481864616e-05\n",
      "[Epoch 48] Training Batch [294/391]: Loss 1.8762972104013897e-05\n",
      "[Epoch 48] Training Batch [295/391]: Loss 1.6277015674859285e-05\n",
      "[Epoch 48] Training Batch [296/391]: Loss 1.4828008715994656e-05\n",
      "[Epoch 48] Training Batch [297/391]: Loss 1.485725624661427e-05\n",
      "[Epoch 48] Training Batch [298/391]: Loss 1.5741910829092376e-05\n",
      "[Epoch 48] Training Batch [299/391]: Loss 1.665042327658739e-05\n",
      "[Epoch 48] Training Batch [300/391]: Loss 2.7864376534125768e-05\n",
      "[Epoch 48] Training Batch [301/391]: Loss 1.7927064618561417e-05\n",
      "[Epoch 48] Training Batch [302/391]: Loss 2.0335923181846738e-05\n",
      "[Epoch 48] Training Batch [303/391]: Loss 1.5314082702388987e-05\n",
      "[Epoch 48] Training Batch [304/391]: Loss 1.7786274838726968e-05\n",
      "[Epoch 48] Training Batch [305/391]: Loss 1.689165219431743e-05\n",
      "[Epoch 48] Training Batch [306/391]: Loss 1.4500175893772393e-05\n",
      "[Epoch 48] Training Batch [307/391]: Loss 8.100281775114127e-06\n",
      "[Epoch 48] Training Batch [308/391]: Loss 1.1918810741917696e-05\n",
      "[Epoch 48] Training Batch [309/391]: Loss 1.936191438289825e-05\n",
      "[Epoch 48] Training Batch [310/391]: Loss 1.904661075968761e-05\n",
      "[Epoch 48] Training Batch [311/391]: Loss 5.131430498295231e-06\n",
      "[Epoch 48] Training Batch [312/391]: Loss 1.740205152600538e-05\n",
      "[Epoch 48] Training Batch [313/391]: Loss 1.9210821847082116e-05\n",
      "[Epoch 48] Training Batch [314/391]: Loss 1.2656973922275938e-05\n",
      "[Epoch 48] Training Batch [315/391]: Loss 1.4209569599188399e-05\n",
      "[Epoch 48] Training Batch [316/391]: Loss 1.746674024616368e-05\n",
      "[Epoch 48] Training Batch [317/391]: Loss 1.6725087334634736e-05\n",
      "[Epoch 48] Training Batch [318/391]: Loss 1.246001193067059e-05\n",
      "[Epoch 48] Training Batch [319/391]: Loss 1.1228047696931753e-05\n",
      "[Epoch 48] Training Batch [320/391]: Loss 1.0958250641124323e-05\n",
      "[Epoch 48] Training Batch [321/391]: Loss 8.888798220141325e-06\n",
      "[Epoch 48] Training Batch [322/391]: Loss 1.5406585589516908e-05\n",
      "[Epoch 48] Training Batch [323/391]: Loss 1.530120243842248e-05\n",
      "[Epoch 48] Training Batch [324/391]: Loss 1.7072579794330522e-05\n",
      "[Epoch 48] Training Batch [325/391]: Loss 1.614030043128878e-05\n",
      "[Epoch 48] Training Batch [326/391]: Loss 9.518582373857498e-06\n",
      "[Epoch 48] Training Batch [327/391]: Loss 1.2274928849365097e-05\n",
      "[Epoch 48] Training Batch [328/391]: Loss 1.1918072232219856e-05\n",
      "[Epoch 48] Training Batch [329/391]: Loss 1.0837227819138207e-05\n",
      "[Epoch 48] Training Batch [330/391]: Loss 1.7566359019838274e-05\n",
      "[Epoch 48] Training Batch [331/391]: Loss 2.404367842245847e-05\n",
      "[Epoch 48] Training Batch [332/391]: Loss 1.630045153433457e-05\n",
      "[Epoch 48] Training Batch [333/391]: Loss 1.712507219053805e-05\n",
      "[Epoch 48] Training Batch [334/391]: Loss 1.6392621546401642e-05\n",
      "[Epoch 48] Training Batch [335/391]: Loss 1.5423845979967155e-05\n",
      "[Epoch 48] Training Batch [336/391]: Loss 2.096716889354866e-05\n",
      "[Epoch 48] Training Batch [337/391]: Loss 1.9799281290033832e-05\n",
      "[Epoch 48] Training Batch [338/391]: Loss 2.0574481823132373e-05\n",
      "[Epoch 48] Training Batch [339/391]: Loss 8.513879947713576e-06\n",
      "[Epoch 48] Training Batch [340/391]: Loss 9.605713785276748e-06\n",
      "[Epoch 48] Training Batch [341/391]: Loss 1.4523678146360908e-05\n",
      "[Epoch 48] Training Batch [342/391]: Loss 1.8170519979321398e-05\n",
      "[Epoch 48] Training Batch [343/391]: Loss 1.0091207514051348e-05\n",
      "[Epoch 48] Training Batch [344/391]: Loss 1.2172328752058093e-05\n",
      "[Epoch 48] Training Batch [345/391]: Loss 2.2010481188772246e-05\n",
      "[Epoch 48] Training Batch [346/391]: Loss 1.522198181191925e-05\n",
      "[Epoch 48] Training Batch [347/391]: Loss 9.406809112988412e-06\n",
      "[Epoch 48] Training Batch [348/391]: Loss 1.8552633264334872e-05\n",
      "[Epoch 48] Training Batch [349/391]: Loss 1.6163794498424977e-05\n",
      "[Epoch 48] Training Batch [350/391]: Loss 1.5193029867077712e-05\n",
      "[Epoch 48] Training Batch [351/391]: Loss 7.879794793552719e-06\n",
      "[Epoch 48] Training Batch [352/391]: Loss 1.7061940525309183e-05\n",
      "[Epoch 48] Training Batch [353/391]: Loss 1.5655205061193556e-05\n",
      "[Epoch 48] Training Batch [354/391]: Loss 1.4184050996846054e-05\n",
      "[Epoch 48] Training Batch [355/391]: Loss 1.577001421537716e-05\n",
      "[Epoch 48] Training Batch [356/391]: Loss 1.5963521946105175e-05\n",
      "[Epoch 48] Training Batch [357/391]: Loss 1.5339812307502143e-05\n",
      "[Epoch 48] Training Batch [358/391]: Loss 1.3890601621824317e-05\n",
      "[Epoch 48] Training Batch [359/391]: Loss 1.3938309166405816e-05\n",
      "[Epoch 48] Training Batch [360/391]: Loss 2.3169388441601768e-05\n",
      "[Epoch 48] Training Batch [361/391]: Loss 1.451137814001413e-05\n",
      "[Epoch 48] Training Batch [362/391]: Loss 1.29846011986956e-05\n",
      "[Epoch 48] Training Batch [363/391]: Loss 1.2516969036369119e-05\n",
      "[Epoch 48] Training Batch [364/391]: Loss 1.267826428374974e-05\n",
      "[Epoch 48] Training Batch [365/391]: Loss 2.3168577172327787e-05\n",
      "[Epoch 48] Training Batch [366/391]: Loss 1.23313593576313e-05\n",
      "[Epoch 48] Training Batch [367/391]: Loss 1.7923632185556926e-05\n",
      "[Epoch 48] Training Batch [368/391]: Loss 2.1094894691486843e-05\n",
      "[Epoch 48] Training Batch [369/391]: Loss 1.969709774130024e-05\n",
      "[Epoch 48] Training Batch [370/391]: Loss 1.1920694305445068e-05\n",
      "[Epoch 48] Training Batch [371/391]: Loss 2.2874803107697517e-05\n",
      "[Epoch 48] Training Batch [372/391]: Loss 2.1588017261819914e-05\n",
      "[Epoch 48] Training Batch [373/391]: Loss 1.864289333752822e-05\n",
      "[Epoch 48] Training Batch [374/391]: Loss 1.3349265827855561e-05\n",
      "[Epoch 48] Training Batch [375/391]: Loss 2.4392791601712815e-05\n",
      "[Epoch 48] Training Batch [376/391]: Loss 1.3909191693528555e-05\n",
      "[Epoch 48] Training Batch [377/391]: Loss 1.2343546586635057e-05\n",
      "[Epoch 48] Training Batch [378/391]: Loss 1.25757997011533e-05\n",
      "[Epoch 48] Training Batch [379/391]: Loss 1.8928691133623943e-05\n",
      "[Epoch 48] Training Batch [380/391]: Loss 2.0422923626028933e-05\n",
      "[Epoch 48] Training Batch [381/391]: Loss 1.0236416528641712e-05\n",
      "[Epoch 48] Training Batch [382/391]: Loss 1.2981652616872452e-05\n",
      "[Epoch 48] Training Batch [383/391]: Loss 6.8560361796699e-06\n",
      "[Epoch 48] Training Batch [384/391]: Loss 1.1319196346448734e-05\n",
      "[Epoch 48] Training Batch [385/391]: Loss 2.031658368650824e-05\n",
      "[Epoch 48] Training Batch [386/391]: Loss 1.2232228073116858e-05\n",
      "[Epoch 48] Training Batch [387/391]: Loss 1.9052640709560364e-05\n",
      "[Epoch 48] Training Batch [388/391]: Loss 8.010650162759703e-06\n",
      "[Epoch 48] Training Batch [389/391]: Loss 1.5363550119218417e-05\n",
      "[Epoch 48] Training Batch [390/391]: Loss 1.6888401660253294e-05\n",
      "[Epoch 48] Training Batch [391/391]: Loss 1.4507715604850091e-05\n",
      "Epoch 48 - Train Loss: 0.0000\n",
      "*********  Epoch 49/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49] Training Batch [1/391]: Loss 8.796019756118767e-06\n",
      "[Epoch 49] Training Batch [2/391]: Loss 2.0774565200554207e-05\n",
      "[Epoch 49] Training Batch [3/391]: Loss 1.7035183191183023e-05\n",
      "[Epoch 49] Training Batch [4/391]: Loss 2.275891529279761e-05\n",
      "[Epoch 49] Training Batch [5/391]: Loss 1.4697946426167618e-05\n",
      "[Epoch 49] Training Batch [6/391]: Loss 1.4823521269136108e-05\n",
      "[Epoch 49] Training Batch [7/391]: Loss 5.822438652103301e-06\n",
      "[Epoch 49] Training Batch [8/391]: Loss 1.7658097931416705e-05\n",
      "[Epoch 49] Training Batch [9/391]: Loss 1.8373966668150388e-05\n",
      "[Epoch 49] Training Batch [10/391]: Loss 1.157462702394696e-05\n",
      "[Epoch 49] Training Batch [11/391]: Loss 2.0476894860621542e-05\n",
      "[Epoch 49] Training Batch [12/391]: Loss 2.1390973415691406e-05\n",
      "[Epoch 49] Training Batch [13/391]: Loss 6.224738172022626e-06\n",
      "[Epoch 49] Training Batch [14/391]: Loss 2.2954971427679993e-05\n",
      "[Epoch 49] Training Batch [15/391]: Loss 1.982951653189957e-05\n",
      "[Epoch 49] Training Batch [16/391]: Loss 1.7240332454093732e-05\n",
      "[Epoch 49] Training Batch [17/391]: Loss 1.641163726162631e-05\n",
      "[Epoch 49] Training Batch [18/391]: Loss 1.4165253560349811e-05\n",
      "[Epoch 49] Training Batch [19/391]: Loss 1.0507456863706466e-05\n",
      "[Epoch 49] Training Batch [20/391]: Loss 9.710100130178034e-06\n",
      "[Epoch 49] Training Batch [21/391]: Loss 1.3840137398801744e-05\n",
      "[Epoch 49] Training Batch [22/391]: Loss 2.1507918063434772e-05\n",
      "[Epoch 49] Training Batch [23/391]: Loss 1.9936440367018804e-05\n",
      "[Epoch 49] Training Batch [24/391]: Loss 1.2009610145469196e-05\n",
      "[Epoch 49] Training Batch [25/391]: Loss 1.1794309102697298e-05\n",
      "[Epoch 49] Training Batch [26/391]: Loss 1.2212277397338767e-05\n",
      "[Epoch 49] Training Batch [27/391]: Loss 1.0387325346528087e-05\n",
      "[Epoch 49] Training Batch [28/391]: Loss 1.2712784155155532e-05\n",
      "[Epoch 49] Training Batch [29/391]: Loss 1.2003675692540128e-05\n",
      "[Epoch 49] Training Batch [30/391]: Loss 1.2253341083123814e-05\n",
      "[Epoch 49] Training Batch [31/391]: Loss 1.6414009223808534e-05\n",
      "[Epoch 49] Training Batch [32/391]: Loss 1.3972352462587878e-05\n",
      "[Epoch 49] Training Batch [33/391]: Loss 1.2337370208115317e-05\n",
      "[Epoch 49] Training Batch [34/391]: Loss 1.4102195564191788e-05\n",
      "[Epoch 49] Training Batch [35/391]: Loss 1.5979281670297496e-05\n",
      "[Epoch 49] Training Batch [36/391]: Loss 1.2952391443832312e-05\n",
      "[Epoch 49] Training Batch [37/391]: Loss 1.0911466233665124e-05\n",
      "[Epoch 49] Training Batch [38/391]: Loss 2.1720979930250905e-05\n",
      "[Epoch 49] Training Batch [39/391]: Loss 2.0513642084551975e-05\n",
      "[Epoch 49] Training Batch [40/391]: Loss 1.734359648253303e-05\n",
      "[Epoch 49] Training Batch [41/391]: Loss 1.2707090718322434e-05\n",
      "[Epoch 49] Training Batch [42/391]: Loss 2.4688206394785084e-05\n",
      "[Epoch 49] Training Batch [43/391]: Loss 2.3304006390389986e-05\n",
      "[Epoch 49] Training Batch [44/391]: Loss 1.0101517545990646e-05\n",
      "[Epoch 49] Training Batch [45/391]: Loss 1.0299899258825462e-05\n",
      "[Epoch 49] Training Batch [46/391]: Loss 1.3588884939963464e-05\n",
      "[Epoch 49] Training Batch [47/391]: Loss 1.2282238458283246e-05\n",
      "[Epoch 49] Training Batch [48/391]: Loss 2.2078975234762765e-05\n",
      "[Epoch 49] Training Batch [49/391]: Loss 1.7954507711692713e-05\n",
      "[Epoch 49] Training Batch [50/391]: Loss 7.934331733849831e-06\n",
      "[Epoch 49] Training Batch [51/391]: Loss 1.080559377442114e-05\n",
      "[Epoch 49] Training Batch [52/391]: Loss 1.316611269430723e-05\n",
      "[Epoch 49] Training Batch [53/391]: Loss 1.1986215213255491e-05\n",
      "[Epoch 49] Training Batch [54/391]: Loss 1.4978724721004255e-05\n",
      "[Epoch 49] Training Batch [55/391]: Loss 1.570816311868839e-05\n",
      "[Epoch 49] Training Batch [56/391]: Loss 1.611050174687989e-05\n",
      "[Epoch 49] Training Batch [57/391]: Loss 2.645376844156999e-05\n",
      "[Epoch 49] Training Batch [58/391]: Loss 1.1894990166183561e-05\n",
      "[Epoch 49] Training Batch [59/391]: Loss 2.635518103488721e-05\n",
      "[Epoch 49] Training Batch [60/391]: Loss 1.577622606419027e-05\n",
      "[Epoch 49] Training Batch [61/391]: Loss 1.4632775673817378e-05\n",
      "[Epoch 49] Training Batch [62/391]: Loss 1.1891120266227517e-05\n",
      "[Epoch 49] Training Batch [63/391]: Loss 1.2131676157878246e-05\n",
      "[Epoch 49] Training Batch [64/391]: Loss 1.2378015526337549e-05\n",
      "[Epoch 49] Training Batch [65/391]: Loss 1.727537892293185e-05\n",
      "[Epoch 49] Training Batch [66/391]: Loss 1.8491778973839246e-05\n",
      "[Epoch 49] Training Batch [67/391]: Loss 1.6271835193037987e-05\n",
      "[Epoch 49] Training Batch [68/391]: Loss 1.1429950063757133e-05\n",
      "[Epoch 49] Training Batch [69/391]: Loss 2.024918285314925e-05\n",
      "[Epoch 49] Training Batch [70/391]: Loss 8.949406037572771e-06\n",
      "[Epoch 49] Training Batch [71/391]: Loss 2.387914719292894e-05\n",
      "[Epoch 49] Training Batch [72/391]: Loss 7.798515980539378e-06\n",
      "[Epoch 49] Training Batch [73/391]: Loss 1.6104762835311703e-05\n",
      "[Epoch 49] Training Batch [74/391]: Loss 1.0337673302274197e-05\n",
      "[Epoch 49] Training Batch [75/391]: Loss 1.69482700584922e-05\n",
      "[Epoch 49] Training Batch [76/391]: Loss 1.239226185134612e-05\n",
      "[Epoch 49] Training Batch [77/391]: Loss 2.34302606259007e-05\n",
      "[Epoch 49] Training Batch [78/391]: Loss 1.1442579307185952e-05\n",
      "[Epoch 49] Training Batch [79/391]: Loss 1.691627585387323e-05\n",
      "[Epoch 49] Training Batch [80/391]: Loss 1.4468884728557896e-05\n",
      "[Epoch 49] Training Batch [81/391]: Loss 1.3530739124689717e-05\n",
      "[Epoch 49] Training Batch [82/391]: Loss 1.3759208741248585e-05\n",
      "[Epoch 49] Training Batch [83/391]: Loss 1.5724874174338765e-05\n",
      "[Epoch 49] Training Batch [84/391]: Loss 1.6028912796173245e-05\n",
      "[Epoch 49] Training Batch [85/391]: Loss 1.9437147784628905e-05\n",
      "[Epoch 49] Training Batch [86/391]: Loss 1.6851776308612898e-05\n",
      "[Epoch 49] Training Batch [87/391]: Loss 2.0207604393363e-05\n",
      "[Epoch 49] Training Batch [88/391]: Loss 8.911246368370485e-06\n",
      "[Epoch 49] Training Batch [89/391]: Loss 1.1804721907537896e-05\n",
      "[Epoch 49] Training Batch [90/391]: Loss 1.3987154488859233e-05\n",
      "[Epoch 49] Training Batch [91/391]: Loss 1.0164292689296417e-05\n",
      "[Epoch 49] Training Batch [92/391]: Loss 9.267014320357703e-06\n",
      "[Epoch 49] Training Batch [93/391]: Loss 2.2811114831711166e-05\n",
      "[Epoch 49] Training Batch [94/391]: Loss 2.3165641323430464e-05\n",
      "[Epoch 49] Training Batch [95/391]: Loss 1.73932385223452e-05\n",
      "[Epoch 49] Training Batch [96/391]: Loss 2.2929096303414553e-05\n",
      "[Epoch 49] Training Batch [97/391]: Loss 2.3180138668976724e-05\n",
      "[Epoch 49] Training Batch [98/391]: Loss 1.9923607396776788e-05\n",
      "[Epoch 49] Training Batch [99/391]: Loss 1.1479352906462736e-05\n",
      "[Epoch 49] Training Batch [100/391]: Loss 1.362485363642918e-05\n",
      "[Epoch 49] Training Batch [101/391]: Loss 1.251580033567734e-05\n",
      "[Epoch 49] Training Batch [102/391]: Loss 1.6989939467748627e-05\n",
      "[Epoch 49] Training Batch [103/391]: Loss 1.9248840544605628e-05\n",
      "[Epoch 49] Training Batch [104/391]: Loss 1.4733040188730229e-05\n",
      "[Epoch 49] Training Batch [105/391]: Loss 1.8619975890032947e-05\n",
      "[Epoch 49] Training Batch [106/391]: Loss 1.140306267188862e-05\n",
      "[Epoch 49] Training Batch [107/391]: Loss 1.7423348253942095e-05\n",
      "[Epoch 49] Training Batch [108/391]: Loss 2.0823195882258005e-05\n",
      "[Epoch 49] Training Batch [109/391]: Loss 1.2781055374944117e-05\n",
      "[Epoch 49] Training Batch [110/391]: Loss 1.3333775314094964e-05\n",
      "[Epoch 49] Training Batch [111/391]: Loss 1.484681160945911e-05\n",
      "[Epoch 49] Training Batch [112/391]: Loss 1.3447594028548338e-05\n",
      "[Epoch 49] Training Batch [113/391]: Loss 2.781086732284166e-05\n",
      "[Epoch 49] Training Batch [114/391]: Loss 2.0713358026114292e-05\n",
      "[Epoch 49] Training Batch [115/391]: Loss 7.418507721013157e-06\n",
      "[Epoch 49] Training Batch [116/391]: Loss 1.1502069355628919e-05\n",
      "[Epoch 49] Training Batch [117/391]: Loss 1.8727665519691072e-05\n",
      "[Epoch 49] Training Batch [118/391]: Loss 9.71685949480161e-06\n",
      "[Epoch 49] Training Batch [119/391]: Loss 2.024480636464432e-05\n",
      "[Epoch 49] Training Batch [120/391]: Loss 1.505820000602398e-05\n",
      "[Epoch 49] Training Batch [121/391]: Loss 7.52294408812304e-06\n",
      "[Epoch 49] Training Batch [122/391]: Loss 1.7181058865389787e-05\n",
      "[Epoch 49] Training Batch [123/391]: Loss 8.468130545224994e-06\n",
      "[Epoch 49] Training Batch [124/391]: Loss 1.733217141008936e-05\n",
      "[Epoch 49] Training Batch [125/391]: Loss 1.0844458302017301e-05\n",
      "[Epoch 49] Training Batch [126/391]: Loss 2.4823333660606295e-05\n",
      "[Epoch 49] Training Batch [127/391]: Loss 1.5088882719282992e-05\n",
      "[Epoch 49] Training Batch [128/391]: Loss 9.038798452820629e-06\n",
      "[Epoch 49] Training Batch [129/391]: Loss 2.0014564142911695e-05\n",
      "[Epoch 49] Training Batch [130/391]: Loss 1.0797156392072793e-05\n",
      "[Epoch 49] Training Batch [131/391]: Loss 6.626141839660704e-06\n",
      "[Epoch 49] Training Batch [132/391]: Loss 1.7768305042409338e-05\n",
      "[Epoch 49] Training Batch [133/391]: Loss 8.650695235701278e-06\n",
      "[Epoch 49] Training Batch [134/391]: Loss 2.0920144379488192e-05\n",
      "[Epoch 49] Training Batch [135/391]: Loss 1.5374220311059617e-05\n",
      "[Epoch 49] Training Batch [136/391]: Loss 1.3814418707625009e-05\n",
      "[Epoch 49] Training Batch [137/391]: Loss 7.638194801984355e-06\n",
      "[Epoch 49] Training Batch [138/391]: Loss 2.313607546966523e-05\n",
      "[Epoch 49] Training Batch [139/391]: Loss 1.2004085874650627e-05\n",
      "[Epoch 49] Training Batch [140/391]: Loss 1.886809513962362e-05\n",
      "[Epoch 49] Training Batch [141/391]: Loss 1.4576315152226016e-05\n",
      "[Epoch 49] Training Batch [142/391]: Loss 9.401199349667877e-06\n",
      "[Epoch 49] Training Batch [143/391]: Loss 2.0392990336404182e-05\n",
      "[Epoch 49] Training Batch [144/391]: Loss 1.5868197806412354e-05\n",
      "[Epoch 49] Training Batch [145/391]: Loss 6.946164376131492e-06\n",
      "[Epoch 49] Training Batch [146/391]: Loss 1.712366793071851e-05\n",
      "[Epoch 49] Training Batch [147/391]: Loss 8.853620784066152e-06\n",
      "[Epoch 49] Training Batch [148/391]: Loss 9.656446309236344e-06\n",
      "[Epoch 49] Training Batch [149/391]: Loss 1.8736007405095734e-05\n",
      "[Epoch 49] Training Batch [150/391]: Loss 1.2633628102776129e-05\n",
      "[Epoch 49] Training Batch [151/391]: Loss 1.70713374245679e-05\n",
      "[Epoch 49] Training Batch [152/391]: Loss 1.7647089407546446e-05\n",
      "[Epoch 49] Training Batch [153/391]: Loss 3.056877903873101e-05\n",
      "[Epoch 49] Training Batch [154/391]: Loss 1.5478419300052337e-05\n",
      "[Epoch 49] Training Batch [155/391]: Loss 1.4427192581933923e-05\n",
      "[Epoch 49] Training Batch [156/391]: Loss 1.0889151781157125e-05\n",
      "[Epoch 49] Training Batch [157/391]: Loss 1.2558996786538046e-05\n",
      "[Epoch 49] Training Batch [158/391]: Loss 1.5504545444855466e-05\n",
      "[Epoch 49] Training Batch [159/391]: Loss 1.2050452824041713e-05\n",
      "[Epoch 49] Training Batch [160/391]: Loss 1.3688190847460646e-05\n",
      "[Epoch 49] Training Batch [161/391]: Loss 1.4758701581740752e-05\n",
      "[Epoch 49] Training Batch [162/391]: Loss 1.7725988072925247e-05\n",
      "[Epoch 49] Training Batch [163/391]: Loss 9.647549632063601e-06\n",
      "[Epoch 49] Training Batch [164/391]: Loss 2.3927916117827408e-05\n",
      "[Epoch 49] Training Batch [165/391]: Loss 2.321882675460074e-05\n",
      "[Epoch 49] Training Batch [166/391]: Loss 1.2507531209848821e-05\n",
      "[Epoch 49] Training Batch [167/391]: Loss 1.2113623597542755e-05\n",
      "[Epoch 49] Training Batch [168/391]: Loss 1.531715133751277e-05\n",
      "[Epoch 49] Training Batch [169/391]: Loss 1.1170286597916856e-05\n",
      "[Epoch 49] Training Batch [170/391]: Loss 1.1685324352583848e-05\n",
      "[Epoch 49] Training Batch [171/391]: Loss 2.132642111973837e-05\n",
      "[Epoch 49] Training Batch [172/391]: Loss 1.3350467270356603e-05\n",
      "[Epoch 49] Training Batch [173/391]: Loss 1.157659244199749e-05\n",
      "[Epoch 49] Training Batch [174/391]: Loss 1.4021801689523272e-05\n",
      "[Epoch 49] Training Batch [175/391]: Loss 1.2806580343749374e-05\n",
      "[Epoch 49] Training Batch [176/391]: Loss 1.0257616850140039e-05\n",
      "[Epoch 49] Training Batch [177/391]: Loss 1.9186367353540845e-05\n",
      "[Epoch 49] Training Batch [178/391]: Loss 1.8525774066802114e-05\n",
      "[Epoch 49] Training Batch [179/391]: Loss 1.7905502318171784e-05\n",
      "[Epoch 49] Training Batch [180/391]: Loss 1.0447152817505412e-05\n",
      "[Epoch 49] Training Batch [181/391]: Loss 1.0627455594658386e-05\n",
      "[Epoch 49] Training Batch [182/391]: Loss 6.917560313013382e-06\n",
      "[Epoch 49] Training Batch [183/391]: Loss 1.073841212928528e-05\n",
      "[Epoch 49] Training Batch [184/391]: Loss 1.15064203782822e-05\n",
      "[Epoch 49] Training Batch [185/391]: Loss 2.144804602721706e-05\n",
      "[Epoch 49] Training Batch [186/391]: Loss 1.9917290046578273e-05\n",
      "[Epoch 49] Training Batch [187/391]: Loss 1.466364847146906e-05\n",
      "[Epoch 49] Training Batch [188/391]: Loss 9.088144906854723e-06\n",
      "[Epoch 49] Training Batch [189/391]: Loss 1.788438021321781e-05\n",
      "[Epoch 49] Training Batch [190/391]: Loss 1.4423354514292441e-05\n",
      "[Epoch 49] Training Batch [191/391]: Loss 1.3560214938479476e-05\n",
      "[Epoch 49] Training Batch [192/391]: Loss 2.0473897166084498e-05\n",
      "[Epoch 49] Training Batch [193/391]: Loss 1.4758578799956013e-05\n",
      "[Epoch 49] Training Batch [194/391]: Loss 1.7508167729829438e-05\n",
      "[Epoch 49] Training Batch [195/391]: Loss 1.4976621059759054e-05\n",
      "[Epoch 49] Training Batch [196/391]: Loss 1.0531606676522642e-05\n",
      "[Epoch 49] Training Batch [197/391]: Loss 2.290007432748098e-05\n",
      "[Epoch 49] Training Batch [198/391]: Loss 1.6421085092588328e-05\n",
      "[Epoch 49] Training Batch [199/391]: Loss 1.7013506294460967e-05\n",
      "[Epoch 49] Training Batch [200/391]: Loss 1.962366332008969e-05\n",
      "[Epoch 49] Training Batch [201/391]: Loss 1.2507876817835495e-05\n",
      "[Epoch 49] Training Batch [202/391]: Loss 1.6798470824141987e-05\n",
      "[Epoch 49] Training Batch [203/391]: Loss 9.728080840432085e-06\n",
      "[Epoch 49] Training Batch [204/391]: Loss 1.5589281247230247e-05\n",
      "[Epoch 49] Training Batch [205/391]: Loss 1.0889213626796845e-05\n",
      "[Epoch 49] Training Batch [206/391]: Loss 9.120019058173057e-06\n",
      "[Epoch 49] Training Batch [207/391]: Loss 6.5187778091058135e-06\n",
      "[Epoch 49] Training Batch [208/391]: Loss 9.488798241363838e-06\n",
      "[Epoch 49] Training Batch [209/391]: Loss 1.8532642570789903e-05\n",
      "[Epoch 49] Training Batch [210/391]: Loss 1.99626010726206e-05\n",
      "[Epoch 49] Training Batch [211/391]: Loss 1.2806558515876532e-05\n",
      "[Epoch 49] Training Batch [212/391]: Loss 2.588340430520475e-05\n",
      "[Epoch 49] Training Batch [213/391]: Loss 1.89067650353536e-05\n",
      "[Epoch 49] Training Batch [214/391]: Loss 9.375146873935591e-06\n",
      "[Epoch 49] Training Batch [215/391]: Loss 2.131568908225745e-05\n",
      "[Epoch 49] Training Batch [216/391]: Loss 1.7303709682892077e-05\n",
      "[Epoch 49] Training Batch [217/391]: Loss 1.4125307643553242e-05\n",
      "[Epoch 49] Training Batch [218/391]: Loss 1.3395466339716222e-05\n",
      "[Epoch 49] Training Batch [219/391]: Loss 1.509806406829739e-05\n",
      "[Epoch 49] Training Batch [220/391]: Loss 1.0149011359317228e-05\n",
      "[Epoch 49] Training Batch [221/391]: Loss 2.7235526431468315e-05\n",
      "[Epoch 49] Training Batch [222/391]: Loss 8.940246516431216e-06\n",
      "[Epoch 49] Training Batch [223/391]: Loss 8.274091669591144e-06\n",
      "[Epoch 49] Training Batch [224/391]: Loss 1.115105169446906e-05\n",
      "[Epoch 49] Training Batch [225/391]: Loss 1.312863332714187e-05\n",
      "[Epoch 49] Training Batch [226/391]: Loss 6.445137387345312e-06\n",
      "[Epoch 49] Training Batch [227/391]: Loss 1.918666748679243e-05\n",
      "[Epoch 49] Training Batch [228/391]: Loss 1.2824428267776966e-05\n",
      "[Epoch 49] Training Batch [229/391]: Loss 1.211194376082858e-05\n",
      "[Epoch 49] Training Batch [230/391]: Loss 1.5977153452695347e-05\n",
      "[Epoch 49] Training Batch [231/391]: Loss 1.4294501852418762e-05\n",
      "[Epoch 49] Training Batch [232/391]: Loss 2.1505760742002167e-05\n",
      "[Epoch 49] Training Batch [233/391]: Loss 1.6904752556001768e-05\n",
      "[Epoch 49] Training Batch [234/391]: Loss 1.6109477655845694e-05\n",
      "[Epoch 49] Training Batch [235/391]: Loss 1.410646473232191e-05\n",
      "[Epoch 49] Training Batch [236/391]: Loss 1.623125353944488e-05\n",
      "[Epoch 49] Training Batch [237/391]: Loss 1.0111513802257832e-05\n",
      "[Epoch 49] Training Batch [238/391]: Loss 1.1392866326787043e-05\n",
      "[Epoch 49] Training Batch [239/391]: Loss 1.1226818060094956e-05\n",
      "[Epoch 49] Training Batch [240/391]: Loss 1.3897973076382186e-05\n",
      "[Epoch 49] Training Batch [241/391]: Loss 1.509820776846027e-05\n",
      "[Epoch 49] Training Batch [242/391]: Loss 1.1787782568717375e-05\n",
      "[Epoch 49] Training Batch [243/391]: Loss 1.8314931367058307e-05\n",
      "[Epoch 49] Training Batch [244/391]: Loss 1.7054173440556042e-05\n",
      "[Epoch 49] Training Batch [245/391]: Loss 2.3249684090842493e-05\n",
      "[Epoch 49] Training Batch [246/391]: Loss 6.3419870457437355e-06\n",
      "[Epoch 49] Training Batch [247/391]: Loss 1.4136492609395646e-05\n",
      "[Epoch 49] Training Batch [248/391]: Loss 1.1658558833005372e-05\n",
      "[Epoch 49] Training Batch [249/391]: Loss 8.100287232082337e-06\n",
      "[Epoch 49] Training Batch [250/391]: Loss 1.0559576367086265e-05\n",
      "[Epoch 49] Training Batch [251/391]: Loss 1.6463231077068485e-05\n",
      "[Epoch 49] Training Batch [252/391]: Loss 6.472516815847484e-06\n",
      "[Epoch 49] Training Batch [253/391]: Loss 1.2465565305319615e-05\n",
      "[Epoch 49] Training Batch [254/391]: Loss 1.588442682987079e-05\n",
      "[Epoch 49] Training Batch [255/391]: Loss 1.2602727110788692e-05\n",
      "[Epoch 49] Training Batch [256/391]: Loss 1.3617934200738091e-05\n",
      "[Epoch 49] Training Batch [257/391]: Loss 1.3749231584370136e-05\n",
      "[Epoch 49] Training Batch [258/391]: Loss 1.3606166248791851e-05\n",
      "[Epoch 49] Training Batch [259/391]: Loss 3.849003405775875e-05\n",
      "[Epoch 49] Training Batch [260/391]: Loss 1.8721373635344207e-05\n",
      "[Epoch 49] Training Batch [261/391]: Loss 1.2434161362762097e-05\n",
      "[Epoch 49] Training Batch [262/391]: Loss 1.847643761720974e-05\n",
      "[Epoch 49] Training Batch [263/391]: Loss 1.1271994480921421e-05\n",
      "[Epoch 49] Training Batch [264/391]: Loss 1.334604894509539e-05\n",
      "[Epoch 49] Training Batch [265/391]: Loss 1.1130395250802394e-05\n",
      "[Epoch 49] Training Batch [266/391]: Loss 1.101765883504413e-05\n",
      "[Epoch 49] Training Batch [267/391]: Loss 1.1735274711099919e-05\n",
      "[Epoch 49] Training Batch [268/391]: Loss 1.392110061715357e-05\n",
      "[Epoch 49] Training Batch [269/391]: Loss 1.814067582017742e-05\n",
      "[Epoch 49] Training Batch [270/391]: Loss 1.3531179320125375e-05\n",
      "[Epoch 49] Training Batch [271/391]: Loss 2.298866820638068e-05\n",
      "[Epoch 49] Training Batch [272/391]: Loss 1.4117464161245152e-05\n",
      "[Epoch 49] Training Batch [273/391]: Loss 7.941098374431022e-06\n",
      "[Epoch 49] Training Batch [274/391]: Loss 1.5540392269031145e-05\n",
      "[Epoch 49] Training Batch [275/391]: Loss 1.1288708265055902e-05\n",
      "[Epoch 49] Training Batch [276/391]: Loss 1.3541926819016226e-05\n",
      "[Epoch 49] Training Batch [277/391]: Loss 2.0258548829588108e-05\n",
      "[Epoch 49] Training Batch [278/391]: Loss 1.1239289051445667e-05\n",
      "[Epoch 49] Training Batch [279/391]: Loss 9.92452260106802e-06\n",
      "[Epoch 49] Training Batch [280/391]: Loss 1.67438447533641e-05\n",
      "[Epoch 49] Training Batch [281/391]: Loss 1.6190682799788192e-05\n",
      "[Epoch 49] Training Batch [282/391]: Loss 2.354000935156364e-05\n",
      "[Epoch 49] Training Batch [283/391]: Loss 1.5222401088976767e-05\n",
      "[Epoch 49] Training Batch [284/391]: Loss 6.504167231469182e-06\n",
      "[Epoch 49] Training Batch [285/391]: Loss 6.073656095395563e-06\n",
      "[Epoch 49] Training Batch [286/391]: Loss 1.589176281413529e-05\n",
      "[Epoch 49] Training Batch [287/391]: Loss 1.3647278137796093e-05\n",
      "[Epoch 49] Training Batch [288/391]: Loss 1.809898458304815e-05\n",
      "[Epoch 49] Training Batch [289/391]: Loss 2.17764827539213e-05\n",
      "[Epoch 49] Training Batch [290/391]: Loss 1.1442381037340965e-05\n",
      "[Epoch 49] Training Batch [291/391]: Loss 7.97264146967791e-06\n",
      "[Epoch 49] Training Batch [292/391]: Loss 1.600063478690572e-05\n",
      "[Epoch 49] Training Batch [293/391]: Loss 1.571120083099231e-05\n",
      "[Epoch 49] Training Batch [294/391]: Loss 1.814513234421611e-05\n",
      "[Epoch 49] Training Batch [295/391]: Loss 1.3558272257796489e-05\n",
      "[Epoch 49] Training Batch [296/391]: Loss 1.5537400031462312e-05\n",
      "[Epoch 49] Training Batch [297/391]: Loss 1.630522274354007e-05\n",
      "[Epoch 49] Training Batch [298/391]: Loss 1.1939448995690327e-05\n",
      "[Epoch 49] Training Batch [299/391]: Loss 1.4371231372933835e-05\n",
      "[Epoch 49] Training Batch [300/391]: Loss 1.579740273882635e-05\n",
      "[Epoch 49] Training Batch [301/391]: Loss 9.21972514333902e-06\n",
      "[Epoch 49] Training Batch [302/391]: Loss 2.091793248837348e-05\n",
      "[Epoch 49] Training Batch [303/391]: Loss 1.0287720215274021e-05\n",
      "[Epoch 49] Training Batch [304/391]: Loss 2.4796829166007228e-05\n",
      "[Epoch 49] Training Batch [305/391]: Loss 1.528046777821146e-05\n",
      "[Epoch 49] Training Batch [306/391]: Loss 1.0561317139945459e-05\n",
      "[Epoch 49] Training Batch [307/391]: Loss 1.2391681593726389e-05\n",
      "[Epoch 49] Training Batch [308/391]: Loss 1.8686130715650506e-05\n",
      "[Epoch 49] Training Batch [309/391]: Loss 1.407936815667199e-05\n",
      "[Epoch 49] Training Batch [310/391]: Loss 2.500130722182803e-05\n",
      "[Epoch 49] Training Batch [311/391]: Loss 9.222604603564832e-06\n",
      "[Epoch 49] Training Batch [312/391]: Loss 2.063008832919877e-05\n",
      "[Epoch 49] Training Batch [313/391]: Loss 1.4938063941372093e-05\n",
      "[Epoch 49] Training Batch [314/391]: Loss 1.859300937212538e-05\n",
      "[Epoch 49] Training Batch [315/391]: Loss 1.3531121112464461e-05\n",
      "[Epoch 49] Training Batch [316/391]: Loss 2.618439066282008e-05\n",
      "[Epoch 49] Training Batch [317/391]: Loss 1.1633355825324543e-05\n",
      "[Epoch 49] Training Batch [318/391]: Loss 1.428258383384673e-05\n",
      "[Epoch 49] Training Batch [319/391]: Loss 2.0932622646796517e-05\n",
      "[Epoch 49] Training Batch [320/391]: Loss 1.1003585314028896e-05\n",
      "[Epoch 49] Training Batch [321/391]: Loss 1.910387072712183e-05\n",
      "[Epoch 49] Training Batch [322/391]: Loss 8.904866263037547e-06\n",
      "[Epoch 49] Training Batch [323/391]: Loss 9.848195986705832e-06\n",
      "[Epoch 49] Training Batch [324/391]: Loss 1.010712639981648e-05\n",
      "[Epoch 49] Training Batch [325/391]: Loss 1.9328028429299593e-05\n",
      "[Epoch 49] Training Batch [326/391]: Loss 1.1445197742432356e-05\n",
      "[Epoch 49] Training Batch [327/391]: Loss 1.868569597718306e-05\n",
      "[Epoch 49] Training Batch [328/391]: Loss 7.667178579140455e-06\n",
      "[Epoch 49] Training Batch [329/391]: Loss 1.6340989532181993e-05\n",
      "[Epoch 49] Training Batch [330/391]: Loss 7.07685785528156e-06\n",
      "[Epoch 49] Training Batch [331/391]: Loss 1.3962117009214126e-05\n",
      "[Epoch 49] Training Batch [332/391]: Loss 1.365416119369911e-05\n",
      "[Epoch 49] Training Batch [333/391]: Loss 1.4787375221203547e-05\n",
      "[Epoch 49] Training Batch [334/391]: Loss 1.3966076039650943e-05\n",
      "[Epoch 49] Training Batch [335/391]: Loss 1.4746074157301337e-05\n",
      "[Epoch 49] Training Batch [336/391]: Loss 1.484094354964327e-05\n",
      "[Epoch 49] Training Batch [337/391]: Loss 1.5395919035654515e-05\n",
      "[Epoch 49] Training Batch [338/391]: Loss 1.7672729882178828e-05\n",
      "[Epoch 49] Training Batch [339/391]: Loss 9.909521395456977e-06\n",
      "[Epoch 49] Training Batch [340/391]: Loss 1.9438974049990065e-05\n",
      "[Epoch 49] Training Batch [341/391]: Loss 2.293251054652501e-05\n",
      "[Epoch 49] Training Batch [342/391]: Loss 1.2661626897170208e-05\n",
      "[Epoch 49] Training Batch [343/391]: Loss 2.2287049432634376e-05\n",
      "[Epoch 49] Training Batch [344/391]: Loss 1.7902142644743435e-05\n",
      "[Epoch 49] Training Batch [345/391]: Loss 1.2224310921737924e-05\n",
      "[Epoch 49] Training Batch [346/391]: Loss 1.1300702681182884e-05\n",
      "[Epoch 49] Training Batch [347/391]: Loss 2.2000383978593163e-05\n",
      "[Epoch 49] Training Batch [348/391]: Loss 8.524956683686469e-06\n",
      "[Epoch 49] Training Batch [349/391]: Loss 2.4804034183034673e-05\n",
      "[Epoch 49] Training Batch [350/391]: Loss 1.1646347957139369e-05\n",
      "[Epoch 49] Training Batch [351/391]: Loss 1.4684687812405173e-05\n",
      "[Epoch 49] Training Batch [352/391]: Loss 2.0505789507296868e-05\n",
      "[Epoch 49] Training Batch [353/391]: Loss 2.7261339710094035e-05\n",
      "[Epoch 49] Training Batch [354/391]: Loss 1.3570060218626168e-05\n",
      "[Epoch 49] Training Batch [355/391]: Loss 1.2844731827499345e-05\n",
      "[Epoch 49] Training Batch [356/391]: Loss 2.513150502636563e-05\n",
      "[Epoch 49] Training Batch [357/391]: Loss 1.845647238951642e-05\n",
      "[Epoch 49] Training Batch [358/391]: Loss 1.5875111785135232e-05\n",
      "[Epoch 49] Training Batch [359/391]: Loss 1.4789315173402429e-05\n",
      "[Epoch 49] Training Batch [360/391]: Loss 1.6408914234489202e-05\n",
      "[Epoch 49] Training Batch [361/391]: Loss 7.271474260051036e-06\n",
      "[Epoch 49] Training Batch [362/391]: Loss 1.9562938177841716e-05\n",
      "[Epoch 49] Training Batch [363/391]: Loss 1.5684998288634233e-05\n",
      "[Epoch 49] Training Batch [364/391]: Loss 1.907268051581923e-05\n",
      "[Epoch 49] Training Batch [365/391]: Loss 1.3977936760056764e-05\n",
      "[Epoch 49] Training Batch [366/391]: Loss 1.1357358744135126e-05\n",
      "[Epoch 49] Training Batch [367/391]: Loss 1.2988143680559006e-05\n",
      "[Epoch 49] Training Batch [368/391]: Loss 1.5985340724000707e-05\n",
      "[Epoch 49] Training Batch [369/391]: Loss 2.6093632186530158e-05\n",
      "[Epoch 49] Training Batch [370/391]: Loss 9.75505099631846e-06\n",
      "[Epoch 49] Training Batch [371/391]: Loss 8.967167559603695e-06\n",
      "[Epoch 49] Training Batch [372/391]: Loss 1.1102419193775859e-05\n",
      "[Epoch 49] Training Batch [373/391]: Loss 1.3202556147007272e-05\n",
      "[Epoch 49] Training Batch [374/391]: Loss 1.1679956514853984e-05\n",
      "[Epoch 49] Training Batch [375/391]: Loss 1.0065204151032958e-05\n",
      "[Epoch 49] Training Batch [376/391]: Loss 1.9474540749797598e-05\n",
      "[Epoch 49] Training Batch [377/391]: Loss 1.1498128515086137e-05\n",
      "[Epoch 49] Training Batch [378/391]: Loss 1.8648897821549326e-05\n",
      "[Epoch 49] Training Batch [379/391]: Loss 1.2577950656122994e-05\n",
      "[Epoch 49] Training Batch [380/391]: Loss 2.0195479009998962e-05\n",
      "[Epoch 49] Training Batch [381/391]: Loss 1.431585133104818e-05\n",
      "[Epoch 49] Training Batch [382/391]: Loss 1.3041787497058976e-05\n",
      "[Epoch 49] Training Batch [383/391]: Loss 1.0273803127347492e-05\n",
      "[Epoch 49] Training Batch [384/391]: Loss 1.7858457795227878e-05\n",
      "[Epoch 49] Training Batch [385/391]: Loss 1.5983754565240815e-05\n",
      "[Epoch 49] Training Batch [386/391]: Loss 6.869170192658203e-06\n",
      "[Epoch 49] Training Batch [387/391]: Loss 1.8817572708940133e-05\n",
      "[Epoch 49] Training Batch [388/391]: Loss 1.5582982086925767e-05\n",
      "[Epoch 49] Training Batch [389/391]: Loss 6.284327355388086e-06\n",
      "[Epoch 49] Training Batch [390/391]: Loss 1.2738334589812439e-05\n",
      "[Epoch 49] Training Batch [391/391]: Loss 1.423373760189861e-05\n",
      "Epoch 49 - Train Loss: 0.0000\n",
      "*********  Epoch 50/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 50] Training Batch [1/391]: Loss 1.5756546417833306e-05\n",
      "[Epoch 50] Training Batch [2/391]: Loss 1.0660982297849841e-05\n",
      "[Epoch 50] Training Batch [3/391]: Loss 1.1529083167260978e-05\n",
      "[Epoch 50] Training Batch [4/391]: Loss 1.3844657587469555e-05\n",
      "[Epoch 50] Training Batch [5/391]: Loss 1.3806320566800423e-05\n",
      "[Epoch 50] Training Batch [6/391]: Loss 1.2910426448797807e-05\n",
      "[Epoch 50] Training Batch [7/391]: Loss 8.33584363135742e-06\n",
      "[Epoch 50] Training Batch [8/391]: Loss 1.3347882486414164e-05\n",
      "[Epoch 50] Training Batch [9/391]: Loss 1.4167085282679182e-05\n",
      "[Epoch 50] Training Batch [10/391]: Loss 1.6885509467101656e-05\n",
      "[Epoch 50] Training Batch [11/391]: Loss 1.6420148313045502e-05\n",
      "[Epoch 50] Training Batch [12/391]: Loss 1.6039439287851565e-05\n",
      "[Epoch 50] Training Batch [13/391]: Loss 1.204422369482927e-05\n",
      "[Epoch 50] Training Batch [14/391]: Loss 2.27041746256873e-05\n",
      "[Epoch 50] Training Batch [15/391]: Loss 1.7624783140490763e-05\n",
      "[Epoch 50] Training Batch [16/391]: Loss 9.18644309422234e-06\n",
      "[Epoch 50] Training Batch [17/391]: Loss 1.667480137257371e-05\n",
      "[Epoch 50] Training Batch [18/391]: Loss 1.3387752915150486e-05\n",
      "[Epoch 50] Training Batch [19/391]: Loss 1.4895200365572236e-05\n",
      "[Epoch 50] Training Batch [20/391]: Loss 1.6707859685993753e-05\n",
      "[Epoch 50] Training Batch [21/391]: Loss 1.14962631414528e-05\n",
      "[Epoch 50] Training Batch [22/391]: Loss 1.9359762518433854e-05\n",
      "[Epoch 50] Training Batch [23/391]: Loss 1.700408938631881e-05\n",
      "[Epoch 50] Training Batch [24/391]: Loss 1.7484804629930295e-05\n",
      "[Epoch 50] Training Batch [25/391]: Loss 2.38627617363818e-05\n",
      "[Epoch 50] Training Batch [26/391]: Loss 1.5820709450053982e-05\n",
      "[Epoch 50] Training Batch [27/391]: Loss 1.54985609697178e-05\n",
      "[Epoch 50] Training Batch [28/391]: Loss 1.0203856618318241e-05\n",
      "[Epoch 50] Training Batch [29/391]: Loss 1.5693956811446697e-05\n",
      "[Epoch 50] Training Batch [30/391]: Loss 1.943965435202699e-05\n",
      "[Epoch 50] Training Batch [31/391]: Loss 1.467969468649244e-05\n",
      "[Epoch 50] Training Batch [32/391]: Loss 1.2088623407180421e-05\n",
      "[Epoch 50] Training Batch [33/391]: Loss 1.775216151145287e-05\n",
      "[Epoch 50] Training Batch [34/391]: Loss 1.2938359759573359e-05\n",
      "[Epoch 50] Training Batch [35/391]: Loss 1.589462044648826e-05\n",
      "[Epoch 50] Training Batch [36/391]: Loss 1.2414609045663383e-05\n",
      "[Epoch 50] Training Batch [37/391]: Loss 1.156893813458737e-05\n",
      "[Epoch 50] Training Batch [38/391]: Loss 1.2818827599403448e-05\n",
      "[Epoch 50] Training Batch [39/391]: Loss 6.134370778454468e-06\n",
      "[Epoch 50] Training Batch [40/391]: Loss 1.8852164430427365e-05\n",
      "[Epoch 50] Training Batch [41/391]: Loss 1.4794745766266715e-05\n",
      "[Epoch 50] Training Batch [42/391]: Loss 9.65899926086422e-06\n",
      "[Epoch 50] Training Batch [43/391]: Loss 1.3543716704589315e-05\n",
      "[Epoch 50] Training Batch [44/391]: Loss 1.3815365491609555e-05\n",
      "[Epoch 50] Training Batch [45/391]: Loss 1.8992286641150713e-05\n",
      "[Epoch 50] Training Batch [46/391]: Loss 7.465922863048036e-06\n",
      "[Epoch 50] Training Batch [47/391]: Loss 1.0680578270694241e-05\n",
      "[Epoch 50] Training Batch [48/391]: Loss 1.8967695723404177e-05\n",
      "[Epoch 50] Training Batch [49/391]: Loss 1.133911337092286e-05\n",
      "[Epoch 50] Training Batch [50/391]: Loss 1.7900034436024725e-05\n",
      "[Epoch 50] Training Batch [51/391]: Loss 8.526654710294679e-06\n",
      "[Epoch 50] Training Batch [52/391]: Loss 1.2719421647489071e-05\n",
      "[Epoch 50] Training Batch [53/391]: Loss 8.86472116690129e-06\n",
      "[Epoch 50] Training Batch [54/391]: Loss 2.0402636437211186e-05\n",
      "[Epoch 50] Training Batch [55/391]: Loss 1.1854921467602253e-05\n",
      "[Epoch 50] Training Batch [56/391]: Loss 1.8991937395185232e-05\n",
      "[Epoch 50] Training Batch [57/391]: Loss 9.660164323577192e-06\n",
      "[Epoch 50] Training Batch [58/391]: Loss 8.577000699006021e-06\n",
      "[Epoch 50] Training Batch [59/391]: Loss 1.0288652447343338e-05\n",
      "[Epoch 50] Training Batch [60/391]: Loss 1.770485869201366e-05\n",
      "[Epoch 50] Training Batch [61/391]: Loss 1.2540166608232539e-05\n",
      "[Epoch 50] Training Batch [62/391]: Loss 1.4490670764644165e-05\n",
      "[Epoch 50] Training Batch [63/391]: Loss 1.534232433186844e-05\n",
      "[Epoch 50] Training Batch [64/391]: Loss 2.0615481844288297e-05\n",
      "[Epoch 50] Training Batch [65/391]: Loss 1.9236849766457453e-05\n",
      "[Epoch 50] Training Batch [66/391]: Loss 9.265979315387085e-06\n",
      "[Epoch 50] Training Batch [67/391]: Loss 1.9007338778465055e-05\n",
      "[Epoch 50] Training Batch [68/391]: Loss 1.287080976908328e-05\n",
      "[Epoch 50] Training Batch [69/391]: Loss 9.129286809184123e-06\n",
      "[Epoch 50] Training Batch [70/391]: Loss 1.3036971722613089e-05\n",
      "[Epoch 50] Training Batch [71/391]: Loss 1.8600789189804345e-05\n",
      "[Epoch 50] Training Batch [72/391]: Loss 1.794158379198052e-05\n",
      "[Epoch 50] Training Batch [73/391]: Loss 1.4416980775422417e-05\n",
      "[Epoch 50] Training Batch [74/391]: Loss 1.3525784197554458e-05\n",
      "[Epoch 50] Training Batch [75/391]: Loss 2.1817884771735407e-05\n",
      "[Epoch 50] Training Batch [76/391]: Loss 1.3789789591100998e-05\n",
      "[Epoch 50] Training Batch [77/391]: Loss 1.21873117677751e-05\n",
      "[Epoch 50] Training Batch [78/391]: Loss 1.6800115190562792e-05\n",
      "[Epoch 50] Training Batch [79/391]: Loss 1.3412094631348737e-05\n",
      "[Epoch 50] Training Batch [80/391]: Loss 2.0562720237649046e-05\n",
      "[Epoch 50] Training Batch [81/391]: Loss 1.4845126315776724e-05\n",
      "[Epoch 50] Training Batch [82/391]: Loss 1.209903166454751e-05\n",
      "[Epoch 50] Training Batch [83/391]: Loss 1.1140661626996007e-05\n",
      "[Epoch 50] Training Batch [84/391]: Loss 1.4985631423769519e-05\n",
      "[Epoch 50] Training Batch [85/391]: Loss 1.6209120076382533e-05\n",
      "[Epoch 50] Training Batch [86/391]: Loss 1.3227001545601524e-05\n",
      "[Epoch 50] Training Batch [87/391]: Loss 1.0444086001371033e-05\n",
      "[Epoch 50] Training Batch [88/391]: Loss 1.7677175492281094e-05\n",
      "[Epoch 50] Training Batch [89/391]: Loss 1.0590213605610188e-05\n",
      "[Epoch 50] Training Batch [90/391]: Loss 2.6519421226112172e-05\n",
      "[Epoch 50] Training Batch [91/391]: Loss 2.3822238290449604e-05\n",
      "[Epoch 50] Training Batch [92/391]: Loss 8.471877663396299e-06\n",
      "[Epoch 50] Training Batch [93/391]: Loss 1.4880323760735337e-05\n",
      "[Epoch 50] Training Batch [94/391]: Loss 4.3640488911478315e-06\n",
      "[Epoch 50] Training Batch [95/391]: Loss 1.0214190297119785e-05\n",
      "[Epoch 50] Training Batch [96/391]: Loss 9.920868251356296e-06\n",
      "[Epoch 50] Training Batch [97/391]: Loss 1.044141663442133e-05\n",
      "[Epoch 50] Training Batch [98/391]: Loss 2.5368883143528365e-05\n",
      "[Epoch 50] Training Batch [99/391]: Loss 1.1636976523732301e-05\n",
      "[Epoch 50] Training Batch [100/391]: Loss 1.4289394130173605e-05\n",
      "[Epoch 50] Training Batch [101/391]: Loss 1.3727494660997763e-05\n",
      "[Epoch 50] Training Batch [102/391]: Loss 1.1671390893752687e-05\n",
      "[Epoch 50] Training Batch [103/391]: Loss 1.1923876627406571e-05\n",
      "[Epoch 50] Training Batch [104/391]: Loss 1.2469057764974423e-05\n",
      "[Epoch 50] Training Batch [105/391]: Loss 1.2044287359458394e-05\n",
      "[Epoch 50] Training Batch [106/391]: Loss 1.4709679817315191e-05\n",
      "[Epoch 50] Training Batch [107/391]: Loss 2.2623007680522278e-05\n",
      "[Epoch 50] Training Batch [108/391]: Loss 2.2748010451323353e-05\n",
      "[Epoch 50] Training Batch [109/391]: Loss 2.4328370272996835e-05\n",
      "[Epoch 50] Training Batch [110/391]: Loss 9.368674909637775e-06\n",
      "[Epoch 50] Training Batch [111/391]: Loss 2.12088598345872e-05\n",
      "[Epoch 50] Training Batch [112/391]: Loss 1.67297366715502e-05\n",
      "[Epoch 50] Training Batch [113/391]: Loss 1.5843168512219563e-05\n",
      "[Epoch 50] Training Batch [114/391]: Loss 1.3500282875611447e-05\n",
      "[Epoch 50] Training Batch [115/391]: Loss 1.284849986404879e-05\n",
      "[Epoch 50] Training Batch [116/391]: Loss 1.6487676475662738e-05\n",
      "[Epoch 50] Training Batch [117/391]: Loss 1.3930337445344776e-05\n",
      "[Epoch 50] Training Batch [118/391]: Loss 2.342636616958771e-05\n",
      "[Epoch 50] Training Batch [119/391]: Loss 1.0672156349755824e-05\n",
      "[Epoch 50] Training Batch [120/391]: Loss 1.604267890797928e-05\n",
      "[Epoch 50] Training Batch [121/391]: Loss 1.3432033483695704e-05\n",
      "[Epoch 50] Training Batch [122/391]: Loss 9.205491551256273e-06\n",
      "[Epoch 50] Training Batch [123/391]: Loss 1.1251636351516936e-05\n",
      "[Epoch 50] Training Batch [124/391]: Loss 1.1822054148069583e-05\n",
      "[Epoch 50] Training Batch [125/391]: Loss 1.12569532575435e-05\n",
      "[Epoch 50] Training Batch [126/391]: Loss 2.2456120859715156e-05\n",
      "[Epoch 50] Training Batch [127/391]: Loss 1.4446839486481622e-05\n",
      "[Epoch 50] Training Batch [128/391]: Loss 1.4598484085581731e-05\n",
      "[Epoch 50] Training Batch [129/391]: Loss 1.36306607601e-05\n",
      "[Epoch 50] Training Batch [130/391]: Loss 1.4040784662938677e-05\n",
      "[Epoch 50] Training Batch [131/391]: Loss 1.628977588552516e-05\n",
      "[Epoch 50] Training Batch [132/391]: Loss 1.6503738152096048e-05\n",
      "[Epoch 50] Training Batch [133/391]: Loss 1.312870517722331e-05\n",
      "[Epoch 50] Training Batch [134/391]: Loss 2.1619582184939645e-05\n",
      "[Epoch 50] Training Batch [135/391]: Loss 1.6538984709768556e-05\n",
      "[Epoch 50] Training Batch [136/391]: Loss 1.796426295186393e-05\n",
      "[Epoch 50] Training Batch [137/391]: Loss 1.4243230907595716e-05\n",
      "[Epoch 50] Training Batch [138/391]: Loss 1.2199964658066165e-05\n",
      "[Epoch 50] Training Batch [139/391]: Loss 1.6633206541882828e-05\n",
      "[Epoch 50] Training Batch [140/391]: Loss 1.3954267160443123e-05\n",
      "[Epoch 50] Training Batch [141/391]: Loss 1.1022473699995317e-05\n",
      "[Epoch 50] Training Batch [142/391]: Loss 1.0778363503050059e-05\n",
      "[Epoch 50] Training Batch [143/391]: Loss 8.540729140804615e-06\n",
      "[Epoch 50] Training Batch [144/391]: Loss 1.3684923942491878e-05\n",
      "[Epoch 50] Training Batch [145/391]: Loss 1.7292935808654875e-05\n",
      "[Epoch 50] Training Batch [146/391]: Loss 1.4840223229839467e-05\n",
      "[Epoch 50] Training Batch [147/391]: Loss 1.903867996588815e-05\n",
      "[Epoch 50] Training Batch [148/391]: Loss 1.3938253687229007e-05\n",
      "[Epoch 50] Training Batch [149/391]: Loss 1.0610907338559628e-05\n",
      "[Epoch 50] Training Batch [150/391]: Loss 1.426023845851887e-05\n",
      "[Epoch 50] Training Batch [151/391]: Loss 1.5801395420567133e-05\n",
      "[Epoch 50] Training Batch [152/391]: Loss 2.2619338778895326e-05\n",
      "[Epoch 50] Training Batch [153/391]: Loss 1.0900449524342548e-05\n",
      "[Epoch 50] Training Batch [154/391]: Loss 9.472907549934462e-06\n",
      "[Epoch 50] Training Batch [155/391]: Loss 5.324202447809512e-06\n",
      "[Epoch 50] Training Batch [156/391]: Loss 1.8891530999098904e-05\n",
      "[Epoch 50] Training Batch [157/391]: Loss 2.135615977749694e-05\n",
      "[Epoch 50] Training Batch [158/391]: Loss 1.5399064068333246e-05\n",
      "[Epoch 50] Training Batch [159/391]: Loss 6.402577128028497e-06\n",
      "[Epoch 50] Training Batch [160/391]: Loss 1.3845063222106546e-05\n",
      "[Epoch 50] Training Batch [161/391]: Loss 9.309163033321965e-06\n",
      "[Epoch 50] Training Batch [162/391]: Loss 1.251543108082842e-05\n",
      "[Epoch 50] Training Batch [163/391]: Loss 1.2729378795484081e-05\n",
      "[Epoch 50] Training Batch [164/391]: Loss 1.1680766874633264e-05\n",
      "[Epoch 50] Training Batch [165/391]: Loss 1.3733005289395805e-05\n",
      "[Epoch 50] Training Batch [166/391]: Loss 9.335991308034863e-06\n",
      "[Epoch 50] Training Batch [167/391]: Loss 1.4991516763984691e-05\n",
      "[Epoch 50] Training Batch [168/391]: Loss 1.2053253158228472e-05\n",
      "[Epoch 50] Training Batch [169/391]: Loss 2.151545777451247e-05\n",
      "[Epoch 50] Training Batch [170/391]: Loss 1.6242764104390517e-05\n",
      "[Epoch 50] Training Batch [171/391]: Loss 1.4107570677879266e-05\n",
      "[Epoch 50] Training Batch [172/391]: Loss 1.5917808923404664e-05\n",
      "[Epoch 50] Training Batch [173/391]: Loss 1.1060335964430124e-05\n",
      "[Epoch 50] Training Batch [174/391]: Loss 9.812719326873776e-06\n",
      "[Epoch 50] Training Batch [175/391]: Loss 1.0293064406141639e-05\n",
      "[Epoch 50] Training Batch [176/391]: Loss 2.9302249458851293e-05\n",
      "[Epoch 50] Training Batch [177/391]: Loss 1.5590894690831192e-05\n",
      "[Epoch 50] Training Batch [178/391]: Loss 9.628411135054193e-06\n",
      "[Epoch 50] Training Batch [179/391]: Loss 1.3935499737272039e-05\n",
      "[Epoch 50] Training Batch [180/391]: Loss 1.6165382476174273e-05\n",
      "[Epoch 50] Training Batch [181/391]: Loss 1.6892285202629864e-05\n",
      "[Epoch 50] Training Batch [182/391]: Loss 1.8487069610273466e-05\n",
      "[Epoch 50] Training Batch [183/391]: Loss 1.9258954125689343e-05\n",
      "[Epoch 50] Training Batch [184/391]: Loss 2.0884419427602552e-05\n",
      "[Epoch 50] Training Batch [185/391]: Loss 1.824524406401906e-05\n",
      "[Epoch 50] Training Batch [186/391]: Loss 1.5547688235528767e-05\n",
      "[Epoch 50] Training Batch [187/391]: Loss 1.4857598216622137e-05\n",
      "[Epoch 50] Training Batch [188/391]: Loss 3.183194166922476e-06\n",
      "[Epoch 50] Training Batch [189/391]: Loss 1.688440897851251e-05\n",
      "[Epoch 50] Training Batch [190/391]: Loss 1.3644277714774944e-05\n",
      "[Epoch 50] Training Batch [191/391]: Loss 7.418651421176037e-06\n",
      "[Epoch 50] Training Batch [192/391]: Loss 9.466435585636646e-06\n",
      "[Epoch 50] Training Batch [193/391]: Loss 8.11990048532607e-06\n",
      "[Epoch 50] Training Batch [194/391]: Loss 1.437773880752502e-05\n",
      "[Epoch 50] Training Batch [195/391]: Loss 1.2204889571876265e-05\n",
      "[Epoch 50] Training Batch [196/391]: Loss 1.106400213757297e-05\n",
      "[Epoch 50] Training Batch [197/391]: Loss 8.197082024707925e-06\n",
      "[Epoch 50] Training Batch [198/391]: Loss 2.4115623091347516e-05\n",
      "[Epoch 50] Training Batch [199/391]: Loss 2.7995814889436588e-05\n",
      "[Epoch 50] Training Batch [200/391]: Loss 1.860137126641348e-05\n",
      "[Epoch 50] Training Batch [201/391]: Loss 1.7988699255511165e-05\n",
      "[Epoch 50] Training Batch [202/391]: Loss 8.569567398808431e-06\n",
      "[Epoch 50] Training Batch [203/391]: Loss 9.181407222058624e-06\n",
      "[Epoch 50] Training Batch [204/391]: Loss 2.1577941879513673e-05\n",
      "[Epoch 50] Training Batch [205/391]: Loss 1.579594572831411e-05\n",
      "[Epoch 50] Training Batch [206/391]: Loss 1.6873402273631655e-05\n",
      "[Epoch 50] Training Batch [207/391]: Loss 1.9684317521750927e-05\n",
      "[Epoch 50] Training Batch [208/391]: Loss 1.307928869209718e-05\n",
      "[Epoch 50] Training Batch [209/391]: Loss 1.2146842891525012e-05\n",
      "[Epoch 50] Training Batch [210/391]: Loss 1.796165270206984e-05\n",
      "[Epoch 50] Training Batch [211/391]: Loss 1.528047869214788e-05\n",
      "[Epoch 50] Training Batch [212/391]: Loss 1.8810200344887562e-05\n",
      "[Epoch 50] Training Batch [213/391]: Loss 1.1925135368073825e-05\n",
      "[Epoch 50] Training Batch [214/391]: Loss 2.1389851099229418e-05\n",
      "[Epoch 50] Training Batch [215/391]: Loss 1.5081111087056343e-05\n",
      "[Epoch 50] Training Batch [216/391]: Loss 1.7030120943672955e-05\n",
      "[Epoch 50] Training Batch [217/391]: Loss 1.607621379662305e-05\n",
      "[Epoch 50] Training Batch [218/391]: Loss 1.9755381799768656e-05\n",
      "[Epoch 50] Training Batch [219/391]: Loss 1.4754461517441086e-05\n",
      "[Epoch 50] Training Batch [220/391]: Loss 1.5141951735131443e-05\n",
      "[Epoch 50] Training Batch [221/391]: Loss 1.5412237189593725e-05\n",
      "[Epoch 50] Training Batch [222/391]: Loss 1.4328109500638675e-05\n",
      "[Epoch 50] Training Batch [223/391]: Loss 1.7626860426389612e-05\n",
      "[Epoch 50] Training Batch [224/391]: Loss 1.2937755855091382e-05\n",
      "[Epoch 50] Training Batch [225/391]: Loss 1.8694569007493556e-05\n",
      "[Epoch 50] Training Batch [226/391]: Loss 9.292993127019145e-06\n",
      "[Epoch 50] Training Batch [227/391]: Loss 1.0910649507422931e-05\n",
      "[Epoch 50] Training Batch [228/391]: Loss 1.9553779566194862e-05\n",
      "[Epoch 50] Training Batch [229/391]: Loss 1.5923324099276215e-05\n",
      "[Epoch 50] Training Batch [230/391]: Loss 5.153559868631419e-06\n",
      "[Epoch 50] Training Batch [231/391]: Loss 2.0720915927086025e-05\n",
      "[Epoch 50] Training Batch [232/391]: Loss 1.0659163308446296e-05\n",
      "[Epoch 50] Training Batch [233/391]: Loss 1.6457692254334688e-05\n",
      "[Epoch 50] Training Batch [234/391]: Loss 1.1554121556400787e-05\n",
      "[Epoch 50] Training Batch [235/391]: Loss 2.6335812435718253e-05\n",
      "[Epoch 50] Training Batch [236/391]: Loss 2.17762262764154e-05\n",
      "[Epoch 50] Training Batch [237/391]: Loss 1.8429569536237977e-05\n",
      "[Epoch 50] Training Batch [238/391]: Loss 8.649371920910198e-06\n",
      "[Epoch 50] Training Batch [239/391]: Loss 2.2874082787893713e-05\n",
      "[Epoch 50] Training Batch [240/391]: Loss 1.2565243196149822e-05\n",
      "[Epoch 50] Training Batch [241/391]: Loss 9.552031769999303e-06\n",
      "[Epoch 50] Training Batch [242/391]: Loss 1.7095835573854856e-05\n",
      "[Epoch 50] Training Batch [243/391]: Loss 1.2355938451946713e-05\n",
      "[Epoch 50] Training Batch [244/391]: Loss 9.027819942275528e-06\n",
      "[Epoch 50] Training Batch [245/391]: Loss 2.329915514565073e-05\n",
      "[Epoch 50] Training Batch [246/391]: Loss 1.542983591207303e-05\n",
      "[Epoch 50] Training Batch [247/391]: Loss 1.9010576579603367e-05\n",
      "[Epoch 50] Training Batch [248/391]: Loss 2.044078428298235e-05\n",
      "[Epoch 50] Training Batch [249/391]: Loss 1.4027545148564968e-05\n",
      "[Epoch 50] Training Batch [250/391]: Loss 1.0168548215006012e-05\n",
      "[Epoch 50] Training Batch [251/391]: Loss 1.0981207196891773e-05\n",
      "[Epoch 50] Training Batch [252/391]: Loss 1.5936111594783142e-05\n",
      "[Epoch 50] Training Batch [253/391]: Loss 9.437651897314936e-06\n",
      "[Epoch 50] Training Batch [254/391]: Loss 6.612182914977893e-06\n",
      "[Epoch 50] Training Batch [255/391]: Loss 9.937358299794141e-06\n",
      "[Epoch 50] Training Batch [256/391]: Loss 1.6082722140708938e-05\n",
      "[Epoch 50] Training Batch [257/391]: Loss 1.4739872312929947e-05\n",
      "[Epoch 50] Training Batch [258/391]: Loss 6.739620403095614e-06\n",
      "[Epoch 50] Training Batch [259/391]: Loss 1.2794318536180072e-05\n",
      "[Epoch 50] Training Batch [260/391]: Loss 1.7105059669120237e-05\n",
      "[Epoch 50] Training Batch [261/391]: Loss 9.797501661523711e-06\n",
      "[Epoch 50] Training Batch [262/391]: Loss 8.588140190113336e-06\n",
      "[Epoch 50] Training Batch [263/391]: Loss 1.583979610586539e-05\n",
      "[Epoch 50] Training Batch [264/391]: Loss 1.2051327757944819e-05\n",
      "[Epoch 50] Training Batch [265/391]: Loss 9.256080375052989e-06\n",
      "[Epoch 50] Training Batch [266/391]: Loss 1.7793045117286965e-05\n",
      "[Epoch 50] Training Batch [267/391]: Loss 1.211938160849968e-05\n",
      "[Epoch 50] Training Batch [268/391]: Loss 1.9780107322731055e-05\n",
      "[Epoch 50] Training Batch [269/391]: Loss 1.1451273167040199e-05\n",
      "[Epoch 50] Training Batch [270/391]: Loss 1.7248330550501123e-05\n",
      "[Epoch 50] Training Batch [271/391]: Loss 2.4722445232328027e-05\n",
      "[Epoch 50] Training Batch [272/391]: Loss 1.8474020180292428e-05\n",
      "[Epoch 50] Training Batch [273/391]: Loss 1.4443698091781698e-05\n",
      "[Epoch 50] Training Batch [274/391]: Loss 1.72624768310925e-05\n",
      "[Epoch 50] Training Batch [275/391]: Loss 1.6706038877600804e-05\n",
      "[Epoch 50] Training Batch [276/391]: Loss 1.8349403035244904e-05\n",
      "[Epoch 50] Training Batch [277/391]: Loss 1.3462197784974705e-05\n",
      "[Epoch 50] Training Batch [278/391]: Loss 1.8054091924568638e-05\n",
      "[Epoch 50] Training Batch [279/391]: Loss 5.6799317462719046e-06\n",
      "[Epoch 50] Training Batch [280/391]: Loss 1.0946776455966756e-05\n",
      "[Epoch 50] Training Batch [281/391]: Loss 1.0874086910916958e-05\n",
      "[Epoch 50] Training Batch [282/391]: Loss 2.0393175873323344e-05\n",
      "[Epoch 50] Training Batch [283/391]: Loss 1.8473096133675426e-05\n",
      "[Epoch 50] Training Batch [284/391]: Loss 1.8187258319812827e-05\n",
      "[Epoch 50] Training Batch [285/391]: Loss 1.3187656804802828e-05\n",
      "[Epoch 50] Training Batch [286/391]: Loss 6.762019893358229e-06\n",
      "[Epoch 50] Training Batch [287/391]: Loss 2.3272363250725903e-05\n",
      "[Epoch 50] Training Batch [288/391]: Loss 1.4544507394020911e-05\n",
      "[Epoch 50] Training Batch [289/391]: Loss 1.084504765458405e-05\n",
      "[Epoch 50] Training Batch [290/391]: Loss 1.5532867109868675e-05\n",
      "[Epoch 50] Training Batch [291/391]: Loss 1.3640010365634225e-05\n",
      "[Epoch 50] Training Batch [292/391]: Loss 1.4709786228195298e-05\n",
      "[Epoch 50] Training Batch [293/391]: Loss 1.9380584490136243e-05\n",
      "[Epoch 50] Training Batch [294/391]: Loss 1.2894031897303648e-05\n",
      "[Epoch 50] Training Batch [295/391]: Loss 1.0821404430316761e-05\n",
      "[Epoch 50] Training Batch [296/391]: Loss 1.5071756934048608e-05\n",
      "[Epoch 50] Training Batch [297/391]: Loss 1.283349502045894e-05\n",
      "[Epoch 50] Training Batch [298/391]: Loss 1.6366690033464693e-05\n",
      "[Epoch 50] Training Batch [299/391]: Loss 1.9810604499070905e-05\n",
      "[Epoch 50] Training Batch [300/391]: Loss 1.4136278878140729e-05\n",
      "[Epoch 50] Training Batch [301/391]: Loss 1.4405994079425e-05\n",
      "[Epoch 50] Training Batch [302/391]: Loss 7.6104915933683515e-06\n",
      "[Epoch 50] Training Batch [303/391]: Loss 7.859166544221807e-06\n",
      "[Epoch 50] Training Batch [304/391]: Loss 9.844356100074947e-06\n",
      "[Epoch 50] Training Batch [305/391]: Loss 7.748230927973054e-06\n",
      "[Epoch 50] Training Batch [306/391]: Loss 1.2141924344177824e-05\n",
      "[Epoch 50] Training Batch [307/391]: Loss 2.1158542949706316e-05\n",
      "[Epoch 50] Training Batch [308/391]: Loss 1.808039814932272e-05\n",
      "[Epoch 50] Training Batch [309/391]: Loss 1.6757439880166203e-05\n",
      "[Epoch 50] Training Batch [310/391]: Loss 1.1219008229090832e-05\n",
      "[Epoch 50] Training Batch [311/391]: Loss 2.013521771004889e-05\n",
      "[Epoch 50] Training Batch [312/391]: Loss 1.5535873899352737e-05\n",
      "[Epoch 50] Training Batch [313/391]: Loss 1.090621117327828e-05\n",
      "[Epoch 50] Training Batch [314/391]: Loss 1.4870389350107871e-05\n",
      "[Epoch 50] Training Batch [315/391]: Loss 1.539275217510294e-05\n",
      "[Epoch 50] Training Batch [316/391]: Loss 9.590155059413519e-06\n",
      "[Epoch 50] Training Batch [317/391]: Loss 2.468091042828746e-05\n",
      "[Epoch 50] Training Batch [318/391]: Loss 1.6813961337902583e-05\n",
      "[Epoch 50] Training Batch [319/391]: Loss 1.3050723282503895e-05\n",
      "[Epoch 50] Training Batch [320/391]: Loss 1.081107711797813e-05\n",
      "[Epoch 50] Training Batch [321/391]: Loss 1.5332940165535547e-05\n",
      "[Epoch 50] Training Batch [322/391]: Loss 6.562792805198114e-06\n",
      "[Epoch 50] Training Batch [323/391]: Loss 1.2489916116464883e-05\n",
      "[Epoch 50] Training Batch [324/391]: Loss 1.430405245628208e-05\n",
      "[Epoch 50] Training Batch [325/391]: Loss 2.1677375116269104e-05\n",
      "[Epoch 50] Training Batch [326/391]: Loss 1.2709796465060208e-05\n",
      "[Epoch 50] Training Batch [327/391]: Loss 1.2072952813468874e-05\n",
      "[Epoch 50] Training Batch [328/391]: Loss 2.347086410736665e-05\n",
      "[Epoch 50] Training Batch [329/391]: Loss 1.655579217185732e-05\n",
      "[Epoch 50] Training Batch [330/391]: Loss 1.633547799428925e-05\n",
      "[Epoch 50] Training Batch [331/391]: Loss 1.5374254871858284e-05\n",
      "[Epoch 50] Training Batch [332/391]: Loss 1.5150829312915448e-05\n",
      "[Epoch 50] Training Batch [333/391]: Loss 2.2106774849817157e-05\n",
      "[Epoch 50] Training Batch [334/391]: Loss 9.841206519922707e-06\n",
      "[Epoch 50] Training Batch [335/391]: Loss 5.095077540318016e-06\n",
      "[Epoch 50] Training Batch [336/391]: Loss 5.617589522444177e-06\n",
      "[Epoch 50] Training Batch [337/391]: Loss 9.39270103117451e-06\n",
      "[Epoch 50] Training Batch [338/391]: Loss 1.1770980563596822e-05\n",
      "[Epoch 50] Training Batch [339/391]: Loss 1.0736413059930783e-05\n",
      "[Epoch 50] Training Batch [340/391]: Loss 1.4629122233600356e-05\n",
      "[Epoch 50] Training Batch [341/391]: Loss 1.0586554708424956e-05\n",
      "[Epoch 50] Training Batch [342/391]: Loss 7.97257962403819e-06\n",
      "[Epoch 50] Training Batch [343/391]: Loss 1.363807859888766e-05\n",
      "[Epoch 50] Training Batch [344/391]: Loss 1.2298331967031118e-05\n",
      "[Epoch 50] Training Batch [345/391]: Loss 1.5576890291413292e-05\n",
      "[Epoch 50] Training Batch [346/391]: Loss 1.4910379832144827e-05\n",
      "[Epoch 50] Training Batch [347/391]: Loss 1.6044365111156367e-05\n",
      "[Epoch 50] Training Batch [348/391]: Loss 1.1686493962770328e-05\n",
      "[Epoch 50] Training Batch [349/391]: Loss 1.776703902578447e-05\n",
      "[Epoch 50] Training Batch [350/391]: Loss 1.8170037947129458e-05\n",
      "[Epoch 50] Training Batch [351/391]: Loss 1.155841709987726e-05\n",
      "[Epoch 50] Training Batch [352/391]: Loss 1.0213293535343837e-05\n",
      "[Epoch 50] Training Batch [353/391]: Loss 1.4506193110719323e-05\n",
      "[Epoch 50] Training Batch [354/391]: Loss 1.6541885997867212e-05\n",
      "[Epoch 50] Training Batch [355/391]: Loss 6.55164831186994e-06\n",
      "[Epoch 50] Training Batch [356/391]: Loss 1.0565156117081642e-05\n",
      "[Epoch 50] Training Batch [357/391]: Loss 1.0150693015020806e-05\n",
      "[Epoch 50] Training Batch [358/391]: Loss 1.3396565009315964e-05\n",
      "[Epoch 50] Training Batch [359/391]: Loss 1.2609506484295707e-05\n",
      "[Epoch 50] Training Batch [360/391]: Loss 1.3377573850448243e-05\n",
      "[Epoch 50] Training Batch [361/391]: Loss 1.436107140762033e-05\n",
      "[Epoch 50] Training Batch [362/391]: Loss 2.0443832909222692e-05\n",
      "[Epoch 50] Training Batch [363/391]: Loss 1.4892366380081512e-05\n",
      "[Epoch 50] Training Batch [364/391]: Loss 1.700070424703881e-05\n",
      "[Epoch 50] Training Batch [365/391]: Loss 2.9350892873480916e-05\n",
      "[Epoch 50] Training Batch [366/391]: Loss 1.2103711924282834e-05\n",
      "[Epoch 50] Training Batch [367/391]: Loss 1.957936729013454e-05\n",
      "[Epoch 50] Training Batch [368/391]: Loss 1.4830244253971614e-05\n",
      "[Epoch 50] Training Batch [369/391]: Loss 1.5042858649394475e-05\n",
      "[Epoch 50] Training Batch [370/391]: Loss 2.8995378670515493e-05\n",
      "[Epoch 50] Training Batch [371/391]: Loss 1.5287590940715745e-05\n",
      "[Epoch 50] Training Batch [372/391]: Loss 1.3738981579081155e-05\n",
      "[Epoch 50] Training Batch [373/391]: Loss 1.2260378753126133e-05\n",
      "[Epoch 50] Training Batch [374/391]: Loss 2.3485779820475727e-05\n",
      "[Epoch 50] Training Batch [375/391]: Loss 1.2023701856378466e-05\n",
      "[Epoch 50] Training Batch [376/391]: Loss 1.6335736290784553e-05\n",
      "[Epoch 50] Training Batch [377/391]: Loss 1.5799785614944994e-05\n",
      "[Epoch 50] Training Batch [378/391]: Loss 4.94793857797049e-06\n",
      "[Epoch 50] Training Batch [379/391]: Loss 8.820106813800521e-06\n",
      "[Epoch 50] Training Batch [380/391]: Loss 1.0513992492633406e-05\n",
      "[Epoch 50] Training Batch [381/391]: Loss 2.288860378030222e-05\n",
      "[Epoch 50] Training Batch [382/391]: Loss 2.7490455977385864e-05\n",
      "[Epoch 50] Training Batch [383/391]: Loss 9.388078069605399e-06\n",
      "[Epoch 50] Training Batch [384/391]: Loss 1.15744423965225e-05\n",
      "[Epoch 50] Training Batch [385/391]: Loss 1.2534591405710671e-05\n",
      "[Epoch 50] Training Batch [386/391]: Loss 1.3969981409900356e-05\n",
      "[Epoch 50] Training Batch [387/391]: Loss 1.2596778105944395e-05\n",
      "[Epoch 50] Training Batch [388/391]: Loss 1.4397003724297974e-05\n",
      "[Epoch 50] Training Batch [389/391]: Loss 1.4417509191844147e-05\n",
      "[Epoch 50] Training Batch [390/391]: Loss 1.3746898730460089e-05\n",
      "[Epoch 50] Training Batch [391/391]: Loss 1.2090236850781366e-05\n",
      "Epoch 50 - Train Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"*********  Epoch {epoch + 1}/{num_epochs}  *********\")\n",
    "    train_loss = train_epoch(epoch)\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), \"vit-s.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66dd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"vit-s.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a191c2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 3, 256, 256])\n",
      "Labels shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOH0lEQVR4nO29a4LjOK8leEBSdmR9t2eDs4ZZwswy5s9s83ZlWCKB+QGABCU5wnZE1qM7WeW0QtaDog7BAxAASUQEv8vv8i8r6e+uwO/yu7xSfgP3d/lXlt/A/V3+leU3cH+Xf2X5Ddzf5V9ZfgP3d/lXlt/A/V3+leU3cH+Xf2X5Ddzf5V9ZyqMH/p//9//71IWJ6P5vZ8fZ90fn7a9Pdq3xEbu+9HucXW3c8uyeNB0DiscfJxnP5x3lg7+O+2JtE1SaUPim8LcfL3YNAcDk+xnie0l27SGjsuHmvf5TJY+tJ0KIh54d3veLQMRqYjcQkTttNV/j//t//q8PDhrlt8T9Xf6V5WGJ+0x5VGoOUfYN1yf9h0Tsj5PuTeFWn9TRrzLte8Cr42XHD7KzhY677/x13OXPHXfKycFjH1G47a7ysa2IECTuXA8JbckmafUHAYhM6hKI7kjdJ98/8M3AfQSwii8a22HrIcDvjjlgv4PTQBwOPwXtCU3Yj5TW/p+WAAd8BuFxBB2qQzLogR9L+7d7qJA/a7z3nUrv8Wv3nDp2vwX171P+hAFcAcDMEBE0ZjDLdA/Z9w46bDxcHgfuHjB3D3uQo4brRjDfv/3xd4odgE7qdE+gE033v1fi6/9IUJxD9CPp2CF7+ClZlRysxyPuXX7ugM51yWovmHUA3ZBxJs16ARGBiJCsMokSKCWtU0ogClezFyAi2FoDs6DWhgrltiyAsEAI4A+k9TPlYeA+qmx9duzpuc8oZntp3cFJ1vg0vYDPrgNY4x3QN17k2WDb9z/0qHvo09TRZsnqiocrnxT+ulORXpvd6NGHbX0W6eCMAJZ+X1dqU0L/TknfS0oJOWfbzkgp9ys7cFkEeatorem1xaQuqCuQ1OvkbeA/PAfgJ6jCcWg6u9UzoLUTXj92evD+DzpI7lx72nvGs3fK9/6nJ8eU+e+dZWIP2hmmO2ifPmO4kkS5avzmoP5HSwONbXIJb9vkwCUkAnIGciYQKYBPgcsCbgyIICdCJZPcIp1L67eCVnaU45nygsQ9vs7Xbn3v+qc/nuyzf6Lk2ovhzvtOT75bnIpNT2xaxalwPlz9I+kYwBL2OJAigH00ov1FAoAl1ia+IzNfETm3xI7z60MSRXoiCtKkNKFkQs5KF0rJKKUgpWTfCh0GQYzKNhaIMBIB3BqqEXbxOpB00AJk79yew5H9YPklVoVXiojcB++ZdiToDeFtMQ77uAHkVJbaeUGf6CpwOOzk3Y9Cx10zjIdePiT3qSE11CFIeVeCRK8g4Vm7XTeMFhTPFWO8/gNG5yECktXdgZuTgnUpCSklLKWgLAUpZSxlQcoZwkBlVcREAGFGtxkbVdAf4hAWxqyDEeVxEfj3A3cC5QcD8R3VXiCGX5pkkAPocMa0M/4RQRv/xRjiwllxYyfodB+Oxx+6icgpeM14NA6k+Z07SMffO24TTxyP9iEsIoCdIuREKDkhJUIpCSUn5JRQSkZOGZwEaAwIoe25sjh4dx0z9qQvjNV/P3ABTG9k4oB7KXuCoABCf5k+nJ8OPqdIRkeChO3p+HtC3IbBPnqbRI6ShHa4mqvgP0bQhtvR7kQhk7Z2vMgQrYiXshOdMkT+E9qNoFQh0QBuSUoRLkvCdcnIOWFZCpZlUYm7qMTlJli3qpdlRiNBmiSuS93Y2+L9XwfvPwO4BtiDpDl71SeSNwK2Xw5PNIsMqXWIHb0nLs9uHjqNE8w97saIsJdGEi4ZznIQ9gcKU6kmbSWAvh8P6XZTkmgIEyRXwvxuAbQ5EUpSfnspCddLRskJy6XgYoBVABe02gAImBkkwAanHyZxp89eCNFh1zPlCeVs3PC++HmuvFzvu/TiSBOeqanstqcudKcX7Gvx0aAwOqacVmwc26F956AgpSmAPlKdrpDRVM/4CApYndFKZrNNJEgp9U9OCdkogloY9NP1Ku1F5ovA9h0A+5RUffxtPS9xfUz8YlT7kZPPKswDrPdwwacGnnDw/kl2rOzDax7vOaTbkMAm+Uzi0QTKfrT9NZji+ACTVigCdahhQNyxxiXugOiwBCf7DAVMP4SUoBI1E3IClkxdyl6WBctSOnD93q01oAm22rBtK9Z1xVabfm8bamto3HT6N4xmU0uFQWTa+WB5XOLupNrz8ixeC6eK1vT7yfZcXCrtjE8f8dt7NwlXPDvxo+sMSDro9opZtJYOEkDTFZ5tSYFIU5qABoBD5f1b2etkYnOJGmiBA/eyJOSk9OByKbavoJRiClqCd8rWGMxNwbptWNcNW624rSu2raExozWTvoFn9/HwCxTBywsc1+XGa+Uz0D55pbvlnvT9HpJzrMc8ShyZ+gxUCWA/m8g5G9j9ShiAgJjEZT1a4vhNiLb3bvrCCUWI1CAnlKwKWeoUwaZ4nRZAZ8TYAFpbQ2sNrSqgmQVsI4HbeL+7vKicfeDp89FZ3wLYUTogXMrSHkAvlicGkzEI0sm9Z4VtdPn9VrztPdCOmwkYIgyAVfJKC7Wg8G1bNPaoBcFmw2iYvBaTrNdLweWSkVPCZclYFt0WgTnOAI0bto2x1Ypt27BuKmlv64qtMVgU1MK9e50/E73+rl7yVRia9/03/BlIvxPErhBJAInWTu7pQZ+XIwn/8FDZo5Z2v31W5A6/Jb/g+BZmsAFWeIVwtXtRcLBPyGkBKJldNg0+W1TRWopK1LdLwdXpwaUYWAk5JRCptFXpKmgsxmcrttpw2yrWraHWhq2xgluAJi7ntWZ9O8yWxQZ8Fg2vSVyCKgH0Kiq+s8wD8bdI3KdrQId7quIv3fvJacHYDltCJ/tjiQJCaYKwSltuDcy1c9guWSnZpEwGkIzbqoRVZYtQsiphOp2r9EAnFxTY3gmMIXR6UNv4KN8VNBGVtMGgsFdy98rZV/juE26NoxYEUk8jgXlWPQeR75G2DtFZuaGOg78CtrvnODNDfGj28oOj+StKJB8+Zgio9NsgXNHqDcxrJwg+dSuUkSBAKspViZETYSlkkwoZ10vR7yXjcilKDy5FfRWAyeLYWLA1Rq2M21rxflPrwW1rWBujVkFldOAyZok7PfEZaJ+ExIvmMPS2fIIOfjPH3QE2VuvEx+C77/thiQ1CDspzbzrnuePVEiKIJ2uuS3FmcKtg3lDrDVxvJmWNbhB0ajYRAAZJQU6CkoCluLUg48fbFdmsB5dSkDNhWYq5VfrEhn4zA7UqTXDgNmasW8NaTUnjoYxxtyRE1XA83zPNeVZenjl7BrB/SblnRvibbj9RBp+e9X24J5x30jbCVhjdqCSqnAkzpDVwq33kdUEmyIA0QBIIjAT3/AJKVoWsZKUIJWdkowglZ7uf0xGeqEJjMUuC8tnKEqjCoBUCfOdc1aF8TTkzmvBQyM4+guLDcx6xWMgHf/nOr7baYz3hlFNTAKjMx+55bCc9XXGJnz0vjtcSM2lxh7lza2IBSUWihJIEJQNLcYmrgHWq0DkuJaSUVaJD48ecw65b69L1tilFYKcPDDSmLmX9mcQqLlMndK2V4l+YfVQ+Ly9JXOVQ6iD8Gcf9KOTmozvoZV8B3q/o4ndMBrs7TlD0l7gbFe/YYHDkuM5aBwfu5IiMFiR1iOGkx5AMrywClNcSoyTBUgiL+R8sJWFZMi6LAjfbBENyCwIAYUFtgroxGitob5tPOjDWqmav2oAqSg1Uro8njErZ2XN/ZYB80arwvEL2v2TZIXaitg7wHUWdXhbNFokD//NzZD7foxS8gydzwladVKC8Fp1Zqs3WZ8qoT+FmdxQPoNVKae3ZfG1bUxB3mmCUoZu93GMtPPl5of71Bb0MwDd4h31EY56jB/fP//sk7wlxnsb+HWJpd9gd+8E4yN/gNK8VjjbH7662SZfiLnFTIkhWsIEFQtz5sFoSGCWL0QTCpZjUXVQpSzmDMEDrQZDMQGuCbdOZsXWrWFelCk4ZWNSK0KDgnQMhP4JvBPBrysnTHNdBNCkYX7AW7M89A+kcNnTKKP3k+7+9XPyeD9i6+p/7Y+hw9HB/JCBIuuP54zoUvsn8EClr7htViKh7Z6kyxsgkyElMKRNTzMwxvKQRO2bTxWKmL51oUJCqH0LDrRpVqIytaieaYh5OjQV7Rn9+zJMU9x/ij/tw+ZwbH4H23eWbzBfhMuesOT5DBK35zyYCSQJJBiFDmGw2LUEAdeimSEMsjMbVptDJxSQnRMBC3WpQmbG1NiYbWNDY+Wys0axCnnW/z1rs2bf1jwDuY1C4c9Tdk2fwnh32OATvGrsePP5e/QZy46TJLJ+5fxwi3TEGGSktIGYINzBZ9jDS8HD3SSAwIA0i1bzKRo4xgaAJwM2mahm4bTqd+75ueF8rtq3i3ehBbXZ8t/buub2WhI+A60TqdQHwJeB+l1x7pPoH98X5x2hXOdTqHoyeazbZ3cOv7HsOBPeDQoMqnwyvMgGYMeDBXeJmShpmgwVJoLNoJGAwmAngZtG6djdp4WMOOiYvmcXsscBm5q5tq7itFT9vAbimoFXe89n9M+jbSiS7/bvmkTEt8+w49kuVs+8sXxmcv2Fg35Wj0nUulR/pLjMbPEqm+NF9BMt3gKQARtYYNM4a6SgCpBHB69RAPcpmqqB+BWJ+CGY5YJtcMIqwGW1w7su9FrGeUa1U78HPqMKZpH60PA7cJ9F5rlCdXPYpi0G81u76k7aP3TEvlDB0n19NzM/o7AUe73us2U7qSr9qYI1iAFBJ64GIOemUbiJBSQUZBGYCcQNYp2yFRR1lAEDM/ZGT+ctyn30DyEDaUJtg3Ri3dcO6NbyvFe9rxVortsrDXituBTnJDUFkiiEh074t9HjGoNg9vdhfrZw9cr9vUmdwz3hy3OfHPT8W9GsJ0NOv4OSm5Jas0XGmrDqnWRfPpGuYJSSf1TfQQkGbIGodII1p8LwHmsjDPMJIc3qllMCSp5ZiFrRWQUSotaLWCkoZ27YBxFjXhvdVgXtbldtuW8NaGzZ3Z5QAMrsyBfuxS/clKf9esgZdEk29EgDMBixdorO4K/zj5RcrZ8PT/zuudZRl59d/VBE723es62j4fvwjD9RPO6cLkWLpdhA9FK0IGPkOTNlKHaxjZKP+XwIRmztl7xrdJdE/TgdAhBok7tbUt9Z9EdhAJjP2jqCFqJN6ou7LuxSzD0tsQQCNQUY7utvAA00ay2vA3T3E/cO+C7Tn1/octB8T/8/rdkIV7p00kX0D08m+XieJsLJ7mSJFovInkTnHQGe+SrK8B0TIAcRJEpi4S1wg61XJ5DczWqsK0lqx1QqkhG2rJnFVEdsq4+dNKUJrasOtPlMmgxiQDTc915g9VSJgsVm5t6XgesnB/j9adLOOoVYMMSL0HHifB+4To+93KkWPAu81gN4pjvg9rT450F8mgGBMp1Pa4Jca8/qe6h4AGGQh52R5uDJF4JL97T6zLn112lZSMo+B1KurVKEBINRWUesGUEKpG4QSbls1esD4+b7htqkitlaxqIYAK2uPZE8Q0/1rHVXSXi8FP64LUvLJEXTJm2hDSk07R21TEz9a/gI77iODdBw0nzlvv48ePO6Re0ataYZZLHvuelY+/lXs2oMWuMlLIxbUUODZEjO5JNbpYLem9haw7B7OlEncn5YBYrBFLVBqqE0TerTa9MNODwJFgCcgQXcsH/U0iW/7JnfJbEnyLMjSM5WLQPPrGk3prf1XK2ePlM+Vs2lM7fvuK12fX/8773kE8bHMYej7o2Y2O67M/Qwxy2ghqLcWqe/sUpQWXMzPICcCSQXappaBViHsExRjkoLstgxzPBdCEg0np/WmTt9UAMp4X5vRAgNxM9Ca9YyMxiR7qJGXAQZUTRKylIQf1wtKzni7XvB2vSARGX/mrpD5Qibdb/fkTXxWfiFwxzh7PlDe2zde+zjqo337bIjfc88ju43K4VnN7t0jFtmdOT7JgWsauQczXorGh12XhGtRjy5pCW1jcNOZribDsaY7R9JQ+HwFHGIBbRskrchVwJIhlLBuYlEMQG2myHm+LyvmEWE0wWiBuUleLBL4uhRcDbhXAy4A0FYhqOa5w/OTy/g8U34hcB+XjvvzHrvW1yTy5/c8P+b8+tFr9qMSpa3/HUxfrnB5psREXaItJWOxpB0CBloCCVt6+kE1pntFiWYO/60xUm2aqC9vALJKWusEwrM0HLVVruCRFK4Y5pSwmE/vZclYzCndvwFCYwE1Nl9hDJeJ0ALPll9KFb5TUfpblLM7NTljx17OSYWE42eHGf3WYT6BUFLCxbji9VJwXTQS9+2Sewh524AkKyoYbEpc/wSqwJARcmN5xmTdwJJAuaEyAUioTKhMaKLfbFO6IgQIjU5loM1QSrPYSPDjWlByxo+3Cy6XC3LOuFx0W6D8dmutS/7YmSJdeKb8L6acfec9P4f70Rx3JlERqG1UooY27uSmJ1TOyfJ5JZVcHkJu+9FSsOnadd3dNEpKowksmpwDlLoNl9AAqgBlNCY0SeoQbt5ljia3w06JROJIYMrY4lI2Z6Q8st/0ynhdWCDBNuz3+gcC97sVpb/6nmfnffzL/B2tBUNeJ6jypYDNHRA/rgve3i5YcsLb2wXXkpGcSxrfpUaoCZCk1yNLw0TmPOMpPsUAKz7ZAIFQhSCBkkAkAdTQJIORdQwQrZ1bDAQAaHSkbJHC11KwFJWyP94WLKXgelmQsiUREbUdMwu2rWLdNv1eV6zbZhMcQwo/W/51ytnZ9X/NPT871/efN/scQRZmwOx3V24SKZddsloTfrwt+OPtglIy/ni7aNYZIg12tCR1simAmUxhEgcr26o2qqzB1hzzHF+qrlWIKHBZCCC1+gqp7ded2s2q1v9W6pJRkiZ7frtYiPv1gh9vFyw543JZ9JpQmtKa2mq3bbNUTRW3VbcbS7c0/MOowvcqZ4/u+3XK2f3jaLd9BlrQbps028wIF9dYsCUMu8tSsJhprFgUg5vJUr+2fibnHM+HAJe8MhLlsQDEUObaNAccEYSScmCITYBYR7aH82ncYvTlUgpKSbiUjKWHuSdzNDfFsNuELSmef8zTLJrFni3/GuXs77vnZ5xXpe5e9o+pUAnGepO4NCRusdSeroT9YRJ3WQr+8+MNJbmkZjWZkaBZLtvsnUEGPdB5fwOsJZ5jZk3VBECkmqRlUAKItBdQIoAyKCUgp17vZKJ3KWruWkrCH28X/HFdgr126ZaE29bU/dHWO6tVfXu3bbOp5RVrbWABKuPFeMJXM9nI6R9/WYkD/OflKwqd/0TTo04APShZ/sLtb1eiMFazIajh/rpklES4XgreDLg/rhf8uC5YiqZGygR4/lsHp9MC/4j/3aVvJFFix6gLozAPSSs6VayOORnue5BtRNBUowrmS1ELx1IyriXjYrnGsq2DRuIZdpQe1K1a3lynCuqV1pp6m3Ubbm/W53z5/hKrwnc62+yH5c8f9shjP9733P3H2SPR0PDgQk82l9JIi6ScVYdZ//ah160IOTkoxMxSCMN+iB+LcWRRhT8rIpbzzWfqWN9NuIY7gHvK/GxuiiXTqJN/yNKlWp00Mrj1nLnx05h7YjzNm/uK0+koX8jWCPNIuwefjxQlu8xXc4mJPAhe4FHl7KOj979Nihf2a4V5PoNk/gZu3lJnmEtJeLssaqNdCt4umrL+ellGys+iaZSELXG+K1zsKUZNkk7rie18HryOrr0zQ6hBeUKDOjxkc+hJaqNNQMpKD8qSkXLBdSk6wZCd16a+FoRwUwfz1lS6NsZ62/B+27C1ZpJXaUNrGiIUWbkT/2fQ8AWJa9yO7pHrT9Sh70iAF9L4vMqV0K9wp+wwPUvZAV6nA0Ris17oEmsxaaWJkotNj2a8XRYsWTX0t8ui0702HGeb8pWm6ZDQ3B6rgNW371SB7R2c6efRkApo/IG+NAZASQD2yQu2qAqbcs6ee0GBe90BF/bsYuE8LEYLasNt3fB+00TPbgqrzZQ1q5IvpfqKFvI6cL9Ab787MznwF7BtGRr2/r6D20aFbDjKZFs3LLsttGvmarC/lKIZFFMKC+Ep6NlmrxowpGpf3cb5LfrTT52qDyiRQpjiBgBgxTG4P4cvj+qZb/qn0xenMNri3R8CYp5nw3qgErb1/GMxf66/sF++erq3CEnPpzJ++saEIPfLLPoOEtY45UeS97M7Hc8k/3+6xkQPIP1FJwun0Rkl95jy2aVk1oOEnFTxUktCxmXJuJYyAJLHskzi9+uVCBzU12wouc+QsWhSEBCQWaUic0JjRuKhvine1SzmoUGJ9LtYhyu77XnxPfWQ8MgKgaBWxmqh7fpdNdCyRX6rDSeg8c5ewM/rSe8Oez+3ln5UPtfz6e5xU93oCO5HaxUZr15r3h9BOwAUF7bTc5ZMuGRNXe9J5kpKffZLTWAD0P7dFR+aOSrCfT1cJ+WE3NTntZTFQMRKJ0RATbMpEilwmZPmXXChDd0mWLYbA25JZjO2GLdMggz3XtOcZMINjVQpaz19vgzAmoP42hOLiKVrsgfDWKXoVaH38uIlM3QfB+1ZRT9S7T7b9+y9HjrvZHu+kkwg9kzgHcTdAdy9vFKnC8WHXqJpKHZNXQE7u2rSrjK6Yo6fo2YpXxeYBKawaTp8kdSlt5/vgYkk3kEwdUJ1YTSLCFzSYrIVj5D2EZvmIO5g7vTAJ0DmFnVXBqK4/7HyTYmdXwftvbMf3fcry14J61LWtnWIVZNVIQelOn8vXbKaj0FOwd9AfWsvxcBsUnes2BiDEGN97NWbOyFblEEriwGpgVnTMBFpGLqGqicIExq5S6zZei2qOHeKIJpLN1m+MRqf1NfkVasAwfKLOZdlUSm76SImW236W0/bZAw7BHGCnrUljPLEAn2w13VWzvY+WKFHrAFnStFfgOLoNh4/w+zl0hNm6iJb5M5WtCGlA0sePLG7swiBUMxV0KwS6fxxu6JFHoZOSDkjC6OUAl4cuITWyKgCwIUtdEYlXrZQ82RLfTUoF1Z6YAoZYNRAwZqkae4IdfxVlstAE00cUqvSAu5p9dtIIuKpSTFiM2K4znjAv4jj/u9UOmBpRwlgktZoQHGwJsuGmNyIb1q3qHZN0sBEEK46K1bUmsClAEjjHUalMNAH57hEYWG9nG1mLA+/BFHJTgCKJIhkMCekJEjNMjqSAGSL8Dn99DXUuKl9tmkwI5N+gKTJR2iswtMz4AiG5cDVOCKcZ6Gkh2XbWflLgTu89M957l9NBT4qA6xuIvJF7dBfdCbgsiSbtrUp2kymgatfQYaCVaqgSkO1yYKUCPJH1ZXIAVvNMQ+ua4pL9NKilEAiymuLvrpi0laE0VJFSrommQORmfvSUD3NksWUNdZWz6kgZZ+6ZaBVleCkZjhpvp4EW9Suui/6SjweCayuiprs2RPj9YkGI7RzhqPXy4vA3StmjwPPzTv7o1+fAHwd7p+d6TbNHslK1B1jPFDwuqhF4O1a8GaLNyc0EFd92SyQVtVDqq6o2wbhprN+wrhcr2oSu1ygraP+rD3fYXczNAWrr2huwGUDLrMC1cDKZJ1DGC0nLNyUHtjwLSYl1XKma6Gp4tIgrOcxNHJCUgJE75NS1rcnRcFapVsOtqbJ86rFrHXfNQIgFCYc9u3//Lv/gsTd2XJfv9DhWq+d+1wNPjt6p0MMZxnT5sdyonmKVliyZRywuC5dFlSHXq4VdVvBJtG2bUVKZA7XzYZy46gk4d5D6mrdVJGTpImZU87qmujTvxDoajswRxoYGI1Ps0bwpu5aSMH1wW200CllqhrqkxKo6dKrxBlEjJgojxnTBINPPkts0PCaYy6fV7DzBHDHTWS3777xKpSDyUPuaHwn5p/T+rzwuB9eL/xmB2SKCtgIwVYbrdpq1aE64e264MdV3f7AGxpVnUligWwNwhV1u2F9/xPbtoFbxVIKhBverm/Yth/67LSAqBhGh7YWeS0ZrwURuFioJDckErOvJl3QxEAsUmy2DZ0qiLjkNSnc3EmGOy0QEVRuIFA3e6WUkVhAuaEJsDZgY0ITYGsIXNcnGdDf72xVejzE9Ky84NYofUkir8AgC/2g081e2WAGUWkSpMt0WuiRzuXPtNJTokHjHIxzD1WKvDseTyNpW/Hh2YDbLQbFgGsA9m1pjMoJDQmoaoaCgavVilY3TYW03pBSwu32jtvt3bIo+iyZ2mcno6OR7mQeW4AgpwzJmivMRwQRhuTcW2Vkf0SXkg5iZp3xqrb8k+cNUyrR0OqmLhHbCtxWgBIo/4SkotMSqaCR2iEa5b4uRG9uN9TKQEps+1fLE+awIGd7hcQE551eQydVo+O270oT0GT+7sPk3nqyB+2OOwewxh6/r23vFDTOK8nCZszz/yBxjRq4f0ExiwILqeXAlSvIyCgjISqAG1pzh+uqmRY5g7khpTgcxdFuJLvrUthog6Zdyjr0pyEIRq5aG9J7CLrY7JqmZ+LG3R2ChdFEFwBsLOBGEGIIJY1bS5pMRDID+QJJFklhLpgivVXxK1Tvp4E7qmAN2oeCuZsN5jmOHqDw3wYQdaZmL3UHS0rh3DgLdD4JEgBKQ8Inu2GvkYQaOhhsW31mM7J9Lx24uZu9ljwsCdeLJnnLOaGhATWBRNcg8/v5UqatVmzrim1dkWhIXBFGSrqwCBEgsoTnMAf1BDD7TFhCymrqAgSwmTJFjVsWYjRwn/jqYTPMgm3dsN60A+VU8S43VBGAG+q26ujQgArtlFUITOrumK4/kC8MygvSkmx9CG/XQRWkj9RftSdoeZoq9JcQKMPoXIOzjL7m0mLufbQDos5ChXBrk1JdN01pHGvoPto2Rg370E/u0D0k1ExDRv1ckoHUj3Yx85TGgRUDawpuhwZiS4ZxMRBXZHBNgBAa+ToMorbc2lCrBQ+uK4gI63rrwC0lYVkKyIb8mF/Wp0ZTSuCUtLNLBrI/g9pyvc3ctOZTygQMJcyagJlxyytyWlErg2hFq7Wvzl63Feu6hlT6wK0JGjLycsHlD8ZVEvKiSabVBAaopXu2IX0HYL08v3o6nB6EisTfQBjpCmcy7vCNvFM/0rmuSwb3bfVjUsKYnyeTSOFqfZsw3cGnUf3c4d4x6hV5bbIe5f6onqllMXqw7NwOs03zqlQbc/okY3juSDGep95ZIVdtoAqt+bRt7v4HY3Zx1N2T2yXSQEd3peydntDXgEg2SdJfWgAuUUIujFwbIEAL7ouTMiXmd8vmvggAqfVncCokIdAytu9eAxJ8DcgvmMO0AcQ5bv9j/DbKXOlIDXrjEswzCWNeHB7ygp6WXYfP1BdMJguldve43g0oehwN4CbSqdLhwOLj1pBmHRBwqhA57sh94IDIySYabHG8VpXXtropAJtHKYSRp0v+pDbW1rBta5e4y1KwXBYQAbVeTEGztiQFGkSQzRFcPA4tjFT9WPc0M67QxzyC1pksCLNPaCQUFiyWgeZyueBy3QACGhI4MagJGqk/hE6YpP4+Ac/xQBMUYujWmQHsFQA/DNzhMyldsPawnVlbOpS9By8ZGAkYvqAAStxOIwy7ZOhicjZkL8uiL5BGHgCQGuypAzdQA5NMrvCcAtfr5hLf6AKRJqLLnWbI6HjwvAbicw0QALWqucuB2w2bcEqi9WbRpBjbtplVoWG5FFy3CxIBtVUQZfhINUaDNEnjadyhANww2jjN6ANif2WEnBklLyDSBB3LUg24Gy6XDQDQhMDEQGIUEITJMtaEUS3qNhbXpnW3uLne0tEO9drU04sSV7zbftJd9vr+cAMc0bBjaCN3o6PhHpgIljBDPak85wAlBS6RzzClLm11H7qh/j5VCGMmdhSGRsCgJlN2qWHRtOLXGR1AzOtKGvd4sChte/PZPVRYKnh1XQaacg/4pIQqZv5MGKOKU5toLTGA9pEtDnP2qFEGadOZbRg2nZwzcmu2QHW26Wh1RM8gpKYytKdZ6tI1stkzcmAgnojaa+WlmTPp4AVM/T09ancGgMFpE6LEVfe7DFYPKiIsybK7EGEpwPWiQ9PlWnC9XEBpgLcD2EDrLzMC95QqdNBGiTjq3ad8YUqjQPknu9HfAWqc1fwQlCqYxBXu3Na7rHcyTXFP2LYNAkHjirfrBdt27TNqzjVTDiNIAmKOr27I9yHbGrsPRhFc/lsPuVcXRinqcA4h1ItK3GXbcLlcQKSWhIqqElcnAtWZPVHoNwIyKavRFcq5fbnVIWu96J4z+vBZeRi4TCn8JXPnij1O/AHifpOw9kk2T68g0bxXbIvLNRIgJxRknaUhjR5wG+nFNHidQco6lJqdiKCOKMmA4SuCd4nrUnNqwXPQ0vQR+FSAMBkgBUxKoTT6wCWor+iYIORWAacIcBquVAtu121oTTvWVnVBvJzV8uAhOn0UQPIb3XlTJkgmZje0e8BljvM9F0TaoVLOyGXBIoTrtYEZKFuF5AVSKkptkFQ1bWgqoLIgpWxWEI/c9fsdFTW/Z9wz/fZgeULi3rmogWBkTzh5+RTogQgorm4oDEZDgqBJAyUBSVaDdh7cKfqsunkH3tspSqMxfHYTGDB+j6LfN3fmEdrX34ALrTE8AoDgi9XZtgGXkneYFO5t9Qz3EDEQuUbOs7WBm2rtAMCJ1SE8yQCj27Y6XYFp9TOLi3INUN3EBc1I/TmemlICGYDLoqHsFyFsnIDUsIgmloYd51RE+v3PwfsxLfhFCUGYsj82OreToQgM8MaqeEZCG6rZJCx7KvgGZl1flsXc/0iAUsB0Va/mVJDh8U/oktfn7ClZJpYUpKxJ3xRMOlFBOXRBHz7DL3uJSwBgztpi0oWZDFi2HaJvW6sABLWDOEx09Ls4xSCAG4g17XxrFXXTdcg0IFKzzGTnuYCmDe2+t3GwjYqP3eUg+AakR45aq1hKyHkBSGfgKBX1yS0bpGjouZQNtDYICI2S2W7JgiEDWAOFeESaPkMWnrAqDLNMFxUevdRtlrPE7d5UpFwV9lKFN0i76foFbQPzpjElXMEkwLKAM4AlgzL1wD311CezqQbgkn8GVSADMUzad67n9Qw0xhEV5e194LJO54oobWCPdjUfAVGfhJoz3H81Wjf0VgYxG9LZEtF5eLfbdLdtQ85ZV4mkhJYDcEP8lwNRXIz3l4YxMsv4Y7Ak6V8d+pSRCpnfb0FZzH9i2YCieb+kbEDZwAJsTd0Zm9hzuPiiPUV5jgp8Vh4H7jS2IigFYcdJ8Zc/Zrs8sUUFWIErbQOEIXUDyLij+bOSrZLojtHd4pDI9DJTuFIwfaVhAuugDOKuT0V2UTtAG+E7ni5OBAhg06rcX4m9MOe45uPANEDbOwjNL09lwPBjGBkOLeCwaQg5WwK7PhTLkLjTxcJjSUBpr2mgRX1bRtsAUJ9bGVPMAkGVhIUBSQ0LAwtD48ksLy8ZpRmXFAwf3O8vX/LH7UZnyDDKI8PzZS09v4CGbKMyuAqYChoyJLE6hzRzu8sWVXBZNOWPO7EknRL2ZG/cdJZHF6QTpQii9s6UCOqLqq3uZqNuQhJzfnFzXoAqDRzDOyMhwllTgyIpABLGKKQDigLKzUhiEnd8cs+FkHIe2n4As8+kuT9DTgmFc++Qbr9V+zCGndjqgbDlwGUMgHchjfm0SVc1IEcd8LZV3GrTEPQ6oh76wtRi428XrLTrpOcIHiLvGYb7gj8uXMHqcfY6e7Sk1KdJl0zd7e+SLYAwE6QCUhNarmhpAVdAmDRcxECZCSil4O3HVdMSldJjp0hYnbHXtZvDknPblDuvHRI3TYqSH0tEnfq4GQ0GaiLqaua90iV2B5MGTIolS/EZpZR0NRrnqTmpTbRlXTNh6ikGYM1V0FCNKqSUwLl0yc2Zu/QXdeOapW4AgNIYi4joUh3d3swcqQKNbZfSYi6KosD9c91QK+PPteJ9qxDRVKHd/zbkTYitdZ8ivE4fXgCublPvstKpQCbPrq1OKNdCPf37JZs7HDFqy2gtQ9AgbNO31iGSRRVcSunRBd2MBR2KGzNSGGL9hRPIhnGTtEknNNxXlWHSLSUAntyYER22Aexmlk7KxIwIPREyQaN3KXagQRdSChw8pel6Ep8x+DBwa2gYExM+AkDOJO6od897ICP3wRlw/TVGsEaJzLZxs2QftTG2WlGrKWfmDSYHThDo0XPC9KHyfASEOOfruiQSxLKxEN4uGT/Mofpa9KPp1xNkE3Al1LShpQ1cAXDrXFT5q7rqlbKgLMVSXFr0q2nrIhiKWJe4Sg/Yh2UHjw3JkkrfJlFlTsinXzGkrv8Nk4DYYZhgkhkmvfvO3k4qXRPYqUGXvDYrVTJKLpBupHIPO+odk2odVgWLJyMQOGd1rHFReJC44325Q3hly98lHmozoiE6mD0FvvF0By3btdfKeLe1fW+bZqnRl5YhlK0NbRbTR6xIGaTvONSTugL3eHncH7ebXGwSAepXQBAsBJ0YKAl/XAr+83YJwFUpfC3qLc9ZULmgclGnDRkrtCjQfDuDfCEMn7YSgBuDuQKBFnTgkibKkKSglJQBt6vCFAWy7WTO3V3YEjRgcFZoOiTJmlfQ69LP7AqgnuH0YM9vNS9YDhxYTFkdI5eYpIUpeRqgaUmXiVC4zBI3WhUCgJkF1RcPMQnpoPU09h4s2SeuLWdtT97RKQAsrZKv8duwsdIs0rBnTBMs8BFkVnHvl+McwGflCUdydw+2adoEFCiPXBLUH9WyaGt+V8Il6aIb6mdrEgK+AmJ4JON38TO4EvUpQ+02HvCMvkW7mvr5+2sqzw33AHbHjobU8TuY7X3buaXMQPF278IlPJv/3uvgndGahDCu5YuNEDDRA+Zm7oRtkrh7e65/fLWbxqyp7Gs1yiAzcOGjzpgy72q3eB5H89Ag6/ymc/TpdgQ9YcdpJXw+L4/z3YeBm6X1S+cEXJJK2UzAdUn4H28LLkvGf/244r/+uJolgC2vgCCjoVlOE53kNStfAK1LJn3LGqItHbzWEII+O2S8xRmyKU16rlMJClIv72y/evjwMBu9nqCzeuOeLm2HwX9IuvHOaIA7vAyfzfM6NFsbzHmnMPfZOzblzH0gAIBL6dx4WBUMsMaHXalrTWcjq+WobU2/b+tmZjbNQiOigY1EhFQWZJu6zWXpIBYfpaDroDGkAzc5NUh5AN+XVHEAT0PWOYhfpcAPAzcZcBOppL3khLdFfVV/XDL+68eCy1Lwf/xxwf/4z1Udl+sGtE0buDGABtgiHJ4qAmZvpTSA5sCVwJniMkS2nEHHdwQJJuk6TFAaxh0UowjcQ083e2eUrHE49v3erQgKTntTB2+wwMdTTshsVMGuxyJdNLMpW43I1ibzBB8K+mgOc9D71HBcZ2HbKt5vN9Ta8PP9hvfbajx3DP8s+vzL9Yrl+oZcCi5XgHLRiiddGwJk4ySFODq3IHTQRv3AR0d0VvXd+tkTwFWqMGbChrvhxZYNuhpVeFsKcgJYzdPqqzrlNXGurIWMDw2XxNBrETgYvMEDVRBMRu6uGrgU3VGFIW2p6w1epP9jGyIdXBA33jtwTfI65RDndTvQxkak4SY50xV/NpOw9qAigpYbqFGnCkxs1EGzkrtPQ2uMWnXBEGbGum64vd+w1Yb39xt+/ny3dKDUOawDlykBqYCFQEVHSZifBZKOZb40ShQOA6zu0O9vILTnyfd3lMeBq8EayEgoUPvs1QD741Lww8Kz/7jqJxFQRdcDYDSd3q0bpFYI1x7pCkq6DEHXQJ1jUZe4Y6oDnWIICClkS1HFanZzdFvvnjJQlAwTcoeef+CNPIA8cQiXskGsTEq+g9QdrrvkzUjGZ+PLdqea4X9AkMKWETwPcFdLk9Qa2lZRW8O6rriZZL3dbvjzz3dsteLPP9/x5ylwVQn+UQVVSKd384KyJBOeqQuVOAJGejWmaBzYUX+9D+JYXAg9Ux4HLo1vTw+/eBaXkvuaW2yRoQTB+v6O2/tPtG3D+vN/4vb+jlYb1ts7bu83tFqRcsFyhX5LRoa1SUogKvpHyl2JSUA3h6WckIq6PlpCWiAVIGfbzkAq2uApj8afFLHYtM5dEUApiKiMEtWHzS6I/DXYteMtzm43K1M85vqDdK/JnHW2ii1tHax1U8latw2bc9nbivW2orWG99uKnz+VKvz5fsPPnzcDrk4YiH2nXCB5Qbpcwci4sNIAEkJCQiZry65zoAsXrWEcGbHbjkCNLlizDvBLOW5QjhFDYXIKeV0BgBW8JGqOqduGuq26MNu6opnSsK4bamvIDEhekIVBWYDialJ0nskzGGTwRlA290Z3r5slrwQrhVCYAp6K7ETBAG1XHvYaBYAx7zCQS3t6gI8B7FRk/3EAu1OPr85IpP7AymUbtpu1a2tYbytut61vr+uqC+Stqy1DOpLSDeCKcWNGKmOJ0sFTR5tGkhfdIO+BcGJdfc/umE9ne87L41YFu7avaFg8IbGtzeVrXjFX1E0Abri9v+Pnn39i21b8/O//xvv7TYF7syGtNpTlgisKclGf00tWLOaUkJFBSeP3u9GLaHSelHpkAMzmq9JWpYSk3IEtXXIoslyiTTKgK2FB0oqrW0e5MLkq7vid940uhQ8fv90scbuHmX0SaWU3c6oXVi57e3/X75/vuN1GuzpYb7cNt9uK2hre33UFnGYJ6XyKtrIglwX5+gOXHxuQF1QGsrjOEZ31nbZFnWMnLffrg+zlAYwCOacHnUiFx8oXqYIt+W4RsCQCaQ0Vmlv19v4TP3/+ifV2w//87//Gzw7czSQDo1wFNV2wLAQUhiwa1rPkBFBBooKUlwmw7iugro2DQyIRKClwxaiC2IREH+66SJTRkJ2URsDOpNUlr+IzaoN0AO3DJYK2z2zxBF6COe5YGBIzY7ut+PnnT2zbhp9//omfP3+iNTaJu442XrWN1RxWLRWoYLOp4MqiuRF+/Adv/6mgYsuZiuoPCT6iKd2SYCHoNH//SAOVoY0Pjx3ae9eeD5YX8iqMP1xi+FQiCKiujTf1JNo29yjS7RayVVdLpFqbgLKgMlAsY7v6IqjSo3xL7+txYGRKhnNfEv0025ds230qppxWkLGaoshYvO4gcUPj4jPXm3nI27+sM6HSlZgueaO03XFgi4wgQIMrW0VttrLNtvVVyqspatVmy5qvemOplJqlYGLRbzLeO5GjoHgNWkA99ZacPI6Ef8c+OjTEkLg073uyPOGPq9BhaGY+nbuu3TSlAY8a+JhJIM202fcV27rh59qwVm24jRM2ZPVXRcEmCcIJqRFQFXQ1CQoaKAOFax+WezohsgyK7glm36WwRqSmpGHXxdIoFe6ZaTyzTIyBi9K2R3f4S+pCWQ32QfRODe82Vs3BJRaK4zRjvNohtSwCoi8A4tkVw73s/pplUaeC66bxaNu6YV1vxnF9cbzaZ9zidYcvjlk2YCsFlYtOPmRdiI9yMaqlNEso9VXQepjPngL0P4wqyLy/CwXfHQXCCwMV8EzoTh8CSMHXRIefpnZEX04oQdcNEG74+ecN7+8qBd5XxlYtOyBbmiIIgIxNsuZgZbX5kggKsSpsiZBbgydiceASoSuIE3Abo2QBJULJgqUpiBdOaEX6mg19fTKIrWg7wDwNcdMbshEgKBQRtH5wjxlz64CElxckvY9WMc4M2I+cerz7MAhzl7Lrtho9uPUp3lptatg+zp1dyiIREmX4VG9ZFuRckLOBNw3gOmipt86RKkQ9wfFx0Aci0APH7W38Qnk6AsIDkJ3oe8J0kmZDcEMSTVS8bhXrpvmytsaotvqKusKRTf0mNJB1CEJlfS5pAJNGAzM4gBXm3GO2UQv889RMCr2GxAmCBpCm1nKgKw8huIqh+yNCZTxt3xWUCJe0MzPogHRJeuTJZ2zBpHqQzqqL7cx1gUow0uDEYRVHT43ka53pJ9COfjGC+yQQWSR1t3erEnZItjIpZWP8cMXz1AYr09e0Uzv5ifnlifICVQAaE7amY09OwEaAeYXrN1cIN6y3G7ab5hhYN1vuXfTFsCSb8cog1oU1wIRWAUq24gtr1K/n5iIa6ZomiQuYgT+hNEZu5tdr/DlnArMa73UFc/SMOeIdgTzdkzUwAe4tFYfC07YRl6DuLsjz+l6dLgRea+dxpwqag0GnqrWNPPOOepCpHYvAqFvFuqlJ0ScdRASt8sht29ynd0hd9+dIZmEh81GYqYLTBLV5e/hRD4I0vcOfiEK7dF2hq7L+rJ1rhcYMvekFAD9NFbp/i3oPq10QosGOsMWRWcwPlLBJAqOASUNzYLNdMJOL5lYtaFRUGePuYKgSkVnnEjpVcHdKdIoAjIQfuTFKZZBx3ItlVtxKwmXLPRqjhxQlYLGwo2JT7u64HgWugzju1x/HzNqI+OXBb6UfBl/ErjZ1gtmqLrdUTVkVAUohgLI51JeeECSbzdw5NnPDsmga0lIKPNO4ZxvX5UkVuLetYd20U1EuQF4sBH1BXha8/fEfXN9+YLlcu7MNgt/IbLMdPHeytMR2idTo5LcDv5X4x2Pl6bwKPiCKjDn17sMq0NUzXKq6OYUIlAvI1smi4KZIuUBsGtZpiFsCiG3ohGhaU7+3SVwiQZIAXB48jJzTQXmtWCKNnMjqaMpdJlv6STtnyiG8uj+3+06E+ruG3Ydx/wzLwHhxem505G5TzBbC6ovDpyL5SjxkuXptskcT7DWdAjYHHPUOGyvq1CrIq1oV0taQ1goRAlwBowTKBXlZcLlcTPKWThuG44w+wUEhC2UC7zxEhX0DPdOJo3meKo9LXBmMhkGo4g4z0hO/DeauLluMDM3qIwCyeSEGE4uQabELJGU0Stq4xltJGCBCspUMPbdYN9CEGTsFMiG1hJxUqUspjfSg2dOGqo/FW8maPbwkyGJT1iX4yu5MNrGFJfzjUQRivhd9mO5UwWmBO3cztqpScLOIgq2q4gqoxKVUNN3U5YKlLJaLt9hET8L1csWyXNBqxY8fP7DeVojY+gsNHbjvqypr77eK91tVU2HKOklj0+A5L/jxX/+Ftx8qccuitEEo+opYOepcR2nrf++YwaSEHfSDu7C7W15QztCzrzCkh/J06e+rEPpg71EIIWcuGdwBBLMLQZDhPic0jK4gtMl81QevgC//Vkea1qeEiwVuuidbImBbMtjCi3DJSBCw/ZYT2VRuCs8dlJTdC3Dp6qHkA8AM8UVzjRe2JgrSrYVVGI0yNB17GQkpF+RcsCxvuF4XW2294LIsyDmBW8Pb2xu4NWzrf7Bt1eoAmOqBrQl+vmsCjz/fN/x8X9EYOqNoo6CQrthz+eM/uP74A3lZUMoC5NxbmXeTAzuZeY7BCaxDSZ2E8QtgjeUJ4PqGDqVu8AdkJDGO4NUtBawNqwouB4EB4zAPbsf2HuJXmoE7uhJmADNGyHqSsYZtIjBbBkgIlgQAGZXV75Uo9eF6kjIYN+lULAybRz+DwW/jgU4pxiJ5qqx6QGMTn+AgHcY7VbCg0WXBsmhIlGTWmLamS6JealVrDcMy6gC1MZA2y/W1QkjXJXOHI/e+I5PsZSlIpYBy2nXax7FBEvfd47ffUx7nuCc37R0KNJQ2lV9Q0IYeOQ29tDO1uE9n7JDiWpEeGqjJAVaifFeDGPXuIKUabLNunPTFJsDyPOjLqSWjso4IbM8yeQzTGD2sGv2ldGZk/0TTU7eIeY2FukthE7XMNLHIAskQaCQB5QW5XFCWBZfrG67XK0rJeLtc8Ha5oBTl6p7u3hc/YbOvN6YucWlRf1xaVqCorwKHFmToqJQvPgkxcj1M7zwg76CcHvAwJOtewp7Q34GJJ3nuSwlBpiRxUbW0ymkyYom70cVVB3Bw1HAFoN8hnOm/9fakYcOm2C7+6+DIANRbDS7tFLRrZVwWVf58FUS3LXfgdsAidDJMLzEWOfxBE4AZDtxkYTBKjVTyMZAWDYnJF+RyRVkuWC4/cH3TlSffrhf8uF5QilIvTxrtqfc7cK1zbFXA6YJSG1q6oaUVtblbo74b1peFVJYQfTI6HtnIOj2W7Z+f90S6xv1ygld7W6+Wp90a7xVtAwqVpOkB3RHFf/OrRn/OR25qbX23IcYQruANUT5aHxnafXeoRpCe8eb7UQIYvbavmbWr3GmdvF0o2LED8fF4LfPGSjkjpdJntHLJGq5fFpSiKak4aSr+1BK4ZbWIMJBYOyCSYGmAUEOpglIBJEbTBghWDw+Ziu9BcGj8M8A+WOTO9lfKCxK3QxHzAxr3DWTeh4541twgQ9qe3+fej0G7j9fqp47z2BqcYXmuACxZ155NLD0TS4pDe/fZHZxzL3HjDNLZyxhUQYHa6QJSl7xsElcpv/kQuMS9XHG5/sDl+oalZFzfLnh7u2ApRVvavMjG9DJ0UUCTuGsTtLQgbw0tLWjpZvZiNsnLoKaT7hopEjLrnI/pJ9QBk7SN++61ib+rr5anJe5cTx/OI3j9QOx+O7/qgCedQFWmm0/U4wS8st8npCY1gxmbjPOFkjOrUjQkr9trd8CF+kGogLo77vVKRkA7r2Wf5hZdSE+647vnmVVFKZULynJFKVcsyxWXyxVLybhcr7heL7qMANDtxRG4mW0NMiGgChZOkFyxMKEwKcmvDTAHHElttOXkbzuGeOweeaZ+u0eP4HCB1b/9hzPF16/8eHlpnbPvEfcU/o3lI0l7v9zVgAmHYX2W/kMZ2V9hMGWfLt2fH0RQfNcubcW4taArfh6lQSmPdE0gZJDlHIu+AyGNauhE45nm53dbMYslovNpX3eyCVPRHB5hXGq0fZ9w+KSfflROpe7pkc+/79eUM+xf/j2uRyc771dyJhn3gH1ymw+upC/aWz/y1d0xBioCJmf1ccSwJXS5FGbHImVQyWoCztNxCiDQmDjKglQacmJdViAzEjNyzlhMwy/uP5AyKOc+nGs0ggSqovkOPKJhawpatRNX9YM2393agptjmCCZaZ+MJ4yg9Y75AIr3h3x8ymu04QtpRj8op2L52Qp+5rg9H3m2DYTXQTScQLrwckoQqmd/nzIcZ0YGmW7Am8Brkk9GeIzHeTEsgUYWpLKokgS1gydhs9fq7FUuy0ECjxwGYkqidyDqUrbWhsqCdXMn86bf5lTeJMz22RSzK2m+mlJQqSfq89Wh9ntGai0vA/f7KMNH5RHa8ESHOBli99dRQ8LxGGfP+5iqkxsEfht9bp0qKPgS6VKmPlFBEnwTki98l3qOX8/6GPUJrYIrfuNe7kDeuoS1fXGiBF4n10OMMwR9orsf3nvcL5fXlbTnlbNPf90TxsCbEAEQEx7JtG8u9/Ye63N/picMhd0JPCoJLmVNEevb4yaTIa9ThCNVELjrp0rZysBmlIFB0BgugIpHXqAnSsk5maQtnSYkGn6y/SOiK5nLkPudKlT1f+gStzaVws1TjSKA1iVpBOh4I/6mBp14vOzfz+eg/8XK2fM3p9OtWW58xGdf75X74qE1LhVP70ZHBuzN7hnRERJRzzFiY6Khg7YJtsbYgunNLQxRuuk/YQIA2gEYFADqkyMw09qukzTGZr4PW63YWgTsblYPEUyzNSh6xz0PwL+mpF9y1UfVyb+sFWa5/Oltd1ThXD6jD/Pd5NOH7ZFblm3GjmUAkbEDYrc2jLvNQ3To5oRwL5iNON5LggN5pAbzqBCfLvLXuaXOv7+rfOV6v045e3Xfcwe8VnaaBk2fQBVkvDIyMUWTxB2g8EQaIurLVs1Ta2uC1RzFh5IWnbMHXglkXnKYQNmVPrgTKfffGe77AA09t4jqdYsU4egfHAE8S9vxx6z0fm95RHv5qPwa4P4Ty16inL6Jo6Sd5dTMbYfTuCtj1E1TTSRQBQ0hqpazgG0GjWjAR8Ebc6WNFPVjatg+IWbPQduCr+9mprCtNrPhjgw1e4m7n7RxmvCrB8OviqQn3BofeZSzfjTzpfvK2T01DCfX/AvLEH8dtDGitx/WeSN1Uxj3yYc4CWFnPpgE43yIHwpZNL25SezcVjtfI3QX4ADeYx1+eXnEQBzKyxL3/Da/Qjm7d/dfAebwak2iuhLmjilj0WnxwzpwVPpJn3jYWFPQ+6RAVMyOFhDpyhYjgtE7hdOEpGse2z3WykoN+jJOGlG9Ve65wnzFx9FuR8AOyXtCD4KZ7NHSF5TZfR/b+7X3+CJwz2E7pOd9qXv/vGfK94J2kmqCAUznsZaLVmBJmPsx4xx20Aq6L0Q1mrA1jzVzJc19IuZnOip3wbLQ6YI6vFcZM2Xrxh3AalFofR0yl/YDnO7YtLOdyGjXvTR20/ffOO4dyrdy3Pty8+NH/ic1iBcBBj2QQA9kJk3RtOTD95CYMknOkRwZmLu47K43+/JGGrJnLj2RXV/fIQI2TDKg3xbHFp/Z/L1Jh+8c5756raeB+5pH5r+k9BmlAFZhowzNpCztwBQl7ph00ORyQzlT/hklaDL1ChgGtz13lVnqEnWq4D4QuhpOw7aZpLVVHzdbh6xfDwAomt/8vhgoClwYCL/HJrr7yyiP0IRj1/hVExAP4fVjmhC3nlHOfllXkXnDpaxYDWUnccdEQ5xZQqcQ3WbLwaY6Sb6gg5i1LeAlSO4wxAtGCiWWIFV58q+tLnHFfRFg3WLvvI/dK6K5EqfNdHwvTwmw3aFP6mGn5XWqcHrzf5Zy9tFREZzu1yqe8cWSxLm9VkykKqDGcM5CnbvWJlirOqivPeXUcCnswzvCcNxDOQbfdKCqeUvNWiyClFeACCXrVO7PnzdNk/9+w0/Lg7tauHsT6Qlc9pMXnh5g/m0cErluPOZTuRjQOOdT24FcPr3SQ+VbOe7o39+jnN0/6zHQ3ruOOFftoBVIignoPDGPUoVuUcCgBQKbbhUZNtTWjBq0LglbsKHyBAoZjxJstW7SSiZR11othatSlJwTtq3i57sC96cBtzXGrdbujeYO8d5JhlDdKWUHlj32z5RhjEqH1j8B7fGF7Pd/jeX+b6OcHcFr39E222fCIsdF574INCEqSF0R8+nWbkud8/JOxQA71aYzE6cKmquhAkgtIVUCc8JWK9ZadV0IW9Gco1KGcyCqgN+BVvbHnlk77Pwz8N7F6S8jeABeAe6vrc/p7Z4F9kfStu+ceKu7BNpidy5xbWbM0alSU/m5Wwj6LJlRBQfS4J0qcQHsTGCRWA7lzM1qZFKcqCJx6vlxU0pYN03wUU3ivq/bmHwwRdCVsOEQP5Ozua12/De4fs5tuVsO68Bdj+AYdOF7yy+Z8n2ELJwxna8NHi+UIOEmALPba8MH1OlCMyml07pur+WRed19BNrI9j0eUK+1149c8nlWHFRgTcqhUyLU1FCr+uRuteJ93XQFI7tXl/pApx0uYRW30Wa7B2r4/fASfP8MPYkPdQfAHcjhe/bxjf4Qz0H7lwD3EbLwPKGI5esQD6ws0AR0quBm21nADFOY+iTQjiIMx+2eqXEvhTp479TLrkekwCdodhpJDBEFbk+R79YGGVXtQJiE+7m01T076bqTuOFAwyj1BcmPD3ZH6uoPd854rfyjlbP75Xvk8uCtmCwMmkKJQ2OTH22TC+osUyVI2zZogktcN0vFOzqvPD6SPlNfy1cUIK2lkQvYkvxVUwCb2XGbWy3mgTw0EwUhNzIMjWEvSNvpvLDvns0sXmivpIm32p5yzO36SvnfRDnb80k4WtGttkJBKbLFWPRAE5BuzKe+3JIDpzL3qVY1SbVgBtu/mD0Dn6vJLKhgJIalELWfgiB1n4g4zdyvFlMonUhfz3i+bxVX3nrNenOdUAG/pIQd4cSx+0gZdoQDr779/33cGgGc9vADz3W6gD4kSjjTR38FuHmCnUy5xus9UzenKWySLM6teXGHHufbvHMmcFie+8ac8Nsz8MTfznJJHJ5rB8ozfnu8C14F79+mnD1aviqN3Uo5rJV7ieef453OIoajlNNlr0Z4zAhOnJUyOukwh9domn9P3wp45uxwku7vURTd/ut3IUCGE01UeyYVKPLYw2PTCR89q/9+A0NPm74HzbJH+Jbya5SzExw8vfIlPRqcfnIq5vaMoD0DMN2r2jR0akijpwStTZMnV3MlHDkLwhTvgXDee2vj5caMayz7ep1FRMxPuK/6ebPTrjpH1Zkgk7XjvMROtT84ZKIYveXkGq+95W+NOesD66EuZ/t+bflYFTw/7vwcg7VLRIz4rsaMJgOsB78Ev9tnz07joCixzj76c/TnPfFF8GudIm5HeD2PxJ36HMovnlh4tPwjlLPD0Q9GBzxW7r36/Su3bdJ/ZjDYETSogntm+eIjMX1+DJOZiOdZhsep6O8MCvw6tCrtx41Rz0EP9KDOb+XICvZAj9kvezLKQ61Ckd1Bu5HFrTX7csT86+/5Lw3dOSuPgPTsiPNBRw4NTrvt6GKi9/ffd5pMyInrWrjGePmkg62c09yKEGy4CIb1/Zh9Z3iWXn/q33vzvGd612XXzh3CZ159wm9P7v15+RiFR77rb8LdGcc3cE9hfa5O3xyefuRa5/vsl2+VrPFuJ/toB+LDwif+sqknBhlccuQ3cCsCy5yHaxje7/A4u5d0aY495nZdKl4tjg/xKe+Tm4+LhH/vKFomNcm/nf/s73KQtvsL72rzDaAFvkQV7kvgR5Sz7wLt2eC7Z3sEhMWrbVlVRKN+6h/EHF32rmxJN7Ofco+mXWu1hHJiSTf6aiXHBgDp6kOCQEViGbJyfwUHrz9rpAv7p73XqgeBH+9wMrvntRCZ5OdU03FF5xrOS3Z3OwP83b8fK39vXoVvvN382kdJQWZ18MIlLgXwkiZX7uJ3RA+4H0CP8+JmGWOqJZnTRMlz9K+9vCiYuqkKExB6zQWdLAC79x2e7p5sv9vEEVuIOD0CdgJb4LKdhgmGWdcl6m6Gcd43P9+RPb9WvjVY8tHyKyjC6X36dw9YMdAOAPsfPbXoxIYBZ/cudTtVcN8EOaMKd55vNzxEBhtUpcBtjzz4eK6fdb9NpxrdeXVzUuZxYM9wGfvk0+VsXPxaeVw5+8gMcvLbAOcHLxLfB+I9PdBrYwdFl7wDol78vexDzUU8VWj0SwgS1xaAHtliwmuieVgXILjgzpJnlsDPJeQ4A+1+z9nrO32nk8VgjFbemc6FNYXr7ca/vQUiiv542+OuD8vXqcKndr3HgRmHyY/2nevKQ0bFNFy+0FOC9JXWAegKkBYes20VzEllnTBKSqg1W1pOz5UgqMxY181yzpojjbsuulfZ7rHvDfsH39a/sOwBqxgN/LXzVf0+YG+61nSV+fgP+sX+rGfL14D7jDH6Y8EL4J7keJww70G7l7RxvHRvMPczAICalO9K1t99lUYNQByK2d51cURKzPX+e2D5Qjm8x+goc/jp5LmeA+t3lC8ESz5Zm2+mtcfLjWFtKGBKC1y+pSC9mQVMbAGJQLbll5gbMiXkrF5g7lCjqzYy1m0beWfNC2w4xTz30H+H1I3SdtrWHWHb/tlJ3TjUy+G3eJ/P933lyV8D7j9k2m9f4oI5BDV7DV4LXSzEjtVIA009nwC0xOBGaE1XKE+JRlCkeKpQwW2ttnh0G6k8YQ7moL1oH3XDuTXgrwTvITJhtz/sCF+z0jb8D44K13nozvy9u+LL5VuUs8/kyq+xhc31GRaEMz48/+1rJpAw2Nby5b54tiawyOay2C0J0T+Bh5O4iBOPb3jGb7S2PBqs+PFxr/726U2/fKW/wB/3Vylnx6vrPDsfryCwld5dx9cPJ3ce1yiDmhJy1Rm1lFyKogOUxVLVxxmzUJtx9bPu8kqZO/2Zmey8fT644h2q8Mi+Qcbib+OXw/EnEv5AT14s35PJ5tN2e0zqPqec7Qdc+0Xm/bP3vSKwOe9lsqlbZcIpjckICrdwIIqg+9/KJHF3uhmN875WPrfVPrwy0Sf04N7fo/36Dx9aF+5J8AN4vzi6/EUREL+CKng5OvhN2sP0AvRv5qG86W+sUlYUuIBOBx+kpsAy3bgH2JCB97Twf4I28Cmn/eR4YPccO976sI047v8ieP/VoTtxEiG6VUPct0qm1u3RvKThLroiqc+auVJHZkqzQZni3WTYbMUz2gi8y3wUZ/F3lGepwKf7ZRK8X7r2dJEXAPyScrZXnEXC9OnJ8f28kwo+okSczq5N3EqhEiHkjoGTxPWGN2dVX2F9r9i5QxCFW/QJYOe1nSJMluI7IH2MKn1neQREj0rij5J6fATgu9+PXOST8pJb4wkff6oCX0/P85k8kw/+2pushidD/I6hMf4ZyTYiaPcSlu7MNMnp5semsP0V9rLc9+3Per5973PcXVXk41f/q1MveflSKn0KCPb0QvQNxPvzMjSnWZaZtBUEqjDXWb+jwVc/0q86pK7s7uVnu2P57Ox3Al7aHyOAHNOrfvyc8a/HlLMYt3av3NX0p22rs/8rx+943GSr/cUA/pI/7gQaA+yjg6KIfNnBZtxrqEnS+e1sYdgne1Mj0uw3pteLQ/6JZJfYaSJF2MdWTBUMl/jracO+PKb5AxMYgwyQk3M+A+13A/nLbo1n4H1UY/wO8O4uOG1/2FQ0wDvTBuCINgrbmP7+dKAXnISafct0xbeUz2ytfd9+Q8bTnwH5o2t9R3mB435ekeeq+gse7N4lO0YDuejmBJodHaaT7l2X7v/0SH3+4eVU8erfRw72FZA+e+avWRL1qfJPkT2/y19adkh9FgUkf5Ua+Lv8Lt9Y/gES93f5XZ4vv4H7u/wry2/g/i7/yvIbuL/Lv7L8Bu7v8q8sv4H7u/wry2/g/i7/yvIbuL/Lv7L8Bu7v8q8s/z9UsW3X3LbHogAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([ 28.4206, -13.9807,   6.6347,  -1.9783,   9.1473,  -2.2799,   5.7235,\n",
      "        -13.8132,   7.5786, -18.9116], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "probabilities: tensor([1.0000e+00, 3.8489e-19, 3.4556e-10, 6.2793e-14, 4.2632e-09, 4.6447e-14,\n",
      "        1.3892e-10, 4.5508e-19, 8.8804e-10, 2.7791e-21], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "predicted class: plane\n",
      "label: plane\n"
     ]
    }
   ],
   "source": [
    "#CIFAR labels to human readable labels\n",
    "CIFAR10_CLASSES = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "data = next(iter(test_loader))\n",
    "images, labels = data\n",
    "\n",
    "# Check the shape of the images and labels\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(2, 2))  # keep this small to avoid blur\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')  # no interpolation\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "idx = 10\n",
    "imshow(images[idx])\n",
    "\n",
    "images = images.to(device)\n",
    "y = model(images) # B, num_classes\n",
    "print(f\"logits: {y[idx]}\")\n",
    "pred = torch.nn.functional.softmax(y, dim=1) # B, num_classes\n",
    "print(f\"probabilities: {pred[idx]}\")\n",
    "pred = torch.argmax(pred, dim=1) # B\n",
    "print(f\"predicted class: {CIFAR10_CLASSES[pred[idx]]}\")\n",
    "\n",
    "print(f\"label: {CIFAR10_CLASSES[labels[idx].item()]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9f953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
