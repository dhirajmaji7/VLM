{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af41cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "from config import VitConfig\n",
    "from model import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4ac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import model, config\n",
    "importlib.reload(config)\n",
    "importlib.reload(model)\n",
    "\n",
    "from config import VitConfig\n",
    "from model import VisionTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27dbe266",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VitConfig()\n",
    "model = VisionTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc45094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((config.img_size,config.img_size))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef9fab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15632017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([64, 3, 256, 256])\n",
      "Labels shape: torch.Size([64])\n",
      "dog\n",
      "tensor([[[0.5216, 0.5216, 0.5216,  ..., 0.6510, 0.6510, 0.6510],\n",
      "         [0.5216, 0.5216, 0.5216,  ..., 0.6510, 0.6510, 0.6510],\n",
      "         [0.5216, 0.5216, 0.5216,  ..., 0.6510, 0.6510, 0.6510],\n",
      "         ...,\n",
      "         [0.5922, 0.5922, 0.5922,  ..., 0.6549, 0.6549, 0.6549],\n",
      "         [0.5922, 0.5922, 0.5922,  ..., 0.6549, 0.6549, 0.6549],\n",
      "         [0.5922, 0.5922, 0.5922,  ..., 0.6549, 0.6549, 0.6549]],\n",
      "\n",
      "        [[0.3804, 0.3804, 0.3804,  ..., 0.6549, 0.6549, 0.6549],\n",
      "         [0.3804, 0.3804, 0.3804,  ..., 0.6549, 0.6549, 0.6549],\n",
      "         [0.3804, 0.3804, 0.3804,  ..., 0.6549, 0.6549, 0.6549],\n",
      "         ...,\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.6118, 0.6118, 0.6118],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.6118, 0.6118, 0.6118],\n",
      "         [0.4549, 0.4549, 0.4549,  ..., 0.6118, 0.6118, 0.6118]],\n",
      "\n",
      "        [[0.2510, 0.2510, 0.2510,  ..., 0.6353, 0.6353, 0.6353],\n",
      "         [0.2510, 0.2510, 0.2510,  ..., 0.6353, 0.6353, 0.6353],\n",
      "         [0.2510, 0.2510, 0.2510,  ..., 0.6353, 0.6353, 0.6353],\n",
      "         ...,\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.5294, 0.5294, 0.5294],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.5294, 0.5294, 0.5294],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.5294, 0.5294, 0.5294]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CIFAR labels to human readable labels\n",
    "CIFAR10_CLASSES = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "data = next(iter(train_loader))\n",
    "images, labels = data\n",
    "\n",
    "# Check the shape of the images and labels\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "#decode the first image\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(2, 2))  # keep this small to avoid blur\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')  # no interpolation\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "print(CIFAR10_CLASSES[labels[0].item()])\n",
    "print(images[0])\n",
    "imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61887ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 50\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_loader), eta_min=1e-6)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf988ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    iter = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Training Batch [{iter + 1}/{len(train_loader)}]: Loss {loss.item()}\")\n",
    "        iter += 1\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2196cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********  Epoch 1/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 2.6427671909332275\n",
      "Training Batch [2/782]: Loss 6.912985801696777\n",
      "Training Batch [3/782]: Loss 10.611411094665527\n",
      "Training Batch [4/782]: Loss 13.058053970336914\n",
      "Training Batch [5/782]: Loss 14.403803825378418\n",
      "Training Batch [6/782]: Loss 11.83556842803955\n",
      "Training Batch [7/782]: Loss 11.036925315856934\n",
      "Training Batch [8/782]: Loss 8.7177095413208\n",
      "Training Batch [9/782]: Loss 7.986027717590332\n",
      "Training Batch [10/782]: Loss 8.994300842285156\n",
      "Training Batch [11/782]: Loss 8.163991928100586\n",
      "Training Batch [12/782]: Loss 7.805727958679199\n",
      "Training Batch [13/782]: Loss 4.646242618560791\n",
      "Training Batch [14/782]: Loss 5.380176067352295\n",
      "Training Batch [15/782]: Loss 6.1363959312438965\n",
      "Training Batch [16/782]: Loss 5.505861282348633\n",
      "Training Batch [17/782]: Loss 4.980743408203125\n",
      "Training Batch [18/782]: Loss 3.9210944175720215\n",
      "Training Batch [19/782]: Loss 3.5745840072631836\n",
      "Training Batch [20/782]: Loss 3.1288793087005615\n",
      "Training Batch [21/782]: Loss 3.6535227298736572\n",
      "Training Batch [22/782]: Loss 3.810528039932251\n",
      "Training Batch [23/782]: Loss 3.406662940979004\n",
      "Training Batch [24/782]: Loss 2.925238847732544\n",
      "Training Batch [25/782]: Loss 3.182964563369751\n",
      "Training Batch [26/782]: Loss 3.347093105316162\n",
      "Training Batch [27/782]: Loss 2.787386417388916\n",
      "Training Batch [28/782]: Loss 2.996971845626831\n",
      "Training Batch [29/782]: Loss 2.936016798019409\n",
      "Training Batch [30/782]: Loss 2.6699821949005127\n",
      "Training Batch [31/782]: Loss 2.6912999153137207\n",
      "Training Batch [32/782]: Loss 3.2608606815338135\n",
      "Training Batch [33/782]: Loss 2.635695219039917\n",
      "Training Batch [34/782]: Loss 2.6974074840545654\n",
      "Training Batch [35/782]: Loss 3.0584781169891357\n",
      "Training Batch [36/782]: Loss 2.854301929473877\n",
      "Training Batch [37/782]: Loss 2.504958391189575\n",
      "Training Batch [38/782]: Loss 2.4606196880340576\n",
      "Training Batch [39/782]: Loss 2.5836853981018066\n",
      "Training Batch [40/782]: Loss 2.8705174922943115\n",
      "Training Batch [41/782]: Loss 2.65167236328125\n",
      "Training Batch [42/782]: Loss 2.61201810836792\n",
      "Training Batch [43/782]: Loss 2.4240121841430664\n",
      "Training Batch [44/782]: Loss 2.4029159545898438\n",
      "Training Batch [45/782]: Loss 2.6724767684936523\n",
      "Training Batch [46/782]: Loss 2.5105538368225098\n",
      "Training Batch [47/782]: Loss 2.359778642654419\n",
      "Training Batch [48/782]: Loss 2.2800371646881104\n",
      "Training Batch [49/782]: Loss 2.499223232269287\n",
      "Training Batch [50/782]: Loss 2.336143970489502\n",
      "Training Batch [51/782]: Loss 2.250379800796509\n",
      "Training Batch [52/782]: Loss 2.259561061859131\n",
      "Training Batch [53/782]: Loss 2.300874948501587\n",
      "Training Batch [54/782]: Loss 2.3926572799682617\n",
      "Training Batch [55/782]: Loss 2.4794068336486816\n",
      "Training Batch [56/782]: Loss 2.3501386642456055\n",
      "Training Batch [57/782]: Loss 2.29746150970459\n",
      "Training Batch [58/782]: Loss 2.2934768199920654\n",
      "Training Batch [59/782]: Loss 2.238783359527588\n",
      "Training Batch [60/782]: Loss 2.4147415161132812\n",
      "Training Batch [61/782]: Loss 2.266430616378784\n",
      "Training Batch [62/782]: Loss 2.3794991970062256\n",
      "Training Batch [63/782]: Loss 2.4065277576446533\n",
      "Training Batch [64/782]: Loss 2.341547966003418\n",
      "Training Batch [65/782]: Loss 2.599644422531128\n",
      "Training Batch [66/782]: Loss 2.501056671142578\n",
      "Training Batch [67/782]: Loss 2.255561113357544\n",
      "Training Batch [68/782]: Loss 2.2077178955078125\n",
      "Training Batch [69/782]: Loss 2.4336204528808594\n",
      "Training Batch [70/782]: Loss 2.564502477645874\n",
      "Training Batch [71/782]: Loss 2.3178091049194336\n",
      "Training Batch [72/782]: Loss 2.2931854724884033\n",
      "Training Batch [73/782]: Loss 2.0202622413635254\n",
      "Training Batch [74/782]: Loss 2.3362510204315186\n",
      "Training Batch [75/782]: Loss 2.2895703315734863\n",
      "Training Batch [76/782]: Loss 2.383082151412964\n",
      "Training Batch [77/782]: Loss 2.3030593395233154\n",
      "Training Batch [78/782]: Loss 2.1620888710021973\n",
      "Training Batch [79/782]: Loss 2.3182404041290283\n",
      "Training Batch [80/782]: Loss 2.246277332305908\n",
      "Training Batch [81/782]: Loss 2.1267991065979004\n",
      "Training Batch [82/782]: Loss 2.304091215133667\n",
      "Training Batch [83/782]: Loss 2.354524850845337\n",
      "Training Batch [84/782]: Loss 2.10921311378479\n",
      "Training Batch [85/782]: Loss 2.343038320541382\n",
      "Training Batch [86/782]: Loss 2.1676502227783203\n",
      "Training Batch [87/782]: Loss 2.2261223793029785\n",
      "Training Batch [88/782]: Loss 2.2573225498199463\n",
      "Training Batch [89/782]: Loss 2.2063779830932617\n",
      "Training Batch [90/782]: Loss 2.119304656982422\n",
      "Training Batch [91/782]: Loss 2.253122568130493\n",
      "Training Batch [92/782]: Loss 2.3323512077331543\n",
      "Training Batch [93/782]: Loss 2.240969181060791\n",
      "Training Batch [94/782]: Loss 2.107403516769409\n",
      "Training Batch [95/782]: Loss 2.2952492237091064\n",
      "Training Batch [96/782]: Loss 2.325571060180664\n",
      "Training Batch [97/782]: Loss 2.1579678058624268\n",
      "Training Batch [98/782]: Loss 2.087104082107544\n",
      "Training Batch [99/782]: Loss 2.240659236907959\n",
      "Training Batch [100/782]: Loss 2.3272557258605957\n",
      "Training Batch [101/782]: Loss 2.164442777633667\n",
      "Training Batch [102/782]: Loss 2.3123209476470947\n",
      "Training Batch [103/782]: Loss 2.1450514793395996\n",
      "Training Batch [104/782]: Loss 2.447136878967285\n",
      "Training Batch [105/782]: Loss 2.222494602203369\n",
      "Training Batch [106/782]: Loss 2.241774559020996\n",
      "Training Batch [107/782]: Loss 2.314262866973877\n",
      "Training Batch [108/782]: Loss 2.1295700073242188\n",
      "Training Batch [109/782]: Loss 2.3109548091888428\n",
      "Training Batch [110/782]: Loss 2.196148157119751\n",
      "Training Batch [111/782]: Loss 2.0911355018615723\n",
      "Training Batch [112/782]: Loss 2.210838794708252\n",
      "Training Batch [113/782]: Loss 2.2486674785614014\n",
      "Training Batch [114/782]: Loss 2.4541611671447754\n",
      "Training Batch [115/782]: Loss 2.19099760055542\n",
      "Training Batch [116/782]: Loss 2.2141129970550537\n",
      "Training Batch [117/782]: Loss 2.3840949535369873\n",
      "Training Batch [118/782]: Loss 2.434490442276001\n",
      "Training Batch [119/782]: Loss 2.410860300064087\n",
      "Training Batch [120/782]: Loss 2.301581621170044\n",
      "Training Batch [121/782]: Loss 2.17097806930542\n",
      "Training Batch [122/782]: Loss 2.3316473960876465\n",
      "Training Batch [123/782]: Loss 2.2413134574890137\n",
      "Training Batch [124/782]: Loss 2.148937225341797\n",
      "Training Batch [125/782]: Loss 2.3081514835357666\n",
      "Training Batch [126/782]: Loss 2.2749409675598145\n",
      "Training Batch [127/782]: Loss 2.08560848236084\n",
      "Training Batch [128/782]: Loss 2.5774216651916504\n",
      "Training Batch [129/782]: Loss 2.380478858947754\n",
      "Training Batch [130/782]: Loss 2.331226110458374\n",
      "Training Batch [131/782]: Loss 2.195462226867676\n",
      "Training Batch [132/782]: Loss 2.2226881980895996\n",
      "Training Batch [133/782]: Loss 2.253364324569702\n",
      "Training Batch [134/782]: Loss 2.0494296550750732\n",
      "Training Batch [135/782]: Loss 2.1486759185791016\n",
      "Training Batch [136/782]: Loss 2.5541720390319824\n",
      "Training Batch [137/782]: Loss 2.4861245155334473\n",
      "Training Batch [138/782]: Loss 2.105543613433838\n",
      "Training Batch [139/782]: Loss 2.249424934387207\n",
      "Training Batch [140/782]: Loss 2.3147690296173096\n",
      "Training Batch [141/782]: Loss 2.4809489250183105\n",
      "Training Batch [142/782]: Loss 2.2451820373535156\n",
      "Training Batch [143/782]: Loss 2.4514763355255127\n",
      "Training Batch [144/782]: Loss 2.3768084049224854\n",
      "Training Batch [145/782]: Loss 2.4445712566375732\n",
      "Training Batch [146/782]: Loss 2.1643409729003906\n",
      "Training Batch [147/782]: Loss 2.2154080867767334\n",
      "Training Batch [148/782]: Loss 2.179929256439209\n",
      "Training Batch [149/782]: Loss 2.0535919666290283\n",
      "Training Batch [150/782]: Loss 2.3435328006744385\n",
      "Training Batch [151/782]: Loss 2.2495856285095215\n",
      "Training Batch [152/782]: Loss 2.322554111480713\n",
      "Training Batch [153/782]: Loss 2.4411661624908447\n",
      "Training Batch [154/782]: Loss 2.041419506072998\n",
      "Training Batch [155/782]: Loss 2.451237440109253\n",
      "Training Batch [156/782]: Loss 2.4012463092803955\n",
      "Training Batch [157/782]: Loss 2.233552932739258\n",
      "Training Batch [158/782]: Loss 1.9841583967208862\n",
      "Training Batch [159/782]: Loss 2.2621002197265625\n",
      "Training Batch [160/782]: Loss 2.1006057262420654\n",
      "Training Batch [161/782]: Loss 2.167231559753418\n",
      "Training Batch [162/782]: Loss 2.5493078231811523\n",
      "Training Batch [163/782]: Loss 2.2582950592041016\n",
      "Training Batch [164/782]: Loss 2.194978713989258\n",
      "Training Batch [165/782]: Loss 2.1317100524902344\n",
      "Training Batch [166/782]: Loss 2.2114696502685547\n",
      "Training Batch [167/782]: Loss 2.1366329193115234\n",
      "Training Batch [168/782]: Loss 2.2250280380249023\n",
      "Training Batch [169/782]: Loss 2.34407377243042\n",
      "Training Batch [170/782]: Loss 2.1273655891418457\n",
      "Training Batch [171/782]: Loss 2.021233558654785\n",
      "Training Batch [172/782]: Loss 2.0019795894622803\n",
      "Training Batch [173/782]: Loss 2.2223458290100098\n",
      "Training Batch [174/782]: Loss 2.1757395267486572\n",
      "Training Batch [175/782]: Loss 2.0914998054504395\n",
      "Training Batch [176/782]: Loss 2.043680191040039\n",
      "Training Batch [177/782]: Loss 1.979272484779358\n",
      "Training Batch [178/782]: Loss 1.9395701885223389\n",
      "Training Batch [179/782]: Loss 2.1038873195648193\n",
      "Training Batch [180/782]: Loss 2.0440824031829834\n",
      "Training Batch [181/782]: Loss 2.1742427349090576\n",
      "Training Batch [182/782]: Loss 2.2223148345947266\n",
      "Training Batch [183/782]: Loss 2.4070138931274414\n",
      "Training Batch [184/782]: Loss 2.0228536128997803\n",
      "Training Batch [185/782]: Loss 2.068174362182617\n",
      "Training Batch [186/782]: Loss 2.1684136390686035\n",
      "Training Batch [187/782]: Loss 1.9780206680297852\n",
      "Training Batch [188/782]: Loss 2.0549259185791016\n",
      "Training Batch [189/782]: Loss 2.030599594116211\n",
      "Training Batch [190/782]: Loss 1.9055129289627075\n",
      "Training Batch [191/782]: Loss 2.2239491939544678\n",
      "Training Batch [192/782]: Loss 2.0292510986328125\n",
      "Training Batch [193/782]: Loss 1.8736155033111572\n",
      "Training Batch [194/782]: Loss 2.023545026779175\n",
      "Training Batch [195/782]: Loss 2.0898032188415527\n",
      "Training Batch [196/782]: Loss 2.31233549118042\n",
      "Training Batch [197/782]: Loss 2.0088255405426025\n",
      "Training Batch [198/782]: Loss 1.9448152780532837\n",
      "Training Batch [199/782]: Loss 2.0234017372131348\n",
      "Training Batch [200/782]: Loss 2.0922398567199707\n",
      "Training Batch [201/782]: Loss 2.3299150466918945\n",
      "Training Batch [202/782]: Loss 2.1383397579193115\n",
      "Training Batch [203/782]: Loss 2.0054407119750977\n",
      "Training Batch [204/782]: Loss 2.3279318809509277\n",
      "Training Batch [205/782]: Loss 2.3297042846679688\n",
      "Training Batch [206/782]: Loss 2.010200262069702\n",
      "Training Batch [207/782]: Loss 1.88282310962677\n",
      "Training Batch [208/782]: Loss 2.1020100116729736\n",
      "Training Batch [209/782]: Loss 2.021951913833618\n",
      "Training Batch [210/782]: Loss 2.146615743637085\n",
      "Training Batch [211/782]: Loss 1.9164507389068604\n",
      "Training Batch [212/782]: Loss 1.8564616441726685\n",
      "Training Batch [213/782]: Loss 2.1464946269989014\n",
      "Training Batch [214/782]: Loss 2.0939433574676514\n",
      "Training Batch [215/782]: Loss 2.0042884349823\n",
      "Training Batch [216/782]: Loss 1.943691611289978\n",
      "Training Batch [217/782]: Loss 2.191683292388916\n",
      "Training Batch [218/782]: Loss 2.09861421585083\n",
      "Training Batch [219/782]: Loss 1.9523916244506836\n",
      "Training Batch [220/782]: Loss 1.9850736856460571\n",
      "Training Batch [221/782]: Loss 1.9137938022613525\n",
      "Training Batch [222/782]: Loss 2.0277042388916016\n",
      "Training Batch [223/782]: Loss 1.7998801469802856\n",
      "Training Batch [224/782]: Loss 1.9224262237548828\n",
      "Training Batch [225/782]: Loss 1.8664770126342773\n",
      "Training Batch [226/782]: Loss 2.0268328189849854\n",
      "Training Batch [227/782]: Loss 1.821545958518982\n",
      "Training Batch [228/782]: Loss 2.054536819458008\n",
      "Training Batch [229/782]: Loss 1.8672016859054565\n",
      "Training Batch [230/782]: Loss 1.901384711265564\n",
      "Training Batch [231/782]: Loss 1.8129205703735352\n",
      "Training Batch [232/782]: Loss 1.9641057252883911\n",
      "Training Batch [233/782]: Loss 2.0800275802612305\n",
      "Training Batch [234/782]: Loss 1.641627550125122\n",
      "Training Batch [235/782]: Loss 1.9969358444213867\n",
      "Training Batch [236/782]: Loss 2.226839780807495\n",
      "Training Batch [237/782]: Loss 1.8687032461166382\n",
      "Training Batch [238/782]: Loss 2.1120219230651855\n",
      "Training Batch [239/782]: Loss 1.8456592559814453\n",
      "Training Batch [240/782]: Loss 1.9074831008911133\n",
      "Training Batch [241/782]: Loss 1.9184815883636475\n",
      "Training Batch [242/782]: Loss 2.030116081237793\n",
      "Training Batch [243/782]: Loss 1.964560866355896\n",
      "Training Batch [244/782]: Loss 1.7415510416030884\n",
      "Training Batch [245/782]: Loss 2.0671653747558594\n",
      "Training Batch [246/782]: Loss 2.502094030380249\n",
      "Training Batch [247/782]: Loss 2.094559669494629\n",
      "Training Batch [248/782]: Loss 2.013376474380493\n",
      "Training Batch [249/782]: Loss 1.961344838142395\n",
      "Training Batch [250/782]: Loss 1.927379846572876\n",
      "Training Batch [251/782]: Loss 2.3115720748901367\n",
      "Training Batch [252/782]: Loss 2.0962092876434326\n",
      "Training Batch [253/782]: Loss 1.9523780345916748\n",
      "Training Batch [254/782]: Loss 1.837838053703308\n",
      "Training Batch [255/782]: Loss 2.097341299057007\n",
      "Training Batch [256/782]: Loss 2.1445541381835938\n",
      "Training Batch [257/782]: Loss 1.8360012769699097\n",
      "Training Batch [258/782]: Loss 1.9150816202163696\n",
      "Training Batch [259/782]: Loss 2.034048557281494\n",
      "Training Batch [260/782]: Loss 1.9023813009262085\n",
      "Training Batch [261/782]: Loss 1.787671685218811\n",
      "Training Batch [262/782]: Loss 2.052473306655884\n",
      "Training Batch [263/782]: Loss 2.0141212940216064\n",
      "Training Batch [264/782]: Loss 1.901511788368225\n",
      "Training Batch [265/782]: Loss 2.1166765689849854\n",
      "Training Batch [266/782]: Loss 2.0705666542053223\n",
      "Training Batch [267/782]: Loss 1.7888227701187134\n",
      "Training Batch [268/782]: Loss 2.0344130992889404\n",
      "Training Batch [269/782]: Loss 2.036203622817993\n",
      "Training Batch [270/782]: Loss 1.8066906929016113\n",
      "Training Batch [271/782]: Loss 1.8045377731323242\n",
      "Training Batch [272/782]: Loss 2.4915382862091064\n",
      "Training Batch [273/782]: Loss 2.195523977279663\n",
      "Training Batch [274/782]: Loss 2.0224390029907227\n",
      "Training Batch [275/782]: Loss 2.1005759239196777\n",
      "Training Batch [276/782]: Loss 2.0056686401367188\n",
      "Training Batch [277/782]: Loss 1.8329746723175049\n",
      "Training Batch [278/782]: Loss 2.2600595951080322\n",
      "Training Batch [279/782]: Loss 1.9072450399398804\n",
      "Training Batch [280/782]: Loss 2.351025104522705\n",
      "Training Batch [281/782]: Loss 1.9657236337661743\n",
      "Training Batch [282/782]: Loss 1.9361814260482788\n",
      "Training Batch [283/782]: Loss 1.9209955930709839\n",
      "Training Batch [284/782]: Loss 1.9855365753173828\n",
      "Training Batch [285/782]: Loss 1.9990761280059814\n",
      "Training Batch [286/782]: Loss 1.8590126037597656\n",
      "Training Batch [287/782]: Loss 1.7985577583312988\n",
      "Training Batch [288/782]: Loss 1.953107237815857\n",
      "Training Batch [289/782]: Loss 1.792879581451416\n",
      "Training Batch [290/782]: Loss 1.8187296390533447\n",
      "Training Batch [291/782]: Loss 1.8974605798721313\n",
      "Training Batch [292/782]: Loss 1.898605465888977\n",
      "Training Batch [293/782]: Loss 1.722190260887146\n",
      "Training Batch [294/782]: Loss 1.986952304840088\n",
      "Training Batch [295/782]: Loss 1.7526483535766602\n",
      "Training Batch [296/782]: Loss 1.9960734844207764\n",
      "Training Batch [297/782]: Loss 2.040823459625244\n",
      "Training Batch [298/782]: Loss 2.002748489379883\n",
      "Training Batch [299/782]: Loss 2.015289306640625\n",
      "Training Batch [300/782]: Loss 1.932594895362854\n",
      "Training Batch [301/782]: Loss 1.9169429540634155\n",
      "Training Batch [302/782]: Loss 2.1137146949768066\n",
      "Training Batch [303/782]: Loss 1.8949812650680542\n",
      "Training Batch [304/782]: Loss 2.075106143951416\n",
      "Training Batch [305/782]: Loss 1.7836170196533203\n",
      "Training Batch [306/782]: Loss 1.862798810005188\n",
      "Training Batch [307/782]: Loss 1.9670099020004272\n",
      "Training Batch [308/782]: Loss 1.763782024383545\n",
      "Training Batch [309/782]: Loss 1.8690143823623657\n",
      "Training Batch [310/782]: Loss 1.9540534019470215\n",
      "Training Batch [311/782]: Loss 1.9700831174850464\n",
      "Training Batch [312/782]: Loss 1.9230927228927612\n",
      "Training Batch [313/782]: Loss 1.881957769393921\n",
      "Training Batch [314/782]: Loss 1.884883165359497\n",
      "Training Batch [315/782]: Loss 2.089683771133423\n",
      "Training Batch [316/782]: Loss 1.9740519523620605\n",
      "Training Batch [317/782]: Loss 1.9954719543457031\n",
      "Training Batch [318/782]: Loss 2.131486415863037\n",
      "Training Batch [319/782]: Loss 1.9938327074050903\n",
      "Training Batch [320/782]: Loss 1.8917021751403809\n",
      "Training Batch [321/782]: Loss 1.7932747602462769\n",
      "Training Batch [322/782]: Loss 1.8817143440246582\n",
      "Training Batch [323/782]: Loss 2.1807632446289062\n",
      "Training Batch [324/782]: Loss 1.7335907220840454\n",
      "Training Batch [325/782]: Loss 1.6801432371139526\n",
      "Training Batch [326/782]: Loss 1.7620679140090942\n",
      "Training Batch [327/782]: Loss 1.7547441720962524\n",
      "Training Batch [328/782]: Loss 2.1361255645751953\n",
      "Training Batch [329/782]: Loss 1.995705008506775\n",
      "Training Batch [330/782]: Loss 1.9058659076690674\n",
      "Training Batch [331/782]: Loss 1.7140257358551025\n",
      "Training Batch [332/782]: Loss 1.6311370134353638\n",
      "Training Batch [333/782]: Loss 1.8530068397521973\n",
      "Training Batch [334/782]: Loss 1.5484699010849\n",
      "Training Batch [335/782]: Loss 1.887878656387329\n",
      "Training Batch [336/782]: Loss 1.7411408424377441\n",
      "Training Batch [337/782]: Loss 1.5546338558197021\n",
      "Training Batch [338/782]: Loss 2.134774923324585\n",
      "Training Batch [339/782]: Loss 1.8912076950073242\n",
      "Training Batch [340/782]: Loss 1.9214158058166504\n",
      "Training Batch [341/782]: Loss 1.8026149272918701\n",
      "Training Batch [342/782]: Loss 1.727693796157837\n",
      "Training Batch [343/782]: Loss 2.0117671489715576\n",
      "Training Batch [344/782]: Loss 1.8929169178009033\n",
      "Training Batch [345/782]: Loss 1.95041823387146\n",
      "Training Batch [346/782]: Loss 1.7900174856185913\n",
      "Training Batch [347/782]: Loss 1.830604910850525\n",
      "Training Batch [348/782]: Loss 1.7519352436065674\n",
      "Training Batch [349/782]: Loss 1.753079891204834\n",
      "Training Batch [350/782]: Loss 1.7496222257614136\n",
      "Training Batch [351/782]: Loss 1.9672101736068726\n",
      "Training Batch [352/782]: Loss 1.965772271156311\n",
      "Training Batch [353/782]: Loss 1.8280539512634277\n",
      "Training Batch [354/782]: Loss 1.8917979001998901\n",
      "Training Batch [355/782]: Loss 1.8755258321762085\n",
      "Training Batch [356/782]: Loss 1.8489773273468018\n",
      "Training Batch [357/782]: Loss 1.8917361497879028\n",
      "Training Batch [358/782]: Loss 2.058022975921631\n",
      "Training Batch [359/782]: Loss 2.021636724472046\n",
      "Training Batch [360/782]: Loss 1.7438077926635742\n",
      "Training Batch [361/782]: Loss 1.6855285167694092\n",
      "Training Batch [362/782]: Loss 2.0651073455810547\n",
      "Training Batch [363/782]: Loss 1.878490924835205\n",
      "Training Batch [364/782]: Loss 1.8452191352844238\n",
      "Training Batch [365/782]: Loss 1.93215012550354\n",
      "Training Batch [366/782]: Loss 2.041623115539551\n",
      "Training Batch [367/782]: Loss 1.7877532243728638\n",
      "Training Batch [368/782]: Loss 1.70932936668396\n",
      "Training Batch [369/782]: Loss 1.7529515027999878\n",
      "Training Batch [370/782]: Loss 1.8318556547164917\n",
      "Training Batch [371/782]: Loss 2.0927419662475586\n",
      "Training Batch [372/782]: Loss 1.6600522994995117\n",
      "Training Batch [373/782]: Loss 1.7070090770721436\n",
      "Training Batch [374/782]: Loss 1.8352699279785156\n",
      "Training Batch [375/782]: Loss 2.2728166580200195\n",
      "Training Batch [376/782]: Loss 1.904358148574829\n",
      "Training Batch [377/782]: Loss 1.7654129266738892\n",
      "Training Batch [378/782]: Loss 2.028581380844116\n",
      "Training Batch [379/782]: Loss 2.04008412361145\n",
      "Training Batch [380/782]: Loss 1.770586609840393\n",
      "Training Batch [381/782]: Loss 1.9118361473083496\n",
      "Training Batch [382/782]: Loss 1.8035928010940552\n",
      "Training Batch [383/782]: Loss 1.7966586351394653\n",
      "Training Batch [384/782]: Loss 1.7535810470581055\n",
      "Training Batch [385/782]: Loss 2.1924164295196533\n",
      "Training Batch [386/782]: Loss 1.9530726671218872\n",
      "Training Batch [387/782]: Loss 1.956567406654358\n",
      "Training Batch [388/782]: Loss 1.916337490081787\n",
      "Training Batch [389/782]: Loss 1.869567632675171\n",
      "Training Batch [390/782]: Loss 1.9229031801223755\n",
      "Training Batch [391/782]: Loss 1.987103819847107\n",
      "Training Batch [392/782]: Loss 2.0364277362823486\n",
      "Training Batch [393/782]: Loss 1.9484161138534546\n",
      "Training Batch [394/782]: Loss 1.8769360780715942\n",
      "Training Batch [395/782]: Loss 1.8741313219070435\n",
      "Training Batch [396/782]: Loss 1.6631927490234375\n",
      "Training Batch [397/782]: Loss 1.7950937747955322\n",
      "Training Batch [398/782]: Loss 1.8639614582061768\n",
      "Training Batch [399/782]: Loss 1.7904231548309326\n",
      "Training Batch [400/782]: Loss 1.7557870149612427\n",
      "Training Batch [401/782]: Loss 1.6783385276794434\n",
      "Training Batch [402/782]: Loss 1.8737951517105103\n",
      "Training Batch [403/782]: Loss 1.984993577003479\n",
      "Training Batch [404/782]: Loss 1.8206020593643188\n",
      "Training Batch [405/782]: Loss 2.104624032974243\n",
      "Training Batch [406/782]: Loss 1.8216028213500977\n",
      "Training Batch [407/782]: Loss 1.9101704359054565\n",
      "Training Batch [408/782]: Loss 1.716665267944336\n",
      "Training Batch [409/782]: Loss 1.8961772918701172\n",
      "Training Batch [410/782]: Loss 1.9678524732589722\n",
      "Training Batch [411/782]: Loss 1.9703290462493896\n",
      "Training Batch [412/782]: Loss 1.8039711713790894\n",
      "Training Batch [413/782]: Loss 1.8220388889312744\n",
      "Training Batch [414/782]: Loss 2.181689739227295\n",
      "Training Batch [415/782]: Loss 1.962274432182312\n",
      "Training Batch [416/782]: Loss 1.8986719846725464\n",
      "Training Batch [417/782]: Loss 1.7904719114303589\n",
      "Training Batch [418/782]: Loss 1.8693228960037231\n",
      "Training Batch [419/782]: Loss 1.7781440019607544\n",
      "Training Batch [420/782]: Loss 1.636254906654358\n",
      "Training Batch [421/782]: Loss 2.1831881999969482\n",
      "Training Batch [422/782]: Loss 1.6896226406097412\n",
      "Training Batch [423/782]: Loss 1.9754071235656738\n",
      "Training Batch [424/782]: Loss 1.8234597444534302\n",
      "Training Batch [425/782]: Loss 1.787535309791565\n",
      "Training Batch [426/782]: Loss 1.8604894876480103\n",
      "Training Batch [427/782]: Loss 1.8382127285003662\n",
      "Training Batch [428/782]: Loss 1.8562679290771484\n",
      "Training Batch [429/782]: Loss 1.608540415763855\n",
      "Training Batch [430/782]: Loss 1.884817123413086\n",
      "Training Batch [431/782]: Loss 1.6486018896102905\n",
      "Training Batch [432/782]: Loss 1.8055510520935059\n",
      "Training Batch [433/782]: Loss 1.8399351835250854\n",
      "Training Batch [434/782]: Loss 1.7192068099975586\n",
      "Training Batch [435/782]: Loss 1.7928613424301147\n",
      "Training Batch [436/782]: Loss 1.6071120500564575\n",
      "Training Batch [437/782]: Loss 1.845068335533142\n",
      "Training Batch [438/782]: Loss 1.930111050605774\n",
      "Training Batch [439/782]: Loss 1.7813670635223389\n",
      "Training Batch [440/782]: Loss 1.669812798500061\n",
      "Training Batch [441/782]: Loss 1.7524200677871704\n",
      "Training Batch [442/782]: Loss 1.643723964691162\n",
      "Training Batch [443/782]: Loss 1.8130147457122803\n",
      "Training Batch [444/782]: Loss 1.7537192106246948\n",
      "Training Batch [445/782]: Loss 1.6880697011947632\n",
      "Training Batch [446/782]: Loss 1.5171011686325073\n",
      "Training Batch [447/782]: Loss 2.0950684547424316\n",
      "Training Batch [448/782]: Loss 1.8075900077819824\n",
      "Training Batch [449/782]: Loss 1.9556224346160889\n",
      "Training Batch [450/782]: Loss 1.737104058265686\n",
      "Training Batch [451/782]: Loss 1.6038142442703247\n",
      "Training Batch [452/782]: Loss 1.9221601486206055\n",
      "Training Batch [453/782]: Loss 1.7527068853378296\n",
      "Training Batch [454/782]: Loss 1.7676541805267334\n",
      "Training Batch [455/782]: Loss 1.709670901298523\n",
      "Training Batch [456/782]: Loss 2.1233699321746826\n",
      "Training Batch [457/782]: Loss 1.6605284214019775\n",
      "Training Batch [458/782]: Loss 1.7099484205245972\n",
      "Training Batch [459/782]: Loss 1.6578320264816284\n",
      "Training Batch [460/782]: Loss 2.0929384231567383\n",
      "Training Batch [461/782]: Loss 1.7991468906402588\n",
      "Training Batch [462/782]: Loss 1.5964936017990112\n",
      "Training Batch [463/782]: Loss 1.708724021911621\n",
      "Training Batch [464/782]: Loss 1.7774840593338013\n",
      "Training Batch [465/782]: Loss 1.8557522296905518\n",
      "Training Batch [466/782]: Loss 1.8454363346099854\n",
      "Training Batch [467/782]: Loss 1.812142014503479\n",
      "Training Batch [468/782]: Loss 1.5072091817855835\n",
      "Training Batch [469/782]: Loss 1.7569036483764648\n",
      "Training Batch [470/782]: Loss 1.7151345014572144\n",
      "Training Batch [471/782]: Loss 1.7339345216751099\n",
      "Training Batch [472/782]: Loss 1.940686583518982\n",
      "Training Batch [473/782]: Loss 1.962682843208313\n",
      "Training Batch [474/782]: Loss 1.8910857439041138\n",
      "Training Batch [475/782]: Loss 1.7296251058578491\n",
      "Training Batch [476/782]: Loss 2.071615219116211\n",
      "Training Batch [477/782]: Loss 1.715490460395813\n",
      "Training Batch [478/782]: Loss 2.041577100753784\n",
      "Training Batch [479/782]: Loss 1.9288249015808105\n",
      "Training Batch [480/782]: Loss 1.791650414466858\n",
      "Training Batch [481/782]: Loss 1.7802484035491943\n",
      "Training Batch [482/782]: Loss 1.7448887825012207\n",
      "Training Batch [483/782]: Loss 1.99415922164917\n",
      "Training Batch [484/782]: Loss 1.6984939575195312\n",
      "Training Batch [485/782]: Loss 1.9613125324249268\n",
      "Training Batch [486/782]: Loss 1.9183624982833862\n",
      "Training Batch [487/782]: Loss 1.8172521591186523\n",
      "Training Batch [488/782]: Loss 1.7818350791931152\n",
      "Training Batch [489/782]: Loss 1.6426750421524048\n",
      "Training Batch [490/782]: Loss 1.8291022777557373\n",
      "Training Batch [491/782]: Loss 1.6119537353515625\n",
      "Training Batch [492/782]: Loss 1.6052666902542114\n",
      "Training Batch [493/782]: Loss 1.7531579732894897\n",
      "Training Batch [494/782]: Loss 1.9949657917022705\n",
      "Training Batch [495/782]: Loss 1.859674334526062\n",
      "Training Batch [496/782]: Loss 1.9865307807922363\n",
      "Training Batch [497/782]: Loss 1.68307363986969\n",
      "Training Batch [498/782]: Loss 1.7425473928451538\n",
      "Training Batch [499/782]: Loss 1.6927566528320312\n",
      "Training Batch [500/782]: Loss 1.813392996788025\n",
      "Training Batch [501/782]: Loss 1.6544665098190308\n",
      "Training Batch [502/782]: Loss 1.956121563911438\n",
      "Training Batch [503/782]: Loss 1.7934118509292603\n",
      "Training Batch [504/782]: Loss 1.7939066886901855\n",
      "Training Batch [505/782]: Loss 1.9764938354492188\n",
      "Training Batch [506/782]: Loss 1.581115961074829\n",
      "Training Batch [507/782]: Loss 1.5720571279525757\n",
      "Training Batch [508/782]: Loss 1.8303947448730469\n",
      "Training Batch [509/782]: Loss 1.7471096515655518\n",
      "Training Batch [510/782]: Loss 1.432631015777588\n",
      "Training Batch [511/782]: Loss 1.6603106260299683\n",
      "Training Batch [512/782]: Loss 1.699074387550354\n",
      "Training Batch [513/782]: Loss 1.8453553915023804\n",
      "Training Batch [514/782]: Loss 1.803022027015686\n",
      "Training Batch [515/782]: Loss 1.5385935306549072\n",
      "Training Batch [516/782]: Loss 1.5969653129577637\n",
      "Training Batch [517/782]: Loss 1.7682057619094849\n",
      "Training Batch [518/782]: Loss 1.7209031581878662\n",
      "Training Batch [519/782]: Loss 2.0077438354492188\n",
      "Training Batch [520/782]: Loss 1.5592912435531616\n",
      "Training Batch [521/782]: Loss 1.5785409212112427\n",
      "Training Batch [522/782]: Loss 1.639482855796814\n",
      "Training Batch [523/782]: Loss 1.7546515464782715\n",
      "Training Batch [524/782]: Loss 1.7349967956542969\n",
      "Training Batch [525/782]: Loss 1.8904410600662231\n",
      "Training Batch [526/782]: Loss 1.7194634675979614\n",
      "Training Batch [527/782]: Loss 1.7467422485351562\n",
      "Training Batch [528/782]: Loss 1.5837857723236084\n",
      "Training Batch [529/782]: Loss 1.9885002374649048\n",
      "Training Batch [530/782]: Loss 1.7476377487182617\n",
      "Training Batch [531/782]: Loss 1.759075403213501\n",
      "Training Batch [532/782]: Loss 1.5570436716079712\n",
      "Training Batch [533/782]: Loss 1.685765027999878\n",
      "Training Batch [534/782]: Loss 1.6325091123580933\n",
      "Training Batch [535/782]: Loss 1.5559343099594116\n",
      "Training Batch [536/782]: Loss 1.611224889755249\n",
      "Training Batch [537/782]: Loss 1.7776540517807007\n",
      "Training Batch [538/782]: Loss 1.5982635021209717\n",
      "Training Batch [539/782]: Loss 1.7157403230667114\n",
      "Training Batch [540/782]: Loss 1.7565062046051025\n",
      "Training Batch [541/782]: Loss 1.8137606382369995\n",
      "Training Batch [542/782]: Loss 1.9517065286636353\n",
      "Training Batch [543/782]: Loss 1.6261131763458252\n",
      "Training Batch [544/782]: Loss 1.5535879135131836\n",
      "Training Batch [545/782]: Loss 1.785254955291748\n",
      "Training Batch [546/782]: Loss 1.8387305736541748\n",
      "Training Batch [547/782]: Loss 1.938197374343872\n",
      "Training Batch [548/782]: Loss 1.681584119796753\n",
      "Training Batch [549/782]: Loss 1.870945692062378\n",
      "Training Batch [550/782]: Loss 1.456909418106079\n",
      "Training Batch [551/782]: Loss 1.8016091585159302\n",
      "Training Batch [552/782]: Loss 1.7358728647232056\n",
      "Training Batch [553/782]: Loss 1.8589637279510498\n",
      "Training Batch [554/782]: Loss 1.5369617938995361\n",
      "Training Batch [555/782]: Loss 1.8228282928466797\n",
      "Training Batch [556/782]: Loss 1.5774176120758057\n",
      "Training Batch [557/782]: Loss 1.8223652839660645\n",
      "Training Batch [558/782]: Loss 1.9717676639556885\n",
      "Training Batch [559/782]: Loss 1.7356557846069336\n",
      "Training Batch [560/782]: Loss 1.7305371761322021\n",
      "Training Batch [561/782]: Loss 1.5286953449249268\n",
      "Training Batch [562/782]: Loss 1.6521732807159424\n",
      "Training Batch [563/782]: Loss 1.5267783403396606\n",
      "Training Batch [564/782]: Loss 1.5439337491989136\n",
      "Training Batch [565/782]: Loss 1.892066240310669\n",
      "Training Batch [566/782]: Loss 1.9428448677062988\n",
      "Training Batch [567/782]: Loss 1.5857853889465332\n",
      "Training Batch [568/782]: Loss 1.8397505283355713\n",
      "Training Batch [569/782]: Loss 1.7790066003799438\n",
      "Training Batch [570/782]: Loss 1.794359564781189\n",
      "Training Batch [571/782]: Loss 1.582828402519226\n",
      "Training Batch [572/782]: Loss 1.7875361442565918\n",
      "Training Batch [573/782]: Loss 1.7127691507339478\n",
      "Training Batch [574/782]: Loss 1.7455322742462158\n",
      "Training Batch [575/782]: Loss 1.8838075399398804\n",
      "Training Batch [576/782]: Loss 1.8761194944381714\n",
      "Training Batch [577/782]: Loss 1.778580665588379\n",
      "Training Batch [578/782]: Loss 1.6866589784622192\n",
      "Training Batch [579/782]: Loss 1.3939374685287476\n",
      "Training Batch [580/782]: Loss 2.067993402481079\n",
      "Training Batch [581/782]: Loss 2.06333327293396\n",
      "Training Batch [582/782]: Loss 2.0057692527770996\n",
      "Training Batch [583/782]: Loss 2.052807569503784\n",
      "Training Batch [584/782]: Loss 1.5878081321716309\n",
      "Training Batch [585/782]: Loss 1.7176756858825684\n",
      "Training Batch [586/782]: Loss 1.5408860445022583\n",
      "Training Batch [587/782]: Loss 1.841629981994629\n",
      "Training Batch [588/782]: Loss 1.7667348384857178\n",
      "Training Batch [589/782]: Loss 1.7259150743484497\n",
      "Training Batch [590/782]: Loss 1.7935805320739746\n",
      "Training Batch [591/782]: Loss 1.7002630233764648\n",
      "Training Batch [592/782]: Loss 1.8068114519119263\n",
      "Training Batch [593/782]: Loss 1.5916224718093872\n",
      "Training Batch [594/782]: Loss 1.6842073202133179\n",
      "Training Batch [595/782]: Loss 1.7714874744415283\n",
      "Training Batch [596/782]: Loss 1.669021725654602\n",
      "Training Batch [597/782]: Loss 1.7098926305770874\n",
      "Training Batch [598/782]: Loss 1.8017538785934448\n",
      "Training Batch [599/782]: Loss 1.824880838394165\n",
      "Training Batch [600/782]: Loss 1.8807275295257568\n",
      "Training Batch [601/782]: Loss 1.9801744222640991\n",
      "Training Batch [602/782]: Loss 1.7492648363113403\n",
      "Training Batch [603/782]: Loss 1.56496000289917\n",
      "Training Batch [604/782]: Loss 1.6548911333084106\n",
      "Training Batch [605/782]: Loss 1.8389623165130615\n",
      "Training Batch [606/782]: Loss 1.9349287748336792\n",
      "Training Batch [607/782]: Loss 1.6225186586380005\n",
      "Training Batch [608/782]: Loss 1.7285406589508057\n",
      "Training Batch [609/782]: Loss 1.603805422782898\n",
      "Training Batch [610/782]: Loss 1.9530088901519775\n",
      "Training Batch [611/782]: Loss 1.6397883892059326\n",
      "Training Batch [612/782]: Loss 1.5588922500610352\n",
      "Training Batch [613/782]: Loss 1.7790852785110474\n",
      "Training Batch [614/782]: Loss 1.4906142950057983\n",
      "Training Batch [615/782]: Loss 1.5717962980270386\n",
      "Training Batch [616/782]: Loss 1.5329232215881348\n",
      "Training Batch [617/782]: Loss 1.4811521768569946\n",
      "Training Batch [618/782]: Loss 1.7334955930709839\n",
      "Training Batch [619/782]: Loss 1.7561863660812378\n",
      "Training Batch [620/782]: Loss 1.7886196374893188\n",
      "Training Batch [621/782]: Loss 1.714849829673767\n",
      "Training Batch [622/782]: Loss 1.4056609869003296\n",
      "Training Batch [623/782]: Loss 1.612177848815918\n",
      "Training Batch [624/782]: Loss 1.6601203680038452\n",
      "Training Batch [625/782]: Loss 1.7827355861663818\n",
      "Training Batch [626/782]: Loss 1.4679856300354004\n",
      "Training Batch [627/782]: Loss 1.5561347007751465\n",
      "Training Batch [628/782]: Loss 1.4970908164978027\n",
      "Training Batch [629/782]: Loss 1.7892311811447144\n",
      "Training Batch [630/782]: Loss 1.721778154373169\n",
      "Training Batch [631/782]: Loss 1.784732460975647\n",
      "Training Batch [632/782]: Loss 1.7309913635253906\n",
      "Training Batch [633/782]: Loss 1.7385526895523071\n",
      "Training Batch [634/782]: Loss 1.5610202550888062\n",
      "Training Batch [635/782]: Loss 1.7528373003005981\n",
      "Training Batch [636/782]: Loss 1.6216000318527222\n",
      "Training Batch [637/782]: Loss 1.6403913497924805\n",
      "Training Batch [638/782]: Loss 1.9323062896728516\n",
      "Training Batch [639/782]: Loss 1.806485652923584\n",
      "Training Batch [640/782]: Loss 1.4358384609222412\n",
      "Training Batch [641/782]: Loss 1.6136394739151\n",
      "Training Batch [642/782]: Loss 1.5638103485107422\n",
      "Training Batch [643/782]: Loss 1.5918246507644653\n",
      "Training Batch [644/782]: Loss 1.7479262351989746\n",
      "Training Batch [645/782]: Loss 1.6443114280700684\n",
      "Training Batch [646/782]: Loss 1.6539013385772705\n",
      "Training Batch [647/782]: Loss 1.4646717309951782\n",
      "Training Batch [648/782]: Loss 1.8180471658706665\n",
      "Training Batch [649/782]: Loss 1.6184965372085571\n",
      "Training Batch [650/782]: Loss 1.5663652420043945\n",
      "Training Batch [651/782]: Loss 1.7814335823059082\n",
      "Training Batch [652/782]: Loss 1.7449771165847778\n",
      "Training Batch [653/782]: Loss 1.6686041355133057\n",
      "Training Batch [654/782]: Loss 1.6867311000823975\n",
      "Training Batch [655/782]: Loss 1.7022275924682617\n",
      "Training Batch [656/782]: Loss 1.7848230600357056\n",
      "Training Batch [657/782]: Loss 1.602797269821167\n",
      "Training Batch [658/782]: Loss 1.617304801940918\n",
      "Training Batch [659/782]: Loss 1.416277289390564\n",
      "Training Batch [660/782]: Loss 1.7404927015304565\n",
      "Training Batch [661/782]: Loss 1.5528513193130493\n",
      "Training Batch [662/782]: Loss 1.6993781328201294\n",
      "Training Batch [663/782]: Loss 1.6769421100616455\n",
      "Training Batch [664/782]: Loss 1.625428318977356\n",
      "Training Batch [665/782]: Loss 1.7637993097305298\n",
      "Training Batch [666/782]: Loss 2.053438901901245\n",
      "Training Batch [667/782]: Loss 1.8490855693817139\n",
      "Training Batch [668/782]: Loss 1.7366114854812622\n",
      "Training Batch [669/782]: Loss 1.4862942695617676\n",
      "Training Batch [670/782]: Loss 1.9041014909744263\n",
      "Training Batch [671/782]: Loss 1.7169228792190552\n",
      "Training Batch [672/782]: Loss 1.5577960014343262\n",
      "Training Batch [673/782]: Loss 1.875990867614746\n",
      "Training Batch [674/782]: Loss 1.673100471496582\n",
      "Training Batch [675/782]: Loss 1.5689537525177002\n",
      "Training Batch [676/782]: Loss 1.6402829885482788\n",
      "Training Batch [677/782]: Loss 1.846778154373169\n",
      "Training Batch [678/782]: Loss 1.8500694036483765\n",
      "Training Batch [679/782]: Loss 1.3992327451705933\n",
      "Training Batch [680/782]: Loss 1.543976068496704\n",
      "Training Batch [681/782]: Loss 1.8638032674789429\n",
      "Training Batch [682/782]: Loss 1.5796071290969849\n",
      "Training Batch [683/782]: Loss 1.6918190717697144\n",
      "Training Batch [684/782]: Loss 1.6710485219955444\n",
      "Training Batch [685/782]: Loss 1.7430578470230103\n",
      "Training Batch [686/782]: Loss 1.683390736579895\n",
      "Training Batch [687/782]: Loss 1.8522169589996338\n",
      "Training Batch [688/782]: Loss 1.5346193313598633\n",
      "Training Batch [689/782]: Loss 1.5488978624343872\n",
      "Training Batch [690/782]: Loss 1.538012146949768\n",
      "Training Batch [691/782]: Loss 1.5552351474761963\n",
      "Training Batch [692/782]: Loss 1.7563755512237549\n",
      "Training Batch [693/782]: Loss 2.0083303451538086\n",
      "Training Batch [694/782]: Loss 1.6360447406768799\n",
      "Training Batch [695/782]: Loss 1.8742324113845825\n",
      "Training Batch [696/782]: Loss 1.8160549402236938\n",
      "Training Batch [697/782]: Loss 1.6102666854858398\n",
      "Training Batch [698/782]: Loss 1.8030891418457031\n",
      "Training Batch [699/782]: Loss 2.0546231269836426\n",
      "Training Batch [700/782]: Loss 1.7291135787963867\n",
      "Training Batch [701/782]: Loss 1.6075571775436401\n",
      "Training Batch [702/782]: Loss 1.5072202682495117\n",
      "Training Batch [703/782]: Loss 1.7610069513320923\n",
      "Training Batch [704/782]: Loss 1.7380987405776978\n",
      "Training Batch [705/782]: Loss 1.4923553466796875\n",
      "Training Batch [706/782]: Loss 1.5317463874816895\n",
      "Training Batch [707/782]: Loss 1.613645076751709\n",
      "Training Batch [708/782]: Loss 1.5658910274505615\n",
      "Training Batch [709/782]: Loss 1.6500266790390015\n",
      "Training Batch [710/782]: Loss 1.70486581325531\n",
      "Training Batch [711/782]: Loss 1.4355679750442505\n",
      "Training Batch [712/782]: Loss 1.7579119205474854\n",
      "Training Batch [713/782]: Loss 1.3577468395233154\n",
      "Training Batch [714/782]: Loss 1.5000125169754028\n",
      "Training Batch [715/782]: Loss 1.5819652080535889\n",
      "Training Batch [716/782]: Loss 1.5162584781646729\n",
      "Training Batch [717/782]: Loss 1.5876480340957642\n",
      "Training Batch [718/782]: Loss 1.8193684816360474\n",
      "Training Batch [719/782]: Loss 1.738525390625\n",
      "Training Batch [720/782]: Loss 1.5391340255737305\n",
      "Training Batch [721/782]: Loss 1.6702183485031128\n",
      "Training Batch [722/782]: Loss 1.4876906871795654\n",
      "Training Batch [723/782]: Loss 1.6776020526885986\n",
      "Training Batch [724/782]: Loss 1.3390882015228271\n",
      "Training Batch [725/782]: Loss 1.5691279172897339\n",
      "Training Batch [726/782]: Loss 1.7297226190567017\n",
      "Training Batch [727/782]: Loss 1.9773715734481812\n",
      "Training Batch [728/782]: Loss 1.5744773149490356\n",
      "Training Batch [729/782]: Loss 1.6058748960494995\n",
      "Training Batch [730/782]: Loss 1.3460487127304077\n",
      "Training Batch [731/782]: Loss 1.6539982557296753\n",
      "Training Batch [732/782]: Loss 1.778502345085144\n",
      "Training Batch [733/782]: Loss 1.6454640626907349\n",
      "Training Batch [734/782]: Loss 1.5526400804519653\n",
      "Training Batch [735/782]: Loss 1.705501914024353\n",
      "Training Batch [736/782]: Loss 1.4121408462524414\n",
      "Training Batch [737/782]: Loss 1.5327777862548828\n",
      "Training Batch [738/782]: Loss 1.703346610069275\n",
      "Training Batch [739/782]: Loss 1.7127296924591064\n",
      "Training Batch [740/782]: Loss 1.522275447845459\n",
      "Training Batch [741/782]: Loss 1.6682865619659424\n",
      "Training Batch [742/782]: Loss 1.516356348991394\n",
      "Training Batch [743/782]: Loss 1.72902250289917\n",
      "Training Batch [744/782]: Loss 1.736100435256958\n",
      "Training Batch [745/782]: Loss 1.6862616539001465\n",
      "Training Batch [746/782]: Loss 1.321928858757019\n",
      "Training Batch [747/782]: Loss 1.3932204246520996\n",
      "Training Batch [748/782]: Loss 1.652833342552185\n",
      "Training Batch [749/782]: Loss 1.6803631782531738\n",
      "Training Batch [750/782]: Loss 1.6346899271011353\n",
      "Training Batch [751/782]: Loss 1.605769395828247\n",
      "Training Batch [752/782]: Loss 1.5219634771347046\n",
      "Training Batch [753/782]: Loss 1.498502254486084\n",
      "Training Batch [754/782]: Loss 1.498246669769287\n",
      "Training Batch [755/782]: Loss 1.646684169769287\n",
      "Training Batch [756/782]: Loss 1.5348107814788818\n",
      "Training Batch [757/782]: Loss 1.675321102142334\n",
      "Training Batch [758/782]: Loss 1.6399370431900024\n",
      "Training Batch [759/782]: Loss 1.481473445892334\n",
      "Training Batch [760/782]: Loss 1.5542597770690918\n",
      "Training Batch [761/782]: Loss 1.5018057823181152\n",
      "Training Batch [762/782]: Loss 1.4274976253509521\n",
      "Training Batch [763/782]: Loss 1.738027572631836\n",
      "Training Batch [764/782]: Loss 1.6761122941970825\n",
      "Training Batch [765/782]: Loss 1.7664473056793213\n",
      "Training Batch [766/782]: Loss 1.7478156089782715\n",
      "Training Batch [767/782]: Loss 1.609139323234558\n",
      "Training Batch [768/782]: Loss 1.6942152976989746\n",
      "Training Batch [769/782]: Loss 1.6824781894683838\n",
      "Training Batch [770/782]: Loss 1.787632703781128\n",
      "Training Batch [771/782]: Loss 1.4383089542388916\n",
      "Training Batch [772/782]: Loss 1.757766604423523\n",
      "Training Batch [773/782]: Loss 1.8343486785888672\n",
      "Training Batch [774/782]: Loss 1.3327932357788086\n",
      "Training Batch [775/782]: Loss 1.610210657119751\n",
      "Training Batch [776/782]: Loss 1.5598469972610474\n",
      "Training Batch [777/782]: Loss 1.6242152452468872\n",
      "Training Batch [778/782]: Loss 1.6075713634490967\n",
      "Training Batch [779/782]: Loss 1.9093937873840332\n",
      "Training Batch [780/782]: Loss 1.4365344047546387\n",
      "Training Batch [781/782]: Loss 1.7039670944213867\n",
      "Training Batch [782/782]: Loss 1.6154170036315918\n",
      "Epoch 1 - Train Loss: 2.0621\n",
      "*********  Epoch 2/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 1.669311285018921\n",
      "Training Batch [2/782]: Loss 1.771156907081604\n",
      "Training Batch [3/782]: Loss 1.334567904472351\n",
      "Training Batch [4/782]: Loss 1.4419646263122559\n",
      "Training Batch [5/782]: Loss 1.5927104949951172\n",
      "Training Batch [6/782]: Loss 1.6542266607284546\n",
      "Training Batch [7/782]: Loss 1.6458332538604736\n",
      "Training Batch [8/782]: Loss 1.6179670095443726\n",
      "Training Batch [9/782]: Loss 1.4127362966537476\n",
      "Training Batch [10/782]: Loss 1.6375575065612793\n",
      "Training Batch [11/782]: Loss 1.48954176902771\n",
      "Training Batch [12/782]: Loss 1.554593563079834\n",
      "Training Batch [13/782]: Loss 1.5206186771392822\n",
      "Training Batch [14/782]: Loss 1.9191505908966064\n",
      "Training Batch [15/782]: Loss 1.3482847213745117\n",
      "Training Batch [16/782]: Loss 1.5136271715164185\n",
      "Training Batch [17/782]: Loss 1.3815851211547852\n",
      "Training Batch [18/782]: Loss 1.6507635116577148\n",
      "Training Batch [19/782]: Loss 1.4106513261795044\n",
      "Training Batch [20/782]: Loss 1.5780584812164307\n",
      "Training Batch [21/782]: Loss 1.5237720012664795\n",
      "Training Batch [22/782]: Loss 1.7436712980270386\n",
      "Training Batch [23/782]: Loss 1.6231821775436401\n",
      "Training Batch [24/782]: Loss 1.578726053237915\n",
      "Training Batch [25/782]: Loss 1.5854096412658691\n",
      "Training Batch [26/782]: Loss 1.3433442115783691\n",
      "Training Batch [27/782]: Loss 1.5787872076034546\n",
      "Training Batch [28/782]: Loss 1.3922951221466064\n",
      "Training Batch [29/782]: Loss 1.4404267072677612\n",
      "Training Batch [30/782]: Loss 1.655487298965454\n",
      "Training Batch [31/782]: Loss 1.559705138206482\n",
      "Training Batch [32/782]: Loss 1.4314570426940918\n",
      "Training Batch [33/782]: Loss 1.4905887842178345\n",
      "Training Batch [34/782]: Loss 1.4358218908309937\n",
      "Training Batch [35/782]: Loss 1.4187945127487183\n",
      "Training Batch [36/782]: Loss 1.641736388206482\n",
      "Training Batch [37/782]: Loss 1.5412174463272095\n",
      "Training Batch [38/782]: Loss 1.6861340999603271\n",
      "Training Batch [39/782]: Loss 1.650628685951233\n",
      "Training Batch [40/782]: Loss 1.5382189750671387\n",
      "Training Batch [41/782]: Loss 1.4343558549880981\n",
      "Training Batch [42/782]: Loss 1.3886531591415405\n",
      "Training Batch [43/782]: Loss 1.711074948310852\n",
      "Training Batch [44/782]: Loss 1.4008491039276123\n",
      "Training Batch [45/782]: Loss 1.534362554550171\n",
      "Training Batch [46/782]: Loss 1.6912108659744263\n",
      "Training Batch [47/782]: Loss 1.6102620363235474\n",
      "Training Batch [48/782]: Loss 1.685663104057312\n",
      "Training Batch [49/782]: Loss 1.5115485191345215\n",
      "Training Batch [50/782]: Loss 1.3603707551956177\n",
      "Training Batch [51/782]: Loss 1.3686397075653076\n",
      "Training Batch [52/782]: Loss 1.7389050722122192\n",
      "Training Batch [53/782]: Loss 1.7372761964797974\n",
      "Training Batch [54/782]: Loss 1.5858772993087769\n",
      "Training Batch [55/782]: Loss 1.7639360427856445\n",
      "Training Batch [56/782]: Loss 1.7033185958862305\n",
      "Training Batch [57/782]: Loss 1.8018336296081543\n",
      "Training Batch [58/782]: Loss 1.3491789102554321\n",
      "Training Batch [59/782]: Loss 1.3904772996902466\n",
      "Training Batch [60/782]: Loss 1.7798734903335571\n",
      "Training Batch [61/782]: Loss 1.5616198778152466\n",
      "Training Batch [62/782]: Loss 1.3482204675674438\n",
      "Training Batch [63/782]: Loss 1.4818154573440552\n",
      "Training Batch [64/782]: Loss 1.7295944690704346\n",
      "Training Batch [65/782]: Loss 1.8007060289382935\n",
      "Training Batch [66/782]: Loss 1.6474632024765015\n",
      "Training Batch [67/782]: Loss 1.553922414779663\n",
      "Training Batch [68/782]: Loss 1.6005319356918335\n",
      "Training Batch [69/782]: Loss 1.711113691329956\n",
      "Training Batch [70/782]: Loss 1.539241075515747\n",
      "Training Batch [71/782]: Loss 1.8378674983978271\n",
      "Training Batch [72/782]: Loss 1.5215280055999756\n",
      "Training Batch [73/782]: Loss 1.5135623216629028\n",
      "Training Batch [74/782]: Loss 1.7173855304718018\n",
      "Training Batch [75/782]: Loss 1.2934538125991821\n",
      "Training Batch [76/782]: Loss 1.490005612373352\n",
      "Training Batch [77/782]: Loss 1.6879059076309204\n",
      "Training Batch [78/782]: Loss 1.4170997142791748\n",
      "Training Batch [79/782]: Loss 1.6039788722991943\n",
      "Training Batch [80/782]: Loss 1.6298041343688965\n",
      "Training Batch [81/782]: Loss 1.4871258735656738\n",
      "Training Batch [82/782]: Loss 1.3242186307907104\n",
      "Training Batch [83/782]: Loss 1.5821876525878906\n",
      "Training Batch [84/782]: Loss 1.330698013305664\n",
      "Training Batch [85/782]: Loss 1.3817241191864014\n",
      "Training Batch [86/782]: Loss 1.567301869392395\n",
      "Training Batch [87/782]: Loss 1.6270098686218262\n",
      "Training Batch [88/782]: Loss 1.4922651052474976\n",
      "Training Batch [89/782]: Loss 1.2510541677474976\n",
      "Training Batch [90/782]: Loss 1.5509947538375854\n",
      "Training Batch [91/782]: Loss 1.7256752252578735\n",
      "Training Batch [92/782]: Loss 1.5255447626113892\n",
      "Training Batch [93/782]: Loss 1.2867306470870972\n",
      "Training Batch [94/782]: Loss 1.6136056184768677\n",
      "Training Batch [95/782]: Loss 1.508457899093628\n",
      "Training Batch [96/782]: Loss 1.533325433731079\n",
      "Training Batch [97/782]: Loss 1.5007829666137695\n",
      "Training Batch [98/782]: Loss 1.784999132156372\n",
      "Training Batch [99/782]: Loss 1.84004545211792\n",
      "Training Batch [100/782]: Loss 1.5677598714828491\n",
      "Training Batch [101/782]: Loss 1.3948420286178589\n",
      "Training Batch [102/782]: Loss 1.3514033555984497\n",
      "Training Batch [103/782]: Loss 1.6182585954666138\n",
      "Training Batch [104/782]: Loss 1.6406091451644897\n",
      "Training Batch [105/782]: Loss 1.8640861511230469\n",
      "Training Batch [106/782]: Loss 1.844736099243164\n",
      "Training Batch [107/782]: Loss 1.7371721267700195\n",
      "Training Batch [108/782]: Loss 1.6550365686416626\n",
      "Training Batch [109/782]: Loss 1.5321933031082153\n",
      "Training Batch [110/782]: Loss 1.483680248260498\n",
      "Training Batch [111/782]: Loss 1.5455498695373535\n",
      "Training Batch [112/782]: Loss 1.7437907457351685\n",
      "Training Batch [113/782]: Loss 1.4171310663223267\n",
      "Training Batch [114/782]: Loss 1.41145658493042\n",
      "Training Batch [115/782]: Loss 1.3704819679260254\n",
      "Training Batch [116/782]: Loss 1.6315429210662842\n",
      "Training Batch [117/782]: Loss 1.4714820384979248\n",
      "Training Batch [118/782]: Loss 1.502678632736206\n",
      "Training Batch [119/782]: Loss 1.7896226644515991\n",
      "Training Batch [120/782]: Loss 1.6165097951889038\n",
      "Training Batch [121/782]: Loss 1.4010498523712158\n",
      "Training Batch [122/782]: Loss 1.7310773134231567\n",
      "Training Batch [123/782]: Loss 1.5897157192230225\n",
      "Training Batch [124/782]: Loss 1.428597092628479\n",
      "Training Batch [125/782]: Loss 1.61666738986969\n",
      "Training Batch [126/782]: Loss 1.3925950527191162\n",
      "Training Batch [127/782]: Loss 1.5631320476531982\n",
      "Training Batch [128/782]: Loss 1.3246619701385498\n",
      "Training Batch [129/782]: Loss 1.472955584526062\n",
      "Training Batch [130/782]: Loss 1.4446396827697754\n",
      "Training Batch [131/782]: Loss 1.5236526727676392\n",
      "Training Batch [132/782]: Loss 1.350041389465332\n",
      "Training Batch [133/782]: Loss 1.637763500213623\n",
      "Training Batch [134/782]: Loss 1.3881938457489014\n",
      "Training Batch [135/782]: Loss 1.683640480041504\n",
      "Training Batch [136/782]: Loss 1.4473991394042969\n",
      "Training Batch [137/782]: Loss 1.856345772743225\n",
      "Training Batch [138/782]: Loss 1.5788170099258423\n",
      "Training Batch [139/782]: Loss 1.414035439491272\n",
      "Training Batch [140/782]: Loss 1.4436841011047363\n",
      "Training Batch [141/782]: Loss 1.8577147722244263\n",
      "Training Batch [142/782]: Loss 1.9080283641815186\n",
      "Training Batch [143/782]: Loss 1.6012341976165771\n",
      "Training Batch [144/782]: Loss 1.3818325996398926\n",
      "Training Batch [145/782]: Loss 1.4373157024383545\n",
      "Training Batch [146/782]: Loss 1.4660048484802246\n",
      "Training Batch [147/782]: Loss 1.617407202720642\n",
      "Training Batch [148/782]: Loss 1.653417944908142\n",
      "Training Batch [149/782]: Loss 1.6184322834014893\n",
      "Training Batch [150/782]: Loss 1.4479484558105469\n",
      "Training Batch [151/782]: Loss 1.3477360010147095\n",
      "Training Batch [152/782]: Loss 1.2722355127334595\n",
      "Training Batch [153/782]: Loss 1.6395071744918823\n",
      "Training Batch [154/782]: Loss 1.8805835247039795\n",
      "Training Batch [155/782]: Loss 1.748995065689087\n",
      "Training Batch [156/782]: Loss 1.5405347347259521\n",
      "Training Batch [157/782]: Loss 1.5568314790725708\n",
      "Training Batch [158/782]: Loss 1.6095377206802368\n",
      "Training Batch [159/782]: Loss 1.4367692470550537\n",
      "Training Batch [160/782]: Loss 1.4184480905532837\n",
      "Training Batch [161/782]: Loss 1.3180150985717773\n",
      "Training Batch [162/782]: Loss 1.495197057723999\n",
      "Training Batch [163/782]: Loss 1.4610469341278076\n",
      "Training Batch [164/782]: Loss 1.5294939279556274\n",
      "Training Batch [165/782]: Loss 1.475988745689392\n",
      "Training Batch [166/782]: Loss 1.2508134841918945\n",
      "Training Batch [167/782]: Loss 1.3908812999725342\n",
      "Training Batch [168/782]: Loss 1.3584779500961304\n",
      "Training Batch [169/782]: Loss 1.2655075788497925\n",
      "Training Batch [170/782]: Loss 1.480292558670044\n",
      "Training Batch [171/782]: Loss 1.4053152799606323\n",
      "Training Batch [172/782]: Loss 1.5366740226745605\n",
      "Training Batch [173/782]: Loss 1.9096665382385254\n",
      "Training Batch [174/782]: Loss 1.6241867542266846\n",
      "Training Batch [175/782]: Loss 1.483986735343933\n",
      "Training Batch [176/782]: Loss 1.3275383710861206\n",
      "Training Batch [177/782]: Loss 1.4662857055664062\n",
      "Training Batch [178/782]: Loss 1.6322147846221924\n",
      "Training Batch [179/782]: Loss 1.344829797744751\n",
      "Training Batch [180/782]: Loss 1.7284250259399414\n",
      "Training Batch [181/782]: Loss 1.361376404762268\n",
      "Training Batch [182/782]: Loss 1.804643988609314\n",
      "Training Batch [183/782]: Loss 1.5177785158157349\n",
      "Training Batch [184/782]: Loss 1.4452333450317383\n",
      "Training Batch [185/782]: Loss 1.3594317436218262\n",
      "Training Batch [186/782]: Loss 1.6343042850494385\n",
      "Training Batch [187/782]: Loss 1.4644235372543335\n",
      "Training Batch [188/782]: Loss 1.6463197469711304\n",
      "Training Batch [189/782]: Loss 1.5138232707977295\n",
      "Training Batch [190/782]: Loss 1.2945492267608643\n",
      "Training Batch [191/782]: Loss 1.2005889415740967\n",
      "Training Batch [192/782]: Loss 1.6702309846878052\n",
      "Training Batch [193/782]: Loss 1.2010225057601929\n",
      "Training Batch [194/782]: Loss 1.687562346458435\n",
      "Training Batch [195/782]: Loss 1.4386004209518433\n",
      "Training Batch [196/782]: Loss 1.4542628526687622\n",
      "Training Batch [197/782]: Loss 1.231907844543457\n",
      "Training Batch [198/782]: Loss 1.4624634981155396\n",
      "Training Batch [199/782]: Loss 1.8073135614395142\n",
      "Training Batch [200/782]: Loss 1.58121657371521\n",
      "Training Batch [201/782]: Loss 1.3698923587799072\n",
      "Training Batch [202/782]: Loss 1.4735639095306396\n",
      "Training Batch [203/782]: Loss 1.8938138484954834\n",
      "Training Batch [204/782]: Loss 1.4972270727157593\n",
      "Training Batch [205/782]: Loss 1.7112430334091187\n",
      "Training Batch [206/782]: Loss 1.2902206182479858\n",
      "Training Batch [207/782]: Loss 1.4253512620925903\n",
      "Training Batch [208/782]: Loss 1.4804078340530396\n",
      "Training Batch [209/782]: Loss 1.3881336450576782\n",
      "Training Batch [210/782]: Loss 1.4561971426010132\n",
      "Training Batch [211/782]: Loss 1.4523470401763916\n",
      "Training Batch [212/782]: Loss 1.4607772827148438\n",
      "Training Batch [213/782]: Loss 1.4162683486938477\n",
      "Training Batch [214/782]: Loss 1.4345862865447998\n",
      "Training Batch [215/782]: Loss 1.6803488731384277\n",
      "Training Batch [216/782]: Loss 1.5450600385665894\n",
      "Training Batch [217/782]: Loss 1.502763271331787\n",
      "Training Batch [218/782]: Loss 1.2799097299575806\n",
      "Training Batch [219/782]: Loss 1.6570122241973877\n",
      "Training Batch [220/782]: Loss 1.3973113298416138\n",
      "Training Batch [221/782]: Loss 1.1963034868240356\n",
      "Training Batch [222/782]: Loss 1.54268217086792\n",
      "Training Batch [223/782]: Loss 1.359877109527588\n",
      "Training Batch [224/782]: Loss 1.4815603494644165\n",
      "Training Batch [225/782]: Loss 1.426459789276123\n",
      "Training Batch [226/782]: Loss 1.356122612953186\n",
      "Training Batch [227/782]: Loss 1.673910140991211\n",
      "Training Batch [228/782]: Loss 1.2727127075195312\n",
      "Training Batch [229/782]: Loss 1.623335361480713\n",
      "Training Batch [230/782]: Loss 1.4516346454620361\n",
      "Training Batch [231/782]: Loss 1.593276858329773\n",
      "Training Batch [232/782]: Loss 1.6947786808013916\n",
      "Training Batch [233/782]: Loss 1.3426412343978882\n",
      "Training Batch [234/782]: Loss 1.4640889167785645\n",
      "Training Batch [235/782]: Loss 1.644344687461853\n",
      "Training Batch [236/782]: Loss 1.464215636253357\n",
      "Training Batch [237/782]: Loss 1.6526539325714111\n",
      "Training Batch [238/782]: Loss 1.3959685564041138\n",
      "Training Batch [239/782]: Loss 1.3504575490951538\n",
      "Training Batch [240/782]: Loss 1.4744775295257568\n",
      "Training Batch [241/782]: Loss 1.5346930027008057\n",
      "Training Batch [242/782]: Loss 1.4459760189056396\n",
      "Training Batch [243/782]: Loss 1.6598100662231445\n",
      "Training Batch [244/782]: Loss 1.5708296298980713\n",
      "Training Batch [245/782]: Loss 1.2811553478240967\n",
      "Training Batch [246/782]: Loss 1.5031358003616333\n",
      "Training Batch [247/782]: Loss 1.5231550931930542\n",
      "Training Batch [248/782]: Loss 1.4528993368148804\n",
      "Training Batch [249/782]: Loss 1.5241897106170654\n",
      "Training Batch [250/782]: Loss 1.5031222105026245\n",
      "Training Batch [251/782]: Loss 1.4108892679214478\n",
      "Training Batch [252/782]: Loss 1.089302897453308\n",
      "Training Batch [253/782]: Loss 1.5911145210266113\n",
      "Training Batch [254/782]: Loss 1.6863247156143188\n",
      "Training Batch [255/782]: Loss 1.3656716346740723\n",
      "Training Batch [256/782]: Loss 1.7798223495483398\n",
      "Training Batch [257/782]: Loss 1.6081717014312744\n",
      "Training Batch [258/782]: Loss 1.2580592632293701\n",
      "Training Batch [259/782]: Loss 1.6036378145217896\n",
      "Training Batch [260/782]: Loss 1.749413251876831\n",
      "Training Batch [261/782]: Loss 1.4026820659637451\n",
      "Training Batch [262/782]: Loss 1.3675897121429443\n",
      "Training Batch [263/782]: Loss 1.4959439039230347\n",
      "Training Batch [264/782]: Loss 1.5949275493621826\n",
      "Training Batch [265/782]: Loss 1.684268832206726\n",
      "Training Batch [266/782]: Loss 1.164313554763794\n",
      "Training Batch [267/782]: Loss 1.4164113998413086\n",
      "Training Batch [268/782]: Loss 1.5198800563812256\n",
      "Training Batch [269/782]: Loss 1.7553297281265259\n",
      "Training Batch [270/782]: Loss 1.5686832666397095\n",
      "Training Batch [271/782]: Loss 1.5734107494354248\n",
      "Training Batch [272/782]: Loss 1.3733984231948853\n",
      "Training Batch [273/782]: Loss 1.651416301727295\n",
      "Training Batch [274/782]: Loss 1.5667500495910645\n",
      "Training Batch [275/782]: Loss 1.6580405235290527\n",
      "Training Batch [276/782]: Loss 1.5069071054458618\n",
      "Training Batch [277/782]: Loss 1.6270606517791748\n",
      "Training Batch [278/782]: Loss 1.5340089797973633\n",
      "Training Batch [279/782]: Loss 1.6591260433197021\n",
      "Training Batch [280/782]: Loss 1.5588486194610596\n",
      "Training Batch [281/782]: Loss 1.4384323358535767\n",
      "Training Batch [282/782]: Loss 1.4298659563064575\n",
      "Training Batch [283/782]: Loss 1.4309239387512207\n",
      "Training Batch [284/782]: Loss 1.5467065572738647\n",
      "Training Batch [285/782]: Loss 1.6791527271270752\n",
      "Training Batch [286/782]: Loss 1.6329421997070312\n",
      "Training Batch [287/782]: Loss 1.2326383590698242\n",
      "Training Batch [288/782]: Loss 1.3012516498565674\n",
      "Training Batch [289/782]: Loss 1.3770869970321655\n",
      "Training Batch [290/782]: Loss 1.3359096050262451\n",
      "Training Batch [291/782]: Loss 1.577822208404541\n",
      "Training Batch [292/782]: Loss 1.4054689407348633\n",
      "Training Batch [293/782]: Loss 1.5690792798995972\n",
      "Training Batch [294/782]: Loss 1.423995852470398\n",
      "Training Batch [295/782]: Loss 1.4128589630126953\n",
      "Training Batch [296/782]: Loss 1.5893645286560059\n",
      "Training Batch [297/782]: Loss 1.5250444412231445\n",
      "Training Batch [298/782]: Loss 1.575852394104004\n",
      "Training Batch [299/782]: Loss 1.6929192543029785\n",
      "Training Batch [300/782]: Loss 1.441115140914917\n",
      "Training Batch [301/782]: Loss 1.2801204919815063\n",
      "Training Batch [302/782]: Loss 1.3678698539733887\n",
      "Training Batch [303/782]: Loss 1.3751857280731201\n",
      "Training Batch [304/782]: Loss 1.311048984527588\n",
      "Training Batch [305/782]: Loss 1.8085582256317139\n",
      "Training Batch [306/782]: Loss 1.4402482509613037\n",
      "Training Batch [307/782]: Loss 1.3884856700897217\n",
      "Training Batch [308/782]: Loss 1.4980827569961548\n",
      "Training Batch [309/782]: Loss 1.6062133312225342\n",
      "Training Batch [310/782]: Loss 1.3435704708099365\n",
      "Training Batch [311/782]: Loss 1.6792010068893433\n",
      "Training Batch [312/782]: Loss 1.4448621273040771\n",
      "Training Batch [313/782]: Loss 1.5126456022262573\n",
      "Training Batch [314/782]: Loss 1.4805057048797607\n",
      "Training Batch [315/782]: Loss 1.3711262941360474\n",
      "Training Batch [316/782]: Loss 1.2647287845611572\n",
      "Training Batch [317/782]: Loss 1.2951714992523193\n",
      "Training Batch [318/782]: Loss 1.4155935049057007\n",
      "Training Batch [319/782]: Loss 1.436115026473999\n",
      "Training Batch [320/782]: Loss 1.4270656108856201\n",
      "Training Batch [321/782]: Loss 1.4415974617004395\n",
      "Training Batch [322/782]: Loss 1.5003588199615479\n",
      "Training Batch [323/782]: Loss 1.8778012990951538\n",
      "Training Batch [324/782]: Loss 1.4580745697021484\n",
      "Training Batch [325/782]: Loss 1.4488978385925293\n",
      "Training Batch [326/782]: Loss 1.1968568563461304\n",
      "Training Batch [327/782]: Loss 1.2891325950622559\n",
      "Training Batch [328/782]: Loss 1.454960584640503\n",
      "Training Batch [329/782]: Loss 1.5002697706222534\n",
      "Training Batch [330/782]: Loss 1.2875306606292725\n",
      "Training Batch [331/782]: Loss 1.2864078283309937\n",
      "Training Batch [332/782]: Loss 1.436367392539978\n",
      "Training Batch [333/782]: Loss 1.425304889678955\n",
      "Training Batch [334/782]: Loss 1.5043301582336426\n",
      "Training Batch [335/782]: Loss 1.330682396888733\n",
      "Training Batch [336/782]: Loss 1.5054218769073486\n",
      "Training Batch [337/782]: Loss 1.3330919742584229\n",
      "Training Batch [338/782]: Loss 1.4665170907974243\n",
      "Training Batch [339/782]: Loss 1.1585938930511475\n",
      "Training Batch [340/782]: Loss 1.297628402709961\n",
      "Training Batch [341/782]: Loss 1.8390092849731445\n",
      "Training Batch [342/782]: Loss 1.4671738147735596\n",
      "Training Batch [343/782]: Loss 1.4240515232086182\n",
      "Training Batch [344/782]: Loss 1.5054080486297607\n",
      "Training Batch [345/782]: Loss 1.5357774496078491\n",
      "Training Batch [346/782]: Loss 1.4802987575531006\n",
      "Training Batch [347/782]: Loss 1.6341092586517334\n",
      "Training Batch [348/782]: Loss 1.400753378868103\n",
      "Training Batch [349/782]: Loss 1.2861595153808594\n",
      "Training Batch [350/782]: Loss 1.3512039184570312\n",
      "Training Batch [351/782]: Loss 1.2329583168029785\n",
      "Training Batch [352/782]: Loss 1.3552618026733398\n",
      "Training Batch [353/782]: Loss 1.4408742189407349\n",
      "Training Batch [354/782]: Loss 1.4072030782699585\n",
      "Training Batch [355/782]: Loss 1.4879875183105469\n",
      "Training Batch [356/782]: Loss 1.2889403104782104\n",
      "Training Batch [357/782]: Loss 1.2452219724655151\n",
      "Training Batch [358/782]: Loss 1.553328037261963\n",
      "Training Batch [359/782]: Loss 1.5562533140182495\n",
      "Training Batch [360/782]: Loss 1.428666353225708\n",
      "Training Batch [361/782]: Loss 1.3953136205673218\n",
      "Training Batch [362/782]: Loss 1.7491892576217651\n",
      "Training Batch [363/782]: Loss 1.4210386276245117\n",
      "Training Batch [364/782]: Loss 1.4827556610107422\n",
      "Training Batch [365/782]: Loss 1.4433008432388306\n",
      "Training Batch [366/782]: Loss 1.5481395721435547\n",
      "Training Batch [367/782]: Loss 1.296580195426941\n",
      "Training Batch [368/782]: Loss 1.604133129119873\n",
      "Training Batch [369/782]: Loss 1.4834097623825073\n",
      "Training Batch [370/782]: Loss 1.7246233224868774\n",
      "Training Batch [371/782]: Loss 1.5874524116516113\n",
      "Training Batch [372/782]: Loss 1.4643480777740479\n",
      "Training Batch [373/782]: Loss 1.4534871578216553\n",
      "Training Batch [374/782]: Loss 1.7265450954437256\n",
      "Training Batch [375/782]: Loss 1.5035749673843384\n",
      "Training Batch [376/782]: Loss 1.2970106601715088\n",
      "Training Batch [377/782]: Loss 1.395578145980835\n",
      "Training Batch [378/782]: Loss 1.504818320274353\n",
      "Training Batch [379/782]: Loss 1.6855769157409668\n",
      "Training Batch [380/782]: Loss 1.4265918731689453\n",
      "Training Batch [381/782]: Loss 1.592387318611145\n",
      "Training Batch [382/782]: Loss 1.70913827419281\n",
      "Training Batch [383/782]: Loss 1.4964545965194702\n",
      "Training Batch [384/782]: Loss 1.8268134593963623\n",
      "Training Batch [385/782]: Loss 1.6324074268341064\n",
      "Training Batch [386/782]: Loss 1.3318424224853516\n",
      "Training Batch [387/782]: Loss 1.4977682828903198\n",
      "Training Batch [388/782]: Loss 1.4707884788513184\n",
      "Training Batch [389/782]: Loss 1.4569019079208374\n",
      "Training Batch [390/782]: Loss 1.6910067796707153\n",
      "Training Batch [391/782]: Loss 1.603413701057434\n",
      "Training Batch [392/782]: Loss 1.41155207157135\n",
      "Training Batch [393/782]: Loss 1.273301124572754\n",
      "Training Batch [394/782]: Loss 1.4363255500793457\n",
      "Training Batch [395/782]: Loss 1.4459742307662964\n",
      "Training Batch [396/782]: Loss 1.7120258808135986\n",
      "Training Batch [397/782]: Loss 1.216661810874939\n",
      "Training Batch [398/782]: Loss 1.5027751922607422\n",
      "Training Batch [399/782]: Loss 1.2841401100158691\n",
      "Training Batch [400/782]: Loss 1.3326340913772583\n",
      "Training Batch [401/782]: Loss 1.4881030321121216\n",
      "Training Batch [402/782]: Loss 1.4158512353897095\n",
      "Training Batch [403/782]: Loss 1.473128080368042\n",
      "Training Batch [404/782]: Loss 1.5373166799545288\n",
      "Training Batch [405/782]: Loss 1.575622797012329\n",
      "Training Batch [406/782]: Loss 1.4314814805984497\n",
      "Training Batch [407/782]: Loss 1.513193964958191\n",
      "Training Batch [408/782]: Loss 1.4428037405014038\n",
      "Training Batch [409/782]: Loss 1.3736493587493896\n",
      "Training Batch [410/782]: Loss 1.4076882600784302\n",
      "Training Batch [411/782]: Loss 1.2078670263290405\n",
      "Training Batch [412/782]: Loss 1.343930959701538\n",
      "Training Batch [413/782]: Loss 1.5973695516586304\n",
      "Training Batch [414/782]: Loss 1.2581192255020142\n",
      "Training Batch [415/782]: Loss 1.430741310119629\n",
      "Training Batch [416/782]: Loss 1.5257470607757568\n",
      "Training Batch [417/782]: Loss 1.4233174324035645\n",
      "Training Batch [418/782]: Loss 1.2709585428237915\n",
      "Training Batch [419/782]: Loss 1.533983826637268\n",
      "Training Batch [420/782]: Loss 1.5750923156738281\n",
      "Training Batch [421/782]: Loss 1.3611996173858643\n",
      "Training Batch [422/782]: Loss 1.3401691913604736\n",
      "Training Batch [423/782]: Loss 1.3937219381332397\n",
      "Training Batch [424/782]: Loss 1.5740751028060913\n",
      "Training Batch [425/782]: Loss 1.2985146045684814\n",
      "Training Batch [426/782]: Loss 1.4239944219589233\n",
      "Training Batch [427/782]: Loss 1.5682581663131714\n",
      "Training Batch [428/782]: Loss 1.3791402578353882\n",
      "Training Batch [429/782]: Loss 1.156313180923462\n",
      "Training Batch [430/782]: Loss 1.6176811456680298\n",
      "Training Batch [431/782]: Loss 1.3349584341049194\n",
      "Training Batch [432/782]: Loss 1.2459559440612793\n",
      "Training Batch [433/782]: Loss 1.2407310009002686\n",
      "Training Batch [434/782]: Loss 1.3021445274353027\n",
      "Training Batch [435/782]: Loss 1.7880765199661255\n",
      "Training Batch [436/782]: Loss 1.8662594556808472\n",
      "Training Batch [437/782]: Loss 1.4825671911239624\n",
      "Training Batch [438/782]: Loss 1.6852272748947144\n",
      "Training Batch [439/782]: Loss 1.1446031332015991\n",
      "Training Batch [440/782]: Loss 1.1222140789031982\n",
      "Training Batch [441/782]: Loss 1.3771653175354004\n",
      "Training Batch [442/782]: Loss 1.759639024734497\n",
      "Training Batch [443/782]: Loss 1.5841987133026123\n",
      "Training Batch [444/782]: Loss 1.465528964996338\n",
      "Training Batch [445/782]: Loss 1.357742190361023\n",
      "Training Batch [446/782]: Loss 1.431105613708496\n",
      "Training Batch [447/782]: Loss 1.5446408987045288\n",
      "Training Batch [448/782]: Loss 1.5369367599487305\n",
      "Training Batch [449/782]: Loss 1.364800214767456\n",
      "Training Batch [450/782]: Loss 1.4608408212661743\n",
      "Training Batch [451/782]: Loss 1.552094578742981\n",
      "Training Batch [452/782]: Loss 1.5941145420074463\n",
      "Training Batch [453/782]: Loss 1.5976845026016235\n",
      "Training Batch [454/782]: Loss 1.587064266204834\n",
      "Training Batch [455/782]: Loss 1.5622813701629639\n",
      "Training Batch [456/782]: Loss 1.860601544380188\n",
      "Training Batch [457/782]: Loss 1.7720046043395996\n",
      "Training Batch [458/782]: Loss 1.3950474262237549\n",
      "Training Batch [459/782]: Loss 1.557982087135315\n",
      "Training Batch [460/782]: Loss 1.8837928771972656\n",
      "Training Batch [461/782]: Loss 1.583801507949829\n",
      "Training Batch [462/782]: Loss 1.4300174713134766\n",
      "Training Batch [463/782]: Loss 1.4057974815368652\n",
      "Training Batch [464/782]: Loss 1.4800446033477783\n",
      "Training Batch [465/782]: Loss 1.434000849723816\n",
      "Training Batch [466/782]: Loss 1.4796831607818604\n",
      "Training Batch [467/782]: Loss 1.5822954177856445\n",
      "Training Batch [468/782]: Loss 1.5538718700408936\n",
      "Training Batch [469/782]: Loss 1.719247579574585\n",
      "Training Batch [470/782]: Loss 1.3336318731307983\n",
      "Training Batch [471/782]: Loss 1.3422925472259521\n",
      "Training Batch [472/782]: Loss 1.3554415702819824\n",
      "Training Batch [473/782]: Loss 1.6645129919052124\n",
      "Training Batch [474/782]: Loss 1.5940070152282715\n",
      "Training Batch [475/782]: Loss 1.4434754848480225\n",
      "Training Batch [476/782]: Loss 1.361432433128357\n",
      "Training Batch [477/782]: Loss 1.4664537906646729\n",
      "Training Batch [478/782]: Loss 1.2437812089920044\n",
      "Training Batch [479/782]: Loss 1.4810484647750854\n",
      "Training Batch [480/782]: Loss 1.2965573072433472\n",
      "Training Batch [481/782]: Loss 1.4379686117172241\n",
      "Training Batch [482/782]: Loss 1.2469466924667358\n",
      "Training Batch [483/782]: Loss 1.3516775369644165\n",
      "Training Batch [484/782]: Loss 1.269913911819458\n",
      "Training Batch [485/782]: Loss 1.605505108833313\n",
      "Training Batch [486/782]: Loss 1.526158332824707\n",
      "Training Batch [487/782]: Loss 1.467973232269287\n",
      "Training Batch [488/782]: Loss 1.3819304704666138\n",
      "Training Batch [489/782]: Loss 1.5744868516921997\n",
      "Training Batch [490/782]: Loss 1.3704899549484253\n",
      "Training Batch [491/782]: Loss 1.4075360298156738\n",
      "Training Batch [492/782]: Loss 1.4460639953613281\n",
      "Training Batch [493/782]: Loss 1.3602081537246704\n",
      "Training Batch [494/782]: Loss 1.0843287706375122\n",
      "Training Batch [495/782]: Loss 1.5861318111419678\n",
      "Training Batch [496/782]: Loss 1.7957395315170288\n",
      "Training Batch [497/782]: Loss 1.4341821670532227\n",
      "Training Batch [498/782]: Loss 1.498824954032898\n",
      "Training Batch [499/782]: Loss 1.757330298423767\n",
      "Training Batch [500/782]: Loss 1.4272876977920532\n",
      "Training Batch [501/782]: Loss 1.527736783027649\n",
      "Training Batch [502/782]: Loss 1.4132601022720337\n",
      "Training Batch [503/782]: Loss 1.3662350177764893\n",
      "Training Batch [504/782]: Loss 1.1385610103607178\n",
      "Training Batch [505/782]: Loss 1.4694057703018188\n",
      "Training Batch [506/782]: Loss 1.286259412765503\n",
      "Training Batch [507/782]: Loss 1.5842667818069458\n",
      "Training Batch [508/782]: Loss 1.5558565855026245\n",
      "Training Batch [509/782]: Loss 1.6892026662826538\n",
      "Training Batch [510/782]: Loss 1.4938009977340698\n",
      "Training Batch [511/782]: Loss 1.0912054777145386\n",
      "Training Batch [512/782]: Loss 1.1498874425888062\n",
      "Training Batch [513/782]: Loss 1.433444619178772\n",
      "Training Batch [514/782]: Loss 1.5027765035629272\n",
      "Training Batch [515/782]: Loss 1.3882695436477661\n",
      "Training Batch [516/782]: Loss 1.6603392362594604\n",
      "Training Batch [517/782]: Loss 1.2751586437225342\n",
      "Training Batch [518/782]: Loss 1.475162386894226\n",
      "Training Batch [519/782]: Loss 1.2235928773880005\n",
      "Training Batch [520/782]: Loss 1.334406852722168\n",
      "Training Batch [521/782]: Loss 1.2859615087509155\n",
      "Training Batch [522/782]: Loss 1.4693934917449951\n",
      "Training Batch [523/782]: Loss 1.600272297859192\n",
      "Training Batch [524/782]: Loss 1.3853628635406494\n",
      "Training Batch [525/782]: Loss 1.2928693294525146\n",
      "Training Batch [526/782]: Loss 1.3640379905700684\n",
      "Training Batch [527/782]: Loss 1.3705251216888428\n",
      "Training Batch [528/782]: Loss 1.2540743350982666\n",
      "Training Batch [529/782]: Loss 1.552152395248413\n",
      "Training Batch [530/782]: Loss 1.3887817859649658\n",
      "Training Batch [531/782]: Loss 1.5213563442230225\n",
      "Training Batch [532/782]: Loss 1.392717957496643\n",
      "Training Batch [533/782]: Loss 1.652350902557373\n",
      "Training Batch [534/782]: Loss 1.3496594429016113\n",
      "Training Batch [535/782]: Loss 1.595880150794983\n",
      "Training Batch [536/782]: Loss 1.3562721014022827\n",
      "Training Batch [537/782]: Loss 1.5067224502563477\n",
      "Training Batch [538/782]: Loss 1.4545578956604004\n",
      "Training Batch [539/782]: Loss 1.526501178741455\n",
      "Training Batch [540/782]: Loss 1.2844033241271973\n",
      "Training Batch [541/782]: Loss 1.4552057981491089\n",
      "Training Batch [542/782]: Loss 1.4004358053207397\n",
      "Training Batch [543/782]: Loss 1.5115994215011597\n",
      "Training Batch [544/782]: Loss 1.7876440286636353\n",
      "Training Batch [545/782]: Loss 1.3779829740524292\n",
      "Training Batch [546/782]: Loss 1.325907826423645\n",
      "Training Batch [547/782]: Loss 1.297431230545044\n",
      "Training Batch [548/782]: Loss 1.3340462446212769\n",
      "Training Batch [549/782]: Loss 1.4576339721679688\n",
      "Training Batch [550/782]: Loss 1.2183228731155396\n",
      "Training Batch [551/782]: Loss 1.2304356098175049\n",
      "Training Batch [552/782]: Loss 1.4675923585891724\n",
      "Training Batch [553/782]: Loss 1.5276165008544922\n",
      "Training Batch [554/782]: Loss 1.7382922172546387\n",
      "Training Batch [555/782]: Loss 1.72966730594635\n",
      "Training Batch [556/782]: Loss 1.4865248203277588\n",
      "Training Batch [557/782]: Loss 1.4691824913024902\n",
      "Training Batch [558/782]: Loss 1.4806147813796997\n",
      "Training Batch [559/782]: Loss 1.3225300312042236\n",
      "Training Batch [560/782]: Loss 1.4629894495010376\n",
      "Training Batch [561/782]: Loss 1.0130701065063477\n",
      "Training Batch [562/782]: Loss 1.3442411422729492\n",
      "Training Batch [563/782]: Loss 1.4008980989456177\n",
      "Training Batch [564/782]: Loss 1.742258071899414\n",
      "Training Batch [565/782]: Loss 1.3782655000686646\n",
      "Training Batch [566/782]: Loss 1.4324419498443604\n",
      "Training Batch [567/782]: Loss 1.392663836479187\n",
      "Training Batch [568/782]: Loss 1.1582893133163452\n",
      "Training Batch [569/782]: Loss 1.3161251544952393\n",
      "Training Batch [570/782]: Loss 1.39238440990448\n",
      "Training Batch [571/782]: Loss 1.2822269201278687\n",
      "Training Batch [572/782]: Loss 1.2961525917053223\n",
      "Training Batch [573/782]: Loss 1.5224391222000122\n",
      "Training Batch [574/782]: Loss 1.483761191368103\n",
      "Training Batch [575/782]: Loss 1.6834455728530884\n",
      "Training Batch [576/782]: Loss 1.4680376052856445\n",
      "Training Batch [577/782]: Loss 1.3685023784637451\n",
      "Training Batch [578/782]: Loss 1.3405827283859253\n",
      "Training Batch [579/782]: Loss 1.236458420753479\n",
      "Training Batch [580/782]: Loss 1.5579296350479126\n",
      "Training Batch [581/782]: Loss 1.1024019718170166\n",
      "Training Batch [582/782]: Loss 1.0741639137268066\n",
      "Training Batch [583/782]: Loss 1.661780595779419\n",
      "Training Batch [584/782]: Loss 1.3622990846633911\n",
      "Training Batch [585/782]: Loss 1.5031991004943848\n",
      "Training Batch [586/782]: Loss 1.6033276319503784\n",
      "Training Batch [587/782]: Loss 1.4718859195709229\n",
      "Training Batch [588/782]: Loss 1.4268008470535278\n",
      "Training Batch [589/782]: Loss 1.279569149017334\n",
      "Training Batch [590/782]: Loss 1.266370415687561\n",
      "Training Batch [591/782]: Loss 1.6505427360534668\n",
      "Training Batch [592/782]: Loss 1.602538824081421\n",
      "Training Batch [593/782]: Loss 1.3894902467727661\n",
      "Training Batch [594/782]: Loss 1.4369173049926758\n",
      "Training Batch [595/782]: Loss 1.3983235359191895\n",
      "Training Batch [596/782]: Loss 1.3204079866409302\n",
      "Training Batch [597/782]: Loss 1.3361947536468506\n",
      "Training Batch [598/782]: Loss 1.519774317741394\n",
      "Training Batch [599/782]: Loss 1.655739426612854\n",
      "Training Batch [600/782]: Loss 1.4598948955535889\n",
      "Training Batch [601/782]: Loss 1.6719106435775757\n",
      "Training Batch [602/782]: Loss 1.3043992519378662\n",
      "Training Batch [603/782]: Loss 1.6321979761123657\n",
      "Training Batch [604/782]: Loss 1.2555146217346191\n",
      "Training Batch [605/782]: Loss 1.238854169845581\n",
      "Training Batch [606/782]: Loss 1.364996314048767\n",
      "Training Batch [607/782]: Loss 1.2296879291534424\n",
      "Training Batch [608/782]: Loss 1.4904862642288208\n",
      "Training Batch [609/782]: Loss 1.5187630653381348\n",
      "Training Batch [610/782]: Loss 1.2903010845184326\n",
      "Training Batch [611/782]: Loss 1.6323920488357544\n",
      "Training Batch [612/782]: Loss 1.4510091543197632\n",
      "Training Batch [613/782]: Loss 1.302763819694519\n",
      "Training Batch [614/782]: Loss 1.250617504119873\n",
      "Training Batch [615/782]: Loss 1.6681703329086304\n",
      "Training Batch [616/782]: Loss 1.3440630435943604\n",
      "Training Batch [617/782]: Loss 1.3186043500900269\n",
      "Training Batch [618/782]: Loss 1.551042914390564\n",
      "Training Batch [619/782]: Loss 1.6270848512649536\n",
      "Training Batch [620/782]: Loss 1.3135864734649658\n",
      "Training Batch [621/782]: Loss 1.4110350608825684\n",
      "Training Batch [622/782]: Loss 1.4544942378997803\n",
      "Training Batch [623/782]: Loss 1.5347161293029785\n",
      "Training Batch [624/782]: Loss 1.2872179746627808\n",
      "Training Batch [625/782]: Loss 1.308296799659729\n",
      "Training Batch [626/782]: Loss 1.64382004737854\n",
      "Training Batch [627/782]: Loss 1.246809959411621\n",
      "Training Batch [628/782]: Loss 1.4567416906356812\n",
      "Training Batch [629/782]: Loss 1.5801618099212646\n",
      "Training Batch [630/782]: Loss 1.472133994102478\n",
      "Training Batch [631/782]: Loss 1.377495527267456\n",
      "Training Batch [632/782]: Loss 1.4255988597869873\n",
      "Training Batch [633/782]: Loss 1.3073747158050537\n",
      "Training Batch [634/782]: Loss 1.495224952697754\n",
      "Training Batch [635/782]: Loss 1.649815559387207\n",
      "Training Batch [636/782]: Loss 1.2653613090515137\n",
      "Training Batch [637/782]: Loss 1.1219265460968018\n",
      "Training Batch [638/782]: Loss 1.2775578498840332\n",
      "Training Batch [639/782]: Loss 1.1795850992202759\n",
      "Training Batch [640/782]: Loss 1.3568516969680786\n",
      "Training Batch [641/782]: Loss 1.3113338947296143\n",
      "Training Batch [642/782]: Loss 1.5732083320617676\n",
      "Training Batch [643/782]: Loss 1.1705238819122314\n",
      "Training Batch [644/782]: Loss 1.4870175123214722\n",
      "Training Batch [645/782]: Loss 1.6525992155075073\n",
      "Training Batch [646/782]: Loss 1.4127287864685059\n",
      "Training Batch [647/782]: Loss 1.391591191291809\n",
      "Training Batch [648/782]: Loss 1.3945937156677246\n",
      "Training Batch [649/782]: Loss 1.3660223484039307\n",
      "Training Batch [650/782]: Loss 1.4422359466552734\n",
      "Training Batch [651/782]: Loss 1.445611596107483\n",
      "Training Batch [652/782]: Loss 1.3301584720611572\n",
      "Training Batch [653/782]: Loss 1.2735743522644043\n",
      "Training Batch [654/782]: Loss 1.6936652660369873\n",
      "Training Batch [655/782]: Loss 1.4622125625610352\n",
      "Training Batch [656/782]: Loss 1.3304102420806885\n",
      "Training Batch [657/782]: Loss 1.5415602922439575\n",
      "Training Batch [658/782]: Loss 1.3472521305084229\n",
      "Training Batch [659/782]: Loss 1.2708218097686768\n",
      "Training Batch [660/782]: Loss 1.7032912969589233\n",
      "Training Batch [661/782]: Loss 1.3890687227249146\n",
      "Training Batch [662/782]: Loss 1.1523702144622803\n",
      "Training Batch [663/782]: Loss 1.4998353719711304\n",
      "Training Batch [664/782]: Loss 1.4228450059890747\n",
      "Training Batch [665/782]: Loss 1.3964829444885254\n",
      "Training Batch [666/782]: Loss 1.4806538820266724\n",
      "Training Batch [667/782]: Loss 1.6111549139022827\n",
      "Training Batch [668/782]: Loss 1.429886817932129\n",
      "Training Batch [669/782]: Loss 1.6354918479919434\n",
      "Training Batch [670/782]: Loss 1.8668887615203857\n",
      "Training Batch [671/782]: Loss 1.425734281539917\n",
      "Training Batch [672/782]: Loss 1.6192131042480469\n",
      "Training Batch [673/782]: Loss 1.2967249155044556\n",
      "Training Batch [674/782]: Loss 1.5919252634048462\n",
      "Training Batch [675/782]: Loss 1.4598020315170288\n",
      "Training Batch [676/782]: Loss 1.5313186645507812\n",
      "Training Batch [677/782]: Loss 1.5422428846359253\n",
      "Training Batch [678/782]: Loss 1.1660505533218384\n",
      "Training Batch [679/782]: Loss 1.1831271648406982\n",
      "Training Batch [680/782]: Loss 1.4184644222259521\n",
      "Training Batch [681/782]: Loss 1.5745258331298828\n",
      "Training Batch [682/782]: Loss 1.22986900806427\n",
      "Training Batch [683/782]: Loss 1.2533926963806152\n",
      "Training Batch [684/782]: Loss 1.4881235361099243\n",
      "Training Batch [685/782]: Loss 1.3163284063339233\n",
      "Training Batch [686/782]: Loss 1.6251531839370728\n",
      "Training Batch [687/782]: Loss 1.3609585762023926\n",
      "Training Batch [688/782]: Loss 1.1829077005386353\n",
      "Training Batch [689/782]: Loss 1.429834246635437\n",
      "Training Batch [690/782]: Loss 1.6475579738616943\n",
      "Training Batch [691/782]: Loss 1.4078248739242554\n",
      "Training Batch [692/782]: Loss 1.490645170211792\n",
      "Training Batch [693/782]: Loss 1.3006854057312012\n",
      "Training Batch [694/782]: Loss 1.6656583547592163\n",
      "Training Batch [695/782]: Loss 1.2480508089065552\n",
      "Training Batch [696/782]: Loss 1.3102195262908936\n",
      "Training Batch [697/782]: Loss 1.3325695991516113\n",
      "Training Batch [698/782]: Loss 1.3795855045318604\n",
      "Training Batch [699/782]: Loss 1.2518885135650635\n",
      "Training Batch [700/782]: Loss 1.556511402130127\n",
      "Training Batch [701/782]: Loss 1.6116496324539185\n",
      "Training Batch [702/782]: Loss 1.6708863973617554\n",
      "Training Batch [703/782]: Loss 1.6864209175109863\n",
      "Training Batch [704/782]: Loss 1.3865470886230469\n",
      "Training Batch [705/782]: Loss 1.592743992805481\n",
      "Training Batch [706/782]: Loss 1.6609123945236206\n",
      "Training Batch [707/782]: Loss 1.322054386138916\n",
      "Training Batch [708/782]: Loss 1.3401967287063599\n",
      "Training Batch [709/782]: Loss 1.4751733541488647\n",
      "Training Batch [710/782]: Loss 1.559675931930542\n",
      "Training Batch [711/782]: Loss 1.66633141040802\n",
      "Training Batch [712/782]: Loss 1.1784768104553223\n",
      "Training Batch [713/782]: Loss 1.380129098892212\n",
      "Training Batch [714/782]: Loss 1.2667499780654907\n",
      "Training Batch [715/782]: Loss 1.3718469142913818\n",
      "Training Batch [716/782]: Loss 1.6120405197143555\n",
      "Training Batch [717/782]: Loss 1.4339154958724976\n",
      "Training Batch [718/782]: Loss 1.669998288154602\n",
      "Training Batch [719/782]: Loss 1.3406765460968018\n",
      "Training Batch [720/782]: Loss 1.607046365737915\n",
      "Training Batch [721/782]: Loss 1.4424397945404053\n",
      "Training Batch [722/782]: Loss 1.406167984008789\n",
      "Training Batch [723/782]: Loss 1.2593739032745361\n",
      "Training Batch [724/782]: Loss 1.3742759227752686\n",
      "Training Batch [725/782]: Loss 1.7947688102722168\n",
      "Training Batch [726/782]: Loss 1.4513237476348877\n",
      "Training Batch [727/782]: Loss 1.2341992855072021\n",
      "Training Batch [728/782]: Loss 1.483515977859497\n",
      "Training Batch [729/782]: Loss 1.2986955642700195\n",
      "Training Batch [730/782]: Loss 1.5384019613265991\n",
      "Training Batch [731/782]: Loss 1.4651141166687012\n",
      "Training Batch [732/782]: Loss 1.4861652851104736\n",
      "Training Batch [733/782]: Loss 1.4640172719955444\n",
      "Training Batch [734/782]: Loss 1.3173716068267822\n",
      "Training Batch [735/782]: Loss 1.4440383911132812\n",
      "Training Batch [736/782]: Loss 1.4007834196090698\n",
      "Training Batch [737/782]: Loss 1.375255823135376\n",
      "Training Batch [738/782]: Loss 1.5347808599472046\n",
      "Training Batch [739/782]: Loss 1.465125322341919\n",
      "Training Batch [740/782]: Loss 1.369799017906189\n",
      "Training Batch [741/782]: Loss 1.325379729270935\n",
      "Training Batch [742/782]: Loss 1.5859800577163696\n",
      "Training Batch [743/782]: Loss 1.6358944177627563\n",
      "Training Batch [744/782]: Loss 1.41128671169281\n",
      "Training Batch [745/782]: Loss 1.0757806301116943\n",
      "Training Batch [746/782]: Loss 1.3423398733139038\n",
      "Training Batch [747/782]: Loss 1.2941492795944214\n",
      "Training Batch [748/782]: Loss 1.755165696144104\n",
      "Training Batch [749/782]: Loss 1.4425723552703857\n",
      "Training Batch [750/782]: Loss 1.6078358888626099\n",
      "Training Batch [751/782]: Loss 1.2514153718948364\n",
      "Training Batch [752/782]: Loss 1.3661993741989136\n",
      "Training Batch [753/782]: Loss 1.439061164855957\n",
      "Training Batch [754/782]: Loss 1.1822271347045898\n",
      "Training Batch [755/782]: Loss 1.436503291130066\n",
      "Training Batch [756/782]: Loss 1.3785004615783691\n",
      "Training Batch [757/782]: Loss 1.318403959274292\n",
      "Training Batch [758/782]: Loss 1.4494527578353882\n",
      "Training Batch [759/782]: Loss 1.3952831029891968\n",
      "Training Batch [760/782]: Loss 1.4717061519622803\n",
      "Training Batch [761/782]: Loss 1.4939265251159668\n",
      "Training Batch [762/782]: Loss 1.2762820720672607\n",
      "Training Batch [763/782]: Loss 1.2903274297714233\n",
      "Training Batch [764/782]: Loss 1.2156416177749634\n",
      "Training Batch [765/782]: Loss 1.2865900993347168\n",
      "Training Batch [766/782]: Loss 1.4612712860107422\n",
      "Training Batch [767/782]: Loss 1.6088684797286987\n",
      "Training Batch [768/782]: Loss 1.4845905303955078\n",
      "Training Batch [769/782]: Loss 1.6728370189666748\n",
      "Training Batch [770/782]: Loss 1.1705892086029053\n",
      "Training Batch [771/782]: Loss 1.241070032119751\n",
      "Training Batch [772/782]: Loss 1.5881388187408447\n",
      "Training Batch [773/782]: Loss 1.430767297744751\n",
      "Training Batch [774/782]: Loss 1.572072982788086\n",
      "Training Batch [775/782]: Loss 1.243618130683899\n",
      "Training Batch [776/782]: Loss 1.64552640914917\n",
      "Training Batch [777/782]: Loss 1.5123873949050903\n",
      "Training Batch [778/782]: Loss 1.3799688816070557\n",
      "Training Batch [779/782]: Loss 1.3148796558380127\n",
      "Training Batch [780/782]: Loss 1.6351029872894287\n",
      "Training Batch [781/782]: Loss 1.5566521883010864\n",
      "Training Batch [782/782]: Loss 1.0443395376205444\n",
      "Epoch 2 - Train Loss: 1.4739\n",
      "*********  Epoch 3/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 1.3507921695709229\n",
      "Training Batch [2/782]: Loss 1.476370930671692\n",
      "Training Batch [3/782]: Loss 1.3606863021850586\n",
      "Training Batch [4/782]: Loss 1.4710832834243774\n",
      "Training Batch [5/782]: Loss 1.3527103662490845\n",
      "Training Batch [6/782]: Loss 1.3261009454727173\n",
      "Training Batch [7/782]: Loss 1.312835454940796\n",
      "Training Batch [8/782]: Loss 1.3032159805297852\n",
      "Training Batch [9/782]: Loss 1.3501642942428589\n",
      "Training Batch [10/782]: Loss 1.3428913354873657\n",
      "Training Batch [11/782]: Loss 1.1251834630966187\n",
      "Training Batch [12/782]: Loss 1.1766877174377441\n",
      "Training Batch [13/782]: Loss 1.5204981565475464\n",
      "Training Batch [14/782]: Loss 1.1831659078598022\n",
      "Training Batch [15/782]: Loss 1.2182105779647827\n",
      "Training Batch [16/782]: Loss 1.1544897556304932\n",
      "Training Batch [17/782]: Loss 1.3017206192016602\n",
      "Training Batch [18/782]: Loss 1.165776014328003\n",
      "Training Batch [19/782]: Loss 1.3892936706542969\n",
      "Training Batch [20/782]: Loss 1.3149158954620361\n",
      "Training Batch [21/782]: Loss 1.5675139427185059\n",
      "Training Batch [22/782]: Loss 1.3922125101089478\n",
      "Training Batch [23/782]: Loss 1.2692949771881104\n",
      "Training Batch [24/782]: Loss 1.5072273015975952\n",
      "Training Batch [25/782]: Loss 1.1748273372650146\n",
      "Training Batch [26/782]: Loss 1.167710542678833\n",
      "Training Batch [27/782]: Loss 1.1437944173812866\n",
      "Training Batch [28/782]: Loss 1.2321550846099854\n",
      "Training Batch [29/782]: Loss 1.5274696350097656\n",
      "Training Batch [30/782]: Loss 1.2797068357467651\n",
      "Training Batch [31/782]: Loss 1.2069265842437744\n",
      "Training Batch [32/782]: Loss 1.2882513999938965\n",
      "Training Batch [33/782]: Loss 1.0991171598434448\n",
      "Training Batch [34/782]: Loss 1.4767531156539917\n",
      "Training Batch [35/782]: Loss 1.16226065158844\n",
      "Training Batch [36/782]: Loss 1.3238542079925537\n",
      "Training Batch [37/782]: Loss 1.2158210277557373\n",
      "Training Batch [38/782]: Loss 1.2230027914047241\n",
      "Training Batch [39/782]: Loss 1.1510580778121948\n",
      "Training Batch [40/782]: Loss 1.393867015838623\n",
      "Training Batch [41/782]: Loss 1.2494659423828125\n",
      "Training Batch [42/782]: Loss 1.3184846639633179\n",
      "Training Batch [43/782]: Loss 1.2791595458984375\n",
      "Training Batch [44/782]: Loss 1.0876058340072632\n",
      "Training Batch [45/782]: Loss 1.3968392610549927\n",
      "Training Batch [46/782]: Loss 1.070601463317871\n",
      "Training Batch [47/782]: Loss 1.1967419385910034\n",
      "Training Batch [48/782]: Loss 1.0538965463638306\n",
      "Training Batch [49/782]: Loss 1.3806630373001099\n",
      "Training Batch [50/782]: Loss 1.4266698360443115\n",
      "Training Batch [51/782]: Loss 1.2899062633514404\n",
      "Training Batch [52/782]: Loss 1.4391030073165894\n",
      "Training Batch [53/782]: Loss 1.5825164318084717\n",
      "Training Batch [54/782]: Loss 1.260189175605774\n",
      "Training Batch [55/782]: Loss 1.2921699285507202\n",
      "Training Batch [56/782]: Loss 1.1930840015411377\n",
      "Training Batch [57/782]: Loss 1.1632500886917114\n",
      "Training Batch [58/782]: Loss 1.50674569606781\n",
      "Training Batch [59/782]: Loss 1.1849493980407715\n",
      "Training Batch [60/782]: Loss 1.471517562866211\n",
      "Training Batch [61/782]: Loss 1.3788398504257202\n",
      "Training Batch [62/782]: Loss 0.9825479984283447\n",
      "Training Batch [63/782]: Loss 1.2673027515411377\n",
      "Training Batch [64/782]: Loss 1.1351916790008545\n",
      "Training Batch [65/782]: Loss 1.157152533531189\n",
      "Training Batch [66/782]: Loss 1.2889726161956787\n",
      "Training Batch [67/782]: Loss 1.3263882398605347\n",
      "Training Batch [68/782]: Loss 1.4309406280517578\n",
      "Training Batch [69/782]: Loss 1.1289520263671875\n",
      "Training Batch [70/782]: Loss 1.0262396335601807\n",
      "Training Batch [71/782]: Loss 1.1158223152160645\n",
      "Training Batch [72/782]: Loss 1.1766748428344727\n",
      "Training Batch [73/782]: Loss 1.236537218093872\n",
      "Training Batch [74/782]: Loss 1.127550721168518\n",
      "Training Batch [75/782]: Loss 1.178130865097046\n",
      "Training Batch [76/782]: Loss 1.189263939857483\n",
      "Training Batch [77/782]: Loss 1.4323153495788574\n",
      "Training Batch [78/782]: Loss 1.3322992324829102\n",
      "Training Batch [79/782]: Loss 1.358381986618042\n",
      "Training Batch [80/782]: Loss 1.2821472883224487\n",
      "Training Batch [81/782]: Loss 1.1944769620895386\n",
      "Training Batch [82/782]: Loss 1.3631950616836548\n",
      "Training Batch [83/782]: Loss 0.9851981401443481\n",
      "Training Batch [84/782]: Loss 1.14091157913208\n",
      "Training Batch [85/782]: Loss 1.1299431324005127\n",
      "Training Batch [86/782]: Loss 1.100998878479004\n",
      "Training Batch [87/782]: Loss 1.2266041040420532\n",
      "Training Batch [88/782]: Loss 1.2108818292617798\n",
      "Training Batch [89/782]: Loss 1.2818584442138672\n",
      "Training Batch [90/782]: Loss 1.3651330471038818\n",
      "Training Batch [91/782]: Loss 1.238992691040039\n",
      "Training Batch [92/782]: Loss 1.3372653722763062\n",
      "Training Batch [93/782]: Loss 1.039042592048645\n",
      "Training Batch [94/782]: Loss 1.1281006336212158\n",
      "Training Batch [95/782]: Loss 1.484328269958496\n",
      "Training Batch [96/782]: Loss 1.4978070259094238\n",
      "Training Batch [97/782]: Loss 1.4071412086486816\n",
      "Training Batch [98/782]: Loss 1.2057440280914307\n",
      "Training Batch [99/782]: Loss 1.272209644317627\n",
      "Training Batch [100/782]: Loss 1.3144116401672363\n",
      "Training Batch [101/782]: Loss 1.358414888381958\n",
      "Training Batch [102/782]: Loss 1.0349156856536865\n",
      "Training Batch [103/782]: Loss 1.3467957973480225\n",
      "Training Batch [104/782]: Loss 1.2729145288467407\n",
      "Training Batch [105/782]: Loss 1.1184062957763672\n",
      "Training Batch [106/782]: Loss 1.1788049936294556\n",
      "Training Batch [107/782]: Loss 1.3031927347183228\n",
      "Training Batch [108/782]: Loss 1.3316317796707153\n",
      "Training Batch [109/782]: Loss 1.1380014419555664\n",
      "Training Batch [110/782]: Loss 1.2493826150894165\n",
      "Training Batch [111/782]: Loss 1.2792844772338867\n",
      "Training Batch [112/782]: Loss 1.5945929288864136\n",
      "Training Batch [113/782]: Loss 1.275593876838684\n",
      "Training Batch [114/782]: Loss 1.1163580417633057\n",
      "Training Batch [115/782]: Loss 1.270790934562683\n",
      "Training Batch [116/782]: Loss 1.1916111707687378\n",
      "Training Batch [117/782]: Loss 1.073498249053955\n",
      "Training Batch [118/782]: Loss 1.1833285093307495\n",
      "Training Batch [119/782]: Loss 1.2430219650268555\n",
      "Training Batch [120/782]: Loss 1.267952799797058\n",
      "Training Batch [121/782]: Loss 1.1536054611206055\n",
      "Training Batch [122/782]: Loss 1.3699219226837158\n",
      "Training Batch [123/782]: Loss 1.6055445671081543\n",
      "Training Batch [124/782]: Loss 1.204683780670166\n",
      "Training Batch [125/782]: Loss 1.4773591756820679\n",
      "Training Batch [126/782]: Loss 1.2583290338516235\n",
      "Training Batch [127/782]: Loss 1.4072637557983398\n",
      "Training Batch [128/782]: Loss 1.4127953052520752\n",
      "Training Batch [129/782]: Loss 1.314881682395935\n",
      "Training Batch [130/782]: Loss 1.0863548517227173\n",
      "Training Batch [131/782]: Loss 1.0428580045700073\n",
      "Training Batch [132/782]: Loss 1.320623517036438\n",
      "Training Batch [133/782]: Loss 1.4730576276779175\n",
      "Training Batch [134/782]: Loss 1.369540810585022\n",
      "Training Batch [135/782]: Loss 1.2681001424789429\n",
      "Training Batch [136/782]: Loss 1.3187346458435059\n",
      "Training Batch [137/782]: Loss 1.1806484460830688\n",
      "Training Batch [138/782]: Loss 1.2694885730743408\n",
      "Training Batch [139/782]: Loss 1.3061590194702148\n",
      "Training Batch [140/782]: Loss 1.268141508102417\n",
      "Training Batch [141/782]: Loss 1.2968144416809082\n",
      "Training Batch [142/782]: Loss 1.3330920934677124\n",
      "Training Batch [143/782]: Loss 1.2578197717666626\n",
      "Training Batch [144/782]: Loss 1.4065872430801392\n",
      "Training Batch [145/782]: Loss 0.9054131507873535\n",
      "Training Batch [146/782]: Loss 1.355649709701538\n",
      "Training Batch [147/782]: Loss 1.3849855661392212\n",
      "Training Batch [148/782]: Loss 1.267227292060852\n",
      "Training Batch [149/782]: Loss 1.2881357669830322\n",
      "Training Batch [150/782]: Loss 1.2581831216812134\n",
      "Training Batch [151/782]: Loss 1.1747920513153076\n",
      "Training Batch [152/782]: Loss 1.0833382606506348\n",
      "Training Batch [153/782]: Loss 1.4769078493118286\n",
      "Training Batch [154/782]: Loss 1.3171627521514893\n",
      "Training Batch [155/782]: Loss 1.2584973573684692\n",
      "Training Batch [156/782]: Loss 1.1293951272964478\n",
      "Training Batch [157/782]: Loss 1.1358447074890137\n",
      "Training Batch [158/782]: Loss 1.1558842658996582\n",
      "Training Batch [159/782]: Loss 1.4780938625335693\n",
      "Training Batch [160/782]: Loss 1.4134900569915771\n",
      "Training Batch [161/782]: Loss 1.1477874517440796\n",
      "Training Batch [162/782]: Loss 1.4161373376846313\n",
      "Training Batch [163/782]: Loss 1.5467183589935303\n",
      "Training Batch [164/782]: Loss 1.2123990058898926\n",
      "Training Batch [165/782]: Loss 1.3149745464324951\n",
      "Training Batch [166/782]: Loss 1.4247535467147827\n",
      "Training Batch [167/782]: Loss 1.3282941579818726\n",
      "Training Batch [168/782]: Loss 1.5653587579727173\n",
      "Training Batch [169/782]: Loss 1.3255342245101929\n",
      "Training Batch [170/782]: Loss 1.2938786745071411\n",
      "Training Batch [171/782]: Loss 1.4259587526321411\n",
      "Training Batch [172/782]: Loss 1.2401615381240845\n",
      "Training Batch [173/782]: Loss 1.34127676486969\n",
      "Training Batch [174/782]: Loss 1.163280725479126\n",
      "Training Batch [175/782]: Loss 1.1971018314361572\n",
      "Training Batch [176/782]: Loss 1.2386621236801147\n",
      "Training Batch [177/782]: Loss 1.322831392288208\n",
      "Training Batch [178/782]: Loss 1.3186596632003784\n",
      "Training Batch [179/782]: Loss 1.2070279121398926\n",
      "Training Batch [180/782]: Loss 1.3210539817810059\n",
      "Training Batch [181/782]: Loss 1.203955888748169\n",
      "Training Batch [182/782]: Loss 1.2108761072158813\n",
      "Training Batch [183/782]: Loss 1.3689513206481934\n",
      "Training Batch [184/782]: Loss 1.2700814008712769\n",
      "Training Batch [185/782]: Loss 1.0715627670288086\n",
      "Training Batch [186/782]: Loss 1.2258721590042114\n",
      "Training Batch [187/782]: Loss 1.0166360139846802\n",
      "Training Batch [188/782]: Loss 1.2629411220550537\n",
      "Training Batch [189/782]: Loss 1.3433070182800293\n",
      "Training Batch [190/782]: Loss 1.1741993427276611\n",
      "Training Batch [191/782]: Loss 1.2629239559173584\n",
      "Training Batch [192/782]: Loss 1.3456900119781494\n",
      "Training Batch [193/782]: Loss 1.0733360052108765\n",
      "Training Batch [194/782]: Loss 1.2098073959350586\n",
      "Training Batch [195/782]: Loss 1.3734617233276367\n",
      "Training Batch [196/782]: Loss 1.0853530168533325\n",
      "Training Batch [197/782]: Loss 1.1947976350784302\n",
      "Training Batch [198/782]: Loss 1.1032171249389648\n",
      "Training Batch [199/782]: Loss 1.0946040153503418\n",
      "Training Batch [200/782]: Loss 1.2746803760528564\n",
      "Training Batch [201/782]: Loss 1.259508728981018\n",
      "Training Batch [202/782]: Loss 1.3329282999038696\n",
      "Training Batch [203/782]: Loss 1.2597506046295166\n",
      "Training Batch [204/782]: Loss 1.0332647562026978\n",
      "Training Batch [205/782]: Loss 1.1694564819335938\n",
      "Training Batch [206/782]: Loss 1.3489454984664917\n",
      "Training Batch [207/782]: Loss 1.3827769756317139\n",
      "Training Batch [208/782]: Loss 1.2802537679672241\n",
      "Training Batch [209/782]: Loss 1.1855266094207764\n",
      "Training Batch [210/782]: Loss 1.4030792713165283\n",
      "Training Batch [211/782]: Loss 1.4853535890579224\n",
      "Training Batch [212/782]: Loss 1.5492194890975952\n",
      "Training Batch [213/782]: Loss 1.4420101642608643\n",
      "Training Batch [214/782]: Loss 1.275893211364746\n",
      "Training Batch [215/782]: Loss 1.178731083869934\n",
      "Training Batch [216/782]: Loss 1.2273890972137451\n",
      "Training Batch [217/782]: Loss 1.2442508935928345\n",
      "Training Batch [218/782]: Loss 1.3848273754119873\n",
      "Training Batch [219/782]: Loss 1.346882939338684\n",
      "Training Batch [220/782]: Loss 1.0403721332550049\n",
      "Training Batch [221/782]: Loss 1.3682420253753662\n",
      "Training Batch [222/782]: Loss 1.2756586074829102\n",
      "Training Batch [223/782]: Loss 0.9803673624992371\n",
      "Training Batch [224/782]: Loss 1.3900715112686157\n",
      "Training Batch [225/782]: Loss 1.0227854251861572\n",
      "Training Batch [226/782]: Loss 1.185590386390686\n",
      "Training Batch [227/782]: Loss 1.3205459117889404\n",
      "Training Batch [228/782]: Loss 1.3828927278518677\n",
      "Training Batch [229/782]: Loss 1.2621450424194336\n",
      "Training Batch [230/782]: Loss 1.5326733589172363\n",
      "Training Batch [231/782]: Loss 1.7691774368286133\n",
      "Training Batch [232/782]: Loss 1.6035116910934448\n",
      "Training Batch [233/782]: Loss 1.2567055225372314\n",
      "Training Batch [234/782]: Loss 1.1650376319885254\n",
      "Training Batch [235/782]: Loss 1.2436704635620117\n",
      "Training Batch [236/782]: Loss 1.1366556882858276\n",
      "Training Batch [237/782]: Loss 1.2030495405197144\n",
      "Training Batch [238/782]: Loss 1.440779209136963\n",
      "Training Batch [239/782]: Loss 1.439575433731079\n",
      "Training Batch [240/782]: Loss 1.31314218044281\n",
      "Training Batch [241/782]: Loss 1.0972644090652466\n",
      "Training Batch [242/782]: Loss 1.3275773525238037\n",
      "Training Batch [243/782]: Loss 1.4823821783065796\n",
      "Training Batch [244/782]: Loss 1.501035213470459\n",
      "Training Batch [245/782]: Loss 1.1182851791381836\n",
      "Training Batch [246/782]: Loss 1.2702893018722534\n",
      "Training Batch [247/782]: Loss 1.286476969718933\n",
      "Training Batch [248/782]: Loss 1.4354383945465088\n",
      "Training Batch [249/782]: Loss 1.2866908311843872\n",
      "Training Batch [250/782]: Loss 1.313417911529541\n",
      "Training Batch [251/782]: Loss 1.3461543321609497\n",
      "Training Batch [252/782]: Loss 1.103861689567566\n",
      "Training Batch [253/782]: Loss 1.4412678480148315\n",
      "Training Batch [254/782]: Loss 1.0619338750839233\n",
      "Training Batch [255/782]: Loss 1.3348503112792969\n",
      "Training Batch [256/782]: Loss 1.4499197006225586\n",
      "Training Batch [257/782]: Loss 1.0424426794052124\n",
      "Training Batch [258/782]: Loss 1.3587415218353271\n",
      "Training Batch [259/782]: Loss 1.1714855432510376\n",
      "Training Batch [260/782]: Loss 1.4709525108337402\n",
      "Training Batch [261/782]: Loss 1.3196128606796265\n",
      "Training Batch [262/782]: Loss 1.1885515451431274\n",
      "Training Batch [263/782]: Loss 1.3120752573013306\n",
      "Training Batch [264/782]: Loss 1.3539190292358398\n",
      "Training Batch [265/782]: Loss 1.3164401054382324\n",
      "Training Batch [266/782]: Loss 1.5105950832366943\n",
      "Training Batch [267/782]: Loss 1.571256399154663\n",
      "Training Batch [268/782]: Loss 1.2551296949386597\n",
      "Training Batch [269/782]: Loss 1.0651487112045288\n",
      "Training Batch [270/782]: Loss 1.3830009698867798\n",
      "Training Batch [271/782]: Loss 1.2609553337097168\n",
      "Training Batch [272/782]: Loss 1.2325252294540405\n",
      "Training Batch [273/782]: Loss 1.240828037261963\n",
      "Training Batch [274/782]: Loss 1.1345889568328857\n",
      "Training Batch [275/782]: Loss 1.342860221862793\n",
      "Training Batch [276/782]: Loss 1.1817231178283691\n",
      "Training Batch [277/782]: Loss 1.1586790084838867\n",
      "Training Batch [278/782]: Loss 1.4511213302612305\n",
      "Training Batch [279/782]: Loss 1.2836487293243408\n",
      "Training Batch [280/782]: Loss 1.1862784624099731\n",
      "Training Batch [281/782]: Loss 1.4496591091156006\n",
      "Training Batch [282/782]: Loss 1.1859562397003174\n",
      "Training Batch [283/782]: Loss 1.368145227432251\n",
      "Training Batch [284/782]: Loss 1.3227829933166504\n",
      "Training Batch [285/782]: Loss 1.2741923332214355\n",
      "Training Batch [286/782]: Loss 1.4475741386413574\n",
      "Training Batch [287/782]: Loss 1.2740317583084106\n",
      "Training Batch [288/782]: Loss 1.508324384689331\n",
      "Training Batch [289/782]: Loss 1.4298205375671387\n",
      "Training Batch [290/782]: Loss 1.494724154472351\n",
      "Training Batch [291/782]: Loss 1.2242971658706665\n",
      "Training Batch [292/782]: Loss 1.4830876588821411\n",
      "Training Batch [293/782]: Loss 1.3337221145629883\n",
      "Training Batch [294/782]: Loss 1.4255245923995972\n",
      "Training Batch [295/782]: Loss 1.203188419342041\n",
      "Training Batch [296/782]: Loss 1.19381582736969\n",
      "Training Batch [297/782]: Loss 1.140376091003418\n",
      "Training Batch [298/782]: Loss 1.617985725402832\n",
      "Training Batch [299/782]: Loss 1.4676448106765747\n",
      "Training Batch [300/782]: Loss 1.3431527614593506\n",
      "Training Batch [301/782]: Loss 1.3180276155471802\n",
      "Training Batch [302/782]: Loss 1.2389543056488037\n",
      "Training Batch [303/782]: Loss 1.3563117980957031\n",
      "Training Batch [304/782]: Loss 1.3140958547592163\n",
      "Training Batch [305/782]: Loss 1.3351171016693115\n",
      "Training Batch [306/782]: Loss 1.2819900512695312\n",
      "Training Batch [307/782]: Loss 1.2120327949523926\n",
      "Training Batch [308/782]: Loss 1.2231322526931763\n",
      "Training Batch [309/782]: Loss 1.1778664588928223\n",
      "Training Batch [310/782]: Loss 1.2505223751068115\n",
      "Training Batch [311/782]: Loss 1.1901984214782715\n",
      "Training Batch [312/782]: Loss 1.237427830696106\n",
      "Training Batch [313/782]: Loss 1.2746468782424927\n",
      "Training Batch [314/782]: Loss 1.2562329769134521\n",
      "Training Batch [315/782]: Loss 1.1560908555984497\n",
      "Training Batch [316/782]: Loss 1.557483196258545\n",
      "Training Batch [317/782]: Loss 0.9643839597702026\n",
      "Training Batch [318/782]: Loss 1.3877664804458618\n",
      "Training Batch [319/782]: Loss 1.5085537433624268\n",
      "Training Batch [320/782]: Loss 1.3025864362716675\n",
      "Training Batch [321/782]: Loss 1.411270022392273\n",
      "Training Batch [322/782]: Loss 1.2906607389450073\n",
      "Training Batch [323/782]: Loss 1.463534951210022\n",
      "Training Batch [324/782]: Loss 1.2349023818969727\n",
      "Training Batch [325/782]: Loss 1.2618204355239868\n",
      "Training Batch [326/782]: Loss 0.9308457374572754\n",
      "Training Batch [327/782]: Loss 1.2409634590148926\n",
      "Training Batch [328/782]: Loss 1.1801350116729736\n",
      "Training Batch [329/782]: Loss 1.170799732208252\n",
      "Training Batch [330/782]: Loss 1.072607398033142\n",
      "Training Batch [331/782]: Loss 1.3766276836395264\n",
      "Training Batch [332/782]: Loss 1.2131224870681763\n",
      "Training Batch [333/782]: Loss 1.351365089416504\n",
      "Training Batch [334/782]: Loss 1.0087453126907349\n",
      "Training Batch [335/782]: Loss 1.0644571781158447\n",
      "Training Batch [336/782]: Loss 1.3438671827316284\n",
      "Training Batch [337/782]: Loss 0.9089540243148804\n",
      "Training Batch [338/782]: Loss 1.3609113693237305\n",
      "Training Batch [339/782]: Loss 1.3044135570526123\n",
      "Training Batch [340/782]: Loss 1.4780917167663574\n",
      "Training Batch [341/782]: Loss 1.481580138206482\n",
      "Training Batch [342/782]: Loss 1.2355053424835205\n",
      "Training Batch [343/782]: Loss 1.0338128805160522\n",
      "Training Batch [344/782]: Loss 1.4285640716552734\n",
      "Training Batch [345/782]: Loss 1.2689493894577026\n",
      "Training Batch [346/782]: Loss 1.3824576139450073\n",
      "Training Batch [347/782]: Loss 1.1190820932388306\n",
      "Training Batch [348/782]: Loss 1.1940432786941528\n",
      "Training Batch [349/782]: Loss 1.2846722602844238\n",
      "Training Batch [350/782]: Loss 1.2450597286224365\n",
      "Training Batch [351/782]: Loss 1.106275200843811\n",
      "Training Batch [352/782]: Loss 1.0292179584503174\n",
      "Training Batch [353/782]: Loss 1.191247582435608\n",
      "Training Batch [354/782]: Loss 1.2403111457824707\n",
      "Training Batch [355/782]: Loss 1.2703810930252075\n",
      "Training Batch [356/782]: Loss 1.260955810546875\n",
      "Training Batch [357/782]: Loss 1.1157641410827637\n",
      "Training Batch [358/782]: Loss 1.221367597579956\n",
      "Training Batch [359/782]: Loss 1.4142855405807495\n",
      "Training Batch [360/782]: Loss 1.3443632125854492\n",
      "Training Batch [361/782]: Loss 1.263777494430542\n",
      "Training Batch [362/782]: Loss 1.161621332168579\n",
      "Training Batch [363/782]: Loss 1.1433409452438354\n",
      "Training Batch [364/782]: Loss 1.3990169763565063\n",
      "Training Batch [365/782]: Loss 1.184468150138855\n",
      "Training Batch [366/782]: Loss 0.956053614616394\n",
      "Training Batch [367/782]: Loss 1.3095662593841553\n",
      "Training Batch [368/782]: Loss 1.2867006063461304\n",
      "Training Batch [369/782]: Loss 1.3584189414978027\n",
      "Training Batch [370/782]: Loss 1.0987975597381592\n",
      "Training Batch [371/782]: Loss 1.4646797180175781\n",
      "Training Batch [372/782]: Loss 1.0482841730117798\n",
      "Training Batch [373/782]: Loss 1.3166768550872803\n",
      "Training Batch [374/782]: Loss 1.3335527181625366\n",
      "Training Batch [375/782]: Loss 1.188744068145752\n",
      "Training Batch [376/782]: Loss 1.4067540168762207\n",
      "Training Batch [377/782]: Loss 1.2700858116149902\n",
      "Training Batch [378/782]: Loss 1.088936686515808\n",
      "Training Batch [379/782]: Loss 1.015708088874817\n",
      "Training Batch [380/782]: Loss 1.1901681423187256\n",
      "Training Batch [381/782]: Loss 1.0708012580871582\n",
      "Training Batch [382/782]: Loss 1.2100729942321777\n",
      "Training Batch [383/782]: Loss 0.8902550935745239\n",
      "Training Batch [384/782]: Loss 1.1226799488067627\n",
      "Training Batch [385/782]: Loss 1.4440523386001587\n",
      "Training Batch [386/782]: Loss 1.3263705968856812\n",
      "Training Batch [387/782]: Loss 1.2244901657104492\n",
      "Training Batch [388/782]: Loss 1.196869134902954\n",
      "Training Batch [389/782]: Loss 1.1519006490707397\n",
      "Training Batch [390/782]: Loss 1.2133910655975342\n",
      "Training Batch [391/782]: Loss 1.1112209558486938\n",
      "Training Batch [392/782]: Loss 1.1816158294677734\n",
      "Training Batch [393/782]: Loss 0.951275646686554\n",
      "Training Batch [394/782]: Loss 1.3123122453689575\n",
      "Training Batch [395/782]: Loss 1.4892141819000244\n",
      "Training Batch [396/782]: Loss 1.263983964920044\n",
      "Training Batch [397/782]: Loss 1.353272557258606\n",
      "Training Batch [398/782]: Loss 1.2940011024475098\n",
      "Training Batch [399/782]: Loss 1.3222519159317017\n",
      "Training Batch [400/782]: Loss 1.1430243253707886\n",
      "Training Batch [401/782]: Loss 1.325790524482727\n",
      "Training Batch [402/782]: Loss 1.5393598079681396\n",
      "Training Batch [403/782]: Loss 1.299911618232727\n",
      "Training Batch [404/782]: Loss 1.2973872423171997\n",
      "Training Batch [405/782]: Loss 1.318202257156372\n",
      "Training Batch [406/782]: Loss 1.096471905708313\n",
      "Training Batch [407/782]: Loss 1.126114845275879\n",
      "Training Batch [408/782]: Loss 1.1046637296676636\n",
      "Training Batch [409/782]: Loss 1.1416946649551392\n",
      "Training Batch [410/782]: Loss 1.5144882202148438\n",
      "Training Batch [411/782]: Loss 1.521298885345459\n",
      "Training Batch [412/782]: Loss 1.3978859186172485\n",
      "Training Batch [413/782]: Loss 1.2707188129425049\n",
      "Training Batch [414/782]: Loss 1.3894730806350708\n",
      "Training Batch [415/782]: Loss 1.318522572517395\n",
      "Training Batch [416/782]: Loss 1.577846884727478\n",
      "Training Batch [417/782]: Loss 1.3680000305175781\n",
      "Training Batch [418/782]: Loss 1.501172423362732\n",
      "Training Batch [419/782]: Loss 1.3777105808258057\n",
      "Training Batch [420/782]: Loss 1.381401538848877\n",
      "Training Batch [421/782]: Loss 1.4529662132263184\n",
      "Training Batch [422/782]: Loss 1.3639004230499268\n",
      "Training Batch [423/782]: Loss 1.2977572679519653\n",
      "Training Batch [424/782]: Loss 1.217379093170166\n",
      "Training Batch [425/782]: Loss 1.1546663045883179\n",
      "Training Batch [426/782]: Loss 1.2744946479797363\n",
      "Training Batch [427/782]: Loss 1.2530962228775024\n",
      "Training Batch [428/782]: Loss 1.1032887697219849\n",
      "Training Batch [429/782]: Loss 1.2120362520217896\n",
      "Training Batch [430/782]: Loss 1.1819506883621216\n",
      "Training Batch [431/782]: Loss 1.0511698722839355\n",
      "Training Batch [432/782]: Loss 1.3396977186203003\n",
      "Training Batch [433/782]: Loss 1.506681203842163\n",
      "Training Batch [434/782]: Loss 1.1054192781448364\n",
      "Training Batch [435/782]: Loss 1.2686632871627808\n",
      "Training Batch [436/782]: Loss 1.0815008878707886\n",
      "Training Batch [437/782]: Loss 0.9653140306472778\n",
      "Training Batch [438/782]: Loss 1.1833893060684204\n",
      "Training Batch [439/782]: Loss 1.313567042350769\n",
      "Training Batch [440/782]: Loss 1.45143723487854\n",
      "Training Batch [441/782]: Loss 1.5459449291229248\n",
      "Training Batch [442/782]: Loss 1.1139981746673584\n",
      "Training Batch [443/782]: Loss 1.1962125301361084\n",
      "Training Batch [444/782]: Loss 1.2470059394836426\n",
      "Training Batch [445/782]: Loss 1.817308783531189\n",
      "Training Batch [446/782]: Loss 1.4868394136428833\n",
      "Training Batch [447/782]: Loss 1.6817840337753296\n",
      "Training Batch [448/782]: Loss 1.2997570037841797\n",
      "Training Batch [449/782]: Loss 1.0695923566818237\n",
      "Training Batch [450/782]: Loss 1.3060998916625977\n",
      "Training Batch [451/782]: Loss 1.339281678199768\n",
      "Training Batch [452/782]: Loss 1.3662828207015991\n",
      "Training Batch [453/782]: Loss 1.6056609153747559\n",
      "Training Batch [454/782]: Loss 1.2092939615249634\n",
      "Training Batch [455/782]: Loss 1.0326480865478516\n",
      "Training Batch [456/782]: Loss 1.3076976537704468\n",
      "Training Batch [457/782]: Loss 1.339799165725708\n",
      "Training Batch [458/782]: Loss 1.2331581115722656\n",
      "Training Batch [459/782]: Loss 1.129220724105835\n",
      "Training Batch [460/782]: Loss 1.2528241872787476\n",
      "Training Batch [461/782]: Loss 1.0475772619247437\n",
      "Training Batch [462/782]: Loss 1.3305093050003052\n",
      "Training Batch [463/782]: Loss 1.2158162593841553\n",
      "Training Batch [464/782]: Loss 1.1235320568084717\n",
      "Training Batch [465/782]: Loss 1.0883393287658691\n",
      "Training Batch [466/782]: Loss 1.497222661972046\n",
      "Training Batch [467/782]: Loss 1.0624260902404785\n",
      "Training Batch [468/782]: Loss 1.4468318223953247\n",
      "Training Batch [469/782]: Loss 1.4485301971435547\n",
      "Training Batch [470/782]: Loss 1.2270910739898682\n",
      "Training Batch [471/782]: Loss 1.3995325565338135\n",
      "Training Batch [472/782]: Loss 1.1719529628753662\n",
      "Training Batch [473/782]: Loss 1.2550532817840576\n",
      "Training Batch [474/782]: Loss 1.1000792980194092\n",
      "Training Batch [475/782]: Loss 1.295759677886963\n",
      "Training Batch [476/782]: Loss 1.4887844324111938\n",
      "Training Batch [477/782]: Loss 1.2621965408325195\n",
      "Training Batch [478/782]: Loss 1.523978352546692\n",
      "Training Batch [479/782]: Loss 1.365586280822754\n",
      "Training Batch [480/782]: Loss 1.2655959129333496\n",
      "Training Batch [481/782]: Loss 1.465429663658142\n",
      "Training Batch [482/782]: Loss 1.0253064632415771\n",
      "Training Batch [483/782]: Loss 1.0575366020202637\n",
      "Training Batch [484/782]: Loss 1.3113665580749512\n",
      "Training Batch [485/782]: Loss 1.4232659339904785\n",
      "Training Batch [486/782]: Loss 1.0791150331497192\n",
      "Training Batch [487/782]: Loss 1.6315994262695312\n",
      "Training Batch [488/782]: Loss 1.2810441255569458\n",
      "Training Batch [489/782]: Loss 1.560524582862854\n",
      "Training Batch [490/782]: Loss 1.141906499862671\n",
      "Training Batch [491/782]: Loss 1.1696933507919312\n",
      "Training Batch [492/782]: Loss 1.4482126235961914\n",
      "Training Batch [493/782]: Loss 1.2640960216522217\n",
      "Training Batch [494/782]: Loss 1.2444658279418945\n",
      "Training Batch [495/782]: Loss 1.2958718538284302\n",
      "Training Batch [496/782]: Loss 1.2971503734588623\n",
      "Training Batch [497/782]: Loss 1.221743106842041\n",
      "Training Batch [498/782]: Loss 1.3246240615844727\n",
      "Training Batch [499/782]: Loss 1.2413043975830078\n",
      "Training Batch [500/782]: Loss 1.5859565734863281\n",
      "Training Batch [501/782]: Loss 1.224483609199524\n",
      "Training Batch [502/782]: Loss 1.2921453714370728\n",
      "Training Batch [503/782]: Loss 1.3222979307174683\n",
      "Training Batch [504/782]: Loss 1.2225055694580078\n",
      "Training Batch [505/782]: Loss 1.471822738647461\n",
      "Training Batch [506/782]: Loss 1.2064074277877808\n",
      "Training Batch [507/782]: Loss 1.798158884048462\n",
      "Training Batch [508/782]: Loss 1.198901891708374\n",
      "Training Batch [509/782]: Loss 1.508220911026001\n",
      "Training Batch [510/782]: Loss 1.1143403053283691\n",
      "Training Batch [511/782]: Loss 1.0469081401824951\n",
      "Training Batch [512/782]: Loss 1.236815333366394\n",
      "Training Batch [513/782]: Loss 1.2569750547409058\n",
      "Training Batch [514/782]: Loss 1.234942078590393\n",
      "Training Batch [515/782]: Loss 1.1606547832489014\n",
      "Training Batch [516/782]: Loss 1.0141123533248901\n",
      "Training Batch [517/782]: Loss 1.4591115713119507\n",
      "Training Batch [518/782]: Loss 1.3526571989059448\n",
      "Training Batch [519/782]: Loss 1.3162587881088257\n",
      "Training Batch [520/782]: Loss 1.2205722332000732\n",
      "Training Batch [521/782]: Loss 1.1962181329727173\n",
      "Training Batch [522/782]: Loss 1.4172847270965576\n",
      "Training Batch [523/782]: Loss 1.2255527973175049\n",
      "Training Batch [524/782]: Loss 1.2937538623809814\n",
      "Training Batch [525/782]: Loss 1.0954025983810425\n",
      "Training Batch [526/782]: Loss 1.132886290550232\n",
      "Training Batch [527/782]: Loss 1.3576710224151611\n",
      "Training Batch [528/782]: Loss 1.3714678287506104\n",
      "Training Batch [529/782]: Loss 1.311553716659546\n",
      "Training Batch [530/782]: Loss 1.0355006456375122\n",
      "Training Batch [531/782]: Loss 0.8801475167274475\n",
      "Training Batch [532/782]: Loss 1.4922757148742676\n",
      "Training Batch [533/782]: Loss 1.0085983276367188\n",
      "Training Batch [534/782]: Loss 1.2628192901611328\n",
      "Training Batch [535/782]: Loss 1.1911529302597046\n",
      "Training Batch [536/782]: Loss 1.448709487915039\n",
      "Training Batch [537/782]: Loss 1.615493655204773\n",
      "Training Batch [538/782]: Loss 1.1410510540008545\n",
      "Training Batch [539/782]: Loss 1.3563040494918823\n",
      "Training Batch [540/782]: Loss 1.0679470300674438\n",
      "Training Batch [541/782]: Loss 1.4604753255844116\n",
      "Training Batch [542/782]: Loss 1.1973185539245605\n",
      "Training Batch [543/782]: Loss 1.0352894067764282\n",
      "Training Batch [544/782]: Loss 1.1486567258834839\n",
      "Training Batch [545/782]: Loss 1.4369641542434692\n",
      "Training Batch [546/782]: Loss 1.4678473472595215\n",
      "Training Batch [547/782]: Loss 1.0166786909103394\n",
      "Training Batch [548/782]: Loss 1.2569223642349243\n",
      "Training Batch [549/782]: Loss 1.2123003005981445\n",
      "Training Batch [550/782]: Loss 1.5782500505447388\n",
      "Training Batch [551/782]: Loss 1.2681941986083984\n",
      "Training Batch [552/782]: Loss 1.356809139251709\n",
      "Training Batch [553/782]: Loss 1.3907431364059448\n",
      "Training Batch [554/782]: Loss 1.261475682258606\n",
      "Training Batch [555/782]: Loss 1.0527794361114502\n",
      "Training Batch [556/782]: Loss 1.3163915872573853\n",
      "Training Batch [557/782]: Loss 1.3079124689102173\n",
      "Training Batch [558/782]: Loss 1.166217565536499\n",
      "Training Batch [559/782]: Loss 1.3316634893417358\n",
      "Training Batch [560/782]: Loss 1.3511323928833008\n",
      "Training Batch [561/782]: Loss 1.1088037490844727\n",
      "Training Batch [562/782]: Loss 1.1409882307052612\n",
      "Training Batch [563/782]: Loss 1.277154803276062\n",
      "Training Batch [564/782]: Loss 1.0563931465148926\n",
      "Training Batch [565/782]: Loss 1.1745388507843018\n",
      "Training Batch [566/782]: Loss 1.4249615669250488\n",
      "Training Batch [567/782]: Loss 1.3044829368591309\n",
      "Training Batch [568/782]: Loss 1.195673942565918\n",
      "Training Batch [569/782]: Loss 0.853759229183197\n",
      "Training Batch [570/782]: Loss 1.2417935132980347\n",
      "Training Batch [571/782]: Loss 1.060794711112976\n",
      "Training Batch [572/782]: Loss 1.6973713636398315\n",
      "Training Batch [573/782]: Loss 1.216206431388855\n",
      "Training Batch [574/782]: Loss 1.350435495376587\n",
      "Training Batch [575/782]: Loss 1.1566659212112427\n",
      "Training Batch [576/782]: Loss 0.9105283617973328\n",
      "Training Batch [577/782]: Loss 1.445292592048645\n",
      "Training Batch [578/782]: Loss 1.4906877279281616\n",
      "Training Batch [579/782]: Loss 1.2042614221572876\n",
      "Training Batch [580/782]: Loss 1.33767569065094\n",
      "Training Batch [581/782]: Loss 1.0628496408462524\n",
      "Training Batch [582/782]: Loss 1.1672275066375732\n",
      "Training Batch [583/782]: Loss 1.4719175100326538\n",
      "Training Batch [584/782]: Loss 1.3448861837387085\n",
      "Training Batch [585/782]: Loss 1.2648494243621826\n",
      "Training Batch [586/782]: Loss 1.2144213914871216\n",
      "Training Batch [587/782]: Loss 0.9921239018440247\n",
      "Training Batch [588/782]: Loss 1.160524606704712\n",
      "Training Batch [589/782]: Loss 1.1274535655975342\n",
      "Training Batch [590/782]: Loss 1.1979804039001465\n",
      "Training Batch [591/782]: Loss 1.2192400693893433\n",
      "Training Batch [592/782]: Loss 1.3816754817962646\n",
      "Training Batch [593/782]: Loss 1.2594507932662964\n",
      "Training Batch [594/782]: Loss 1.219971776008606\n",
      "Training Batch [595/782]: Loss 1.1989483833312988\n",
      "Training Batch [596/782]: Loss 1.424752116203308\n",
      "Training Batch [597/782]: Loss 1.1478328704833984\n",
      "Training Batch [598/782]: Loss 1.221261739730835\n",
      "Training Batch [599/782]: Loss 1.1487772464752197\n",
      "Training Batch [600/782]: Loss 1.6259652376174927\n",
      "Training Batch [601/782]: Loss 1.126790165901184\n",
      "Training Batch [602/782]: Loss 1.196942687034607\n",
      "Training Batch [603/782]: Loss 0.9635119438171387\n",
      "Training Batch [604/782]: Loss 1.3746222257614136\n",
      "Training Batch [605/782]: Loss 1.1598485708236694\n",
      "Training Batch [606/782]: Loss 1.3534414768218994\n",
      "Training Batch [607/782]: Loss 1.0731170177459717\n",
      "Training Batch [608/782]: Loss 1.4799973964691162\n",
      "Training Batch [609/782]: Loss 1.1341822147369385\n",
      "Training Batch [610/782]: Loss 1.5319551229476929\n",
      "Training Batch [611/782]: Loss 1.3690508604049683\n",
      "Training Batch [612/782]: Loss 1.0901669263839722\n",
      "Training Batch [613/782]: Loss 1.1836639642715454\n",
      "Training Batch [614/782]: Loss 1.260021448135376\n",
      "Training Batch [615/782]: Loss 1.2998343706130981\n",
      "Training Batch [616/782]: Loss 1.2091437578201294\n",
      "Training Batch [617/782]: Loss 1.3955961465835571\n",
      "Training Batch [618/782]: Loss 1.0572509765625\n",
      "Training Batch [619/782]: Loss 1.1498501300811768\n",
      "Training Batch [620/782]: Loss 1.0327337980270386\n",
      "Training Batch [621/782]: Loss 1.1483781337738037\n",
      "Training Batch [622/782]: Loss 0.9226061701774597\n",
      "Training Batch [623/782]: Loss 1.5819296836853027\n",
      "Training Batch [624/782]: Loss 1.2247982025146484\n",
      "Training Batch [625/782]: Loss 1.1547633409500122\n",
      "Training Batch [626/782]: Loss 0.9940866231918335\n",
      "Training Batch [627/782]: Loss 0.9793819189071655\n",
      "Training Batch [628/782]: Loss 1.3119685649871826\n",
      "Training Batch [629/782]: Loss 1.1082123517990112\n",
      "Training Batch [630/782]: Loss 1.1321839094161987\n",
      "Training Batch [631/782]: Loss 1.5598984956741333\n",
      "Training Batch [632/782]: Loss 1.3200467824935913\n",
      "Training Batch [633/782]: Loss 1.1436882019042969\n",
      "Training Batch [634/782]: Loss 1.0935990810394287\n",
      "Training Batch [635/782]: Loss 1.1834014654159546\n",
      "Training Batch [636/782]: Loss 1.5638647079467773\n",
      "Training Batch [637/782]: Loss 1.4590160846710205\n",
      "Training Batch [638/782]: Loss 1.1251300573349\n",
      "Training Batch [639/782]: Loss 1.2510813474655151\n",
      "Training Batch [640/782]: Loss 1.2710298299789429\n",
      "Training Batch [641/782]: Loss 1.0046321153640747\n",
      "Training Batch [642/782]: Loss 1.2873300313949585\n",
      "Training Batch [643/782]: Loss 1.4688998460769653\n",
      "Training Batch [644/782]: Loss 1.4402930736541748\n",
      "Training Batch [645/782]: Loss 1.3729256391525269\n",
      "Training Batch [646/782]: Loss 1.1847199201583862\n",
      "Training Batch [647/782]: Loss 1.3236483335494995\n",
      "Training Batch [648/782]: Loss 1.46499764919281\n",
      "Training Batch [649/782]: Loss 1.4915225505828857\n",
      "Training Batch [650/782]: Loss 1.2074908018112183\n",
      "Training Batch [651/782]: Loss 1.4349827766418457\n",
      "Training Batch [652/782]: Loss 1.007204294204712\n",
      "Training Batch [653/782]: Loss 1.4746500253677368\n",
      "Training Batch [654/782]: Loss 1.084710717201233\n",
      "Training Batch [655/782]: Loss 1.2678437232971191\n",
      "Training Batch [656/782]: Loss 1.1839078664779663\n",
      "Training Batch [657/782]: Loss 1.2830878496170044\n",
      "Training Batch [658/782]: Loss 1.3162784576416016\n",
      "Training Batch [659/782]: Loss 1.554854154586792\n",
      "Training Batch [660/782]: Loss 1.3096671104431152\n",
      "Training Batch [661/782]: Loss 1.1773427724838257\n",
      "Training Batch [662/782]: Loss 1.2078717947006226\n",
      "Training Batch [663/782]: Loss 1.3037326335906982\n",
      "Training Batch [664/782]: Loss 1.4438576698303223\n",
      "Training Batch [665/782]: Loss 1.2978832721710205\n",
      "Training Batch [666/782]: Loss 1.2355302572250366\n",
      "Training Batch [667/782]: Loss 0.944779634475708\n",
      "Training Batch [668/782]: Loss 1.1564288139343262\n",
      "Training Batch [669/782]: Loss 1.2074769735336304\n",
      "Training Batch [670/782]: Loss 1.2106900215148926\n",
      "Training Batch [671/782]: Loss 1.5270053148269653\n",
      "Training Batch [672/782]: Loss 1.464016079902649\n",
      "Training Batch [673/782]: Loss 0.9551417231559753\n",
      "Training Batch [674/782]: Loss 1.230259656906128\n",
      "Training Batch [675/782]: Loss 1.400682806968689\n",
      "Training Batch [676/782]: Loss 1.2260135412216187\n",
      "Training Batch [677/782]: Loss 1.2503504753112793\n",
      "Training Batch [678/782]: Loss 1.1881424188613892\n",
      "Training Batch [679/782]: Loss 1.270401954650879\n",
      "Training Batch [680/782]: Loss 1.309417486190796\n",
      "Training Batch [681/782]: Loss 1.2402491569519043\n",
      "Training Batch [682/782]: Loss 1.1932653188705444\n",
      "Training Batch [683/782]: Loss 1.22703218460083\n",
      "Training Batch [684/782]: Loss 1.024774432182312\n",
      "Training Batch [685/782]: Loss 1.2657309770584106\n",
      "Training Batch [686/782]: Loss 1.5437235832214355\n",
      "Training Batch [687/782]: Loss 1.2638567686080933\n",
      "Training Batch [688/782]: Loss 1.1533621549606323\n",
      "Training Batch [689/782]: Loss 1.2089208364486694\n",
      "Training Batch [690/782]: Loss 1.1353225708007812\n",
      "Training Batch [691/782]: Loss 1.2009893655776978\n",
      "Training Batch [692/782]: Loss 1.2343133687973022\n",
      "Training Batch [693/782]: Loss 1.284279704093933\n",
      "Training Batch [694/782]: Loss 1.0269333124160767\n",
      "Training Batch [695/782]: Loss 1.1263362169265747\n",
      "Training Batch [696/782]: Loss 1.3630423545837402\n",
      "Training Batch [697/782]: Loss 1.1501996517181396\n",
      "Training Batch [698/782]: Loss 1.2223647832870483\n",
      "Training Batch [699/782]: Loss 1.0634289979934692\n",
      "Training Batch [700/782]: Loss 1.3688688278198242\n",
      "Training Batch [701/782]: Loss 1.034520149230957\n",
      "Training Batch [702/782]: Loss 1.1141464710235596\n",
      "Training Batch [703/782]: Loss 1.153085470199585\n",
      "Training Batch [704/782]: Loss 1.4759620428085327\n",
      "Training Batch [705/782]: Loss 1.4214856624603271\n",
      "Training Batch [706/782]: Loss 1.421208143234253\n",
      "Training Batch [707/782]: Loss 1.4955906867980957\n",
      "Training Batch [708/782]: Loss 1.3661450147628784\n",
      "Training Batch [709/782]: Loss 1.437606930732727\n",
      "Training Batch [710/782]: Loss 1.2305636405944824\n",
      "Training Batch [711/782]: Loss 1.0731017589569092\n",
      "Training Batch [712/782]: Loss 1.1277368068695068\n",
      "Training Batch [713/782]: Loss 1.335044503211975\n",
      "Training Batch [714/782]: Loss 1.2030917406082153\n",
      "Training Batch [715/782]: Loss 1.1174538135528564\n",
      "Training Batch [716/782]: Loss 1.0968953371047974\n",
      "Training Batch [717/782]: Loss 1.181951642036438\n",
      "Training Batch [718/782]: Loss 1.4824072122573853\n",
      "Training Batch [719/782]: Loss 1.214748740196228\n",
      "Training Batch [720/782]: Loss 1.1574840545654297\n",
      "Training Batch [721/782]: Loss 1.1794989109039307\n",
      "Training Batch [722/782]: Loss 1.2615925073623657\n",
      "Training Batch [723/782]: Loss 1.1309870481491089\n",
      "Training Batch [724/782]: Loss 1.0766605138778687\n",
      "Training Batch [725/782]: Loss 1.201918125152588\n",
      "Training Batch [726/782]: Loss 1.113864541053772\n",
      "Training Batch [727/782]: Loss 1.398543357849121\n",
      "Training Batch [728/782]: Loss 1.1826987266540527\n",
      "Training Batch [729/782]: Loss 0.9323183298110962\n",
      "Training Batch [730/782]: Loss 1.1772016286849976\n",
      "Training Batch [731/782]: Loss 1.0233412981033325\n",
      "Training Batch [732/782]: Loss 1.4219118356704712\n",
      "Training Batch [733/782]: Loss 1.3440942764282227\n",
      "Training Batch [734/782]: Loss 1.1898739337921143\n",
      "Training Batch [735/782]: Loss 0.9614478349685669\n",
      "Training Batch [736/782]: Loss 1.1079678535461426\n",
      "Training Batch [737/782]: Loss 1.4063001871109009\n",
      "Training Batch [738/782]: Loss 1.1615891456604004\n",
      "Training Batch [739/782]: Loss 1.387481451034546\n",
      "Training Batch [740/782]: Loss 1.1022746562957764\n",
      "Training Batch [741/782]: Loss 1.2610872983932495\n",
      "Training Batch [742/782]: Loss 1.1537418365478516\n",
      "Training Batch [743/782]: Loss 1.2584694623947144\n",
      "Training Batch [744/782]: Loss 1.269244909286499\n",
      "Training Batch [745/782]: Loss 1.1305137872695923\n",
      "Training Batch [746/782]: Loss 1.1601272821426392\n",
      "Training Batch [747/782]: Loss 1.3741910457611084\n",
      "Training Batch [748/782]: Loss 1.389696478843689\n",
      "Training Batch [749/782]: Loss 1.616218090057373\n",
      "Training Batch [750/782]: Loss 1.1189258098602295\n",
      "Training Batch [751/782]: Loss 1.2433733940124512\n",
      "Training Batch [752/782]: Loss 1.1825286149978638\n",
      "Training Batch [753/782]: Loss 1.2763800621032715\n",
      "Training Batch [754/782]: Loss 1.2746742963790894\n",
      "Training Batch [755/782]: Loss 1.1050056219100952\n",
      "Training Batch [756/782]: Loss 1.2456066608428955\n",
      "Training Batch [757/782]: Loss 1.0987777709960938\n",
      "Training Batch [758/782]: Loss 1.2523550987243652\n",
      "Training Batch [759/782]: Loss 1.1378506422042847\n",
      "Training Batch [760/782]: Loss 1.224534273147583\n",
      "Training Batch [761/782]: Loss 1.0893826484680176\n",
      "Training Batch [762/782]: Loss 1.0115972757339478\n",
      "Training Batch [763/782]: Loss 1.4195270538330078\n",
      "Training Batch [764/782]: Loss 1.357818603515625\n",
      "Training Batch [765/782]: Loss 1.2101516723632812\n",
      "Training Batch [766/782]: Loss 1.186232566833496\n",
      "Training Batch [767/782]: Loss 1.1631499528884888\n",
      "Training Batch [768/782]: Loss 1.137439489364624\n",
      "Training Batch [769/782]: Loss 1.0991791486740112\n",
      "Training Batch [770/782]: Loss 1.279541254043579\n",
      "Training Batch [771/782]: Loss 1.115526795387268\n",
      "Training Batch [772/782]: Loss 1.0711308717727661\n",
      "Training Batch [773/782]: Loss 1.2622114419937134\n",
      "Training Batch [774/782]: Loss 1.2026506662368774\n",
      "Training Batch [775/782]: Loss 1.4067813158035278\n",
      "Training Batch [776/782]: Loss 1.231297492980957\n",
      "Training Batch [777/782]: Loss 0.9927391409873962\n",
      "Training Batch [778/782]: Loss 1.0270148515701294\n",
      "Training Batch [779/782]: Loss 1.2800718545913696\n",
      "Training Batch [780/782]: Loss 0.8921182155609131\n",
      "Training Batch [781/782]: Loss 0.9981566667556763\n",
      "Training Batch [782/782]: Loss 0.9280996322631836\n",
      "Epoch 3 - Train Loss: 1.2634\n",
      "*********  Epoch 4/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 1.1561529636383057\n",
      "Training Batch [2/782]: Loss 1.0931406021118164\n",
      "Training Batch [3/782]: Loss 1.4568461179733276\n",
      "Training Batch [4/782]: Loss 1.2978914976119995\n",
      "Training Batch [5/782]: Loss 1.0057791471481323\n",
      "Training Batch [6/782]: Loss 1.2686883211135864\n",
      "Training Batch [7/782]: Loss 0.9160135984420776\n",
      "Training Batch [8/782]: Loss 1.4593256711959839\n",
      "Training Batch [9/782]: Loss 0.9919157028198242\n",
      "Training Batch [10/782]: Loss 0.9837603569030762\n",
      "Training Batch [11/782]: Loss 1.1561076641082764\n",
      "Training Batch [12/782]: Loss 1.4466747045516968\n",
      "Training Batch [13/782]: Loss 1.1446185111999512\n",
      "Training Batch [14/782]: Loss 0.9964779615402222\n",
      "Training Batch [15/782]: Loss 0.9481492638587952\n",
      "Training Batch [16/782]: Loss 1.2090100049972534\n",
      "Training Batch [17/782]: Loss 1.3957868814468384\n",
      "Training Batch [18/782]: Loss 1.4167758226394653\n",
      "Training Batch [19/782]: Loss 1.1294256448745728\n",
      "Training Batch [20/782]: Loss 1.0301140546798706\n",
      "Training Batch [21/782]: Loss 1.339525818824768\n",
      "Training Batch [22/782]: Loss 1.0694375038146973\n",
      "Training Batch [23/782]: Loss 1.1403758525848389\n",
      "Training Batch [24/782]: Loss 1.339355230331421\n",
      "Training Batch [25/782]: Loss 0.9690882563591003\n",
      "Training Batch [26/782]: Loss 1.3142750263214111\n",
      "Training Batch [27/782]: Loss 1.065122365951538\n",
      "Training Batch [28/782]: Loss 1.2546930313110352\n",
      "Training Batch [29/782]: Loss 1.1510591506958008\n",
      "Training Batch [30/782]: Loss 1.2678639888763428\n",
      "Training Batch [31/782]: Loss 1.023354411125183\n",
      "Training Batch [32/782]: Loss 1.053835391998291\n",
      "Training Batch [33/782]: Loss 0.9420832991600037\n",
      "Training Batch [34/782]: Loss 1.0019760131835938\n",
      "Training Batch [35/782]: Loss 1.2245770692825317\n",
      "Training Batch [36/782]: Loss 1.0819387435913086\n",
      "Training Batch [37/782]: Loss 1.2290135622024536\n",
      "Training Batch [38/782]: Loss 0.9812420606613159\n",
      "Training Batch [39/782]: Loss 1.2514526844024658\n",
      "Training Batch [40/782]: Loss 0.9221436381340027\n",
      "Training Batch [41/782]: Loss 1.3198145627975464\n",
      "Training Batch [42/782]: Loss 1.005794882774353\n",
      "Training Batch [43/782]: Loss 0.925225555896759\n",
      "Training Batch [44/782]: Loss 1.1344412565231323\n",
      "Training Batch [45/782]: Loss 0.9414355754852295\n",
      "Training Batch [46/782]: Loss 1.1108684539794922\n",
      "Training Batch [47/782]: Loss 1.0968514680862427\n",
      "Training Batch [48/782]: Loss 0.925609290599823\n",
      "Training Batch [49/782]: Loss 1.1110131740570068\n",
      "Training Batch [50/782]: Loss 1.2426214218139648\n",
      "Training Batch [51/782]: Loss 1.0346269607543945\n",
      "Training Batch [52/782]: Loss 1.1159346103668213\n",
      "Training Batch [53/782]: Loss 1.0369952917099\n",
      "Training Batch [54/782]: Loss 0.9928900003433228\n",
      "Training Batch [55/782]: Loss 1.1729998588562012\n",
      "Training Batch [56/782]: Loss 1.1562614440917969\n",
      "Training Batch [57/782]: Loss 1.0524662733078003\n",
      "Training Batch [58/782]: Loss 1.1139192581176758\n",
      "Training Batch [59/782]: Loss 1.281461477279663\n",
      "Training Batch [60/782]: Loss 1.155973196029663\n",
      "Training Batch [61/782]: Loss 0.8434003591537476\n",
      "Training Batch [62/782]: Loss 1.2152189016342163\n",
      "Training Batch [63/782]: Loss 1.058883786201477\n",
      "Training Batch [64/782]: Loss 1.2122668027877808\n",
      "Training Batch [65/782]: Loss 1.0417869091033936\n",
      "Training Batch [66/782]: Loss 1.2066662311553955\n",
      "Training Batch [67/782]: Loss 0.9364591836929321\n",
      "Training Batch [68/782]: Loss 1.1459704637527466\n",
      "Training Batch [69/782]: Loss 1.3886767625808716\n",
      "Training Batch [70/782]: Loss 1.0805854797363281\n",
      "Training Batch [71/782]: Loss 1.166589617729187\n",
      "Training Batch [72/782]: Loss 0.8699296712875366\n",
      "Training Batch [73/782]: Loss 0.9449262619018555\n",
      "Training Batch [74/782]: Loss 1.2548134326934814\n",
      "Training Batch [75/782]: Loss 1.2212109565734863\n",
      "Training Batch [76/782]: Loss 1.2022740840911865\n",
      "Training Batch [77/782]: Loss 1.2381397485733032\n",
      "Training Batch [78/782]: Loss 0.9289566278457642\n",
      "Training Batch [79/782]: Loss 0.9802180528640747\n",
      "Training Batch [80/782]: Loss 1.1098135709762573\n",
      "Training Batch [81/782]: Loss 1.1088333129882812\n",
      "Training Batch [82/782]: Loss 0.8343572020530701\n",
      "Training Batch [83/782]: Loss 1.0073750019073486\n",
      "Training Batch [84/782]: Loss 1.1937296390533447\n",
      "Training Batch [85/782]: Loss 0.9018226265907288\n",
      "Training Batch [86/782]: Loss 0.8798400163650513\n",
      "Training Batch [87/782]: Loss 1.3875035047531128\n",
      "Training Batch [88/782]: Loss 1.2699358463287354\n",
      "Training Batch [89/782]: Loss 1.140916109085083\n",
      "Training Batch [90/782]: Loss 0.9697950482368469\n",
      "Training Batch [91/782]: Loss 1.0333446264266968\n",
      "Training Batch [92/782]: Loss 0.960867702960968\n",
      "Training Batch [93/782]: Loss 1.3948206901550293\n",
      "Training Batch [94/782]: Loss 1.0217357873916626\n",
      "Training Batch [95/782]: Loss 1.1941485404968262\n",
      "Training Batch [96/782]: Loss 1.1040648221969604\n",
      "Training Batch [97/782]: Loss 1.2023812532424927\n",
      "Training Batch [98/782]: Loss 0.9987354278564453\n",
      "Training Batch [99/782]: Loss 1.2949063777923584\n",
      "Training Batch [100/782]: Loss 0.9331850409507751\n",
      "Training Batch [101/782]: Loss 0.8754834532737732\n",
      "Training Batch [102/782]: Loss 1.0037668943405151\n",
      "Training Batch [103/782]: Loss 1.1220496892929077\n",
      "Training Batch [104/782]: Loss 1.2019799947738647\n",
      "Training Batch [105/782]: Loss 0.9763326048851013\n",
      "Training Batch [106/782]: Loss 1.1566392183303833\n",
      "Training Batch [107/782]: Loss 1.209648847579956\n",
      "Training Batch [108/782]: Loss 1.4474345445632935\n",
      "Training Batch [109/782]: Loss 1.2517249584197998\n",
      "Training Batch [110/782]: Loss 0.9640624523162842\n",
      "Training Batch [111/782]: Loss 1.250726342201233\n",
      "Training Batch [112/782]: Loss 1.0283600091934204\n",
      "Training Batch [113/782]: Loss 1.2370506525039673\n",
      "Training Batch [114/782]: Loss 0.9556753039360046\n",
      "Training Batch [115/782]: Loss 0.9355263113975525\n",
      "Training Batch [116/782]: Loss 1.4068201780319214\n",
      "Training Batch [117/782]: Loss 0.7774153351783752\n",
      "Training Batch [118/782]: Loss 1.2109163999557495\n",
      "Training Batch [119/782]: Loss 1.0454996824264526\n",
      "Training Batch [120/782]: Loss 1.1460402011871338\n",
      "Training Batch [121/782]: Loss 1.5089887380599976\n",
      "Training Batch [122/782]: Loss 0.9155621528625488\n",
      "Training Batch [123/782]: Loss 1.0329540967941284\n",
      "Training Batch [124/782]: Loss 1.273797631263733\n",
      "Training Batch [125/782]: Loss 1.2200984954833984\n",
      "Training Batch [126/782]: Loss 1.204838514328003\n",
      "Training Batch [127/782]: Loss 0.9977313876152039\n",
      "Training Batch [128/782]: Loss 1.0877914428710938\n",
      "Training Batch [129/782]: Loss 1.0593703985214233\n",
      "Training Batch [130/782]: Loss 1.3015342950820923\n",
      "Training Batch [131/782]: Loss 1.1468700170516968\n",
      "Training Batch [132/782]: Loss 1.0803935527801514\n",
      "Training Batch [133/782]: Loss 1.2868086099624634\n",
      "Training Batch [134/782]: Loss 1.0462487936019897\n",
      "Training Batch [135/782]: Loss 1.128040075302124\n",
      "Training Batch [136/782]: Loss 1.1895232200622559\n",
      "Training Batch [137/782]: Loss 1.1019290685653687\n",
      "Training Batch [138/782]: Loss 1.2363042831420898\n",
      "Training Batch [139/782]: Loss 1.0064204931259155\n",
      "Training Batch [140/782]: Loss 0.8575876951217651\n",
      "Training Batch [141/782]: Loss 1.2268621921539307\n",
      "Training Batch [142/782]: Loss 1.2236775159835815\n",
      "Training Batch [143/782]: Loss 0.887660801410675\n",
      "Training Batch [144/782]: Loss 1.0559099912643433\n",
      "Training Batch [145/782]: Loss 0.9648374915122986\n",
      "Training Batch [146/782]: Loss 1.1571218967437744\n",
      "Training Batch [147/782]: Loss 1.0506889820098877\n",
      "Training Batch [148/782]: Loss 0.9055392742156982\n",
      "Training Batch [149/782]: Loss 1.1233421564102173\n",
      "Training Batch [150/782]: Loss 0.9291386008262634\n",
      "Training Batch [151/782]: Loss 0.911318302154541\n",
      "Training Batch [152/782]: Loss 1.1918230056762695\n",
      "Training Batch [153/782]: Loss 1.0990113019943237\n",
      "Training Batch [154/782]: Loss 1.0694754123687744\n",
      "Training Batch [155/782]: Loss 1.1277709007263184\n",
      "Training Batch [156/782]: Loss 0.9891582131385803\n",
      "Training Batch [157/782]: Loss 1.3296459913253784\n",
      "Training Batch [158/782]: Loss 0.8656020760536194\n",
      "Training Batch [159/782]: Loss 1.314751386642456\n",
      "Training Batch [160/782]: Loss 1.138275384902954\n",
      "Training Batch [161/782]: Loss 1.3310465812683105\n",
      "Training Batch [162/782]: Loss 1.0080149173736572\n",
      "Training Batch [163/782]: Loss 1.2823156118392944\n",
      "Training Batch [164/782]: Loss 0.7842196822166443\n",
      "Training Batch [165/782]: Loss 1.043564796447754\n",
      "Training Batch [166/782]: Loss 1.239030122756958\n",
      "Training Batch [167/782]: Loss 1.2160085439682007\n",
      "Training Batch [168/782]: Loss 1.0883731842041016\n",
      "Training Batch [169/782]: Loss 1.507228970527649\n",
      "Training Batch [170/782]: Loss 1.0721359252929688\n",
      "Training Batch [171/782]: Loss 1.2277451753616333\n",
      "Training Batch [172/782]: Loss 1.2080405950546265\n",
      "Training Batch [173/782]: Loss 1.0926343202590942\n",
      "Training Batch [174/782]: Loss 1.0787436962127686\n",
      "Training Batch [175/782]: Loss 0.9943358302116394\n",
      "Training Batch [176/782]: Loss 0.9168800115585327\n",
      "Training Batch [177/782]: Loss 1.0409555435180664\n",
      "Training Batch [178/782]: Loss 1.0244508981704712\n",
      "Training Batch [179/782]: Loss 1.0209875106811523\n",
      "Training Batch [180/782]: Loss 1.0998228788375854\n",
      "Training Batch [181/782]: Loss 1.5429580211639404\n",
      "Training Batch [182/782]: Loss 1.0850956439971924\n",
      "Training Batch [183/782]: Loss 1.0056557655334473\n",
      "Training Batch [184/782]: Loss 1.3301244974136353\n",
      "Training Batch [185/782]: Loss 1.2154268026351929\n",
      "Training Batch [186/782]: Loss 1.1279722452163696\n",
      "Training Batch [187/782]: Loss 1.076786994934082\n",
      "Training Batch [188/782]: Loss 0.9522965550422668\n",
      "Training Batch [189/782]: Loss 0.9583356380462646\n",
      "Training Batch [190/782]: Loss 1.1087615489959717\n",
      "Training Batch [191/782]: Loss 0.9431155920028687\n",
      "Training Batch [192/782]: Loss 1.0359938144683838\n",
      "Training Batch [193/782]: Loss 1.2177565097808838\n",
      "Training Batch [194/782]: Loss 0.956218421459198\n",
      "Training Batch [195/782]: Loss 0.7808791995048523\n",
      "Training Batch [196/782]: Loss 1.2007991075515747\n",
      "Training Batch [197/782]: Loss 1.195000171661377\n",
      "Training Batch [198/782]: Loss 1.1329227685928345\n",
      "Training Batch [199/782]: Loss 0.9322865009307861\n",
      "Training Batch [200/782]: Loss 1.3423645496368408\n",
      "Training Batch [201/782]: Loss 1.0778405666351318\n",
      "Training Batch [202/782]: Loss 0.9435234069824219\n",
      "Training Batch [203/782]: Loss 0.7766710519790649\n",
      "Training Batch [204/782]: Loss 1.2289897203445435\n",
      "Training Batch [205/782]: Loss 1.2903553247451782\n",
      "Training Batch [206/782]: Loss 1.1679770946502686\n",
      "Training Batch [207/782]: Loss 1.3096635341644287\n",
      "Training Batch [208/782]: Loss 1.0048414468765259\n",
      "Training Batch [209/782]: Loss 1.426652193069458\n",
      "Training Batch [210/782]: Loss 1.2335326671600342\n",
      "Training Batch [211/782]: Loss 0.9805160760879517\n",
      "Training Batch [212/782]: Loss 0.9456650614738464\n",
      "Training Batch [213/782]: Loss 0.9254379272460938\n",
      "Training Batch [214/782]: Loss 0.8735101819038391\n",
      "Training Batch [215/782]: Loss 1.2592682838439941\n",
      "Training Batch [216/782]: Loss 1.0341107845306396\n",
      "Training Batch [217/782]: Loss 0.9864892959594727\n",
      "Training Batch [218/782]: Loss 1.0088549852371216\n",
      "Training Batch [219/782]: Loss 0.9967474341392517\n",
      "Training Batch [220/782]: Loss 1.1428840160369873\n",
      "Training Batch [221/782]: Loss 1.084560513496399\n",
      "Training Batch [222/782]: Loss 1.2268155813217163\n",
      "Training Batch [223/782]: Loss 1.187135100364685\n",
      "Training Batch [224/782]: Loss 1.035692811012268\n",
      "Training Batch [225/782]: Loss 1.1303908824920654\n",
      "Training Batch [226/782]: Loss 0.7621210217475891\n",
      "Training Batch [227/782]: Loss 1.019734501838684\n",
      "Training Batch [228/782]: Loss 1.0661897659301758\n",
      "Training Batch [229/782]: Loss 0.9575698375701904\n",
      "Training Batch [230/782]: Loss 1.223122239112854\n",
      "Training Batch [231/782]: Loss 1.1923352479934692\n",
      "Training Batch [232/782]: Loss 1.1429643630981445\n",
      "Training Batch [233/782]: Loss 0.900733470916748\n",
      "Training Batch [234/782]: Loss 1.013275146484375\n",
      "Training Batch [235/782]: Loss 1.274706482887268\n",
      "Training Batch [236/782]: Loss 1.0196301937103271\n",
      "Training Batch [237/782]: Loss 1.1083043813705444\n",
      "Training Batch [238/782]: Loss 0.9534239768981934\n",
      "Training Batch [239/782]: Loss 0.9384143948554993\n",
      "Training Batch [240/782]: Loss 1.0535768270492554\n",
      "Training Batch [241/782]: Loss 1.0835171937942505\n",
      "Training Batch [242/782]: Loss 1.0940431356430054\n",
      "Training Batch [243/782]: Loss 0.9226316809654236\n",
      "Training Batch [244/782]: Loss 0.9393553137779236\n",
      "Training Batch [245/782]: Loss 1.08707594871521\n",
      "Training Batch [246/782]: Loss 0.9377707242965698\n",
      "Training Batch [247/782]: Loss 1.1433601379394531\n",
      "Training Batch [248/782]: Loss 1.2654982805252075\n",
      "Training Batch [249/782]: Loss 0.9945205450057983\n",
      "Training Batch [250/782]: Loss 0.9165858030319214\n",
      "Training Batch [251/782]: Loss 0.9539094567298889\n",
      "Training Batch [252/782]: Loss 1.067685842514038\n",
      "Training Batch [253/782]: Loss 1.046379804611206\n",
      "Training Batch [254/782]: Loss 0.7992744445800781\n",
      "Training Batch [255/782]: Loss 1.0470026731491089\n",
      "Training Batch [256/782]: Loss 1.1865313053131104\n",
      "Training Batch [257/782]: Loss 1.1851269006729126\n",
      "Training Batch [258/782]: Loss 1.1235359907150269\n",
      "Training Batch [259/782]: Loss 1.3355419635772705\n",
      "Training Batch [260/782]: Loss 1.079856514930725\n",
      "Training Batch [261/782]: Loss 0.8570370078086853\n",
      "Training Batch [262/782]: Loss 1.3307479619979858\n",
      "Training Batch [263/782]: Loss 1.067723274230957\n",
      "Training Batch [264/782]: Loss 1.4814931154251099\n",
      "Training Batch [265/782]: Loss 1.3018755912780762\n",
      "Training Batch [266/782]: Loss 1.2412689924240112\n",
      "Training Batch [267/782]: Loss 1.091303825378418\n",
      "Training Batch [268/782]: Loss 1.2227004766464233\n",
      "Training Batch [269/782]: Loss 1.1857198476791382\n",
      "Training Batch [270/782]: Loss 1.121639609336853\n",
      "Training Batch [271/782]: Loss 1.1898248195648193\n",
      "Training Batch [272/782]: Loss 1.0778436660766602\n",
      "Training Batch [273/782]: Loss 1.1229461431503296\n",
      "Training Batch [274/782]: Loss 1.248726725578308\n",
      "Training Batch [275/782]: Loss 1.0535460710525513\n",
      "Training Batch [276/782]: Loss 1.083420991897583\n",
      "Training Batch [277/782]: Loss 1.038778305053711\n",
      "Training Batch [278/782]: Loss 0.9364120364189148\n",
      "Training Batch [279/782]: Loss 1.2055883407592773\n",
      "Training Batch [280/782]: Loss 1.153118371963501\n",
      "Training Batch [281/782]: Loss 1.0506612062454224\n",
      "Training Batch [282/782]: Loss 1.0852230787277222\n",
      "Training Batch [283/782]: Loss 1.03935706615448\n",
      "Training Batch [284/782]: Loss 0.8276473879814148\n",
      "Training Batch [285/782]: Loss 1.0725102424621582\n",
      "Training Batch [286/782]: Loss 1.0157474279403687\n",
      "Training Batch [287/782]: Loss 0.9675977230072021\n",
      "Training Batch [288/782]: Loss 1.1139369010925293\n",
      "Training Batch [289/782]: Loss 0.9584243893623352\n",
      "Training Batch [290/782]: Loss 1.2313991785049438\n",
      "Training Batch [291/782]: Loss 0.9063493013381958\n",
      "Training Batch [292/782]: Loss 1.0440608263015747\n",
      "Training Batch [293/782]: Loss 0.9490957856178284\n",
      "Training Batch [294/782]: Loss 1.411192536354065\n",
      "Training Batch [295/782]: Loss 1.2352294921875\n",
      "Training Batch [296/782]: Loss 1.2263662815093994\n",
      "Training Batch [297/782]: Loss 1.1854405403137207\n",
      "Training Batch [298/782]: Loss 1.0288729667663574\n",
      "Training Batch [299/782]: Loss 1.1872481107711792\n",
      "Training Batch [300/782]: Loss 0.9399244785308838\n",
      "Training Batch [301/782]: Loss 1.0939394235610962\n",
      "Training Batch [302/782]: Loss 1.155707597732544\n",
      "Training Batch [303/782]: Loss 1.1069977283477783\n",
      "Training Batch [304/782]: Loss 1.068129539489746\n",
      "Training Batch [305/782]: Loss 1.2590622901916504\n",
      "Training Batch [306/782]: Loss 1.3052453994750977\n",
      "Training Batch [307/782]: Loss 1.1229103803634644\n",
      "Training Batch [308/782]: Loss 1.109803557395935\n",
      "Training Batch [309/782]: Loss 1.0838059186935425\n",
      "Training Batch [310/782]: Loss 1.086517333984375\n",
      "Training Batch [311/782]: Loss 1.3483920097351074\n",
      "Training Batch [312/782]: Loss 0.8480951189994812\n",
      "Training Batch [313/782]: Loss 1.3409326076507568\n",
      "Training Batch [314/782]: Loss 0.9403222799301147\n",
      "Training Batch [315/782]: Loss 1.158530831336975\n",
      "Training Batch [316/782]: Loss 0.9925961494445801\n",
      "Training Batch [317/782]: Loss 1.2399609088897705\n",
      "Training Batch [318/782]: Loss 1.2078312635421753\n",
      "Training Batch [319/782]: Loss 1.290608286857605\n",
      "Training Batch [320/782]: Loss 1.0504207611083984\n",
      "Training Batch [321/782]: Loss 0.9651284217834473\n",
      "Training Batch [322/782]: Loss 1.1269853115081787\n",
      "Training Batch [323/782]: Loss 1.4873080253601074\n",
      "Training Batch [324/782]: Loss 0.7563530802726746\n",
      "Training Batch [325/782]: Loss 0.9728342294692993\n",
      "Training Batch [326/782]: Loss 1.2823994159698486\n",
      "Training Batch [327/782]: Loss 1.2881017923355103\n",
      "Training Batch [328/782]: Loss 1.1990610361099243\n",
      "Training Batch [329/782]: Loss 0.959212601184845\n",
      "Training Batch [330/782]: Loss 1.1710106134414673\n",
      "Training Batch [331/782]: Loss 1.0474098920822144\n",
      "Training Batch [332/782]: Loss 1.1065800189971924\n",
      "Training Batch [333/782]: Loss 1.1331019401550293\n",
      "Training Batch [334/782]: Loss 1.3203723430633545\n",
      "Training Batch [335/782]: Loss 1.149968147277832\n",
      "Training Batch [336/782]: Loss 1.1834725141525269\n",
      "Training Batch [337/782]: Loss 0.986940860748291\n",
      "Training Batch [338/782]: Loss 1.2089667320251465\n",
      "Training Batch [339/782]: Loss 1.0936914682388306\n",
      "Training Batch [340/782]: Loss 1.2805041074752808\n",
      "Training Batch [341/782]: Loss 1.1805126667022705\n",
      "Training Batch [342/782]: Loss 1.1584392786026\n",
      "Training Batch [343/782]: Loss 1.1503889560699463\n",
      "Training Batch [344/782]: Loss 1.024790644645691\n",
      "Training Batch [345/782]: Loss 0.8568795919418335\n",
      "Training Batch [346/782]: Loss 1.0739552974700928\n",
      "Training Batch [347/782]: Loss 1.3030503988265991\n",
      "Training Batch [348/782]: Loss 1.1969592571258545\n",
      "Training Batch [349/782]: Loss 1.051443338394165\n",
      "Training Batch [350/782]: Loss 1.055092453956604\n",
      "Training Batch [351/782]: Loss 1.0724008083343506\n",
      "Training Batch [352/782]: Loss 1.275349736213684\n",
      "Training Batch [353/782]: Loss 0.7621300220489502\n",
      "Training Batch [354/782]: Loss 1.1737439632415771\n",
      "Training Batch [355/782]: Loss 0.9868342280387878\n",
      "Training Batch [356/782]: Loss 1.1860361099243164\n",
      "Training Batch [357/782]: Loss 1.2370864152908325\n",
      "Training Batch [358/782]: Loss 1.2113611698150635\n",
      "Training Batch [359/782]: Loss 1.0361753702163696\n",
      "Training Batch [360/782]: Loss 0.7877673506736755\n",
      "Training Batch [361/782]: Loss 1.461678147315979\n",
      "Training Batch [362/782]: Loss 1.1616560220718384\n",
      "Training Batch [363/782]: Loss 0.9614476561546326\n",
      "Training Batch [364/782]: Loss 0.9808827042579651\n",
      "Training Batch [365/782]: Loss 0.937719464302063\n",
      "Training Batch [366/782]: Loss 1.0640170574188232\n",
      "Training Batch [367/782]: Loss 1.3156909942626953\n",
      "Training Batch [368/782]: Loss 1.183203935623169\n",
      "Training Batch [369/782]: Loss 0.9073461294174194\n",
      "Training Batch [370/782]: Loss 1.3895378112792969\n",
      "Training Batch [371/782]: Loss 1.048063039779663\n",
      "Training Batch [372/782]: Loss 1.1274703741073608\n",
      "Training Batch [373/782]: Loss 1.1793478727340698\n",
      "Training Batch [374/782]: Loss 1.169979453086853\n",
      "Training Batch [375/782]: Loss 0.9383257031440735\n",
      "Training Batch [376/782]: Loss 1.1632664203643799\n",
      "Training Batch [377/782]: Loss 1.0347304344177246\n",
      "Training Batch [378/782]: Loss 1.0501655340194702\n",
      "Training Batch [379/782]: Loss 0.8868340849876404\n",
      "Training Batch [380/782]: Loss 1.2458332777023315\n",
      "Training Batch [381/782]: Loss 0.9639933109283447\n",
      "Training Batch [382/782]: Loss 0.9601331949234009\n",
      "Training Batch [383/782]: Loss 0.7837799787521362\n",
      "Training Batch [384/782]: Loss 1.1713780164718628\n",
      "Training Batch [385/782]: Loss 0.9698357582092285\n",
      "Training Batch [386/782]: Loss 1.2763237953186035\n",
      "Training Batch [387/782]: Loss 0.9718616604804993\n",
      "Training Batch [388/782]: Loss 1.017370343208313\n",
      "Training Batch [389/782]: Loss 1.1854238510131836\n",
      "Training Batch [390/782]: Loss 1.146568775177002\n",
      "Training Batch [391/782]: Loss 0.9860753417015076\n",
      "Training Batch [392/782]: Loss 1.1629865169525146\n",
      "Training Batch [393/782]: Loss 1.4382671117782593\n",
      "Training Batch [394/782]: Loss 0.9794058203697205\n",
      "Training Batch [395/782]: Loss 1.3313796520233154\n",
      "Training Batch [396/782]: Loss 1.1653409004211426\n",
      "Training Batch [397/782]: Loss 1.0001146793365479\n",
      "Training Batch [398/782]: Loss 1.1143771409988403\n",
      "Training Batch [399/782]: Loss 0.8728079199790955\n",
      "Training Batch [400/782]: Loss 1.2320712804794312\n",
      "Training Batch [401/782]: Loss 1.1357991695404053\n",
      "Training Batch [402/782]: Loss 1.1280642747879028\n",
      "Training Batch [403/782]: Loss 1.0019795894622803\n",
      "Training Batch [404/782]: Loss 1.1712825298309326\n",
      "Training Batch [405/782]: Loss 1.0358504056930542\n",
      "Training Batch [406/782]: Loss 0.9040912985801697\n",
      "Training Batch [407/782]: Loss 1.0246691703796387\n",
      "Training Batch [408/782]: Loss 1.1141990423202515\n",
      "Training Batch [409/782]: Loss 1.100224494934082\n",
      "Training Batch [410/782]: Loss 1.3582780361175537\n",
      "Training Batch [411/782]: Loss 0.7714148163795471\n",
      "Training Batch [412/782]: Loss 1.03431236743927\n",
      "Training Batch [413/782]: Loss 1.3766335248947144\n",
      "Training Batch [414/782]: Loss 0.9655964374542236\n",
      "Training Batch [415/782]: Loss 1.2018401622772217\n",
      "Training Batch [416/782]: Loss 1.1554011106491089\n",
      "Training Batch [417/782]: Loss 1.1655144691467285\n",
      "Training Batch [418/782]: Loss 1.018852949142456\n",
      "Training Batch [419/782]: Loss 1.032812237739563\n",
      "Training Batch [420/782]: Loss 1.0999912023544312\n",
      "Training Batch [421/782]: Loss 0.8191800713539124\n",
      "Training Batch [422/782]: Loss 1.2920364141464233\n",
      "Training Batch [423/782]: Loss 0.9993256330490112\n",
      "Training Batch [424/782]: Loss 0.9903380870819092\n",
      "Training Batch [425/782]: Loss 1.2360970973968506\n",
      "Training Batch [426/782]: Loss 1.2324891090393066\n",
      "Training Batch [427/782]: Loss 1.0303478240966797\n",
      "Training Batch [428/782]: Loss 1.1399750709533691\n",
      "Training Batch [429/782]: Loss 1.1335289478302002\n",
      "Training Batch [430/782]: Loss 1.1660391092300415\n",
      "Training Batch [431/782]: Loss 1.1950206756591797\n",
      "Training Batch [432/782]: Loss 1.0978283882141113\n",
      "Training Batch [433/782]: Loss 1.0512181520462036\n",
      "Training Batch [434/782]: Loss 1.10438871383667\n",
      "Training Batch [435/782]: Loss 0.8462149500846863\n",
      "Training Batch [436/782]: Loss 0.9778661727905273\n",
      "Training Batch [437/782]: Loss 1.081860899925232\n",
      "Training Batch [438/782]: Loss 0.982097327709198\n",
      "Training Batch [439/782]: Loss 1.2151027917861938\n",
      "Training Batch [440/782]: Loss 1.1339753866195679\n",
      "Training Batch [441/782]: Loss 1.223405122756958\n",
      "Training Batch [442/782]: Loss 1.158427119255066\n",
      "Training Batch [443/782]: Loss 0.963768720626831\n",
      "Training Batch [444/782]: Loss 1.0907227993011475\n",
      "Training Batch [445/782]: Loss 1.186671495437622\n",
      "Training Batch [446/782]: Loss 1.113279104232788\n",
      "Training Batch [447/782]: Loss 1.2010616064071655\n",
      "Training Batch [448/782]: Loss 1.234473466873169\n",
      "Training Batch [449/782]: Loss 0.9566722512245178\n",
      "Training Batch [450/782]: Loss 1.180974006652832\n",
      "Training Batch [451/782]: Loss 0.9975116848945618\n",
      "Training Batch [452/782]: Loss 1.049448013305664\n",
      "Training Batch [453/782]: Loss 1.0993690490722656\n",
      "Training Batch [454/782]: Loss 0.955750048160553\n",
      "Training Batch [455/782]: Loss 1.0928380489349365\n",
      "Training Batch [456/782]: Loss 1.3148314952850342\n",
      "Training Batch [457/782]: Loss 1.3382127285003662\n",
      "Training Batch [458/782]: Loss 1.1093735694885254\n",
      "Training Batch [459/782]: Loss 0.913935124874115\n",
      "Training Batch [460/782]: Loss 1.0610692501068115\n",
      "Training Batch [461/782]: Loss 0.999252438545227\n",
      "Training Batch [462/782]: Loss 1.2935315370559692\n",
      "Training Batch [463/782]: Loss 1.3283352851867676\n",
      "Training Batch [464/782]: Loss 0.902814507484436\n",
      "Training Batch [465/782]: Loss 0.7620867490768433\n",
      "Training Batch [466/782]: Loss 0.9253823161125183\n",
      "Training Batch [467/782]: Loss 1.185989260673523\n",
      "Training Batch [468/782]: Loss 1.2038143873214722\n",
      "Training Batch [469/782]: Loss 1.0286942720413208\n",
      "Training Batch [470/782]: Loss 1.0636237859725952\n",
      "Training Batch [471/782]: Loss 0.9489759802818298\n",
      "Training Batch [472/782]: Loss 0.8271490335464478\n",
      "Training Batch [473/782]: Loss 1.0414087772369385\n",
      "Training Batch [474/782]: Loss 0.9889501333236694\n",
      "Training Batch [475/782]: Loss 1.0426441431045532\n",
      "Training Batch [476/782]: Loss 1.1901893615722656\n",
      "Training Batch [477/782]: Loss 0.9301339387893677\n",
      "Training Batch [478/782]: Loss 0.8504697680473328\n",
      "Training Batch [479/782]: Loss 0.9154310822486877\n",
      "Training Batch [480/782]: Loss 1.2051670551300049\n",
      "Training Batch [481/782]: Loss 0.906882107257843\n",
      "Training Batch [482/782]: Loss 1.0756021738052368\n",
      "Training Batch [483/782]: Loss 1.0582804679870605\n",
      "Training Batch [484/782]: Loss 1.0134332180023193\n",
      "Training Batch [485/782]: Loss 1.0548826456069946\n",
      "Training Batch [486/782]: Loss 1.0570276975631714\n",
      "Training Batch [487/782]: Loss 1.0888235569000244\n",
      "Training Batch [488/782]: Loss 0.9635009765625\n",
      "Training Batch [489/782]: Loss 1.1607904434204102\n",
      "Training Batch [490/782]: Loss 1.0230461359024048\n",
      "Training Batch [491/782]: Loss 1.0677261352539062\n",
      "Training Batch [492/782]: Loss 1.096044898033142\n",
      "Training Batch [493/782]: Loss 1.1903504133224487\n",
      "Training Batch [494/782]: Loss 1.3030776977539062\n",
      "Training Batch [495/782]: Loss 1.2638713121414185\n",
      "Training Batch [496/782]: Loss 1.202012062072754\n",
      "Training Batch [497/782]: Loss 1.2344094514846802\n",
      "Training Batch [498/782]: Loss 1.1681482791900635\n",
      "Training Batch [499/782]: Loss 1.033823013305664\n",
      "Training Batch [500/782]: Loss 1.1779084205627441\n",
      "Training Batch [501/782]: Loss 0.9697439074516296\n",
      "Training Batch [502/782]: Loss 1.1452782154083252\n",
      "Training Batch [503/782]: Loss 1.1473379135131836\n",
      "Training Batch [504/782]: Loss 1.016384482383728\n",
      "Training Batch [505/782]: Loss 1.129414439201355\n",
      "Training Batch [506/782]: Loss 1.1624492406845093\n",
      "Training Batch [507/782]: Loss 1.0693905353546143\n",
      "Training Batch [508/782]: Loss 1.1098105907440186\n",
      "Training Batch [509/782]: Loss 1.0223239660263062\n",
      "Training Batch [510/782]: Loss 0.982953667640686\n",
      "Training Batch [511/782]: Loss 1.080539584159851\n",
      "Training Batch [512/782]: Loss 1.2391375303268433\n",
      "Training Batch [513/782]: Loss 1.2148559093475342\n",
      "Training Batch [514/782]: Loss 1.1083649396896362\n",
      "Training Batch [515/782]: Loss 1.0447008609771729\n",
      "Training Batch [516/782]: Loss 1.153564691543579\n",
      "Training Batch [517/782]: Loss 0.9851508736610413\n",
      "Training Batch [518/782]: Loss 0.8769524693489075\n",
      "Training Batch [519/782]: Loss 1.172034740447998\n",
      "Training Batch [520/782]: Loss 0.9281910061836243\n",
      "Training Batch [521/782]: Loss 0.9051589965820312\n",
      "Training Batch [522/782]: Loss 1.089928388595581\n",
      "Training Batch [523/782]: Loss 1.2658562660217285\n",
      "Training Batch [524/782]: Loss 1.0588752031326294\n",
      "Training Batch [525/782]: Loss 0.9634772539138794\n",
      "Training Batch [526/782]: Loss 0.7956977486610413\n",
      "Training Batch [527/782]: Loss 0.9811868071556091\n",
      "Training Batch [528/782]: Loss 1.1319208145141602\n",
      "Training Batch [529/782]: Loss 1.092484712600708\n",
      "Training Batch [530/782]: Loss 0.9674399495124817\n",
      "Training Batch [531/782]: Loss 1.2223713397979736\n",
      "Training Batch [532/782]: Loss 1.089505672454834\n",
      "Training Batch [533/782]: Loss 0.9363003969192505\n",
      "Training Batch [534/782]: Loss 1.2082380056381226\n",
      "Training Batch [535/782]: Loss 1.0297706127166748\n",
      "Training Batch [536/782]: Loss 1.275564432144165\n",
      "Training Batch [537/782]: Loss 1.1364850997924805\n",
      "Training Batch [538/782]: Loss 0.964941680431366\n",
      "Training Batch [539/782]: Loss 1.202303171157837\n",
      "Training Batch [540/782]: Loss 0.8535558581352234\n",
      "Training Batch [541/782]: Loss 1.2270281314849854\n",
      "Training Batch [542/782]: Loss 1.3170979022979736\n",
      "Training Batch [543/782]: Loss 1.0062259435653687\n",
      "Training Batch [544/782]: Loss 0.8573814630508423\n",
      "Training Batch [545/782]: Loss 0.9299501180648804\n",
      "Training Batch [546/782]: Loss 0.9088018536567688\n",
      "Training Batch [547/782]: Loss 1.1857861280441284\n",
      "Training Batch [548/782]: Loss 1.0373269319534302\n",
      "Training Batch [549/782]: Loss 0.9908507466316223\n",
      "Training Batch [550/782]: Loss 1.1640461683273315\n",
      "Training Batch [551/782]: Loss 0.890632688999176\n",
      "Training Batch [552/782]: Loss 1.1752640008926392\n",
      "Training Batch [553/782]: Loss 1.0731911659240723\n",
      "Training Batch [554/782]: Loss 1.096153736114502\n",
      "Training Batch [555/782]: Loss 0.9166574478149414\n",
      "Training Batch [556/782]: Loss 1.1920711994171143\n",
      "Training Batch [557/782]: Loss 1.449945092201233\n",
      "Training Batch [558/782]: Loss 1.1006810665130615\n",
      "Training Batch [559/782]: Loss 1.1791396141052246\n",
      "Training Batch [560/782]: Loss 0.9323105216026306\n",
      "Training Batch [561/782]: Loss 1.06980562210083\n",
      "Training Batch [562/782]: Loss 1.1099233627319336\n",
      "Training Batch [563/782]: Loss 0.9818285703659058\n",
      "Training Batch [564/782]: Loss 0.9909994006156921\n",
      "Training Batch [565/782]: Loss 1.1901572942733765\n",
      "Training Batch [566/782]: Loss 1.100415825843811\n",
      "Training Batch [567/782]: Loss 0.936083197593689\n",
      "Training Batch [568/782]: Loss 1.1638284921646118\n",
      "Training Batch [569/782]: Loss 0.9217191338539124\n",
      "Training Batch [570/782]: Loss 1.100555181503296\n",
      "Training Batch [571/782]: Loss 1.0877842903137207\n",
      "Training Batch [572/782]: Loss 1.1953638792037964\n",
      "Training Batch [573/782]: Loss 1.3487308025360107\n",
      "Training Batch [574/782]: Loss 1.0589858293533325\n",
      "Training Batch [575/782]: Loss 0.9377733469009399\n",
      "Training Batch [576/782]: Loss 0.8994774222373962\n",
      "Training Batch [577/782]: Loss 1.1360384225845337\n",
      "Training Batch [578/782]: Loss 1.151529312133789\n",
      "Training Batch [579/782]: Loss 1.1815043687820435\n",
      "Training Batch [580/782]: Loss 1.201324462890625\n",
      "Training Batch [581/782]: Loss 0.9198527932167053\n",
      "Training Batch [582/782]: Loss 1.2136942148208618\n",
      "Training Batch [583/782]: Loss 0.8413156867027283\n",
      "Training Batch [584/782]: Loss 1.104089617729187\n",
      "Training Batch [585/782]: Loss 1.2075451612472534\n",
      "Training Batch [586/782]: Loss 0.9110264182090759\n",
      "Training Batch [587/782]: Loss 1.1644117832183838\n",
      "Training Batch [588/782]: Loss 1.3165427446365356\n",
      "Training Batch [589/782]: Loss 1.0701193809509277\n",
      "Training Batch [590/782]: Loss 1.1569987535476685\n",
      "Training Batch [591/782]: Loss 1.3105340003967285\n",
      "Training Batch [592/782]: Loss 1.3304530382156372\n",
      "Training Batch [593/782]: Loss 1.202351450920105\n",
      "Training Batch [594/782]: Loss 0.8875000476837158\n",
      "Training Batch [595/782]: Loss 1.3972830772399902\n",
      "Training Batch [596/782]: Loss 0.8988502025604248\n",
      "Training Batch [597/782]: Loss 0.9799850583076477\n",
      "Training Batch [598/782]: Loss 0.9950703978538513\n",
      "Training Batch [599/782]: Loss 1.3077688217163086\n",
      "Training Batch [600/782]: Loss 1.1976324319839478\n",
      "Training Batch [601/782]: Loss 1.2331494092941284\n",
      "Training Batch [602/782]: Loss 1.2049182653427124\n",
      "Training Batch [603/782]: Loss 1.137485384941101\n",
      "Training Batch [604/782]: Loss 1.2310447692871094\n",
      "Training Batch [605/782]: Loss 1.0294857025146484\n",
      "Training Batch [606/782]: Loss 0.9367846846580505\n",
      "Training Batch [607/782]: Loss 0.8137391805648804\n",
      "Training Batch [608/782]: Loss 1.333165168762207\n",
      "Training Batch [609/782]: Loss 1.2958743572235107\n",
      "Training Batch [610/782]: Loss 1.2359721660614014\n",
      "Training Batch [611/782]: Loss 1.3546463251113892\n",
      "Training Batch [612/782]: Loss 1.2691947221755981\n",
      "Training Batch [613/782]: Loss 1.188721776008606\n",
      "Training Batch [614/782]: Loss 1.1388635635375977\n",
      "Training Batch [615/782]: Loss 0.9405565857887268\n",
      "Training Batch [616/782]: Loss 1.0057902336120605\n",
      "Training Batch [617/782]: Loss 1.378993272781372\n",
      "Training Batch [618/782]: Loss 0.9427319169044495\n",
      "Training Batch [619/782]: Loss 1.0909180641174316\n",
      "Training Batch [620/782]: Loss 1.1666340827941895\n",
      "Training Batch [621/782]: Loss 1.118750810623169\n",
      "Training Batch [622/782]: Loss 1.2489327192306519\n",
      "Training Batch [623/782]: Loss 0.8480031490325928\n",
      "Training Batch [624/782]: Loss 0.8051429986953735\n",
      "Training Batch [625/782]: Loss 1.0582996606826782\n",
      "Training Batch [626/782]: Loss 1.3877673149108887\n",
      "Training Batch [627/782]: Loss 1.1100997924804688\n",
      "Training Batch [628/782]: Loss 1.0533450841903687\n",
      "Training Batch [629/782]: Loss 1.1581456661224365\n",
      "Training Batch [630/782]: Loss 1.2122437953948975\n",
      "Training Batch [631/782]: Loss 0.8879277110099792\n",
      "Training Batch [632/782]: Loss 1.2357957363128662\n",
      "Training Batch [633/782]: Loss 1.2281134128570557\n",
      "Training Batch [634/782]: Loss 0.8941749930381775\n",
      "Training Batch [635/782]: Loss 1.1311067342758179\n",
      "Training Batch [636/782]: Loss 1.377805471420288\n",
      "Training Batch [637/782]: Loss 1.0813336372375488\n",
      "Training Batch [638/782]: Loss 1.1533598899841309\n",
      "Training Batch [639/782]: Loss 0.8780755400657654\n",
      "Training Batch [640/782]: Loss 1.3000972270965576\n",
      "Training Batch [641/782]: Loss 1.3057572841644287\n",
      "Training Batch [642/782]: Loss 1.4574567079544067\n",
      "Training Batch [643/782]: Loss 1.0059127807617188\n",
      "Training Batch [644/782]: Loss 0.9362384080886841\n",
      "Training Batch [645/782]: Loss 1.0489650964736938\n",
      "Training Batch [646/782]: Loss 1.2262743711471558\n",
      "Training Batch [647/782]: Loss 0.8590892553329468\n",
      "Training Batch [648/782]: Loss 1.3045668601989746\n",
      "Training Batch [649/782]: Loss 1.1825549602508545\n",
      "Training Batch [650/782]: Loss 1.0749883651733398\n",
      "Training Batch [651/782]: Loss 0.8768299221992493\n",
      "Training Batch [652/782]: Loss 1.1059070825576782\n",
      "Training Batch [653/782]: Loss 0.9169278740882874\n",
      "Training Batch [654/782]: Loss 1.1152366399765015\n",
      "Training Batch [655/782]: Loss 1.1174323558807373\n",
      "Training Batch [656/782]: Loss 0.9566550254821777\n",
      "Training Batch [657/782]: Loss 0.9626538753509521\n",
      "Training Batch [658/782]: Loss 1.2646859884262085\n",
      "Training Batch [659/782]: Loss 1.1325201988220215\n",
      "Training Batch [660/782]: Loss 1.0888558626174927\n",
      "Training Batch [661/782]: Loss 0.9610534310340881\n",
      "Training Batch [662/782]: Loss 0.9527509212493896\n",
      "Training Batch [663/782]: Loss 0.916920006275177\n",
      "Training Batch [664/782]: Loss 0.9927300214767456\n",
      "Training Batch [665/782]: Loss 1.269148349761963\n",
      "Training Batch [666/782]: Loss 0.9313058853149414\n",
      "Training Batch [667/782]: Loss 1.1577156782150269\n",
      "Training Batch [668/782]: Loss 1.2135114669799805\n",
      "Training Batch [669/782]: Loss 1.1093989610671997\n",
      "Training Batch [670/782]: Loss 1.1920191049575806\n",
      "Training Batch [671/782]: Loss 0.9429326057434082\n",
      "Training Batch [672/782]: Loss 1.0643202066421509\n",
      "Training Batch [673/782]: Loss 1.1818740367889404\n",
      "Training Batch [674/782]: Loss 0.997527003288269\n",
      "Training Batch [675/782]: Loss 1.14128839969635\n",
      "Training Batch [676/782]: Loss 1.2562103271484375\n",
      "Training Batch [677/782]: Loss 1.0938633680343628\n",
      "Training Batch [678/782]: Loss 0.9752752780914307\n",
      "Training Batch [679/782]: Loss 1.0632528066635132\n",
      "Training Batch [680/782]: Loss 1.0156863927841187\n",
      "Training Batch [681/782]: Loss 1.3615591526031494\n",
      "Training Batch [682/782]: Loss 1.02006196975708\n",
      "Training Batch [683/782]: Loss 1.2365895509719849\n",
      "Training Batch [684/782]: Loss 1.0726780891418457\n",
      "Training Batch [685/782]: Loss 1.0835306644439697\n",
      "Training Batch [686/782]: Loss 1.0698832273483276\n",
      "Training Batch [687/782]: Loss 1.2584342956542969\n",
      "Training Batch [688/782]: Loss 0.9917348623275757\n",
      "Training Batch [689/782]: Loss 1.048097848892212\n",
      "Training Batch [690/782]: Loss 1.0418623685836792\n",
      "Training Batch [691/782]: Loss 0.8115670084953308\n",
      "Training Batch [692/782]: Loss 0.7721379399299622\n",
      "Training Batch [693/782]: Loss 1.0747381448745728\n",
      "Training Batch [694/782]: Loss 1.14612877368927\n",
      "Training Batch [695/782]: Loss 1.0168575048446655\n",
      "Training Batch [696/782]: Loss 1.348073959350586\n",
      "Training Batch [697/782]: Loss 1.165787696838379\n",
      "Training Batch [698/782]: Loss 1.125040888786316\n",
      "Training Batch [699/782]: Loss 0.9684122800827026\n",
      "Training Batch [700/782]: Loss 0.8613239526748657\n",
      "Training Batch [701/782]: Loss 1.0174282789230347\n",
      "Training Batch [702/782]: Loss 1.0076813697814941\n",
      "Training Batch [703/782]: Loss 0.9109894037246704\n",
      "Training Batch [704/782]: Loss 1.2171951532363892\n",
      "Training Batch [705/782]: Loss 1.1526845693588257\n",
      "Training Batch [706/782]: Loss 0.9995863437652588\n",
      "Training Batch [707/782]: Loss 0.848828136920929\n",
      "Training Batch [708/782]: Loss 1.1409951448440552\n",
      "Training Batch [709/782]: Loss 1.3031044006347656\n",
      "Training Batch [710/782]: Loss 0.8443050384521484\n",
      "Training Batch [711/782]: Loss 0.9535907506942749\n",
      "Training Batch [712/782]: Loss 1.095300316810608\n",
      "Training Batch [713/782]: Loss 1.0962563753128052\n",
      "Training Batch [714/782]: Loss 0.9610219597816467\n",
      "Training Batch [715/782]: Loss 0.8607025146484375\n",
      "Training Batch [716/782]: Loss 0.817333459854126\n",
      "Training Batch [717/782]: Loss 1.1242194175720215\n",
      "Training Batch [718/782]: Loss 1.0418411493301392\n",
      "Training Batch [719/782]: Loss 0.9986122250556946\n",
      "Training Batch [720/782]: Loss 1.0786938667297363\n",
      "Training Batch [721/782]: Loss 0.9898340702056885\n",
      "Training Batch [722/782]: Loss 1.0369356870651245\n",
      "Training Batch [723/782]: Loss 1.0871905088424683\n",
      "Training Batch [724/782]: Loss 1.2362197637557983\n",
      "Training Batch [725/782]: Loss 1.1500612497329712\n",
      "Training Batch [726/782]: Loss 1.0558185577392578\n",
      "Training Batch [727/782]: Loss 1.3774781227111816\n",
      "Training Batch [728/782]: Loss 1.0049077272415161\n",
      "Training Batch [729/782]: Loss 1.1339772939682007\n",
      "Training Batch [730/782]: Loss 0.8803063035011292\n",
      "Training Batch [731/782]: Loss 1.0858207941055298\n",
      "Training Batch [732/782]: Loss 0.857453465461731\n",
      "Training Batch [733/782]: Loss 1.038780689239502\n",
      "Training Batch [734/782]: Loss 1.3169227838516235\n",
      "Training Batch [735/782]: Loss 1.3056241273880005\n",
      "Training Batch [736/782]: Loss 1.0544087886810303\n",
      "Training Batch [737/782]: Loss 1.3671824932098389\n",
      "Training Batch [738/782]: Loss 0.978568971157074\n",
      "Training Batch [739/782]: Loss 0.9461489915847778\n",
      "Training Batch [740/782]: Loss 1.3380731344223022\n",
      "Training Batch [741/782]: Loss 1.1118422746658325\n",
      "Training Batch [742/782]: Loss 0.8984450101852417\n",
      "Training Batch [743/782]: Loss 1.0991668701171875\n",
      "Training Batch [744/782]: Loss 1.1560215950012207\n",
      "Training Batch [745/782]: Loss 1.1068246364593506\n",
      "Training Batch [746/782]: Loss 1.1583071947097778\n",
      "Training Batch [747/782]: Loss 1.0598307847976685\n",
      "Training Batch [748/782]: Loss 1.288903832435608\n",
      "Training Batch [749/782]: Loss 1.000465989112854\n",
      "Training Batch [750/782]: Loss 1.059889554977417\n",
      "Training Batch [751/782]: Loss 1.1298980712890625\n",
      "Training Batch [752/782]: Loss 0.926089882850647\n",
      "Training Batch [753/782]: Loss 1.1388964653015137\n",
      "Training Batch [754/782]: Loss 1.137943148612976\n",
      "Training Batch [755/782]: Loss 1.0863990783691406\n",
      "Training Batch [756/782]: Loss 0.928070068359375\n",
      "Training Batch [757/782]: Loss 1.181282639503479\n",
      "Training Batch [758/782]: Loss 0.9347710609436035\n",
      "Training Batch [759/782]: Loss 0.9243398308753967\n",
      "Training Batch [760/782]: Loss 1.4020637273788452\n",
      "Training Batch [761/782]: Loss 0.8231487274169922\n",
      "Training Batch [762/782]: Loss 0.9942467212677002\n",
      "Training Batch [763/782]: Loss 1.05857253074646\n",
      "Training Batch [764/782]: Loss 1.055702805519104\n",
      "Training Batch [765/782]: Loss 1.0898716449737549\n",
      "Training Batch [766/782]: Loss 1.0620229244232178\n",
      "Training Batch [767/782]: Loss 0.9046597480773926\n",
      "Training Batch [768/782]: Loss 1.2484594583511353\n",
      "Training Batch [769/782]: Loss 0.7837203145027161\n",
      "Training Batch [770/782]: Loss 1.3096524477005005\n",
      "Training Batch [771/782]: Loss 1.1846946477890015\n",
      "Training Batch [772/782]: Loss 1.1653002500534058\n",
      "Training Batch [773/782]: Loss 1.0804953575134277\n",
      "Training Batch [774/782]: Loss 1.1211192607879639\n",
      "Training Batch [775/782]: Loss 1.1239084005355835\n",
      "Training Batch [776/782]: Loss 1.1811953783035278\n",
      "Training Batch [777/782]: Loss 0.8304005861282349\n",
      "Training Batch [778/782]: Loss 1.0546317100524902\n",
      "Training Batch [779/782]: Loss 1.0390756130218506\n",
      "Training Batch [780/782]: Loss 1.1841872930526733\n",
      "Training Batch [781/782]: Loss 0.8768166899681091\n",
      "Training Batch [782/782]: Loss 0.8947252035140991\n",
      "Epoch 4 - Train Loss: 1.0961\n",
      "*********  Epoch 5/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.86085045337677\n",
      "Training Batch [2/782]: Loss 0.9971923232078552\n",
      "Training Batch [3/782]: Loss 0.9103062152862549\n",
      "Training Batch [4/782]: Loss 0.896773099899292\n",
      "Training Batch [5/782]: Loss 0.9750640392303467\n",
      "Training Batch [6/782]: Loss 1.2752254009246826\n",
      "Training Batch [7/782]: Loss 1.0048805475234985\n",
      "Training Batch [8/782]: Loss 0.827017605304718\n",
      "Training Batch [9/782]: Loss 0.9521006345748901\n",
      "Training Batch [10/782]: Loss 0.9269885420799255\n",
      "Training Batch [11/782]: Loss 0.9615671038627625\n",
      "Training Batch [12/782]: Loss 0.9071106910705566\n",
      "Training Batch [13/782]: Loss 1.0126230716705322\n",
      "Training Batch [14/782]: Loss 0.9281578660011292\n",
      "Training Batch [15/782]: Loss 0.9500395059585571\n",
      "Training Batch [16/782]: Loss 0.9941131472587585\n",
      "Training Batch [17/782]: Loss 1.0426025390625\n",
      "Training Batch [18/782]: Loss 0.8729043006896973\n",
      "Training Batch [19/782]: Loss 0.7761616706848145\n",
      "Training Batch [20/782]: Loss 0.8410122394561768\n",
      "Training Batch [21/782]: Loss 0.873001754283905\n",
      "Training Batch [22/782]: Loss 0.7981579303741455\n",
      "Training Batch [23/782]: Loss 1.0877069234848022\n",
      "Training Batch [24/782]: Loss 1.1034783124923706\n",
      "Training Batch [25/782]: Loss 1.0571823120117188\n",
      "Training Batch [26/782]: Loss 1.0159441232681274\n",
      "Training Batch [27/782]: Loss 1.09710693359375\n",
      "Training Batch [28/782]: Loss 0.976976752281189\n",
      "Training Batch [29/782]: Loss 0.8411040306091309\n",
      "Training Batch [30/782]: Loss 0.9334489703178406\n",
      "Training Batch [31/782]: Loss 0.9064446091651917\n",
      "Training Batch [32/782]: Loss 1.051254391670227\n",
      "Training Batch [33/782]: Loss 0.9320709109306335\n",
      "Training Batch [34/782]: Loss 0.6967823505401611\n",
      "Training Batch [35/782]: Loss 1.0346357822418213\n",
      "Training Batch [36/782]: Loss 0.6799721717834473\n",
      "Training Batch [37/782]: Loss 0.8968857526779175\n",
      "Training Batch [38/782]: Loss 0.9048272967338562\n",
      "Training Batch [39/782]: Loss 0.8336427211761475\n",
      "Training Batch [40/782]: Loss 0.7894858717918396\n",
      "Training Batch [41/782]: Loss 0.8714384436607361\n",
      "Training Batch [42/782]: Loss 0.8582547307014465\n",
      "Training Batch [43/782]: Loss 0.7530850172042847\n",
      "Training Batch [44/782]: Loss 0.7527941465377808\n",
      "Training Batch [45/782]: Loss 0.6824489831924438\n",
      "Training Batch [46/782]: Loss 0.8398387432098389\n",
      "Training Batch [47/782]: Loss 1.1523958444595337\n",
      "Training Batch [48/782]: Loss 0.9308069348335266\n",
      "Training Batch [49/782]: Loss 0.7517715692520142\n",
      "Training Batch [50/782]: Loss 0.8208628296852112\n",
      "Training Batch [51/782]: Loss 0.9050402641296387\n",
      "Training Batch [52/782]: Loss 1.0438427925109863\n",
      "Training Batch [53/782]: Loss 1.0086992979049683\n",
      "Training Batch [54/782]: Loss 0.7285502552986145\n",
      "Training Batch [55/782]: Loss 0.8338526487350464\n",
      "Training Batch [56/782]: Loss 0.9116150140762329\n",
      "Training Batch [57/782]: Loss 0.9358333945274353\n",
      "Training Batch [58/782]: Loss 0.9554604887962341\n",
      "Training Batch [59/782]: Loss 0.9358304738998413\n",
      "Training Batch [60/782]: Loss 0.800993800163269\n",
      "Training Batch [61/782]: Loss 1.0549755096435547\n",
      "Training Batch [62/782]: Loss 0.7273529767990112\n",
      "Training Batch [63/782]: Loss 0.7984790802001953\n",
      "Training Batch [64/782]: Loss 1.0544686317443848\n",
      "Training Batch [65/782]: Loss 0.9687157273292542\n",
      "Training Batch [66/782]: Loss 0.8502630591392517\n",
      "Training Batch [67/782]: Loss 0.9377173185348511\n",
      "Training Batch [68/782]: Loss 0.736991286277771\n",
      "Training Batch [69/782]: Loss 0.833946704864502\n",
      "Training Batch [70/782]: Loss 0.7149353623390198\n",
      "Training Batch [71/782]: Loss 0.9115371108055115\n",
      "Training Batch [72/782]: Loss 0.8050354719161987\n",
      "Training Batch [73/782]: Loss 0.9282801747322083\n",
      "Training Batch [74/782]: Loss 1.1730616092681885\n",
      "Training Batch [75/782]: Loss 0.8706544637680054\n",
      "Training Batch [76/782]: Loss 0.7396227121353149\n",
      "Training Batch [77/782]: Loss 0.9750412106513977\n",
      "Training Batch [78/782]: Loss 0.8609507083892822\n",
      "Training Batch [79/782]: Loss 0.9702710509300232\n",
      "Training Batch [80/782]: Loss 0.8677513003349304\n",
      "Training Batch [81/782]: Loss 1.1097878217697144\n",
      "Training Batch [82/782]: Loss 0.8402396440505981\n",
      "Training Batch [83/782]: Loss 0.8574315309524536\n",
      "Training Batch [84/782]: Loss 0.8935709595680237\n",
      "Training Batch [85/782]: Loss 0.8639523386955261\n",
      "Training Batch [86/782]: Loss 0.7352698445320129\n",
      "Training Batch [87/782]: Loss 0.9087690114974976\n",
      "Training Batch [88/782]: Loss 1.0385278463363647\n",
      "Training Batch [89/782]: Loss 1.0249288082122803\n",
      "Training Batch [90/782]: Loss 0.9045789837837219\n",
      "Training Batch [91/782]: Loss 0.8048756718635559\n",
      "Training Batch [92/782]: Loss 1.0351366996765137\n",
      "Training Batch [93/782]: Loss 1.0134427547454834\n",
      "Training Batch [94/782]: Loss 0.8140603303909302\n",
      "Training Batch [95/782]: Loss 0.7189372777938843\n",
      "Training Batch [96/782]: Loss 0.7567781209945679\n",
      "Training Batch [97/782]: Loss 0.7512269616127014\n",
      "Training Batch [98/782]: Loss 0.9008281826972961\n",
      "Training Batch [99/782]: Loss 0.7629401087760925\n",
      "Training Batch [100/782]: Loss 1.0829726457595825\n",
      "Training Batch [101/782]: Loss 0.8197186589241028\n",
      "Training Batch [102/782]: Loss 0.9901480674743652\n",
      "Training Batch [103/782]: Loss 0.9034377336502075\n",
      "Training Batch [104/782]: Loss 0.8815903663635254\n",
      "Training Batch [105/782]: Loss 0.9879283308982849\n",
      "Training Batch [106/782]: Loss 0.854867696762085\n",
      "Training Batch [107/782]: Loss 1.0040346384048462\n",
      "Training Batch [108/782]: Loss 0.8931729793548584\n",
      "Training Batch [109/782]: Loss 0.9364383220672607\n",
      "Training Batch [110/782]: Loss 0.8887842893600464\n",
      "Training Batch [111/782]: Loss 0.9207690358161926\n",
      "Training Batch [112/782]: Loss 0.643301248550415\n",
      "Training Batch [113/782]: Loss 0.7253844141960144\n",
      "Training Batch [114/782]: Loss 1.0469253063201904\n",
      "Training Batch [115/782]: Loss 1.0397701263427734\n",
      "Training Batch [116/782]: Loss 0.7423222661018372\n",
      "Training Batch [117/782]: Loss 0.7814253568649292\n",
      "Training Batch [118/782]: Loss 0.7047567367553711\n",
      "Training Batch [119/782]: Loss 0.9239881038665771\n",
      "Training Batch [120/782]: Loss 1.0036394596099854\n",
      "Training Batch [121/782]: Loss 1.1193335056304932\n",
      "Training Batch [122/782]: Loss 0.5968886017799377\n",
      "Training Batch [123/782]: Loss 0.9173319935798645\n",
      "Training Batch [124/782]: Loss 0.7590613961219788\n",
      "Training Batch [125/782]: Loss 1.0831011533737183\n",
      "Training Batch [126/782]: Loss 0.7688595652580261\n",
      "Training Batch [127/782]: Loss 1.2085845470428467\n",
      "Training Batch [128/782]: Loss 0.9923490285873413\n",
      "Training Batch [129/782]: Loss 0.9669508934020996\n",
      "Training Batch [130/782]: Loss 0.8673741221427917\n",
      "Training Batch [131/782]: Loss 0.8965703248977661\n",
      "Training Batch [132/782]: Loss 0.870130181312561\n",
      "Training Batch [133/782]: Loss 0.9417040944099426\n",
      "Training Batch [134/782]: Loss 1.254060983657837\n",
      "Training Batch [135/782]: Loss 0.7281410098075867\n",
      "Training Batch [136/782]: Loss 0.7759001851081848\n",
      "Training Batch [137/782]: Loss 1.0934195518493652\n",
      "Training Batch [138/782]: Loss 0.9329820275306702\n",
      "Training Batch [139/782]: Loss 0.9025335311889648\n",
      "Training Batch [140/782]: Loss 0.9525330066680908\n",
      "Training Batch [141/782]: Loss 1.313935399055481\n",
      "Training Batch [142/782]: Loss 1.1442803144454956\n",
      "Training Batch [143/782]: Loss 0.9366116523742676\n",
      "Training Batch [144/782]: Loss 0.8565341234207153\n",
      "Training Batch [145/782]: Loss 1.0476021766662598\n",
      "Training Batch [146/782]: Loss 1.0847632884979248\n",
      "Training Batch [147/782]: Loss 0.8975496888160706\n",
      "Training Batch [148/782]: Loss 1.1899229288101196\n",
      "Training Batch [149/782]: Loss 0.8775278329849243\n",
      "Training Batch [150/782]: Loss 0.9424710869789124\n",
      "Training Batch [151/782]: Loss 0.6951481699943542\n",
      "Training Batch [152/782]: Loss 1.0093251466751099\n",
      "Training Batch [153/782]: Loss 0.8606574535369873\n",
      "Training Batch [154/782]: Loss 0.9167641997337341\n",
      "Training Batch [155/782]: Loss 1.1381040811538696\n",
      "Training Batch [156/782]: Loss 1.435091495513916\n",
      "Training Batch [157/782]: Loss 0.8155078291893005\n",
      "Training Batch [158/782]: Loss 0.9943256974220276\n",
      "Training Batch [159/782]: Loss 1.0301735401153564\n",
      "Training Batch [160/782]: Loss 1.1253116130828857\n",
      "Training Batch [161/782]: Loss 1.3083248138427734\n",
      "Training Batch [162/782]: Loss 0.9186034798622131\n",
      "Training Batch [163/782]: Loss 0.8057656288146973\n",
      "Training Batch [164/782]: Loss 0.9533082246780396\n",
      "Training Batch [165/782]: Loss 0.7793415188789368\n",
      "Training Batch [166/782]: Loss 0.8983591198921204\n",
      "Training Batch [167/782]: Loss 0.8409178853034973\n",
      "Training Batch [168/782]: Loss 0.9569094777107239\n",
      "Training Batch [169/782]: Loss 0.9392711520195007\n",
      "Training Batch [170/782]: Loss 0.8213796019554138\n",
      "Training Batch [171/782]: Loss 0.9192689061164856\n",
      "Training Batch [172/782]: Loss 0.8812723755836487\n",
      "Training Batch [173/782]: Loss 0.9229026436805725\n",
      "Training Batch [174/782]: Loss 0.7742246985435486\n",
      "Training Batch [175/782]: Loss 0.9485335946083069\n",
      "Training Batch [176/782]: Loss 0.6260738372802734\n",
      "Training Batch [177/782]: Loss 1.036806583404541\n",
      "Training Batch [178/782]: Loss 0.9926838874816895\n",
      "Training Batch [179/782]: Loss 0.7803118228912354\n",
      "Training Batch [180/782]: Loss 0.8235907554626465\n",
      "Training Batch [181/782]: Loss 0.8954420685768127\n",
      "Training Batch [182/782]: Loss 1.060839056968689\n",
      "Training Batch [183/782]: Loss 0.8785048127174377\n",
      "Training Batch [184/782]: Loss 0.6554601788520813\n",
      "Training Batch [185/782]: Loss 0.9748857617378235\n",
      "Training Batch [186/782]: Loss 0.8957108855247498\n",
      "Training Batch [187/782]: Loss 0.7469368577003479\n",
      "Training Batch [188/782]: Loss 0.7776305079460144\n",
      "Training Batch [189/782]: Loss 0.8635676503181458\n",
      "Training Batch [190/782]: Loss 1.055798053741455\n",
      "Training Batch [191/782]: Loss 0.6804584860801697\n",
      "Training Batch [192/782]: Loss 0.8856364488601685\n",
      "Training Batch [193/782]: Loss 0.9762961268424988\n",
      "Training Batch [194/782]: Loss 0.9492788910865784\n",
      "Training Batch [195/782]: Loss 0.9024071097373962\n",
      "Training Batch [196/782]: Loss 0.9505696296691895\n",
      "Training Batch [197/782]: Loss 1.1691216230392456\n",
      "Training Batch [198/782]: Loss 0.8963479399681091\n",
      "Training Batch [199/782]: Loss 0.9867282509803772\n",
      "Training Batch [200/782]: Loss 1.0167008638381958\n",
      "Training Batch [201/782]: Loss 0.717523992061615\n",
      "Training Batch [202/782]: Loss 1.024775505065918\n",
      "Training Batch [203/782]: Loss 0.9089272618293762\n",
      "Training Batch [204/782]: Loss 1.1060256958007812\n",
      "Training Batch [205/782]: Loss 0.6768900156021118\n",
      "Training Batch [206/782]: Loss 0.9067918062210083\n",
      "Training Batch [207/782]: Loss 0.7235537767410278\n",
      "Training Batch [208/782]: Loss 1.0250370502471924\n",
      "Training Batch [209/782]: Loss 0.8916402459144592\n",
      "Training Batch [210/782]: Loss 0.8400517702102661\n",
      "Training Batch [211/782]: Loss 0.8209319710731506\n",
      "Training Batch [212/782]: Loss 1.109314203262329\n",
      "Training Batch [213/782]: Loss 0.8175795078277588\n",
      "Training Batch [214/782]: Loss 1.1229387521743774\n",
      "Training Batch [215/782]: Loss 0.991553544998169\n",
      "Training Batch [216/782]: Loss 0.7354071140289307\n",
      "Training Batch [217/782]: Loss 0.9460115432739258\n",
      "Training Batch [218/782]: Loss 0.6981600522994995\n",
      "Training Batch [219/782]: Loss 1.0137091875076294\n",
      "Training Batch [220/782]: Loss 1.0935271978378296\n",
      "Training Batch [221/782]: Loss 0.8770115375518799\n",
      "Training Batch [222/782]: Loss 1.0694694519042969\n",
      "Training Batch [223/782]: Loss 1.0315990447998047\n",
      "Training Batch [224/782]: Loss 0.9683690071105957\n",
      "Training Batch [225/782]: Loss 1.0924280881881714\n",
      "Training Batch [226/782]: Loss 0.7975283265113831\n",
      "Training Batch [227/782]: Loss 0.8302403688430786\n",
      "Training Batch [228/782]: Loss 1.015851378440857\n",
      "Training Batch [229/782]: Loss 0.8821088671684265\n",
      "Training Batch [230/782]: Loss 0.8591715693473816\n",
      "Training Batch [231/782]: Loss 1.02704918384552\n",
      "Training Batch [232/782]: Loss 0.8860612511634827\n",
      "Training Batch [233/782]: Loss 1.2155427932739258\n",
      "Training Batch [234/782]: Loss 1.1500394344329834\n",
      "Training Batch [235/782]: Loss 0.8737392425537109\n",
      "Training Batch [236/782]: Loss 0.9217506647109985\n",
      "Training Batch [237/782]: Loss 1.0552200078964233\n",
      "Training Batch [238/782]: Loss 0.9705002903938293\n",
      "Training Batch [239/782]: Loss 1.1515388488769531\n",
      "Training Batch [240/782]: Loss 0.9737313389778137\n",
      "Training Batch [241/782]: Loss 0.8385875821113586\n",
      "Training Batch [242/782]: Loss 0.8739129304885864\n",
      "Training Batch [243/782]: Loss 0.9760364890098572\n",
      "Training Batch [244/782]: Loss 0.903089702129364\n",
      "Training Batch [245/782]: Loss 0.9245518445968628\n",
      "Training Batch [246/782]: Loss 0.8986879587173462\n",
      "Training Batch [247/782]: Loss 0.8574390411376953\n",
      "Training Batch [248/782]: Loss 0.9682270884513855\n",
      "Training Batch [249/782]: Loss 0.8917196393013\n",
      "Training Batch [250/782]: Loss 1.1033272743225098\n",
      "Training Batch [251/782]: Loss 0.9566571712493896\n",
      "Training Batch [252/782]: Loss 1.1165229082107544\n",
      "Training Batch [253/782]: Loss 0.8828632831573486\n",
      "Training Batch [254/782]: Loss 0.879090428352356\n",
      "Training Batch [255/782]: Loss 1.0026079416275024\n",
      "Training Batch [256/782]: Loss 1.1053110361099243\n",
      "Training Batch [257/782]: Loss 1.0088056325912476\n",
      "Training Batch [258/782]: Loss 0.806242823600769\n",
      "Training Batch [259/782]: Loss 1.2062387466430664\n",
      "Training Batch [260/782]: Loss 0.8320071697235107\n",
      "Training Batch [261/782]: Loss 0.8466399312019348\n",
      "Training Batch [262/782]: Loss 1.002315878868103\n",
      "Training Batch [263/782]: Loss 0.9690157771110535\n",
      "Training Batch [264/782]: Loss 1.003511905670166\n",
      "Training Batch [265/782]: Loss 0.808535635471344\n",
      "Training Batch [266/782]: Loss 1.0905717611312866\n",
      "Training Batch [267/782]: Loss 0.9644663333892822\n",
      "Training Batch [268/782]: Loss 0.7417619824409485\n",
      "Training Batch [269/782]: Loss 0.8701499700546265\n",
      "Training Batch [270/782]: Loss 1.1233749389648438\n",
      "Training Batch [271/782]: Loss 1.0182650089263916\n",
      "Training Batch [272/782]: Loss 0.7792891263961792\n",
      "Training Batch [273/782]: Loss 0.848285436630249\n",
      "Training Batch [274/782]: Loss 0.8056288957595825\n",
      "Training Batch [275/782]: Loss 0.9703111052513123\n",
      "Training Batch [276/782]: Loss 0.915368378162384\n",
      "Training Batch [277/782]: Loss 0.779233455657959\n",
      "Training Batch [278/782]: Loss 0.9319022297859192\n",
      "Training Batch [279/782]: Loss 0.7841389179229736\n",
      "Training Batch [280/782]: Loss 0.8480798006057739\n",
      "Training Batch [281/782]: Loss 1.0534672737121582\n",
      "Training Batch [282/782]: Loss 0.7802996635437012\n",
      "Training Batch [283/782]: Loss 0.8235282301902771\n",
      "Training Batch [284/782]: Loss 1.074314832687378\n",
      "Training Batch [285/782]: Loss 1.1530061960220337\n",
      "Training Batch [286/782]: Loss 0.6659660935401917\n",
      "Training Batch [287/782]: Loss 1.0318009853363037\n",
      "Training Batch [288/782]: Loss 0.8891192674636841\n",
      "Training Batch [289/782]: Loss 1.1739168167114258\n",
      "Training Batch [290/782]: Loss 0.7842822074890137\n",
      "Training Batch [291/782]: Loss 0.9057220816612244\n",
      "Training Batch [292/782]: Loss 0.8255853652954102\n",
      "Training Batch [293/782]: Loss 0.7048123478889465\n",
      "Training Batch [294/782]: Loss 1.0093798637390137\n",
      "Training Batch [295/782]: Loss 0.818720817565918\n",
      "Training Batch [296/782]: Loss 0.7458950877189636\n",
      "Training Batch [297/782]: Loss 0.908876895904541\n",
      "Training Batch [298/782]: Loss 1.0172357559204102\n",
      "Training Batch [299/782]: Loss 1.148853063583374\n",
      "Training Batch [300/782]: Loss 0.7181007862091064\n",
      "Training Batch [301/782]: Loss 0.9838249683380127\n",
      "Training Batch [302/782]: Loss 0.9723401665687561\n",
      "Training Batch [303/782]: Loss 1.2302104234695435\n",
      "Training Batch [304/782]: Loss 0.9867364764213562\n",
      "Training Batch [305/782]: Loss 0.9998141527175903\n",
      "Training Batch [306/782]: Loss 0.7773950099945068\n",
      "Training Batch [307/782]: Loss 1.0376567840576172\n",
      "Training Batch [308/782]: Loss 1.087393045425415\n",
      "Training Batch [309/782]: Loss 1.0212730169296265\n",
      "Training Batch [310/782]: Loss 1.0244836807250977\n",
      "Training Batch [311/782]: Loss 0.8154810667037964\n",
      "Training Batch [312/782]: Loss 1.0010145902633667\n",
      "Training Batch [313/782]: Loss 1.196908712387085\n",
      "Training Batch [314/782]: Loss 0.878422737121582\n",
      "Training Batch [315/782]: Loss 0.9256791472434998\n",
      "Training Batch [316/782]: Loss 0.7995516061782837\n",
      "Training Batch [317/782]: Loss 1.0125179290771484\n",
      "Training Batch [318/782]: Loss 0.8027952909469604\n",
      "Training Batch [319/782]: Loss 1.00797438621521\n",
      "Training Batch [320/782]: Loss 0.8617576956748962\n",
      "Training Batch [321/782]: Loss 0.7360199093818665\n",
      "Training Batch [322/782]: Loss 0.909871518611908\n",
      "Training Batch [323/782]: Loss 0.8390828371047974\n",
      "Training Batch [324/782]: Loss 0.8450272083282471\n",
      "Training Batch [325/782]: Loss 0.6624053120613098\n",
      "Training Batch [326/782]: Loss 0.9273110032081604\n",
      "Training Batch [327/782]: Loss 1.0764703750610352\n",
      "Training Batch [328/782]: Loss 0.9851586818695068\n",
      "Training Batch [329/782]: Loss 1.0415054559707642\n",
      "Training Batch [330/782]: Loss 0.9429426193237305\n",
      "Training Batch [331/782]: Loss 0.9766197204589844\n",
      "Training Batch [332/782]: Loss 0.7928227782249451\n",
      "Training Batch [333/782]: Loss 0.9594185948371887\n",
      "Training Batch [334/782]: Loss 0.943621039390564\n",
      "Training Batch [335/782]: Loss 0.9819754362106323\n",
      "Training Batch [336/782]: Loss 0.906731903553009\n",
      "Training Batch [337/782]: Loss 0.8213265538215637\n",
      "Training Batch [338/782]: Loss 0.9835424423217773\n",
      "Training Batch [339/782]: Loss 0.8758598566055298\n",
      "Training Batch [340/782]: Loss 1.1645394563674927\n",
      "Training Batch [341/782]: Loss 0.9509550929069519\n",
      "Training Batch [342/782]: Loss 0.7786598801612854\n",
      "Training Batch [343/782]: Loss 0.7409722208976746\n",
      "Training Batch [344/782]: Loss 0.8849503397941589\n",
      "Training Batch [345/782]: Loss 1.0467089414596558\n",
      "Training Batch [346/782]: Loss 0.8532467484474182\n",
      "Training Batch [347/782]: Loss 1.0858794450759888\n",
      "Training Batch [348/782]: Loss 0.8610199093818665\n",
      "Training Batch [349/782]: Loss 0.7814878821372986\n",
      "Training Batch [350/782]: Loss 0.8557007908821106\n",
      "Training Batch [351/782]: Loss 0.9176992774009705\n",
      "Training Batch [352/782]: Loss 1.0142143964767456\n",
      "Training Batch [353/782]: Loss 0.7833159565925598\n",
      "Training Batch [354/782]: Loss 0.9644216895103455\n",
      "Training Batch [355/782]: Loss 0.7929771542549133\n",
      "Training Batch [356/782]: Loss 0.6353620290756226\n",
      "Training Batch [357/782]: Loss 0.791123628616333\n",
      "Training Batch [358/782]: Loss 0.8683294057846069\n",
      "Training Batch [359/782]: Loss 0.9771461486816406\n",
      "Training Batch [360/782]: Loss 0.9879631996154785\n",
      "Training Batch [361/782]: Loss 1.1220893859863281\n",
      "Training Batch [362/782]: Loss 0.6646242141723633\n",
      "Training Batch [363/782]: Loss 0.6541287899017334\n",
      "Training Batch [364/782]: Loss 0.9941253066062927\n",
      "Training Batch [365/782]: Loss 1.150800347328186\n",
      "Training Batch [366/782]: Loss 0.8966633677482605\n",
      "Training Batch [367/782]: Loss 0.7953230738639832\n",
      "Training Batch [368/782]: Loss 0.7413817644119263\n",
      "Training Batch [369/782]: Loss 0.9355067014694214\n",
      "Training Batch [370/782]: Loss 1.0441887378692627\n",
      "Training Batch [371/782]: Loss 0.9403804540634155\n",
      "Training Batch [372/782]: Loss 0.9944887161254883\n",
      "Training Batch [373/782]: Loss 0.9352307319641113\n",
      "Training Batch [374/782]: Loss 0.9258849620819092\n",
      "Training Batch [375/782]: Loss 0.8683573007583618\n",
      "Training Batch [376/782]: Loss 0.9349755644798279\n",
      "Training Batch [377/782]: Loss 0.926984429359436\n",
      "Training Batch [378/782]: Loss 0.6887402534484863\n",
      "Training Batch [379/782]: Loss 0.9100691080093384\n",
      "Training Batch [380/782]: Loss 1.1273211240768433\n",
      "Training Batch [381/782]: Loss 0.6943308711051941\n",
      "Training Batch [382/782]: Loss 0.815419614315033\n",
      "Training Batch [383/782]: Loss 1.000814437866211\n",
      "Training Batch [384/782]: Loss 0.8719965219497681\n",
      "Training Batch [385/782]: Loss 0.8168851137161255\n",
      "Training Batch [386/782]: Loss 0.7844130396842957\n",
      "Training Batch [387/782]: Loss 0.6728957891464233\n",
      "Training Batch [388/782]: Loss 0.8671619892120361\n",
      "Training Batch [389/782]: Loss 0.9608314037322998\n",
      "Training Batch [390/782]: Loss 0.699539065361023\n",
      "Training Batch [391/782]: Loss 0.9499794840812683\n",
      "Training Batch [392/782]: Loss 0.8321261405944824\n",
      "Training Batch [393/782]: Loss 0.8978484272956848\n",
      "Training Batch [394/782]: Loss 0.9545338153839111\n",
      "Training Batch [395/782]: Loss 0.9714711308479309\n",
      "Training Batch [396/782]: Loss 0.7902480959892273\n",
      "Training Batch [397/782]: Loss 0.8712199926376343\n",
      "Training Batch [398/782]: Loss 0.7592541575431824\n",
      "Training Batch [399/782]: Loss 1.114016056060791\n",
      "Training Batch [400/782]: Loss 0.733039140701294\n",
      "Training Batch [401/782]: Loss 0.9695755839347839\n",
      "Training Batch [402/782]: Loss 1.002423882484436\n",
      "Training Batch [403/782]: Loss 0.9048465490341187\n",
      "Training Batch [404/782]: Loss 0.9877170920372009\n",
      "Training Batch [405/782]: Loss 0.9189627170562744\n",
      "Training Batch [406/782]: Loss 0.9105930328369141\n",
      "Training Batch [407/782]: Loss 1.1211203336715698\n",
      "Training Batch [408/782]: Loss 0.7836964726448059\n",
      "Training Batch [409/782]: Loss 1.1070284843444824\n",
      "Training Batch [410/782]: Loss 1.1510021686553955\n",
      "Training Batch [411/782]: Loss 0.8646811842918396\n",
      "Training Batch [412/782]: Loss 0.6959903836250305\n",
      "Training Batch [413/782]: Loss 0.8401775360107422\n",
      "Training Batch [414/782]: Loss 0.8616489768028259\n",
      "Training Batch [415/782]: Loss 1.0872856378555298\n",
      "Training Batch [416/782]: Loss 0.943772554397583\n",
      "Training Batch [417/782]: Loss 0.9054933190345764\n",
      "Training Batch [418/782]: Loss 0.9950069785118103\n",
      "Training Batch [419/782]: Loss 1.2315356731414795\n",
      "Training Batch [420/782]: Loss 1.2675226926803589\n",
      "Training Batch [421/782]: Loss 0.8328166604042053\n",
      "Training Batch [422/782]: Loss 0.9829286336898804\n",
      "Training Batch [423/782]: Loss 0.8275212049484253\n",
      "Training Batch [424/782]: Loss 1.3316733837127686\n",
      "Training Batch [425/782]: Loss 1.0452980995178223\n",
      "Training Batch [426/782]: Loss 0.9605017900466919\n",
      "Training Batch [427/782]: Loss 1.0880829095840454\n",
      "Training Batch [428/782]: Loss 0.919886589050293\n",
      "Training Batch [429/782]: Loss 1.2571059465408325\n",
      "Training Batch [430/782]: Loss 0.8401517868041992\n",
      "Training Batch [431/782]: Loss 0.8777514696121216\n",
      "Training Batch [432/782]: Loss 1.0837271213531494\n",
      "Training Batch [433/782]: Loss 0.9981902837753296\n",
      "Training Batch [434/782]: Loss 0.9050813317298889\n",
      "Training Batch [435/782]: Loss 1.0267771482467651\n",
      "Training Batch [436/782]: Loss 1.0015777349472046\n",
      "Training Batch [437/782]: Loss 1.007503867149353\n",
      "Training Batch [438/782]: Loss 0.8891275525093079\n",
      "Training Batch [439/782]: Loss 0.8871966600418091\n",
      "Training Batch [440/782]: Loss 1.1765984296798706\n",
      "Training Batch [441/782]: Loss 0.9234801530838013\n",
      "Training Batch [442/782]: Loss 0.8387401103973389\n",
      "Training Batch [443/782]: Loss 0.8373420238494873\n",
      "Training Batch [444/782]: Loss 1.1953394412994385\n",
      "Training Batch [445/782]: Loss 1.0049550533294678\n",
      "Training Batch [446/782]: Loss 0.9725875854492188\n",
      "Training Batch [447/782]: Loss 1.1673904657363892\n",
      "Training Batch [448/782]: Loss 0.9534891247749329\n",
      "Training Batch [449/782]: Loss 0.8560910820960999\n",
      "Training Batch [450/782]: Loss 0.9437136054039001\n",
      "Training Batch [451/782]: Loss 0.8324155807495117\n",
      "Training Batch [452/782]: Loss 0.8687807321548462\n",
      "Training Batch [453/782]: Loss 1.1235792636871338\n",
      "Training Batch [454/782]: Loss 0.8528032302856445\n",
      "Training Batch [455/782]: Loss 0.9360541701316833\n",
      "Training Batch [456/782]: Loss 0.8034656047821045\n",
      "Training Batch [457/782]: Loss 1.096992015838623\n",
      "Training Batch [458/782]: Loss 1.2380285263061523\n",
      "Training Batch [459/782]: Loss 1.0918300151824951\n",
      "Training Batch [460/782]: Loss 0.8162049055099487\n",
      "Training Batch [461/782]: Loss 1.420982837677002\n",
      "Training Batch [462/782]: Loss 1.0196611881256104\n",
      "Training Batch [463/782]: Loss 0.8954505324363708\n",
      "Training Batch [464/782]: Loss 1.0836372375488281\n",
      "Training Batch [465/782]: Loss 1.084218978881836\n",
      "Training Batch [466/782]: Loss 1.2675050497055054\n",
      "Training Batch [467/782]: Loss 0.9038709998130798\n",
      "Training Batch [468/782]: Loss 1.2070139646530151\n",
      "Training Batch [469/782]: Loss 0.7285410761833191\n",
      "Training Batch [470/782]: Loss 0.903755784034729\n",
      "Training Batch [471/782]: Loss 0.8460200428962708\n",
      "Training Batch [472/782]: Loss 1.039315104484558\n",
      "Training Batch [473/782]: Loss 0.934962809085846\n",
      "Training Batch [474/782]: Loss 0.8848519921302795\n",
      "Training Batch [475/782]: Loss 0.8125935792922974\n",
      "Training Batch [476/782]: Loss 1.013990044593811\n",
      "Training Batch [477/782]: Loss 1.0545055866241455\n",
      "Training Batch [478/782]: Loss 0.9758375287055969\n",
      "Training Batch [479/782]: Loss 0.8834583759307861\n",
      "Training Batch [480/782]: Loss 0.7917696833610535\n",
      "Training Batch [481/782]: Loss 0.9022161364555359\n",
      "Training Batch [482/782]: Loss 0.9754154086112976\n",
      "Training Batch [483/782]: Loss 0.822657585144043\n",
      "Training Batch [484/782]: Loss 0.7486991882324219\n",
      "Training Batch [485/782]: Loss 0.8719959855079651\n",
      "Training Batch [486/782]: Loss 0.9652701020240784\n",
      "Training Batch [487/782]: Loss 0.9174541234970093\n",
      "Training Batch [488/782]: Loss 1.0757131576538086\n",
      "Training Batch [489/782]: Loss 0.8890369534492493\n",
      "Training Batch [490/782]: Loss 1.0690159797668457\n",
      "Training Batch [491/782]: Loss 1.1435555219650269\n",
      "Training Batch [492/782]: Loss 1.0517531633377075\n",
      "Training Batch [493/782]: Loss 1.0208510160446167\n",
      "Training Batch [494/782]: Loss 0.8526994585990906\n",
      "Training Batch [495/782]: Loss 0.961388349533081\n",
      "Training Batch [496/782]: Loss 0.8883757591247559\n",
      "Training Batch [497/782]: Loss 1.3535776138305664\n",
      "Training Batch [498/782]: Loss 0.8499309420585632\n",
      "Training Batch [499/782]: Loss 0.8868860602378845\n",
      "Training Batch [500/782]: Loss 0.8829066157341003\n",
      "Training Batch [501/782]: Loss 0.8196790814399719\n",
      "Training Batch [502/782]: Loss 0.819320559501648\n",
      "Training Batch [503/782]: Loss 1.054738998413086\n",
      "Training Batch [504/782]: Loss 1.0437870025634766\n",
      "Training Batch [505/782]: Loss 1.331439733505249\n",
      "Training Batch [506/782]: Loss 1.12651526927948\n",
      "Training Batch [507/782]: Loss 1.0267772674560547\n",
      "Training Batch [508/782]: Loss 1.0065102577209473\n",
      "Training Batch [509/782]: Loss 1.1983871459960938\n",
      "Training Batch [510/782]: Loss 0.8704238533973694\n",
      "Training Batch [511/782]: Loss 1.0256609916687012\n",
      "Training Batch [512/782]: Loss 0.9065765142440796\n",
      "Training Batch [513/782]: Loss 0.9198063611984253\n",
      "Training Batch [514/782]: Loss 0.6914048790931702\n",
      "Training Batch [515/782]: Loss 0.654586672782898\n",
      "Training Batch [516/782]: Loss 0.7996675968170166\n",
      "Training Batch [517/782]: Loss 0.9095885753631592\n",
      "Training Batch [518/782]: Loss 1.0088249444961548\n",
      "Training Batch [519/782]: Loss 1.0801353454589844\n",
      "Training Batch [520/782]: Loss 0.8041685223579407\n",
      "Training Batch [521/782]: Loss 0.9195035099983215\n",
      "Training Batch [522/782]: Loss 1.0430915355682373\n",
      "Training Batch [523/782]: Loss 0.8838748335838318\n",
      "Training Batch [524/782]: Loss 0.8323229551315308\n",
      "Training Batch [525/782]: Loss 0.9271760582923889\n",
      "Training Batch [526/782]: Loss 0.8155825138092041\n",
      "Training Batch [527/782]: Loss 0.913906455039978\n",
      "Training Batch [528/782]: Loss 0.7499067187309265\n",
      "Training Batch [529/782]: Loss 0.8949519395828247\n",
      "Training Batch [530/782]: Loss 1.0231773853302002\n",
      "Training Batch [531/782]: Loss 1.1093189716339111\n",
      "Training Batch [532/782]: Loss 1.0620245933532715\n",
      "Training Batch [533/782]: Loss 0.998966634273529\n",
      "Training Batch [534/782]: Loss 0.8107710480690002\n",
      "Training Batch [535/782]: Loss 0.7508006691932678\n",
      "Training Batch [536/782]: Loss 0.949565589427948\n",
      "Training Batch [537/782]: Loss 0.8822433352470398\n",
      "Training Batch [538/782]: Loss 0.7054556012153625\n",
      "Training Batch [539/782]: Loss 1.048659086227417\n",
      "Training Batch [540/782]: Loss 0.6592786908149719\n",
      "Training Batch [541/782]: Loss 0.9989215135574341\n",
      "Training Batch [542/782]: Loss 0.5895745158195496\n",
      "Training Batch [543/782]: Loss 1.1230627298355103\n",
      "Training Batch [544/782]: Loss 0.8218930959701538\n",
      "Training Batch [545/782]: Loss 1.034292221069336\n",
      "Training Batch [546/782]: Loss 0.869830846786499\n",
      "Training Batch [547/782]: Loss 1.0323243141174316\n",
      "Training Batch [548/782]: Loss 0.8510894775390625\n",
      "Training Batch [549/782]: Loss 0.7873916625976562\n",
      "Training Batch [550/782]: Loss 0.8155187368392944\n",
      "Training Batch [551/782]: Loss 0.9216820001602173\n",
      "Training Batch [552/782]: Loss 0.797308623790741\n",
      "Training Batch [553/782]: Loss 0.8742160797119141\n",
      "Training Batch [554/782]: Loss 0.8103840351104736\n",
      "Training Batch [555/782]: Loss 0.7955181002616882\n",
      "Training Batch [556/782]: Loss 0.9314928650856018\n",
      "Training Batch [557/782]: Loss 1.2249836921691895\n",
      "Training Batch [558/782]: Loss 1.0530436038970947\n",
      "Training Batch [559/782]: Loss 0.9695860147476196\n",
      "Training Batch [560/782]: Loss 0.8738457560539246\n",
      "Training Batch [561/782]: Loss 0.9292793869972229\n",
      "Training Batch [562/782]: Loss 0.8986063003540039\n",
      "Training Batch [563/782]: Loss 0.9205679297447205\n",
      "Training Batch [564/782]: Loss 1.0093369483947754\n",
      "Training Batch [565/782]: Loss 0.7873892188072205\n",
      "Training Batch [566/782]: Loss 0.7959279417991638\n",
      "Training Batch [567/782]: Loss 0.8532477021217346\n",
      "Training Batch [568/782]: Loss 0.9851588606834412\n",
      "Training Batch [569/782]: Loss 0.9846659898757935\n",
      "Training Batch [570/782]: Loss 1.2013882398605347\n",
      "Training Batch [571/782]: Loss 0.9291318655014038\n",
      "Training Batch [572/782]: Loss 0.9223797917366028\n",
      "Training Batch [573/782]: Loss 0.9939701557159424\n",
      "Training Batch [574/782]: Loss 1.046473503112793\n",
      "Training Batch [575/782]: Loss 0.8388670086860657\n",
      "Training Batch [576/782]: Loss 0.8071343302726746\n",
      "Training Batch [577/782]: Loss 1.1458660364151\n",
      "Training Batch [578/782]: Loss 1.0689148902893066\n",
      "Training Batch [579/782]: Loss 0.9333767890930176\n",
      "Training Batch [580/782]: Loss 1.1659270524978638\n",
      "Training Batch [581/782]: Loss 0.8650516271591187\n",
      "Training Batch [582/782]: Loss 0.8716647028923035\n",
      "Training Batch [583/782]: Loss 0.960077166557312\n",
      "Training Batch [584/782]: Loss 1.1598631143569946\n",
      "Training Batch [585/782]: Loss 0.8353261351585388\n",
      "Training Batch [586/782]: Loss 0.7544160485267639\n",
      "Training Batch [587/782]: Loss 1.032153606414795\n",
      "Training Batch [588/782]: Loss 1.142637848854065\n",
      "Training Batch [589/782]: Loss 0.96985924243927\n",
      "Training Batch [590/782]: Loss 0.6163873076438904\n",
      "Training Batch [591/782]: Loss 0.9499082565307617\n",
      "Training Batch [592/782]: Loss 0.7735108137130737\n",
      "Training Batch [593/782]: Loss 0.8970445990562439\n",
      "Training Batch [594/782]: Loss 0.9359258413314819\n",
      "Training Batch [595/782]: Loss 0.9133068323135376\n",
      "Training Batch [596/782]: Loss 0.8239877223968506\n",
      "Training Batch [597/782]: Loss 0.7977964282035828\n",
      "Training Batch [598/782]: Loss 0.8015113472938538\n",
      "Training Batch [599/782]: Loss 0.82700115442276\n",
      "Training Batch [600/782]: Loss 0.9496647715568542\n",
      "Training Batch [601/782]: Loss 1.0348832607269287\n",
      "Training Batch [602/782]: Loss 1.3172262907028198\n",
      "Training Batch [603/782]: Loss 0.7542682886123657\n",
      "Training Batch [604/782]: Loss 0.6665222644805908\n",
      "Training Batch [605/782]: Loss 1.1503698825836182\n",
      "Training Batch [606/782]: Loss 0.9881120920181274\n",
      "Training Batch [607/782]: Loss 0.8395029902458191\n",
      "Training Batch [608/782]: Loss 1.0656795501708984\n",
      "Training Batch [609/782]: Loss 0.9489040374755859\n",
      "Training Batch [610/782]: Loss 0.9962499141693115\n",
      "Training Batch [611/782]: Loss 0.9287304878234863\n",
      "Training Batch [612/782]: Loss 0.928234875202179\n",
      "Training Batch [613/782]: Loss 1.0875016450881958\n",
      "Training Batch [614/782]: Loss 1.0174435377120972\n",
      "Training Batch [615/782]: Loss 1.0456706285476685\n",
      "Training Batch [616/782]: Loss 0.9113842844963074\n",
      "Training Batch [617/782]: Loss 0.8298206329345703\n",
      "Training Batch [618/782]: Loss 1.0631318092346191\n",
      "Training Batch [619/782]: Loss 0.9171770215034485\n",
      "Training Batch [620/782]: Loss 1.0000171661376953\n",
      "Training Batch [621/782]: Loss 0.9459502696990967\n",
      "Training Batch [622/782]: Loss 0.8290120363235474\n",
      "Training Batch [623/782]: Loss 0.797585129737854\n",
      "Training Batch [624/782]: Loss 0.753808319568634\n",
      "Training Batch [625/782]: Loss 1.2298839092254639\n",
      "Training Batch [626/782]: Loss 0.8161062598228455\n",
      "Training Batch [627/782]: Loss 0.9532449245452881\n",
      "Training Batch [628/782]: Loss 0.9755622744560242\n",
      "Training Batch [629/782]: Loss 1.1249476671218872\n",
      "Training Batch [630/782]: Loss 1.1086440086364746\n",
      "Training Batch [631/782]: Loss 0.7892747521400452\n",
      "Training Batch [632/782]: Loss 1.290175437927246\n",
      "Training Batch [633/782]: Loss 0.793778121471405\n",
      "Training Batch [634/782]: Loss 0.8209060430526733\n",
      "Training Batch [635/782]: Loss 0.7076540589332581\n",
      "Training Batch [636/782]: Loss 0.910222589969635\n",
      "Training Batch [637/782]: Loss 0.8940576910972595\n",
      "Training Batch [638/782]: Loss 1.0812827348709106\n",
      "Training Batch [639/782]: Loss 0.7522853016853333\n",
      "Training Batch [640/782]: Loss 1.0150580406188965\n",
      "Training Batch [641/782]: Loss 1.0635327100753784\n",
      "Training Batch [642/782]: Loss 1.0433727502822876\n",
      "Training Batch [643/782]: Loss 1.0531615018844604\n",
      "Training Batch [644/782]: Loss 1.0163699388504028\n",
      "Training Batch [645/782]: Loss 1.2954440116882324\n",
      "Training Batch [646/782]: Loss 1.074785828590393\n",
      "Training Batch [647/782]: Loss 0.9063483476638794\n",
      "Training Batch [648/782]: Loss 1.0060149431228638\n",
      "Training Batch [649/782]: Loss 1.1122701168060303\n",
      "Training Batch [650/782]: Loss 0.7278001308441162\n",
      "Training Batch [651/782]: Loss 0.8119962811470032\n",
      "Training Batch [652/782]: Loss 0.855713427066803\n",
      "Training Batch [653/782]: Loss 0.8616009950637817\n",
      "Training Batch [654/782]: Loss 0.9640748500823975\n",
      "Training Batch [655/782]: Loss 1.07969331741333\n",
      "Training Batch [656/782]: Loss 0.8521555662155151\n",
      "Training Batch [657/782]: Loss 0.825126051902771\n",
      "Training Batch [658/782]: Loss 1.0098040103912354\n",
      "Training Batch [659/782]: Loss 0.7745814919471741\n",
      "Training Batch [660/782]: Loss 0.7257486581802368\n",
      "Training Batch [661/782]: Loss 0.9814265370368958\n",
      "Training Batch [662/782]: Loss 0.8934410214424133\n",
      "Training Batch [663/782]: Loss 0.7958306074142456\n",
      "Training Batch [664/782]: Loss 0.8303166627883911\n",
      "Training Batch [665/782]: Loss 0.6751925945281982\n",
      "Training Batch [666/782]: Loss 0.9147593379020691\n",
      "Training Batch [667/782]: Loss 1.023613452911377\n",
      "Training Batch [668/782]: Loss 0.8419481515884399\n",
      "Training Batch [669/782]: Loss 0.6249918341636658\n",
      "Training Batch [670/782]: Loss 1.237924337387085\n",
      "Training Batch [671/782]: Loss 0.8097223043441772\n",
      "Training Batch [672/782]: Loss 0.986059844493866\n",
      "Training Batch [673/782]: Loss 0.8099062442779541\n",
      "Training Batch [674/782]: Loss 0.7883819341659546\n",
      "Training Batch [675/782]: Loss 0.8903043270111084\n",
      "Training Batch [676/782]: Loss 1.1287729740142822\n",
      "Training Batch [677/782]: Loss 1.1613725423812866\n",
      "Training Batch [678/782]: Loss 0.8432153463363647\n",
      "Training Batch [679/782]: Loss 0.7789812684059143\n",
      "Training Batch [680/782]: Loss 0.8867833018302917\n",
      "Training Batch [681/782]: Loss 0.9632178544998169\n",
      "Training Batch [682/782]: Loss 0.7444959282875061\n",
      "Training Batch [683/782]: Loss 0.9939873218536377\n",
      "Training Batch [684/782]: Loss 1.1306822299957275\n",
      "Training Batch [685/782]: Loss 1.0530952215194702\n",
      "Training Batch [686/782]: Loss 0.8310103416442871\n",
      "Training Batch [687/782]: Loss 0.8417003750801086\n",
      "Training Batch [688/782]: Loss 0.9403806328773499\n",
      "Training Batch [689/782]: Loss 0.9994433522224426\n",
      "Training Batch [690/782]: Loss 0.8642054200172424\n",
      "Training Batch [691/782]: Loss 1.0533605813980103\n",
      "Training Batch [692/782]: Loss 0.8797194361686707\n",
      "Training Batch [693/782]: Loss 1.0256900787353516\n",
      "Training Batch [694/782]: Loss 0.7716013193130493\n",
      "Training Batch [695/782]: Loss 0.6735285520553589\n",
      "Training Batch [696/782]: Loss 0.7187918424606323\n",
      "Training Batch [697/782]: Loss 1.3256722688674927\n",
      "Training Batch [698/782]: Loss 1.0675028562545776\n",
      "Training Batch [699/782]: Loss 1.0338383913040161\n",
      "Training Batch [700/782]: Loss 0.9491550922393799\n",
      "Training Batch [701/782]: Loss 1.046278953552246\n",
      "Training Batch [702/782]: Loss 1.052138090133667\n",
      "Training Batch [703/782]: Loss 1.019757628440857\n",
      "Training Batch [704/782]: Loss 1.079374074935913\n",
      "Training Batch [705/782]: Loss 1.0073939561843872\n",
      "Training Batch [706/782]: Loss 1.237480878829956\n",
      "Training Batch [707/782]: Loss 0.7096075415611267\n",
      "Training Batch [708/782]: Loss 0.7115837931632996\n",
      "Training Batch [709/782]: Loss 0.8227830529212952\n",
      "Training Batch [710/782]: Loss 1.0269221067428589\n",
      "Training Batch [711/782]: Loss 1.054772973060608\n",
      "Training Batch [712/782]: Loss 1.0493313074111938\n",
      "Training Batch [713/782]: Loss 1.1101466417312622\n",
      "Training Batch [714/782]: Loss 1.0410313606262207\n",
      "Training Batch [715/782]: Loss 0.8637635111808777\n",
      "Training Batch [716/782]: Loss 1.0236423015594482\n",
      "Training Batch [717/782]: Loss 0.7054072618484497\n",
      "Training Batch [718/782]: Loss 0.8771955966949463\n",
      "Training Batch [719/782]: Loss 1.1309077739715576\n",
      "Training Batch [720/782]: Loss 0.9774627685546875\n",
      "Training Batch [721/782]: Loss 0.933928906917572\n",
      "Training Batch [722/782]: Loss 0.8405738472938538\n",
      "Training Batch [723/782]: Loss 0.835366427898407\n",
      "Training Batch [724/782]: Loss 0.941939651966095\n",
      "Training Batch [725/782]: Loss 1.2111353874206543\n",
      "Training Batch [726/782]: Loss 0.7493681907653809\n",
      "Training Batch [727/782]: Loss 0.7309576272964478\n",
      "Training Batch [728/782]: Loss 0.7819918990135193\n",
      "Training Batch [729/782]: Loss 0.9455779790878296\n",
      "Training Batch [730/782]: Loss 0.944550096988678\n",
      "Training Batch [731/782]: Loss 0.9577397108078003\n",
      "Training Batch [732/782]: Loss 0.942263662815094\n",
      "Training Batch [733/782]: Loss 0.8156083822250366\n",
      "Training Batch [734/782]: Loss 0.9752458333969116\n",
      "Training Batch [735/782]: Loss 0.9771275520324707\n",
      "Training Batch [736/782]: Loss 0.8683291077613831\n",
      "Training Batch [737/782]: Loss 0.9019765853881836\n",
      "Training Batch [738/782]: Loss 0.8093476295471191\n",
      "Training Batch [739/782]: Loss 0.8983791470527649\n",
      "Training Batch [740/782]: Loss 1.100079894065857\n",
      "Training Batch [741/782]: Loss 1.2062077522277832\n",
      "Training Batch [742/782]: Loss 1.0729644298553467\n",
      "Training Batch [743/782]: Loss 0.7531178593635559\n",
      "Training Batch [744/782]: Loss 0.6140503287315369\n",
      "Training Batch [745/782]: Loss 0.8901205062866211\n",
      "Training Batch [746/782]: Loss 0.9141802787780762\n",
      "Training Batch [747/782]: Loss 1.1451709270477295\n",
      "Training Batch [748/782]: Loss 0.9481515288352966\n",
      "Training Batch [749/782]: Loss 0.7942265272140503\n",
      "Training Batch [750/782]: Loss 1.2342476844787598\n",
      "Training Batch [751/782]: Loss 1.2406381368637085\n",
      "Training Batch [752/782]: Loss 0.9818087220191956\n",
      "Training Batch [753/782]: Loss 0.7502285242080688\n",
      "Training Batch [754/782]: Loss 1.0072979927062988\n",
      "Training Batch [755/782]: Loss 1.1709275245666504\n",
      "Training Batch [756/782]: Loss 1.0619100332260132\n",
      "Training Batch [757/782]: Loss 0.9184889197349548\n",
      "Training Batch [758/782]: Loss 0.8547582030296326\n",
      "Training Batch [759/782]: Loss 0.9273881912231445\n",
      "Training Batch [760/782]: Loss 1.0378469228744507\n",
      "Training Batch [761/782]: Loss 1.2062368392944336\n",
      "Training Batch [762/782]: Loss 0.9907046556472778\n",
      "Training Batch [763/782]: Loss 0.9901685118675232\n",
      "Training Batch [764/782]: Loss 1.035643458366394\n",
      "Training Batch [765/782]: Loss 0.7869256138801575\n",
      "Training Batch [766/782]: Loss 1.0224742889404297\n",
      "Training Batch [767/782]: Loss 1.1085047721862793\n",
      "Training Batch [768/782]: Loss 0.8143476843833923\n",
      "Training Batch [769/782]: Loss 0.9243956804275513\n",
      "Training Batch [770/782]: Loss 0.8741405010223389\n",
      "Training Batch [771/782]: Loss 1.103539228439331\n",
      "Training Batch [772/782]: Loss 1.1098711490631104\n",
      "Training Batch [773/782]: Loss 1.1285853385925293\n",
      "Training Batch [774/782]: Loss 0.902346134185791\n",
      "Training Batch [775/782]: Loss 0.9845669269561768\n",
      "Training Batch [776/782]: Loss 0.9223244786262512\n",
      "Training Batch [777/782]: Loss 1.0427225828170776\n",
      "Training Batch [778/782]: Loss 0.9976070523262024\n",
      "Training Batch [779/782]: Loss 1.0716345310211182\n",
      "Training Batch [780/782]: Loss 0.7553713917732239\n",
      "Training Batch [781/782]: Loss 0.6941319704055786\n",
      "Training Batch [782/782]: Loss 1.17171049118042\n",
      "Epoch 5 - Train Loss: 0.9335\n",
      "*********  Epoch 6/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.9212702512741089\n",
      "Training Batch [2/782]: Loss 0.6790826320648193\n",
      "Training Batch [3/782]: Loss 0.8891717195510864\n",
      "Training Batch [4/782]: Loss 0.7957009673118591\n",
      "Training Batch [5/782]: Loss 0.7205362319946289\n",
      "Training Batch [6/782]: Loss 0.6673457622528076\n",
      "Training Batch [7/782]: Loss 0.6240573525428772\n",
      "Training Batch [8/782]: Loss 0.6749611496925354\n",
      "Training Batch [9/782]: Loss 0.7992047667503357\n",
      "Training Batch [10/782]: Loss 1.0011423826217651\n",
      "Training Batch [11/782]: Loss 0.70534348487854\n",
      "Training Batch [12/782]: Loss 0.6683110594749451\n",
      "Training Batch [13/782]: Loss 0.8855049014091492\n",
      "Training Batch [14/782]: Loss 0.8454811573028564\n",
      "Training Batch [15/782]: Loss 0.7013623714447021\n",
      "Training Batch [16/782]: Loss 0.6714210510253906\n",
      "Training Batch [17/782]: Loss 0.6593284606933594\n",
      "Training Batch [18/782]: Loss 0.6465762257575989\n",
      "Training Batch [19/782]: Loss 0.7726589441299438\n",
      "Training Batch [20/782]: Loss 0.7548289895057678\n",
      "Training Batch [21/782]: Loss 0.542232096195221\n",
      "Training Batch [22/782]: Loss 0.461822509765625\n",
      "Training Batch [23/782]: Loss 0.49582433700561523\n",
      "Training Batch [24/782]: Loss 0.7075490355491638\n",
      "Training Batch [25/782]: Loss 0.6728605031967163\n",
      "Training Batch [26/782]: Loss 0.6356036067008972\n",
      "Training Batch [27/782]: Loss 0.6701815128326416\n",
      "Training Batch [28/782]: Loss 0.8232706785202026\n",
      "Training Batch [29/782]: Loss 0.7269638180732727\n",
      "Training Batch [30/782]: Loss 0.6741413474082947\n",
      "Training Batch [31/782]: Loss 0.5745738744735718\n",
      "Training Batch [32/782]: Loss 0.7046116590499878\n",
      "Training Batch [33/782]: Loss 0.7346215844154358\n",
      "Training Batch [34/782]: Loss 0.49130013585090637\n",
      "Training Batch [35/782]: Loss 0.688117265701294\n",
      "Training Batch [36/782]: Loss 0.5016869306564331\n",
      "Training Batch [37/782]: Loss 0.8649793863296509\n",
      "Training Batch [38/782]: Loss 0.6278414726257324\n",
      "Training Batch [39/782]: Loss 0.5227143168449402\n",
      "Training Batch [40/782]: Loss 0.696872889995575\n",
      "Training Batch [41/782]: Loss 0.8797251582145691\n",
      "Training Batch [42/782]: Loss 0.5500066876411438\n",
      "Training Batch [43/782]: Loss 0.6176440119743347\n",
      "Training Batch [44/782]: Loss 0.7881632447242737\n",
      "Training Batch [45/782]: Loss 0.5152684450149536\n",
      "Training Batch [46/782]: Loss 0.6048213243484497\n",
      "Training Batch [47/782]: Loss 0.49598896503448486\n",
      "Training Batch [48/782]: Loss 0.8174715042114258\n",
      "Training Batch [49/782]: Loss 0.5765173435211182\n",
      "Training Batch [50/782]: Loss 0.8219839334487915\n",
      "Training Batch [51/782]: Loss 0.6528946757316589\n",
      "Training Batch [52/782]: Loss 0.5379841327667236\n",
      "Training Batch [53/782]: Loss 0.6786173582077026\n",
      "Training Batch [54/782]: Loss 0.6740595698356628\n",
      "Training Batch [55/782]: Loss 0.5473918318748474\n",
      "Training Batch [56/782]: Loss 0.579490065574646\n",
      "Training Batch [57/782]: Loss 0.604642391204834\n",
      "Training Batch [58/782]: Loss 0.6451618671417236\n",
      "Training Batch [59/782]: Loss 0.5479966998100281\n",
      "Training Batch [60/782]: Loss 0.6435495615005493\n",
      "Training Batch [61/782]: Loss 0.6792539358139038\n",
      "Training Batch [62/782]: Loss 0.8083993792533875\n",
      "Training Batch [63/782]: Loss 0.7967008948326111\n",
      "Training Batch [64/782]: Loss 0.8868685960769653\n",
      "Training Batch [65/782]: Loss 0.7382311224937439\n",
      "Training Batch [66/782]: Loss 0.9525584578514099\n",
      "Training Batch [67/782]: Loss 0.5179523825645447\n",
      "Training Batch [68/782]: Loss 0.6638004183769226\n",
      "Training Batch [69/782]: Loss 0.5179523229598999\n",
      "Training Batch [70/782]: Loss 0.5631466507911682\n",
      "Training Batch [71/782]: Loss 0.9092355370521545\n",
      "Training Batch [72/782]: Loss 0.5107946395874023\n",
      "Training Batch [73/782]: Loss 0.9888884425163269\n",
      "Training Batch [74/782]: Loss 0.7655131816864014\n",
      "Training Batch [75/782]: Loss 0.6024714112281799\n",
      "Training Batch [76/782]: Loss 0.6306149959564209\n",
      "Training Batch [77/782]: Loss 0.543607234954834\n",
      "Training Batch [78/782]: Loss 0.5614113807678223\n",
      "Training Batch [79/782]: Loss 0.49728044867515564\n",
      "Training Batch [80/782]: Loss 0.5932143926620483\n",
      "Training Batch [81/782]: Loss 0.6343921422958374\n",
      "Training Batch [82/782]: Loss 0.5721942782402039\n",
      "Training Batch [83/782]: Loss 0.6828470230102539\n",
      "Training Batch [84/782]: Loss 0.6595084071159363\n",
      "Training Batch [85/782]: Loss 0.6491385698318481\n",
      "Training Batch [86/782]: Loss 0.6134719848632812\n",
      "Training Batch [87/782]: Loss 0.7400515079498291\n",
      "Training Batch [88/782]: Loss 0.5836001634597778\n",
      "Training Batch [89/782]: Loss 0.8966676592826843\n",
      "Training Batch [90/782]: Loss 0.6155665516853333\n",
      "Training Batch [91/782]: Loss 0.8917120695114136\n",
      "Training Batch [92/782]: Loss 0.7314125299453735\n",
      "Training Batch [93/782]: Loss 0.6758451461791992\n",
      "Training Batch [94/782]: Loss 0.8193544149398804\n",
      "Training Batch [95/782]: Loss 0.6928136944770813\n",
      "Training Batch [96/782]: Loss 0.5766190886497498\n",
      "Training Batch [97/782]: Loss 0.5087538957595825\n",
      "Training Batch [98/782]: Loss 0.7891933917999268\n",
      "Training Batch [99/782]: Loss 0.5139263868331909\n",
      "Training Batch [100/782]: Loss 0.6156327128410339\n",
      "Training Batch [101/782]: Loss 0.6117841005325317\n",
      "Training Batch [102/782]: Loss 0.567173182964325\n",
      "Training Batch [103/782]: Loss 0.6025272011756897\n",
      "Training Batch [104/782]: Loss 0.7916743159294128\n",
      "Training Batch [105/782]: Loss 0.7574909925460815\n",
      "Training Batch [106/782]: Loss 0.7684844732284546\n",
      "Training Batch [107/782]: Loss 0.6916468143463135\n",
      "Training Batch [108/782]: Loss 0.5515193343162537\n",
      "Training Batch [109/782]: Loss 0.7329905033111572\n",
      "Training Batch [110/782]: Loss 0.5044833421707153\n",
      "Training Batch [111/782]: Loss 0.6337658166885376\n",
      "Training Batch [112/782]: Loss 0.8412347435951233\n",
      "Training Batch [113/782]: Loss 0.5526989102363586\n",
      "Training Batch [114/782]: Loss 0.5286605358123779\n",
      "Training Batch [115/782]: Loss 0.5514587163925171\n",
      "Training Batch [116/782]: Loss 0.6443890333175659\n",
      "Training Batch [117/782]: Loss 0.7562132477760315\n",
      "Training Batch [118/782]: Loss 0.7650153636932373\n",
      "Training Batch [119/782]: Loss 0.6415663361549377\n",
      "Training Batch [120/782]: Loss 0.5807203054428101\n",
      "Training Batch [121/782]: Loss 0.7141134142875671\n",
      "Training Batch [122/782]: Loss 0.6519081592559814\n",
      "Training Batch [123/782]: Loss 0.557223916053772\n",
      "Training Batch [124/782]: Loss 0.7951329946517944\n",
      "Training Batch [125/782]: Loss 0.7828601598739624\n",
      "Training Batch [126/782]: Loss 0.7470371723175049\n",
      "Training Batch [127/782]: Loss 0.608452558517456\n",
      "Training Batch [128/782]: Loss 0.6497138142585754\n",
      "Training Batch [129/782]: Loss 0.658508837223053\n",
      "Training Batch [130/782]: Loss 0.6442676782608032\n",
      "Training Batch [131/782]: Loss 0.45796921849250793\n",
      "Training Batch [132/782]: Loss 0.5186966061592102\n",
      "Training Batch [133/782]: Loss 0.685352087020874\n",
      "Training Batch [134/782]: Loss 0.6591442227363586\n",
      "Training Batch [135/782]: Loss 0.6834232211112976\n",
      "Training Batch [136/782]: Loss 0.5796405673027039\n",
      "Training Batch [137/782]: Loss 0.6635662317276001\n",
      "Training Batch [138/782]: Loss 0.6944993734359741\n",
      "Training Batch [139/782]: Loss 0.8136913776397705\n",
      "Training Batch [140/782]: Loss 0.6956719160079956\n",
      "Training Batch [141/782]: Loss 0.600565493106842\n",
      "Training Batch [142/782]: Loss 0.6753621697425842\n",
      "Training Batch [143/782]: Loss 0.6479158401489258\n",
      "Training Batch [144/782]: Loss 0.5953890681266785\n",
      "Training Batch [145/782]: Loss 0.7228827476501465\n",
      "Training Batch [146/782]: Loss 0.6741246581077576\n",
      "Training Batch [147/782]: Loss 0.7001335024833679\n",
      "Training Batch [148/782]: Loss 0.6161012053489685\n",
      "Training Batch [149/782]: Loss 1.036216378211975\n",
      "Training Batch [150/782]: Loss 0.7227430939674377\n",
      "Training Batch [151/782]: Loss 0.8158785104751587\n",
      "Training Batch [152/782]: Loss 0.6466497182846069\n",
      "Training Batch [153/782]: Loss 0.6031681299209595\n",
      "Training Batch [154/782]: Loss 0.7135522961616516\n",
      "Training Batch [155/782]: Loss 0.6351990699768066\n",
      "Training Batch [156/782]: Loss 0.8886249661445618\n",
      "Training Batch [157/782]: Loss 0.7746623754501343\n",
      "Training Batch [158/782]: Loss 0.6403553485870361\n",
      "Training Batch [159/782]: Loss 0.556056797504425\n",
      "Training Batch [160/782]: Loss 0.6921529769897461\n",
      "Training Batch [161/782]: Loss 0.9030724763870239\n",
      "Training Batch [162/782]: Loss 0.7433493733406067\n",
      "Training Batch [163/782]: Loss 0.8130106925964355\n",
      "Training Batch [164/782]: Loss 0.7788121104240417\n",
      "Training Batch [165/782]: Loss 0.799024224281311\n",
      "Training Batch [166/782]: Loss 0.7204115986824036\n",
      "Training Batch [167/782]: Loss 0.679656445980072\n",
      "Training Batch [168/782]: Loss 0.6622207760810852\n",
      "Training Batch [169/782]: Loss 0.9406231045722961\n",
      "Training Batch [170/782]: Loss 0.7000607848167419\n",
      "Training Batch [171/782]: Loss 0.717417299747467\n",
      "Training Batch [172/782]: Loss 0.7476690411567688\n",
      "Training Batch [173/782]: Loss 0.644424319267273\n",
      "Training Batch [174/782]: Loss 0.6621713042259216\n",
      "Training Batch [175/782]: Loss 0.8238269686698914\n",
      "Training Batch [176/782]: Loss 0.7761040925979614\n",
      "Training Batch [177/782]: Loss 0.9289575815200806\n",
      "Training Batch [178/782]: Loss 0.5458078980445862\n",
      "Training Batch [179/782]: Loss 0.7842536568641663\n",
      "Training Batch [180/782]: Loss 0.5674983859062195\n",
      "Training Batch [181/782]: Loss 0.7425016760826111\n",
      "Training Batch [182/782]: Loss 0.6994194984436035\n",
      "Training Batch [183/782]: Loss 0.672825276851654\n",
      "Training Batch [184/782]: Loss 0.6930456757545471\n",
      "Training Batch [185/782]: Loss 0.7950299978256226\n",
      "Training Batch [186/782]: Loss 0.756890058517456\n",
      "Training Batch [187/782]: Loss 0.7015255093574524\n",
      "Training Batch [188/782]: Loss 0.6794281601905823\n",
      "Training Batch [189/782]: Loss 0.702761173248291\n",
      "Training Batch [190/782]: Loss 0.7018833756446838\n",
      "Training Batch [191/782]: Loss 0.7835052609443665\n",
      "Training Batch [192/782]: Loss 0.7157246470451355\n",
      "Training Batch [193/782]: Loss 0.8343551158905029\n",
      "Training Batch [194/782]: Loss 0.6327619552612305\n",
      "Training Batch [195/782]: Loss 0.588514506816864\n",
      "Training Batch [196/782]: Loss 0.6642062664031982\n",
      "Training Batch [197/782]: Loss 0.8434451222419739\n",
      "Training Batch [198/782]: Loss 0.6674665212631226\n",
      "Training Batch [199/782]: Loss 0.7803782820701599\n",
      "Training Batch [200/782]: Loss 0.7934299111366272\n",
      "Training Batch [201/782]: Loss 0.6205989718437195\n",
      "Training Batch [202/782]: Loss 0.7036464214324951\n",
      "Training Batch [203/782]: Loss 0.6724526286125183\n",
      "Training Batch [204/782]: Loss 0.5813591480255127\n",
      "Training Batch [205/782]: Loss 0.8997740149497986\n",
      "Training Batch [206/782]: Loss 0.5556037425994873\n",
      "Training Batch [207/782]: Loss 0.5801054835319519\n",
      "Training Batch [208/782]: Loss 0.692581832408905\n",
      "Training Batch [209/782]: Loss 0.8865334987640381\n",
      "Training Batch [210/782]: Loss 0.7168567776679993\n",
      "Training Batch [211/782]: Loss 0.9651308655738831\n",
      "Training Batch [212/782]: Loss 0.5468396544456482\n",
      "Training Batch [213/782]: Loss 0.8431750535964966\n",
      "Training Batch [214/782]: Loss 0.8009540438652039\n",
      "Training Batch [215/782]: Loss 0.7061578631401062\n",
      "Training Batch [216/782]: Loss 0.9492034912109375\n",
      "Training Batch [217/782]: Loss 0.5933612585067749\n",
      "Training Batch [218/782]: Loss 0.8034224510192871\n",
      "Training Batch [219/782]: Loss 0.8011370897293091\n",
      "Training Batch [220/782]: Loss 0.7664493918418884\n",
      "Training Batch [221/782]: Loss 0.5856054425239563\n",
      "Training Batch [222/782]: Loss 0.9187777042388916\n",
      "Training Batch [223/782]: Loss 0.8017116189002991\n",
      "Training Batch [224/782]: Loss 0.588067352771759\n",
      "Training Batch [225/782]: Loss 0.7077120542526245\n",
      "Training Batch [226/782]: Loss 0.718757688999176\n",
      "Training Batch [227/782]: Loss 0.7963430285453796\n",
      "Training Batch [228/782]: Loss 0.8176930546760559\n",
      "Training Batch [229/782]: Loss 0.5794089436531067\n",
      "Training Batch [230/782]: Loss 0.984894335269928\n",
      "Training Batch [231/782]: Loss 0.8719805479049683\n",
      "Training Batch [232/782]: Loss 0.8693603873252869\n",
      "Training Batch [233/782]: Loss 0.8696737885475159\n",
      "Training Batch [234/782]: Loss 0.7233986854553223\n",
      "Training Batch [235/782]: Loss 0.8201789259910583\n",
      "Training Batch [236/782]: Loss 0.7472110390663147\n",
      "Training Batch [237/782]: Loss 0.6244007349014282\n",
      "Training Batch [238/782]: Loss 0.8991174697875977\n",
      "Training Batch [239/782]: Loss 0.7101395726203918\n",
      "Training Batch [240/782]: Loss 0.848722517490387\n",
      "Training Batch [241/782]: Loss 0.892054557800293\n",
      "Training Batch [242/782]: Loss 0.7392027974128723\n",
      "Training Batch [243/782]: Loss 0.6298678517341614\n",
      "Training Batch [244/782]: Loss 0.7726643085479736\n",
      "Training Batch [245/782]: Loss 1.0184961557388306\n",
      "Training Batch [246/782]: Loss 0.7366886138916016\n",
      "Training Batch [247/782]: Loss 0.667314350605011\n",
      "Training Batch [248/782]: Loss 0.7751145362854004\n",
      "Training Batch [249/782]: Loss 0.9853223562240601\n",
      "Training Batch [250/782]: Loss 0.8783546090126038\n",
      "Training Batch [251/782]: Loss 0.6612089276313782\n",
      "Training Batch [252/782]: Loss 0.4932878911495209\n",
      "Training Batch [253/782]: Loss 0.7389733791351318\n",
      "Training Batch [254/782]: Loss 0.6284794807434082\n",
      "Training Batch [255/782]: Loss 0.8455314040184021\n",
      "Training Batch [256/782]: Loss 0.4028617739677429\n",
      "Training Batch [257/782]: Loss 0.8402736186981201\n",
      "Training Batch [258/782]: Loss 0.7803674936294556\n",
      "Training Batch [259/782]: Loss 0.5720791220664978\n",
      "Training Batch [260/782]: Loss 0.749017596244812\n",
      "Training Batch [261/782]: Loss 0.5551661849021912\n",
      "Training Batch [262/782]: Loss 0.7049409747123718\n",
      "Training Batch [263/782]: Loss 0.6823191046714783\n",
      "Training Batch [264/782]: Loss 0.8805020451545715\n",
      "Training Batch [265/782]: Loss 0.7230530977249146\n",
      "Training Batch [266/782]: Loss 1.0411399602890015\n",
      "Training Batch [267/782]: Loss 0.8782173991203308\n",
      "Training Batch [268/782]: Loss 0.6630812883377075\n",
      "Training Batch [269/782]: Loss 0.48030513525009155\n",
      "Training Batch [270/782]: Loss 0.6792865991592407\n",
      "Training Batch [271/782]: Loss 0.9950544238090515\n",
      "Training Batch [272/782]: Loss 0.7482000589370728\n",
      "Training Batch [273/782]: Loss 0.7574667930603027\n",
      "Training Batch [274/782]: Loss 0.7043986916542053\n",
      "Training Batch [275/782]: Loss 0.7148047089576721\n",
      "Training Batch [276/782]: Loss 0.8145892024040222\n",
      "Training Batch [277/782]: Loss 0.7992498874664307\n",
      "Training Batch [278/782]: Loss 0.7249233722686768\n",
      "Training Batch [279/782]: Loss 0.4216350317001343\n",
      "Training Batch [280/782]: Loss 0.8076844811439514\n",
      "Training Batch [281/782]: Loss 0.6503669023513794\n",
      "Training Batch [282/782]: Loss 0.703039288520813\n",
      "Training Batch [283/782]: Loss 0.7013489603996277\n",
      "Training Batch [284/782]: Loss 0.7667106986045837\n",
      "Training Batch [285/782]: Loss 0.9264169335365295\n",
      "Training Batch [286/782]: Loss 0.7845189571380615\n",
      "Training Batch [287/782]: Loss 0.725197970867157\n",
      "Training Batch [288/782]: Loss 0.7676638960838318\n",
      "Training Batch [289/782]: Loss 0.9789986610412598\n",
      "Training Batch [290/782]: Loss 0.5672875642776489\n",
      "Training Batch [291/782]: Loss 0.7273830771446228\n",
      "Training Batch [292/782]: Loss 0.7447826266288757\n",
      "Training Batch [293/782]: Loss 0.8097141981124878\n",
      "Training Batch [294/782]: Loss 0.8367589116096497\n",
      "Training Batch [295/782]: Loss 0.7683765292167664\n",
      "Training Batch [296/782]: Loss 0.6765154600143433\n",
      "Training Batch [297/782]: Loss 0.5284982919692993\n",
      "Training Batch [298/782]: Loss 0.7242935299873352\n",
      "Training Batch [299/782]: Loss 0.6634069681167603\n",
      "Training Batch [300/782]: Loss 0.5702663064002991\n",
      "Training Batch [301/782]: Loss 1.0782798528671265\n",
      "Training Batch [302/782]: Loss 0.5084850788116455\n",
      "Training Batch [303/782]: Loss 0.7631314396858215\n",
      "Training Batch [304/782]: Loss 0.5879884958267212\n",
      "Training Batch [305/782]: Loss 0.9352585077285767\n",
      "Training Batch [306/782]: Loss 0.7552471160888672\n",
      "Training Batch [307/782]: Loss 0.9151068329811096\n",
      "Training Batch [308/782]: Loss 0.8105820417404175\n",
      "Training Batch [309/782]: Loss 0.6684970855712891\n",
      "Training Batch [310/782]: Loss 0.6422101855278015\n",
      "Training Batch [311/782]: Loss 0.42324334383010864\n",
      "Training Batch [312/782]: Loss 0.7122077941894531\n",
      "Training Batch [313/782]: Loss 0.6729184985160828\n",
      "Training Batch [314/782]: Loss 0.6021594405174255\n",
      "Training Batch [315/782]: Loss 0.803905725479126\n",
      "Training Batch [316/782]: Loss 0.5439556837081909\n",
      "Training Batch [317/782]: Loss 0.7550041675567627\n",
      "Training Batch [318/782]: Loss 0.7989823222160339\n",
      "Training Batch [319/782]: Loss 0.7444218397140503\n",
      "Training Batch [320/782]: Loss 0.503871738910675\n",
      "Training Batch [321/782]: Loss 0.6741353273391724\n",
      "Training Batch [322/782]: Loss 0.7396054267883301\n",
      "Training Batch [323/782]: Loss 0.5397005677223206\n",
      "Training Batch [324/782]: Loss 0.64759761095047\n",
      "Training Batch [325/782]: Loss 0.6060362458229065\n",
      "Training Batch [326/782]: Loss 0.8801528215408325\n",
      "Training Batch [327/782]: Loss 1.0452327728271484\n",
      "Training Batch [328/782]: Loss 0.8125212788581848\n",
      "Training Batch [329/782]: Loss 0.7031190991401672\n",
      "Training Batch [330/782]: Loss 0.7880720496177673\n",
      "Training Batch [331/782]: Loss 0.6780751347541809\n",
      "Training Batch [332/782]: Loss 0.8505501747131348\n",
      "Training Batch [333/782]: Loss 0.743429958820343\n",
      "Training Batch [334/782]: Loss 1.0048654079437256\n",
      "Training Batch [335/782]: Loss 0.7837008237838745\n",
      "Training Batch [336/782]: Loss 0.7918148040771484\n",
      "Training Batch [337/782]: Loss 0.6960762143135071\n",
      "Training Batch [338/782]: Loss 0.813052237033844\n",
      "Training Batch [339/782]: Loss 0.7572680711746216\n",
      "Training Batch [340/782]: Loss 0.7518559694290161\n",
      "Training Batch [341/782]: Loss 1.0201913118362427\n",
      "Training Batch [342/782]: Loss 0.759476363658905\n",
      "Training Batch [343/782]: Loss 0.6464078426361084\n",
      "Training Batch [344/782]: Loss 0.8000102043151855\n",
      "Training Batch [345/782]: Loss 0.8005053997039795\n",
      "Training Batch [346/782]: Loss 0.8222957253456116\n",
      "Training Batch [347/782]: Loss 0.6341021060943604\n",
      "Training Batch [348/782]: Loss 0.8017485737800598\n",
      "Training Batch [349/782]: Loss 0.8260830640792847\n",
      "Training Batch [350/782]: Loss 0.7722402215003967\n",
      "Training Batch [351/782]: Loss 0.5837446451187134\n",
      "Training Batch [352/782]: Loss 0.7640394568443298\n",
      "Training Batch [353/782]: Loss 0.7559617757797241\n",
      "Training Batch [354/782]: Loss 0.7109247446060181\n",
      "Training Batch [355/782]: Loss 0.7712864875793457\n",
      "Training Batch [356/782]: Loss 0.9719626903533936\n",
      "Training Batch [357/782]: Loss 0.5537600517272949\n",
      "Training Batch [358/782]: Loss 0.777292788028717\n",
      "Training Batch [359/782]: Loss 0.6397960186004639\n",
      "Training Batch [360/782]: Loss 0.9965198636054993\n",
      "Training Batch [361/782]: Loss 0.7808107733726501\n",
      "Training Batch [362/782]: Loss 0.8122597932815552\n",
      "Training Batch [363/782]: Loss 0.793391764163971\n",
      "Training Batch [364/782]: Loss 0.5867587327957153\n",
      "Training Batch [365/782]: Loss 0.8066645860671997\n",
      "Training Batch [366/782]: Loss 0.8021169304847717\n",
      "Training Batch [367/782]: Loss 0.7351785898208618\n",
      "Training Batch [368/782]: Loss 0.8338858485221863\n",
      "Training Batch [369/782]: Loss 0.709134578704834\n",
      "Training Batch [370/782]: Loss 0.8334382772445679\n",
      "Training Batch [371/782]: Loss 0.8756577968597412\n",
      "Training Batch [372/782]: Loss 0.7133795619010925\n",
      "Training Batch [373/782]: Loss 0.622261106967926\n",
      "Training Batch [374/782]: Loss 0.5676661133766174\n",
      "Training Batch [375/782]: Loss 0.8065084218978882\n",
      "Training Batch [376/782]: Loss 0.8553628921508789\n",
      "Training Batch [377/782]: Loss 0.8248631358146667\n",
      "Training Batch [378/782]: Loss 1.1483211517333984\n",
      "Training Batch [379/782]: Loss 0.8771653771400452\n",
      "Training Batch [380/782]: Loss 1.0221245288848877\n",
      "Training Batch [381/782]: Loss 0.6332129240036011\n",
      "Training Batch [382/782]: Loss 0.9714232683181763\n",
      "Training Batch [383/782]: Loss 0.8654747605323792\n",
      "Training Batch [384/782]: Loss 0.7845607399940491\n",
      "Training Batch [385/782]: Loss 0.8089040517807007\n",
      "Training Batch [386/782]: Loss 0.7898041009902954\n",
      "Training Batch [387/782]: Loss 0.7495183944702148\n",
      "Training Batch [388/782]: Loss 0.594574511051178\n",
      "Training Batch [389/782]: Loss 0.5878645777702332\n",
      "Training Batch [390/782]: Loss 0.7698582410812378\n",
      "Training Batch [391/782]: Loss 0.6468619704246521\n",
      "Training Batch [392/782]: Loss 0.8706096410751343\n",
      "Training Batch [393/782]: Loss 0.573169469833374\n",
      "Training Batch [394/782]: Loss 0.859533429145813\n",
      "Training Batch [395/782]: Loss 0.7429618835449219\n",
      "Training Batch [396/782]: Loss 1.0698368549346924\n",
      "Training Batch [397/782]: Loss 0.7257744669914246\n",
      "Training Batch [398/782]: Loss 0.6280430555343628\n",
      "Training Batch [399/782]: Loss 0.5555108785629272\n",
      "Training Batch [400/782]: Loss 0.9771800637245178\n",
      "Training Batch [401/782]: Loss 0.7586225271224976\n",
      "Training Batch [402/782]: Loss 0.7700924277305603\n",
      "Training Batch [403/782]: Loss 0.6229625344276428\n",
      "Training Batch [404/782]: Loss 0.8158606290817261\n",
      "Training Batch [405/782]: Loss 0.6582061648368835\n",
      "Training Batch [406/782]: Loss 0.8467258810997009\n",
      "Training Batch [407/782]: Loss 0.8211019039154053\n",
      "Training Batch [408/782]: Loss 0.9019525051116943\n",
      "Training Batch [409/782]: Loss 0.7894659638404846\n",
      "Training Batch [410/782]: Loss 0.8287096619606018\n",
      "Training Batch [411/782]: Loss 0.6466253995895386\n",
      "Training Batch [412/782]: Loss 0.8694565296173096\n",
      "Training Batch [413/782]: Loss 0.6443244218826294\n",
      "Training Batch [414/782]: Loss 1.0063971281051636\n",
      "Training Batch [415/782]: Loss 0.9777266979217529\n",
      "Training Batch [416/782]: Loss 0.9514474272727966\n",
      "Training Batch [417/782]: Loss 0.6122792959213257\n",
      "Training Batch [418/782]: Loss 0.8457183837890625\n",
      "Training Batch [419/782]: Loss 0.6804504990577698\n",
      "Training Batch [420/782]: Loss 0.6266493797302246\n",
      "Training Batch [421/782]: Loss 0.6087679266929626\n",
      "Training Batch [422/782]: Loss 0.7760140895843506\n",
      "Training Batch [423/782]: Loss 0.8193329572677612\n",
      "Training Batch [424/782]: Loss 0.8066229224205017\n",
      "Training Batch [425/782]: Loss 0.8551315069198608\n",
      "Training Batch [426/782]: Loss 0.9653478860855103\n",
      "Training Batch [427/782]: Loss 0.7544644474983215\n",
      "Training Batch [428/782]: Loss 0.8721110820770264\n",
      "Training Batch [429/782]: Loss 0.9703497886657715\n",
      "Training Batch [430/782]: Loss 0.7256730198860168\n",
      "Training Batch [431/782]: Loss 0.7459564208984375\n",
      "Training Batch [432/782]: Loss 0.5293580293655396\n",
      "Training Batch [433/782]: Loss 0.9543527960777283\n",
      "Training Batch [434/782]: Loss 0.8484863638877869\n",
      "Training Batch [435/782]: Loss 0.7750290036201477\n",
      "Training Batch [436/782]: Loss 0.8203123211860657\n",
      "Training Batch [437/782]: Loss 0.8010604977607727\n",
      "Training Batch [438/782]: Loss 0.8087173700332642\n",
      "Training Batch [439/782]: Loss 0.9511248469352722\n",
      "Training Batch [440/782]: Loss 0.9893798828125\n",
      "Training Batch [441/782]: Loss 0.9500702619552612\n",
      "Training Batch [442/782]: Loss 0.7411409616470337\n",
      "Training Batch [443/782]: Loss 0.4605279862880707\n",
      "Training Batch [444/782]: Loss 0.6077955365180969\n",
      "Training Batch [445/782]: Loss 0.7309820652008057\n",
      "Training Batch [446/782]: Loss 0.7722393870353699\n",
      "Training Batch [447/782]: Loss 0.7144575715065002\n",
      "Training Batch [448/782]: Loss 0.9831597208976746\n",
      "Training Batch [449/782]: Loss 0.8111932277679443\n",
      "Training Batch [450/782]: Loss 0.8547665476799011\n",
      "Training Batch [451/782]: Loss 0.8423675298690796\n",
      "Training Batch [452/782]: Loss 0.8510470390319824\n",
      "Training Batch [453/782]: Loss 0.7689396739006042\n",
      "Training Batch [454/782]: Loss 0.8058498501777649\n",
      "Training Batch [455/782]: Loss 0.6149392127990723\n",
      "Training Batch [456/782]: Loss 0.7554933428764343\n",
      "Training Batch [457/782]: Loss 0.90603107213974\n",
      "Training Batch [458/782]: Loss 0.9097128510475159\n",
      "Training Batch [459/782]: Loss 0.7978229522705078\n",
      "Training Batch [460/782]: Loss 0.860093355178833\n",
      "Training Batch [461/782]: Loss 0.8411858677864075\n",
      "Training Batch [462/782]: Loss 0.6242937445640564\n",
      "Training Batch [463/782]: Loss 0.658486008644104\n",
      "Training Batch [464/782]: Loss 0.7661427855491638\n",
      "Training Batch [465/782]: Loss 0.8323505520820618\n",
      "Training Batch [466/782]: Loss 1.0302072763442993\n",
      "Training Batch [467/782]: Loss 0.7197891473770142\n",
      "Training Batch [468/782]: Loss 0.7339608669281006\n",
      "Training Batch [469/782]: Loss 0.9930530190467834\n",
      "Training Batch [470/782]: Loss 0.9004663825035095\n",
      "Training Batch [471/782]: Loss 0.6766249537467957\n",
      "Training Batch [472/782]: Loss 0.955840528011322\n",
      "Training Batch [473/782]: Loss 0.7128461599349976\n",
      "Training Batch [474/782]: Loss 0.7747673988342285\n",
      "Training Batch [475/782]: Loss 0.7941771745681763\n",
      "Training Batch [476/782]: Loss 0.8100354671478271\n",
      "Training Batch [477/782]: Loss 0.6780406832695007\n",
      "Training Batch [478/782]: Loss 0.8350882530212402\n",
      "Training Batch [479/782]: Loss 0.7147958278656006\n",
      "Training Batch [480/782]: Loss 0.9933828711509705\n",
      "Training Batch [481/782]: Loss 0.9684383273124695\n",
      "Training Batch [482/782]: Loss 0.7770500779151917\n",
      "Training Batch [483/782]: Loss 0.8720015287399292\n",
      "Training Batch [484/782]: Loss 0.8548303842544556\n",
      "Training Batch [485/782]: Loss 0.6172060370445251\n",
      "Training Batch [486/782]: Loss 0.809110701084137\n",
      "Training Batch [487/782]: Loss 0.641312837600708\n",
      "Training Batch [488/782]: Loss 0.617624819278717\n",
      "Training Batch [489/782]: Loss 0.644686222076416\n",
      "Training Batch [490/782]: Loss 0.9332550168037415\n",
      "Training Batch [491/782]: Loss 0.8660542964935303\n",
      "Training Batch [492/782]: Loss 0.7592474222183228\n",
      "Training Batch [493/782]: Loss 0.7352578043937683\n",
      "Training Batch [494/782]: Loss 0.7784976959228516\n",
      "Training Batch [495/782]: Loss 0.8456686735153198\n",
      "Training Batch [496/782]: Loss 0.6878203749656677\n",
      "Training Batch [497/782]: Loss 0.6496533751487732\n",
      "Training Batch [498/782]: Loss 0.7237847447395325\n",
      "Training Batch [499/782]: Loss 0.9472894072532654\n",
      "Training Batch [500/782]: Loss 0.6352180242538452\n",
      "Training Batch [501/782]: Loss 0.7413268089294434\n",
      "Training Batch [502/782]: Loss 0.7990590333938599\n",
      "Training Batch [503/782]: Loss 0.5287768244743347\n",
      "Training Batch [504/782]: Loss 0.9743689894676208\n",
      "Training Batch [505/782]: Loss 0.988060712814331\n",
      "Training Batch [506/782]: Loss 0.8218412399291992\n",
      "Training Batch [507/782]: Loss 0.8621861934661865\n",
      "Training Batch [508/782]: Loss 0.6007872819900513\n",
      "Training Batch [509/782]: Loss 0.6432719826698303\n",
      "Training Batch [510/782]: Loss 0.8416581153869629\n",
      "Training Batch [511/782]: Loss 0.856565535068512\n",
      "Training Batch [512/782]: Loss 0.69749915599823\n",
      "Training Batch [513/782]: Loss 1.1332416534423828\n",
      "Training Batch [514/782]: Loss 0.7702971696853638\n",
      "Training Batch [515/782]: Loss 0.8245135545730591\n",
      "Training Batch [516/782]: Loss 0.9849016666412354\n",
      "Training Batch [517/782]: Loss 0.9169612526893616\n",
      "Training Batch [518/782]: Loss 0.8152402639389038\n",
      "Training Batch [519/782]: Loss 0.7125917077064514\n",
      "Training Batch [520/782]: Loss 0.9891773462295532\n",
      "Training Batch [521/782]: Loss 0.554619550704956\n",
      "Training Batch [522/782]: Loss 0.7568426728248596\n",
      "Training Batch [523/782]: Loss 0.7896903157234192\n",
      "Training Batch [524/782]: Loss 0.8221457004547119\n",
      "Training Batch [525/782]: Loss 0.7141121029853821\n",
      "Training Batch [526/782]: Loss 0.8116133809089661\n",
      "Training Batch [527/782]: Loss 0.8159575462341309\n",
      "Training Batch [528/782]: Loss 0.9443508982658386\n",
      "Training Batch [529/782]: Loss 0.7155084013938904\n",
      "Training Batch [530/782]: Loss 1.2280564308166504\n",
      "Training Batch [531/782]: Loss 0.6473526954650879\n",
      "Training Batch [532/782]: Loss 0.68350750207901\n",
      "Training Batch [533/782]: Loss 0.6823490858078003\n",
      "Training Batch [534/782]: Loss 0.8416445255279541\n",
      "Training Batch [535/782]: Loss 0.4927966594696045\n",
      "Training Batch [536/782]: Loss 0.8120034337043762\n",
      "Training Batch [537/782]: Loss 1.1059916019439697\n",
      "Training Batch [538/782]: Loss 0.6147995591163635\n",
      "Training Batch [539/782]: Loss 0.8191332817077637\n",
      "Training Batch [540/782]: Loss 0.9011302590370178\n",
      "Training Batch [541/782]: Loss 0.5638946294784546\n",
      "Training Batch [542/782]: Loss 1.0206501483917236\n",
      "Training Batch [543/782]: Loss 0.923149824142456\n",
      "Training Batch [544/782]: Loss 0.8010457158088684\n",
      "Training Batch [545/782]: Loss 0.899009644985199\n",
      "Training Batch [546/782]: Loss 0.6740235686302185\n",
      "Training Batch [547/782]: Loss 0.9080920219421387\n",
      "Training Batch [548/782]: Loss 0.8329663276672363\n",
      "Training Batch [549/782]: Loss 0.6198456287384033\n",
      "Training Batch [550/782]: Loss 0.9456974864006042\n",
      "Training Batch [551/782]: Loss 0.6736589074134827\n",
      "Training Batch [552/782]: Loss 0.7134682536125183\n",
      "Training Batch [553/782]: Loss 0.8555912971496582\n",
      "Training Batch [554/782]: Loss 0.8039100766181946\n",
      "Training Batch [555/782]: Loss 0.9736785888671875\n",
      "Training Batch [556/782]: Loss 0.8373094201087952\n",
      "Training Batch [557/782]: Loss 0.6914458870887756\n",
      "Training Batch [558/782]: Loss 0.6995648145675659\n",
      "Training Batch [559/782]: Loss 0.5423375368118286\n",
      "Training Batch [560/782]: Loss 0.7986836433410645\n",
      "Training Batch [561/782]: Loss 0.7478896379470825\n",
      "Training Batch [562/782]: Loss 0.92430180311203\n",
      "Training Batch [563/782]: Loss 0.7531580328941345\n",
      "Training Batch [564/782]: Loss 0.8492819666862488\n",
      "Training Batch [565/782]: Loss 0.8840050101280212\n",
      "Training Batch [566/782]: Loss 0.6168017983436584\n",
      "Training Batch [567/782]: Loss 0.744624674320221\n",
      "Training Batch [568/782]: Loss 0.5406872630119324\n",
      "Training Batch [569/782]: Loss 1.0645935535430908\n",
      "Training Batch [570/782]: Loss 0.6811426281929016\n",
      "Training Batch [571/782]: Loss 0.8761276006698608\n",
      "Training Batch [572/782]: Loss 1.1324679851531982\n",
      "Training Batch [573/782]: Loss 0.7506147623062134\n",
      "Training Batch [574/782]: Loss 0.596696138381958\n",
      "Training Batch [575/782]: Loss 0.8068826198577881\n",
      "Training Batch [576/782]: Loss 0.8352552652359009\n",
      "Training Batch [577/782]: Loss 0.700971782207489\n",
      "Training Batch [578/782]: Loss 0.6470551490783691\n",
      "Training Batch [579/782]: Loss 0.8058342933654785\n",
      "Training Batch [580/782]: Loss 0.6412709951400757\n",
      "Training Batch [581/782]: Loss 0.939329981803894\n",
      "Training Batch [582/782]: Loss 1.0917655229568481\n",
      "Training Batch [583/782]: Loss 0.5599155426025391\n",
      "Training Batch [584/782]: Loss 0.6690347790718079\n",
      "Training Batch [585/782]: Loss 0.7794771790504456\n",
      "Training Batch [586/782]: Loss 0.6379374861717224\n",
      "Training Batch [587/782]: Loss 0.737080454826355\n",
      "Training Batch [588/782]: Loss 0.8693482875823975\n",
      "Training Batch [589/782]: Loss 0.6987013816833496\n",
      "Training Batch [590/782]: Loss 0.5957510471343994\n",
      "Training Batch [591/782]: Loss 0.6545443534851074\n",
      "Training Batch [592/782]: Loss 0.8153061866760254\n",
      "Training Batch [593/782]: Loss 0.9473926424980164\n",
      "Training Batch [594/782]: Loss 0.7690245509147644\n",
      "Training Batch [595/782]: Loss 0.9835604429244995\n",
      "Training Batch [596/782]: Loss 0.7588889002799988\n",
      "Training Batch [597/782]: Loss 0.7521604299545288\n",
      "Training Batch [598/782]: Loss 0.820490300655365\n",
      "Training Batch [599/782]: Loss 0.8221563696861267\n",
      "Training Batch [600/782]: Loss 0.9230664968490601\n",
      "Training Batch [601/782]: Loss 0.782737672328949\n",
      "Training Batch [602/782]: Loss 0.8777763247489929\n",
      "Training Batch [603/782]: Loss 0.6796176433563232\n",
      "Training Batch [604/782]: Loss 0.8942300081253052\n",
      "Training Batch [605/782]: Loss 0.8134510517120361\n",
      "Training Batch [606/782]: Loss 0.8417680263519287\n",
      "Training Batch [607/782]: Loss 0.9983941316604614\n",
      "Training Batch [608/782]: Loss 0.7952084541320801\n",
      "Training Batch [609/782]: Loss 0.6085599660873413\n",
      "Training Batch [610/782]: Loss 0.7516963481903076\n",
      "Training Batch [611/782]: Loss 0.9942106008529663\n",
      "Training Batch [612/782]: Loss 0.4782402813434601\n",
      "Training Batch [613/782]: Loss 0.45291128754615784\n",
      "Training Batch [614/782]: Loss 1.0581943988800049\n",
      "Training Batch [615/782]: Loss 0.8868862986564636\n",
      "Training Batch [616/782]: Loss 0.8222991824150085\n",
      "Training Batch [617/782]: Loss 0.7014675140380859\n",
      "Training Batch [618/782]: Loss 0.7323461771011353\n",
      "Training Batch [619/782]: Loss 0.5104138255119324\n",
      "Training Batch [620/782]: Loss 0.840196967124939\n",
      "Training Batch [621/782]: Loss 1.1791601181030273\n",
      "Training Batch [622/782]: Loss 0.723897397518158\n",
      "Training Batch [623/782]: Loss 0.8436527848243713\n",
      "Training Batch [624/782]: Loss 0.8741603493690491\n",
      "Training Batch [625/782]: Loss 0.8924526572227478\n",
      "Training Batch [626/782]: Loss 0.7012636661529541\n",
      "Training Batch [627/782]: Loss 0.742959201335907\n",
      "Training Batch [628/782]: Loss 0.9164945483207703\n",
      "Training Batch [629/782]: Loss 0.7788546681404114\n",
      "Training Batch [630/782]: Loss 0.8102437257766724\n",
      "Training Batch [631/782]: Loss 0.7404358983039856\n",
      "Training Batch [632/782]: Loss 0.6288724541664124\n",
      "Training Batch [633/782]: Loss 0.737343966960907\n",
      "Training Batch [634/782]: Loss 0.891278862953186\n",
      "Training Batch [635/782]: Loss 1.1003040075302124\n",
      "Training Batch [636/782]: Loss 0.7321772575378418\n",
      "Training Batch [637/782]: Loss 0.6875004172325134\n",
      "Training Batch [638/782]: Loss 0.6926092505455017\n",
      "Training Batch [639/782]: Loss 0.5342245101928711\n",
      "Training Batch [640/782]: Loss 0.7219324707984924\n",
      "Training Batch [641/782]: Loss 0.7611663937568665\n",
      "Training Batch [642/782]: Loss 0.6752244830131531\n",
      "Training Batch [643/782]: Loss 0.7291854023933411\n",
      "Training Batch [644/782]: Loss 0.7466781735420227\n",
      "Training Batch [645/782]: Loss 0.9777834415435791\n",
      "Training Batch [646/782]: Loss 0.933146059513092\n",
      "Training Batch [647/782]: Loss 0.920620858669281\n",
      "Training Batch [648/782]: Loss 0.8227128982543945\n",
      "Training Batch [649/782]: Loss 0.7457477450370789\n",
      "Training Batch [650/782]: Loss 0.662416934967041\n",
      "Training Batch [651/782]: Loss 0.6923810839653015\n",
      "Training Batch [652/782]: Loss 0.8289988040924072\n",
      "Training Batch [653/782]: Loss 0.820138692855835\n",
      "Training Batch [654/782]: Loss 0.6156443357467651\n",
      "Training Batch [655/782]: Loss 0.6651269197463989\n",
      "Training Batch [656/782]: Loss 0.9837904572486877\n",
      "Training Batch [657/782]: Loss 0.60201495885849\n",
      "Training Batch [658/782]: Loss 0.8279480934143066\n",
      "Training Batch [659/782]: Loss 1.0637837648391724\n",
      "Training Batch [660/782]: Loss 0.857158362865448\n",
      "Training Batch [661/782]: Loss 0.9239780306816101\n",
      "Training Batch [662/782]: Loss 0.9017195105552673\n",
      "Training Batch [663/782]: Loss 0.7932060360908508\n",
      "Training Batch [664/782]: Loss 0.7252559661865234\n",
      "Training Batch [665/782]: Loss 0.8288169503211975\n",
      "Training Batch [666/782]: Loss 0.796424150466919\n",
      "Training Batch [667/782]: Loss 1.0447204113006592\n",
      "Training Batch [668/782]: Loss 0.6053711771965027\n",
      "Training Batch [669/782]: Loss 0.8698790073394775\n",
      "Training Batch [670/782]: Loss 0.7012999653816223\n",
      "Training Batch [671/782]: Loss 0.5430213212966919\n",
      "Training Batch [672/782]: Loss 0.6735542416572571\n",
      "Training Batch [673/782]: Loss 0.8545116782188416\n",
      "Training Batch [674/782]: Loss 0.9471468329429626\n",
      "Training Batch [675/782]: Loss 0.9505347609519958\n",
      "Training Batch [676/782]: Loss 0.8144479990005493\n",
      "Training Batch [677/782]: Loss 0.8801727890968323\n",
      "Training Batch [678/782]: Loss 0.6312220692634583\n",
      "Training Batch [679/782]: Loss 0.7203328013420105\n",
      "Training Batch [680/782]: Loss 0.6430943608283997\n",
      "Training Batch [681/782]: Loss 0.6143938899040222\n",
      "Training Batch [682/782]: Loss 0.9310198426246643\n",
      "Training Batch [683/782]: Loss 0.6299152970314026\n",
      "Training Batch [684/782]: Loss 0.7067625522613525\n",
      "Training Batch [685/782]: Loss 0.7201122641563416\n",
      "Training Batch [686/782]: Loss 0.6234177350997925\n",
      "Training Batch [687/782]: Loss 1.0983624458312988\n",
      "Training Batch [688/782]: Loss 0.6771539449691772\n",
      "Training Batch [689/782]: Loss 0.7571303248405457\n",
      "Training Batch [690/782]: Loss 0.7072970867156982\n",
      "Training Batch [691/782]: Loss 0.8627923727035522\n",
      "Training Batch [692/782]: Loss 0.8494874238967896\n",
      "Training Batch [693/782]: Loss 0.7947748899459839\n",
      "Training Batch [694/782]: Loss 0.8790982365608215\n",
      "Training Batch [695/782]: Loss 0.632004976272583\n",
      "Training Batch [696/782]: Loss 0.6292012333869934\n",
      "Training Batch [697/782]: Loss 1.028674840927124\n",
      "Training Batch [698/782]: Loss 0.902129590511322\n",
      "Training Batch [699/782]: Loss 0.9046804308891296\n",
      "Training Batch [700/782]: Loss 0.5975435376167297\n",
      "Training Batch [701/782]: Loss 0.7548680305480957\n",
      "Training Batch [702/782]: Loss 0.5422620177268982\n",
      "Training Batch [703/782]: Loss 0.9756175875663757\n",
      "Training Batch [704/782]: Loss 0.9027078747749329\n",
      "Training Batch [705/782]: Loss 0.597835123538971\n",
      "Training Batch [706/782]: Loss 0.7999281287193298\n",
      "Training Batch [707/782]: Loss 0.6702216267585754\n",
      "Training Batch [708/782]: Loss 0.8260964751243591\n",
      "Training Batch [709/782]: Loss 0.7718602418899536\n",
      "Training Batch [710/782]: Loss 0.7142173647880554\n",
      "Training Batch [711/782]: Loss 1.1203912496566772\n",
      "Training Batch [712/782]: Loss 0.9565188884735107\n",
      "Training Batch [713/782]: Loss 1.0342860221862793\n",
      "Training Batch [714/782]: Loss 1.0602883100509644\n",
      "Training Batch [715/782]: Loss 0.7737430930137634\n",
      "Training Batch [716/782]: Loss 0.8346133828163147\n",
      "Training Batch [717/782]: Loss 0.8209264278411865\n",
      "Training Batch [718/782]: Loss 0.8180044293403625\n",
      "Training Batch [719/782]: Loss 0.9565994739532471\n",
      "Training Batch [720/782]: Loss 0.5774402022361755\n",
      "Training Batch [721/782]: Loss 0.920128583908081\n",
      "Training Batch [722/782]: Loss 0.7709798216819763\n",
      "Training Batch [723/782]: Loss 0.965732216835022\n",
      "Training Batch [724/782]: Loss 0.8672501444816589\n",
      "Training Batch [725/782]: Loss 0.9046648144721985\n",
      "Training Batch [726/782]: Loss 0.8925951719284058\n",
      "Training Batch [727/782]: Loss 0.6309102773666382\n",
      "Training Batch [728/782]: Loss 0.6882296800613403\n",
      "Training Batch [729/782]: Loss 1.0708699226379395\n",
      "Training Batch [730/782]: Loss 0.8044111728668213\n",
      "Training Batch [731/782]: Loss 0.7461695075035095\n",
      "Training Batch [732/782]: Loss 0.798312246799469\n",
      "Training Batch [733/782]: Loss 1.1321794986724854\n",
      "Training Batch [734/782]: Loss 0.5456851720809937\n",
      "Training Batch [735/782]: Loss 0.7766607403755188\n",
      "Training Batch [736/782]: Loss 0.6836534142494202\n",
      "Training Batch [737/782]: Loss 0.7858295440673828\n",
      "Training Batch [738/782]: Loss 1.054883360862732\n",
      "Training Batch [739/782]: Loss 0.6130266189575195\n",
      "Training Batch [740/782]: Loss 0.857312798500061\n",
      "Training Batch [741/782]: Loss 0.73537278175354\n",
      "Training Batch [742/782]: Loss 0.5651936531066895\n",
      "Training Batch [743/782]: Loss 0.9283656477928162\n",
      "Training Batch [744/782]: Loss 1.0398662090301514\n",
      "Training Batch [745/782]: Loss 0.8847914338111877\n",
      "Training Batch [746/782]: Loss 0.7081258296966553\n",
      "Training Batch [747/782]: Loss 0.6110885739326477\n",
      "Training Batch [748/782]: Loss 0.760841965675354\n",
      "Training Batch [749/782]: Loss 0.6243158578872681\n",
      "Training Batch [750/782]: Loss 0.8110018968582153\n",
      "Training Batch [751/782]: Loss 0.9342714548110962\n",
      "Training Batch [752/782]: Loss 0.6585424542427063\n",
      "Training Batch [753/782]: Loss 0.779742956161499\n",
      "Training Batch [754/782]: Loss 0.9394726753234863\n",
      "Training Batch [755/782]: Loss 0.8099637031555176\n",
      "Training Batch [756/782]: Loss 0.7622280120849609\n",
      "Training Batch [757/782]: Loss 0.9746081233024597\n",
      "Training Batch [758/782]: Loss 0.7997993230819702\n",
      "Training Batch [759/782]: Loss 0.5826383233070374\n",
      "Training Batch [760/782]: Loss 0.8299371004104614\n",
      "Training Batch [761/782]: Loss 0.8816731572151184\n",
      "Training Batch [762/782]: Loss 0.7042181491851807\n",
      "Training Batch [763/782]: Loss 0.7425617575645447\n",
      "Training Batch [764/782]: Loss 1.1995234489440918\n",
      "Training Batch [765/782]: Loss 0.46886759996414185\n",
      "Training Batch [766/782]: Loss 0.8065442442893982\n",
      "Training Batch [767/782]: Loss 0.9274144172668457\n",
      "Training Batch [768/782]: Loss 0.6652696132659912\n",
      "Training Batch [769/782]: Loss 0.7556655406951904\n",
      "Training Batch [770/782]: Loss 1.026928424835205\n",
      "Training Batch [771/782]: Loss 0.8230975270271301\n",
      "Training Batch [772/782]: Loss 0.728045642375946\n",
      "Training Batch [773/782]: Loss 0.8389384150505066\n",
      "Training Batch [774/782]: Loss 0.534446656703949\n",
      "Training Batch [775/782]: Loss 0.8121256828308105\n",
      "Training Batch [776/782]: Loss 0.7317896485328674\n",
      "Training Batch [777/782]: Loss 0.7663509845733643\n",
      "Training Batch [778/782]: Loss 1.0949033498764038\n",
      "Training Batch [779/782]: Loss 0.5852128267288208\n",
      "Training Batch [780/782]: Loss 0.9646825790405273\n",
      "Training Batch [781/782]: Loss 0.7548782825469971\n",
      "Training Batch [782/782]: Loss 1.3070849180221558\n",
      "Epoch 6 - Train Loss: 0.7575\n",
      "*********  Epoch 7/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.6933326721191406\n",
      "Training Batch [2/782]: Loss 0.5644760727882385\n",
      "Training Batch [3/782]: Loss 0.5489718914031982\n",
      "Training Batch [4/782]: Loss 0.5565447211265564\n",
      "Training Batch [5/782]: Loss 0.5556743144989014\n",
      "Training Batch [6/782]: Loss 0.5935924053192139\n",
      "Training Batch [7/782]: Loss 0.5282039642333984\n",
      "Training Batch [8/782]: Loss 0.70393306016922\n",
      "Training Batch [9/782]: Loss 0.4386971890926361\n",
      "Training Batch [10/782]: Loss 0.4922192394733429\n",
      "Training Batch [11/782]: Loss 0.5751290917396545\n",
      "Training Batch [12/782]: Loss 0.5034543871879578\n",
      "Training Batch [13/782]: Loss 0.44199231266975403\n",
      "Training Batch [14/782]: Loss 0.4690139889717102\n",
      "Training Batch [15/782]: Loss 0.4883616268634796\n",
      "Training Batch [16/782]: Loss 0.5073044300079346\n",
      "Training Batch [17/782]: Loss 0.5322139859199524\n",
      "Training Batch [18/782]: Loss 0.7037750482559204\n",
      "Training Batch [19/782]: Loss 0.43537837266921997\n",
      "Training Batch [20/782]: Loss 0.42309287190437317\n",
      "Training Batch [21/782]: Loss 0.4927203357219696\n",
      "Training Batch [22/782]: Loss 0.5621383190155029\n",
      "Training Batch [23/782]: Loss 0.6924778819084167\n",
      "Training Batch [24/782]: Loss 0.6990404725074768\n",
      "Training Batch [25/782]: Loss 0.48519033193588257\n",
      "Training Batch [26/782]: Loss 0.41510576009750366\n",
      "Training Batch [27/782]: Loss 0.40907588601112366\n",
      "Training Batch [28/782]: Loss 0.36751195788383484\n",
      "Training Batch [29/782]: Loss 0.48163896799087524\n",
      "Training Batch [30/782]: Loss 0.4249277412891388\n",
      "Training Batch [31/782]: Loss 0.5276917815208435\n",
      "Training Batch [32/782]: Loss 0.492387980222702\n",
      "Training Batch [33/782]: Loss 0.39239880442619324\n",
      "Training Batch [34/782]: Loss 0.6652497053146362\n",
      "Training Batch [35/782]: Loss 0.5361427664756775\n",
      "Training Batch [36/782]: Loss 0.4593717157840729\n",
      "Training Batch [37/782]: Loss 0.47496217489242554\n",
      "Training Batch [38/782]: Loss 0.4037889540195465\n",
      "Training Batch [39/782]: Loss 0.6729522347450256\n",
      "Training Batch [40/782]: Loss 0.5699095129966736\n",
      "Training Batch [41/782]: Loss 0.3888761103153229\n",
      "Training Batch [42/782]: Loss 0.4934791624546051\n",
      "Training Batch [43/782]: Loss 0.4824288487434387\n",
      "Training Batch [44/782]: Loss 0.4549701511859894\n",
      "Training Batch [45/782]: Loss 0.38959237933158875\n",
      "Training Batch [46/782]: Loss 0.5963779091835022\n",
      "Training Batch [47/782]: Loss 0.3922000825405121\n",
      "Training Batch [48/782]: Loss 0.31767722964286804\n",
      "Training Batch [49/782]: Loss 0.35309165716171265\n",
      "Training Batch [50/782]: Loss 0.27335530519485474\n",
      "Training Batch [51/782]: Loss 0.43022847175598145\n",
      "Training Batch [52/782]: Loss 0.21847689151763916\n",
      "Training Batch [53/782]: Loss 0.3095821142196655\n",
      "Training Batch [54/782]: Loss 0.44060561060905457\n",
      "Training Batch [55/782]: Loss 0.4590607285499573\n",
      "Training Batch [56/782]: Loss 0.32721227407455444\n",
      "Training Batch [57/782]: Loss 0.40838301181793213\n",
      "Training Batch [58/782]: Loss 0.31263476610183716\n",
      "Training Batch [59/782]: Loss 0.4870278239250183\n",
      "Training Batch [60/782]: Loss 0.6304510235786438\n",
      "Training Batch [61/782]: Loss 0.6306919455528259\n",
      "Training Batch [62/782]: Loss 0.6713297367095947\n",
      "Training Batch [63/782]: Loss 0.32485732436180115\n",
      "Training Batch [64/782]: Loss 0.5287370681762695\n",
      "Training Batch [65/782]: Loss 0.37662041187286377\n",
      "Training Batch [66/782]: Loss 0.2604904770851135\n",
      "Training Batch [67/782]: Loss 0.43013516068458557\n",
      "Training Batch [68/782]: Loss 0.4760541617870331\n",
      "Training Batch [69/782]: Loss 0.44389471411705017\n",
      "Training Batch [70/782]: Loss 0.41150590777397156\n",
      "Training Batch [71/782]: Loss 0.47347137331962585\n",
      "Training Batch [72/782]: Loss 0.4632877707481384\n",
      "Training Batch [73/782]: Loss 0.3955412209033966\n",
      "Training Batch [74/782]: Loss 0.3571356534957886\n",
      "Training Batch [75/782]: Loss 0.5763469338417053\n",
      "Training Batch [76/782]: Loss 0.4889579117298126\n",
      "Training Batch [77/782]: Loss 0.4569222033023834\n",
      "Training Batch [78/782]: Loss 0.6029521822929382\n",
      "Training Batch [79/782]: Loss 0.39293423295021057\n",
      "Training Batch [80/782]: Loss 0.5071061849594116\n",
      "Training Batch [81/782]: Loss 0.5182478427886963\n",
      "Training Batch [82/782]: Loss 0.31277674436569214\n",
      "Training Batch [83/782]: Loss 0.5685298442840576\n",
      "Training Batch [84/782]: Loss 0.34926414489746094\n",
      "Training Batch [85/782]: Loss 0.3868826925754547\n",
      "Training Batch [86/782]: Loss 0.330841600894928\n",
      "Training Batch [87/782]: Loss 0.553468644618988\n",
      "Training Batch [88/782]: Loss 0.27336058020591736\n",
      "Training Batch [89/782]: Loss 0.3515382707118988\n",
      "Training Batch [90/782]: Loss 0.365021675825119\n",
      "Training Batch [91/782]: Loss 0.42543062567710876\n",
      "Training Batch [92/782]: Loss 0.3205963373184204\n",
      "Training Batch [93/782]: Loss 0.6310701370239258\n",
      "Training Batch [94/782]: Loss 0.449320524930954\n",
      "Training Batch [95/782]: Loss 0.410625159740448\n",
      "Training Batch [96/782]: Loss 0.3163297474384308\n",
      "Training Batch [97/782]: Loss 0.38228362798690796\n",
      "Training Batch [98/782]: Loss 0.34708595275878906\n",
      "Training Batch [99/782]: Loss 0.4290272295475006\n",
      "Training Batch [100/782]: Loss 0.5476192235946655\n",
      "Training Batch [101/782]: Loss 0.4012153446674347\n",
      "Training Batch [102/782]: Loss 0.5644868016242981\n",
      "Training Batch [103/782]: Loss 0.4105476140975952\n",
      "Training Batch [104/782]: Loss 0.5367246866226196\n",
      "Training Batch [105/782]: Loss 0.4676330089569092\n",
      "Training Batch [106/782]: Loss 0.5216975808143616\n",
      "Training Batch [107/782]: Loss 0.3797341287136078\n",
      "Training Batch [108/782]: Loss 0.43280503153800964\n",
      "Training Batch [109/782]: Loss 0.4334195554256439\n",
      "Training Batch [110/782]: Loss 0.402241975069046\n",
      "Training Batch [111/782]: Loss 0.5047948360443115\n",
      "Training Batch [112/782]: Loss 0.4411340057849884\n",
      "Training Batch [113/782]: Loss 0.4004935324192047\n",
      "Training Batch [114/782]: Loss 0.29322683811187744\n",
      "Training Batch [115/782]: Loss 0.48438942432403564\n",
      "Training Batch [116/782]: Loss 0.5800155401229858\n",
      "Training Batch [117/782]: Loss 0.5480873584747314\n",
      "Training Batch [118/782]: Loss 0.4025461971759796\n",
      "Training Batch [119/782]: Loss 0.522566556930542\n",
      "Training Batch [120/782]: Loss 0.30354297161102295\n",
      "Training Batch [121/782]: Loss 0.43202298879623413\n",
      "Training Batch [122/782]: Loss 0.4249711036682129\n",
      "Training Batch [123/782]: Loss 0.4347776770591736\n",
      "Training Batch [124/782]: Loss 0.37684890627861023\n",
      "Training Batch [125/782]: Loss 0.4262637495994568\n",
      "Training Batch [126/782]: Loss 0.5583796501159668\n",
      "Training Batch [127/782]: Loss 0.5072231292724609\n",
      "Training Batch [128/782]: Loss 0.4762551486492157\n",
      "Training Batch [129/782]: Loss 0.5142184495925903\n",
      "Training Batch [130/782]: Loss 0.4451572895050049\n",
      "Training Batch [131/782]: Loss 0.3829794228076935\n",
      "Training Batch [132/782]: Loss 0.4769805669784546\n",
      "Training Batch [133/782]: Loss 0.3250596225261688\n",
      "Training Batch [134/782]: Loss 0.4844498038291931\n",
      "Training Batch [135/782]: Loss 0.38958683609962463\n",
      "Training Batch [136/782]: Loss 0.5418269634246826\n",
      "Training Batch [137/782]: Loss 0.41863760352134705\n",
      "Training Batch [138/782]: Loss 0.4196430444717407\n",
      "Training Batch [139/782]: Loss 0.4111001789569855\n",
      "Training Batch [140/782]: Loss 0.3332007825374603\n",
      "Training Batch [141/782]: Loss 0.4111391007900238\n",
      "Training Batch [142/782]: Loss 0.44265687465667725\n",
      "Training Batch [143/782]: Loss 0.5900031924247742\n",
      "Training Batch [144/782]: Loss 0.33768635988235474\n",
      "Training Batch [145/782]: Loss 0.3848365545272827\n",
      "Training Batch [146/782]: Loss 0.41032838821411133\n",
      "Training Batch [147/782]: Loss 0.46301841735839844\n",
      "Training Batch [148/782]: Loss 0.5198109149932861\n",
      "Training Batch [149/782]: Loss 0.5211357474327087\n",
      "Training Batch [150/782]: Loss 0.7612693309783936\n",
      "Training Batch [151/782]: Loss 0.35269516706466675\n",
      "Training Batch [152/782]: Loss 0.5408301949501038\n",
      "Training Batch [153/782]: Loss 0.5093410015106201\n",
      "Training Batch [154/782]: Loss 0.4906163811683655\n",
      "Training Batch [155/782]: Loss 0.3595692813396454\n",
      "Training Batch [156/782]: Loss 0.3696569502353668\n",
      "Training Batch [157/782]: Loss 0.6122791767120361\n",
      "Training Batch [158/782]: Loss 0.49952083826065063\n",
      "Training Batch [159/782]: Loss 0.33315956592559814\n",
      "Training Batch [160/782]: Loss 0.5490106344223022\n",
      "Training Batch [161/782]: Loss 0.5445033311843872\n",
      "Training Batch [162/782]: Loss 0.6026085019111633\n",
      "Training Batch [163/782]: Loss 0.5623399615287781\n",
      "Training Batch [164/782]: Loss 0.38225725293159485\n",
      "Training Batch [165/782]: Loss 0.6024764180183411\n",
      "Training Batch [166/782]: Loss 0.485019326210022\n",
      "Training Batch [167/782]: Loss 0.6151335835456848\n",
      "Training Batch [168/782]: Loss 0.6172214150428772\n",
      "Training Batch [169/782]: Loss 0.5419804453849792\n",
      "Training Batch [170/782]: Loss 0.6019708514213562\n",
      "Training Batch [171/782]: Loss 0.43647265434265137\n",
      "Training Batch [172/782]: Loss 0.308806449174881\n",
      "Training Batch [173/782]: Loss 0.43816077709198\n",
      "Training Batch [174/782]: Loss 0.46939000487327576\n",
      "Training Batch [175/782]: Loss 0.6904495358467102\n",
      "Training Batch [176/782]: Loss 0.4639316499233246\n",
      "Training Batch [177/782]: Loss 0.5056867003440857\n",
      "Training Batch [178/782]: Loss 0.4848199486732483\n",
      "Training Batch [179/782]: Loss 0.5255482792854309\n",
      "Training Batch [180/782]: Loss 0.4045090079307556\n",
      "Training Batch [181/782]: Loss 0.43402430415153503\n",
      "Training Batch [182/782]: Loss 0.5672300457954407\n",
      "Training Batch [183/782]: Loss 0.5785253643989563\n",
      "Training Batch [184/782]: Loss 0.4223509430885315\n",
      "Training Batch [185/782]: Loss 0.5211343169212341\n",
      "Training Batch [186/782]: Loss 0.43832746148109436\n",
      "Training Batch [187/782]: Loss 0.3851953148841858\n",
      "Training Batch [188/782]: Loss 0.42647334933280945\n",
      "Training Batch [189/782]: Loss 0.5327860116958618\n",
      "Training Batch [190/782]: Loss 0.388377845287323\n",
      "Training Batch [191/782]: Loss 0.4099772274494171\n",
      "Training Batch [192/782]: Loss 0.5172291994094849\n",
      "Training Batch [193/782]: Loss 0.4340919554233551\n",
      "Training Batch [194/782]: Loss 0.561743438243866\n",
      "Training Batch [195/782]: Loss 0.37762704491615295\n",
      "Training Batch [196/782]: Loss 0.4511239528656006\n",
      "Training Batch [197/782]: Loss 0.4722102880477905\n",
      "Training Batch [198/782]: Loss 0.45276498794555664\n",
      "Training Batch [199/782]: Loss 0.39726805686950684\n",
      "Training Batch [200/782]: Loss 0.37284180521965027\n",
      "Training Batch [201/782]: Loss 0.29270607233047485\n",
      "Training Batch [202/782]: Loss 0.4443508982658386\n",
      "Training Batch [203/782]: Loss 0.4779500365257263\n",
      "Training Batch [204/782]: Loss 0.301631897687912\n",
      "Training Batch [205/782]: Loss 0.6364601254463196\n",
      "Training Batch [206/782]: Loss 0.39016059041023254\n",
      "Training Batch [207/782]: Loss 0.3544140160083771\n",
      "Training Batch [208/782]: Loss 0.4298284947872162\n",
      "Training Batch [209/782]: Loss 0.6425220966339111\n",
      "Training Batch [210/782]: Loss 0.48853883147239685\n",
      "Training Batch [211/782]: Loss 0.27622246742248535\n",
      "Training Batch [212/782]: Loss 0.4209222197532654\n",
      "Training Batch [213/782]: Loss 0.5560786724090576\n",
      "Training Batch [214/782]: Loss 0.45103588700294495\n",
      "Training Batch [215/782]: Loss 0.5515100955963135\n",
      "Training Batch [216/782]: Loss 0.5764844417572021\n",
      "Training Batch [217/782]: Loss 0.5156413316726685\n",
      "Training Batch [218/782]: Loss 0.4734652638435364\n",
      "Training Batch [219/782]: Loss 0.7094601392745972\n",
      "Training Batch [220/782]: Loss 0.42236626148223877\n",
      "Training Batch [221/782]: Loss 0.4401395320892334\n",
      "Training Batch [222/782]: Loss 0.2892386317253113\n",
      "Training Batch [223/782]: Loss 0.5041471719741821\n",
      "Training Batch [224/782]: Loss 0.5101532340049744\n",
      "Training Batch [225/782]: Loss 0.5714364647865295\n",
      "Training Batch [226/782]: Loss 0.37181100249290466\n",
      "Training Batch [227/782]: Loss 0.4888555407524109\n",
      "Training Batch [228/782]: Loss 0.4701988995075226\n",
      "Training Batch [229/782]: Loss 0.462303102016449\n",
      "Training Batch [230/782]: Loss 0.5331094861030579\n",
      "Training Batch [231/782]: Loss 0.44498735666275024\n",
      "Training Batch [232/782]: Loss 0.46905526518821716\n",
      "Training Batch [233/782]: Loss 0.41904154419898987\n",
      "Training Batch [234/782]: Loss 0.4373745918273926\n",
      "Training Batch [235/782]: Loss 0.6280627250671387\n",
      "Training Batch [236/782]: Loss 0.5164855718612671\n",
      "Training Batch [237/782]: Loss 0.5433529019355774\n",
      "Training Batch [238/782]: Loss 0.48457732796669006\n",
      "Training Batch [239/782]: Loss 0.4646156132221222\n",
      "Training Batch [240/782]: Loss 0.41234442591667175\n",
      "Training Batch [241/782]: Loss 0.5410118103027344\n",
      "Training Batch [242/782]: Loss 0.40667951107025146\n",
      "Training Batch [243/782]: Loss 0.6110119819641113\n",
      "Training Batch [244/782]: Loss 0.28236642479896545\n",
      "Training Batch [245/782]: Loss 0.6081439256668091\n",
      "Training Batch [246/782]: Loss 0.38971176743507385\n",
      "Training Batch [247/782]: Loss 0.5584831833839417\n",
      "Training Batch [248/782]: Loss 0.5578815340995789\n",
      "Training Batch [249/782]: Loss 0.5954246520996094\n",
      "Training Batch [250/782]: Loss 0.5976691246032715\n",
      "Training Batch [251/782]: Loss 0.6515154838562012\n",
      "Training Batch [252/782]: Loss 0.6407874226570129\n",
      "Training Batch [253/782]: Loss 0.4646676480770111\n",
      "Training Batch [254/782]: Loss 0.46825119853019714\n",
      "Training Batch [255/782]: Loss 0.49480268359184265\n",
      "Training Batch [256/782]: Loss 0.47645190358161926\n",
      "Training Batch [257/782]: Loss 0.6596187353134155\n",
      "Training Batch [258/782]: Loss 0.352245569229126\n",
      "Training Batch [259/782]: Loss 0.6077740788459778\n",
      "Training Batch [260/782]: Loss 0.5221220254898071\n",
      "Training Batch [261/782]: Loss 0.4992314279079437\n",
      "Training Batch [262/782]: Loss 0.4639683961868286\n",
      "Training Batch [263/782]: Loss 0.49362272024154663\n",
      "Training Batch [264/782]: Loss 0.6115690469741821\n",
      "Training Batch [265/782]: Loss 0.7734061479568481\n",
      "Training Batch [266/782]: Loss 0.4020364582538605\n",
      "Training Batch [267/782]: Loss 0.6571504473686218\n",
      "Training Batch [268/782]: Loss 0.825958251953125\n",
      "Training Batch [269/782]: Loss 0.6498553156852722\n",
      "Training Batch [270/782]: Loss 0.4507143795490265\n",
      "Training Batch [271/782]: Loss 0.43227145075798035\n",
      "Training Batch [272/782]: Loss 0.35231685638427734\n",
      "Training Batch [273/782]: Loss 0.4593651294708252\n",
      "Training Batch [274/782]: Loss 0.6827545166015625\n",
      "Training Batch [275/782]: Loss 0.39675742387771606\n",
      "Training Batch [276/782]: Loss 0.635549783706665\n",
      "Training Batch [277/782]: Loss 0.46896395087242126\n",
      "Training Batch [278/782]: Loss 0.4863809049129486\n",
      "Training Batch [279/782]: Loss 0.4997576177120209\n",
      "Training Batch [280/782]: Loss 0.6240239143371582\n",
      "Training Batch [281/782]: Loss 0.48272624611854553\n",
      "Training Batch [282/782]: Loss 0.6603760123252869\n",
      "Training Batch [283/782]: Loss 0.4983615279197693\n",
      "Training Batch [284/782]: Loss 0.5760360360145569\n",
      "Training Batch [285/782]: Loss 0.6614590883255005\n",
      "Training Batch [286/782]: Loss 0.6822080612182617\n",
      "Training Batch [287/782]: Loss 0.6606009602546692\n",
      "Training Batch [288/782]: Loss 0.5498654842376709\n",
      "Training Batch [289/782]: Loss 0.6645457148551941\n",
      "Training Batch [290/782]: Loss 0.5684831142425537\n",
      "Training Batch [291/782]: Loss 0.6306028366088867\n",
      "Training Batch [292/782]: Loss 0.4118315875530243\n",
      "Training Batch [293/782]: Loss 0.3613360822200775\n",
      "Training Batch [294/782]: Loss 0.4105190336704254\n",
      "Training Batch [295/782]: Loss 0.5145434737205505\n",
      "Training Batch [296/782]: Loss 0.5777873396873474\n",
      "Training Batch [297/782]: Loss 0.3355218172073364\n",
      "Training Batch [298/782]: Loss 0.4996049404144287\n",
      "Training Batch [299/782]: Loss 0.6223862171173096\n",
      "Training Batch [300/782]: Loss 0.46258172392845154\n",
      "Training Batch [301/782]: Loss 0.7159016132354736\n",
      "Training Batch [302/782]: Loss 0.7141929864883423\n",
      "Training Batch [303/782]: Loss 0.5573140382766724\n",
      "Training Batch [304/782]: Loss 0.6528791189193726\n",
      "Training Batch [305/782]: Loss 0.521406352519989\n",
      "Training Batch [306/782]: Loss 0.6609939932823181\n",
      "Training Batch [307/782]: Loss 0.4983054995536804\n",
      "Training Batch [308/782]: Loss 0.4604150056838989\n",
      "Training Batch [309/782]: Loss 0.6826775074005127\n",
      "Training Batch [310/782]: Loss 0.7574028968811035\n",
      "Training Batch [311/782]: Loss 0.7604032754898071\n",
      "Training Batch [312/782]: Loss 0.48940160870552063\n",
      "Training Batch [313/782]: Loss 0.44040942192077637\n",
      "Training Batch [314/782]: Loss 0.7136228680610657\n",
      "Training Batch [315/782]: Loss 0.46062034368515015\n",
      "Training Batch [316/782]: Loss 0.7774174213409424\n",
      "Training Batch [317/782]: Loss 0.5325182676315308\n",
      "Training Batch [318/782]: Loss 0.5817528963088989\n",
      "Training Batch [319/782]: Loss 0.4647682309150696\n",
      "Training Batch [320/782]: Loss 0.47511622309684753\n",
      "Training Batch [321/782]: Loss 0.5503976345062256\n",
      "Training Batch [322/782]: Loss 0.4757293164730072\n",
      "Training Batch [323/782]: Loss 0.455230712890625\n",
      "Training Batch [324/782]: Loss 0.7424742579460144\n",
      "Training Batch [325/782]: Loss 0.595578670501709\n",
      "Training Batch [326/782]: Loss 0.7610296607017517\n",
      "Training Batch [327/782]: Loss 0.5373018383979797\n",
      "Training Batch [328/782]: Loss 0.5583116412162781\n",
      "Training Batch [329/782]: Loss 0.5570074915885925\n",
      "Training Batch [330/782]: Loss 0.4686647951602936\n",
      "Training Batch [331/782]: Loss 0.5249755382537842\n",
      "Training Batch [332/782]: Loss 0.6000449061393738\n",
      "Training Batch [333/782]: Loss 0.5268423557281494\n",
      "Training Batch [334/782]: Loss 0.44347888231277466\n",
      "Training Batch [335/782]: Loss 0.4109754264354706\n",
      "Training Batch [336/782]: Loss 0.7303881645202637\n",
      "Training Batch [337/782]: Loss 0.5559223890304565\n",
      "Training Batch [338/782]: Loss 0.46964317560195923\n",
      "Training Batch [339/782]: Loss 0.6066471338272095\n",
      "Training Batch [340/782]: Loss 0.6598761081695557\n",
      "Training Batch [341/782]: Loss 0.5391569137573242\n",
      "Training Batch [342/782]: Loss 0.43845659494400024\n",
      "Training Batch [343/782]: Loss 0.457176148891449\n",
      "Training Batch [344/782]: Loss 0.5503110289573669\n",
      "Training Batch [345/782]: Loss 0.36578360199928284\n",
      "Training Batch [346/782]: Loss 0.4768466055393219\n",
      "Training Batch [347/782]: Loss 0.3709257245063782\n",
      "Training Batch [348/782]: Loss 0.5695858001708984\n",
      "Training Batch [349/782]: Loss 0.5100981593132019\n",
      "Training Batch [350/782]: Loss 0.38012969493865967\n",
      "Training Batch [351/782]: Loss 0.48028868436813354\n",
      "Training Batch [352/782]: Loss 0.5283989906311035\n",
      "Training Batch [353/782]: Loss 0.3904231786727905\n",
      "Training Batch [354/782]: Loss 0.7789602279663086\n",
      "Training Batch [355/782]: Loss 0.7283763289451599\n",
      "Training Batch [356/782]: Loss 0.6461069583892822\n",
      "Training Batch [357/782]: Loss 0.39275774359703064\n",
      "Training Batch [358/782]: Loss 0.5703660845756531\n",
      "Training Batch [359/782]: Loss 0.404293030500412\n",
      "Training Batch [360/782]: Loss 0.785884439945221\n",
      "Training Batch [361/782]: Loss 0.6612922549247742\n",
      "Training Batch [362/782]: Loss 0.5925949811935425\n",
      "Training Batch [363/782]: Loss 0.4676884114742279\n",
      "Training Batch [364/782]: Loss 0.416138619184494\n",
      "Training Batch [365/782]: Loss 0.48640087246894836\n",
      "Training Batch [366/782]: Loss 0.48969918489456177\n",
      "Training Batch [367/782]: Loss 0.4411395192146301\n",
      "Training Batch [368/782]: Loss 0.6600580215454102\n",
      "Training Batch [369/782]: Loss 0.572625994682312\n",
      "Training Batch [370/782]: Loss 0.6595481634140015\n",
      "Training Batch [371/782]: Loss 0.6274107098579407\n",
      "Training Batch [372/782]: Loss 0.493522584438324\n",
      "Training Batch [373/782]: Loss 0.40889883041381836\n",
      "Training Batch [374/782]: Loss 0.4858815371990204\n",
      "Training Batch [375/782]: Loss 0.5637266635894775\n",
      "Training Batch [376/782]: Loss 0.5348080992698669\n",
      "Training Batch [377/782]: Loss 0.6348211765289307\n",
      "Training Batch [378/782]: Loss 0.47924312949180603\n",
      "Training Batch [379/782]: Loss 0.46068528294563293\n",
      "Training Batch [380/782]: Loss 0.5271365642547607\n",
      "Training Batch [381/782]: Loss 0.6410273313522339\n",
      "Training Batch [382/782]: Loss 0.49934685230255127\n",
      "Training Batch [383/782]: Loss 0.6977056264877319\n",
      "Training Batch [384/782]: Loss 0.8017218708992004\n",
      "Training Batch [385/782]: Loss 0.5946409106254578\n",
      "Training Batch [386/782]: Loss 0.6443454027175903\n",
      "Training Batch [387/782]: Loss 0.5712106227874756\n",
      "Training Batch [388/782]: Loss 0.3587835133075714\n",
      "Training Batch [389/782]: Loss 0.3998473882675171\n",
      "Training Batch [390/782]: Loss 0.4261881113052368\n",
      "Training Batch [391/782]: Loss 0.43695515394210815\n",
      "Training Batch [392/782]: Loss 0.5954621434211731\n",
      "Training Batch [393/782]: Loss 0.7104076147079468\n",
      "Training Batch [394/782]: Loss 0.6180769205093384\n",
      "Training Batch [395/782]: Loss 0.426324725151062\n",
      "Training Batch [396/782]: Loss 0.43936869502067566\n",
      "Training Batch [397/782]: Loss 0.666465163230896\n",
      "Training Batch [398/782]: Loss 0.5428484082221985\n",
      "Training Batch [399/782]: Loss 0.4709396958351135\n",
      "Training Batch [400/782]: Loss 0.7869986891746521\n",
      "Training Batch [401/782]: Loss 0.7025890946388245\n",
      "Training Batch [402/782]: Loss 0.4567483067512512\n",
      "Training Batch [403/782]: Loss 0.39972782135009766\n",
      "Training Batch [404/782]: Loss 0.5226196050643921\n",
      "Training Batch [405/782]: Loss 0.3682118356227875\n",
      "Training Batch [406/782]: Loss 0.6783685088157654\n",
      "Training Batch [407/782]: Loss 0.5149267315864563\n",
      "Training Batch [408/782]: Loss 0.8130577206611633\n",
      "Training Batch [409/782]: Loss 0.621849000453949\n",
      "Training Batch [410/782]: Loss 0.7424205541610718\n",
      "Training Batch [411/782]: Loss 0.7866686582565308\n",
      "Training Batch [412/782]: Loss 0.5952032804489136\n",
      "Training Batch [413/782]: Loss 0.41684049367904663\n",
      "Training Batch [414/782]: Loss 0.4829022288322449\n",
      "Training Batch [415/782]: Loss 0.6498777866363525\n",
      "Training Batch [416/782]: Loss 0.8749150037765503\n",
      "Training Batch [417/782]: Loss 0.5274299383163452\n",
      "Training Batch [418/782]: Loss 0.5148025751113892\n",
      "Training Batch [419/782]: Loss 0.536420464515686\n",
      "Training Batch [420/782]: Loss 0.6734018921852112\n",
      "Training Batch [421/782]: Loss 0.6557109355926514\n",
      "Training Batch [422/782]: Loss 0.540895402431488\n",
      "Training Batch [423/782]: Loss 0.5563724040985107\n",
      "Training Batch [424/782]: Loss 0.23357565701007843\n",
      "Training Batch [425/782]: Loss 0.5717676281929016\n",
      "Training Batch [426/782]: Loss 0.6612728238105774\n",
      "Training Batch [427/782]: Loss 0.42750948667526245\n",
      "Training Batch [428/782]: Loss 0.620495617389679\n",
      "Training Batch [429/782]: Loss 0.6929017901420593\n",
      "Training Batch [430/782]: Loss 0.6053869128227234\n",
      "Training Batch [431/782]: Loss 0.5917141437530518\n",
      "Training Batch [432/782]: Loss 0.43276867270469666\n",
      "Training Batch [433/782]: Loss 0.28823721408843994\n",
      "Training Batch [434/782]: Loss 0.5743412971496582\n",
      "Training Batch [435/782]: Loss 0.6211997866630554\n",
      "Training Batch [436/782]: Loss 0.5834049582481384\n",
      "Training Batch [437/782]: Loss 0.6025919318199158\n",
      "Training Batch [438/782]: Loss 0.7588304877281189\n",
      "Training Batch [439/782]: Loss 0.6630825400352478\n",
      "Training Batch [440/782]: Loss 0.42897674441337585\n",
      "Training Batch [441/782]: Loss 0.3953578770160675\n",
      "Training Batch [442/782]: Loss 0.44508278369903564\n",
      "Training Batch [443/782]: Loss 0.4207076132297516\n",
      "Training Batch [444/782]: Loss 0.40621230006217957\n",
      "Training Batch [445/782]: Loss 0.4702184498310089\n",
      "Training Batch [446/782]: Loss 0.6704975962638855\n",
      "Training Batch [447/782]: Loss 0.6021791100502014\n",
      "Training Batch [448/782]: Loss 0.5991405844688416\n",
      "Training Batch [449/782]: Loss 0.6225067377090454\n",
      "Training Batch [450/782]: Loss 0.3652275502681732\n",
      "Training Batch [451/782]: Loss 0.520129382610321\n",
      "Training Batch [452/782]: Loss 0.6392176747322083\n",
      "Training Batch [453/782]: Loss 0.6462321877479553\n",
      "Training Batch [454/782]: Loss 0.47037920355796814\n",
      "Training Batch [455/782]: Loss 0.6427853107452393\n",
      "Training Batch [456/782]: Loss 0.6339707374572754\n",
      "Training Batch [457/782]: Loss 0.6907410025596619\n",
      "Training Batch [458/782]: Loss 0.441650390625\n",
      "Training Batch [459/782]: Loss 0.5849978923797607\n",
      "Training Batch [460/782]: Loss 0.4925254285335541\n",
      "Training Batch [461/782]: Loss 0.3539130687713623\n",
      "Training Batch [462/782]: Loss 0.6206796169281006\n",
      "Training Batch [463/782]: Loss 0.6394172310829163\n",
      "Training Batch [464/782]: Loss 0.6329669952392578\n",
      "Training Batch [465/782]: Loss 0.711866021156311\n",
      "Training Batch [466/782]: Loss 0.4244547188282013\n",
      "Training Batch [467/782]: Loss 0.4574151039123535\n",
      "Training Batch [468/782]: Loss 0.6722865700721741\n",
      "Training Batch [469/782]: Loss 0.5492451786994934\n",
      "Training Batch [470/782]: Loss 0.46391943097114563\n",
      "Training Batch [471/782]: Loss 0.4387224316596985\n",
      "Training Batch [472/782]: Loss 0.6683064103126526\n",
      "Training Batch [473/782]: Loss 0.48155444860458374\n",
      "Training Batch [474/782]: Loss 0.47694146633148193\n",
      "Training Batch [475/782]: Loss 0.43875575065612793\n",
      "Training Batch [476/782]: Loss 0.38432812690734863\n",
      "Training Batch [477/782]: Loss 0.6387008428573608\n",
      "Training Batch [478/782]: Loss 0.6120185852050781\n",
      "Training Batch [479/782]: Loss 0.5387828350067139\n",
      "Training Batch [480/782]: Loss 0.47149941325187683\n",
      "Training Batch [481/782]: Loss 0.554868221282959\n",
      "Training Batch [482/782]: Loss 0.6760962605476379\n",
      "Training Batch [483/782]: Loss 0.47622382640838623\n",
      "Training Batch [484/782]: Loss 0.6052286028862\n",
      "Training Batch [485/782]: Loss 0.46590283513069153\n",
      "Training Batch [486/782]: Loss 0.5332726836204529\n",
      "Training Batch [487/782]: Loss 0.3750745356082916\n",
      "Training Batch [488/782]: Loss 0.5134715437889099\n",
      "Training Batch [489/782]: Loss 0.5315953493118286\n",
      "Training Batch [490/782]: Loss 0.5583405494689941\n",
      "Training Batch [491/782]: Loss 0.4831784963607788\n",
      "Training Batch [492/782]: Loss 0.47806671261787415\n",
      "Training Batch [493/782]: Loss 0.8230446577072144\n",
      "Training Batch [494/782]: Loss 0.5710163116455078\n",
      "Training Batch [495/782]: Loss 0.9432543516159058\n",
      "Training Batch [496/782]: Loss 0.5642992854118347\n",
      "Training Batch [497/782]: Loss 0.6591668128967285\n",
      "Training Batch [498/782]: Loss 0.4124016761779785\n",
      "Training Batch [499/782]: Loss 0.5200647711753845\n",
      "Training Batch [500/782]: Loss 0.5878134369850159\n",
      "Training Batch [501/782]: Loss 0.6593233942985535\n",
      "Training Batch [502/782]: Loss 0.42810580134391785\n",
      "Training Batch [503/782]: Loss 0.41094955801963806\n",
      "Training Batch [504/782]: Loss 0.604592502117157\n",
      "Training Batch [505/782]: Loss 0.5596534609794617\n",
      "Training Batch [506/782]: Loss 0.7825129628181458\n",
      "Training Batch [507/782]: Loss 0.32281291484832764\n",
      "Training Batch [508/782]: Loss 0.6193798780441284\n",
      "Training Batch [509/782]: Loss 0.7354859709739685\n",
      "Training Batch [510/782]: Loss 0.3992900550365448\n",
      "Training Batch [511/782]: Loss 0.5497346520423889\n",
      "Training Batch [512/782]: Loss 0.6161484122276306\n",
      "Training Batch [513/782]: Loss 0.7497944831848145\n",
      "Training Batch [514/782]: Loss 0.4961327314376831\n",
      "Training Batch [515/782]: Loss 0.7876871228218079\n",
      "Training Batch [516/782]: Loss 0.7114561796188354\n",
      "Training Batch [517/782]: Loss 0.5931334495544434\n",
      "Training Batch [518/782]: Loss 0.7326148152351379\n",
      "Training Batch [519/782]: Loss 0.41182997822761536\n",
      "Training Batch [520/782]: Loss 0.5705509781837463\n",
      "Training Batch [521/782]: Loss 0.6425154209136963\n",
      "Training Batch [522/782]: Loss 0.5915188193321228\n",
      "Training Batch [523/782]: Loss 0.6628777980804443\n",
      "Training Batch [524/782]: Loss 0.7262208461761475\n",
      "Training Batch [525/782]: Loss 0.5319687128067017\n",
      "Training Batch [526/782]: Loss 0.38232922554016113\n",
      "Training Batch [527/782]: Loss 0.7308099269866943\n",
      "Training Batch [528/782]: Loss 0.4077755808830261\n",
      "Training Batch [529/782]: Loss 0.8100678324699402\n",
      "Training Batch [530/782]: Loss 0.4835642874240875\n",
      "Training Batch [531/782]: Loss 0.6077225804328918\n",
      "Training Batch [532/782]: Loss 0.394284725189209\n",
      "Training Batch [533/782]: Loss 0.45524030923843384\n",
      "Training Batch [534/782]: Loss 0.4535902738571167\n",
      "Training Batch [535/782]: Loss 0.5990849137306213\n",
      "Training Batch [536/782]: Loss 0.56829434633255\n",
      "Training Batch [537/782]: Loss 0.7864925265312195\n",
      "Training Batch [538/782]: Loss 0.4375480115413666\n",
      "Training Batch [539/782]: Loss 0.6698731780052185\n",
      "Training Batch [540/782]: Loss 0.40958642959594727\n",
      "Training Batch [541/782]: Loss 0.5111860632896423\n",
      "Training Batch [542/782]: Loss 0.5140991806983948\n",
      "Training Batch [543/782]: Loss 0.4617542624473572\n",
      "Training Batch [544/782]: Loss 0.555415153503418\n",
      "Training Batch [545/782]: Loss 0.5271828770637512\n",
      "Training Batch [546/782]: Loss 0.4826151132583618\n",
      "Training Batch [547/782]: Loss 0.6654059290885925\n",
      "Training Batch [548/782]: Loss 0.7315648198127747\n",
      "Training Batch [549/782]: Loss 0.5817340016365051\n",
      "Training Batch [550/782]: Loss 0.6665279865264893\n",
      "Training Batch [551/782]: Loss 0.5088632702827454\n",
      "Training Batch [552/782]: Loss 0.4923734962940216\n",
      "Training Batch [553/782]: Loss 0.6329227685928345\n",
      "Training Batch [554/782]: Loss 0.48969417810440063\n",
      "Training Batch [555/782]: Loss 0.47518062591552734\n",
      "Training Batch [556/782]: Loss 0.7324689626693726\n",
      "Training Batch [557/782]: Loss 0.3883184790611267\n",
      "Training Batch [558/782]: Loss 0.46765652298927307\n",
      "Training Batch [559/782]: Loss 0.4967212378978729\n",
      "Training Batch [560/782]: Loss 0.7903723120689392\n",
      "Training Batch [561/782]: Loss 0.41040945053100586\n",
      "Training Batch [562/782]: Loss 0.5408169627189636\n",
      "Training Batch [563/782]: Loss 0.48629847168922424\n",
      "Training Batch [564/782]: Loss 0.7927109599113464\n",
      "Training Batch [565/782]: Loss 0.9396668076515198\n",
      "Training Batch [566/782]: Loss 0.729397177696228\n",
      "Training Batch [567/782]: Loss 0.5806993246078491\n",
      "Training Batch [568/782]: Loss 0.614600658416748\n",
      "Training Batch [569/782]: Loss 0.6534383893013\n",
      "Training Batch [570/782]: Loss 0.47538307309150696\n",
      "Training Batch [571/782]: Loss 0.7759257555007935\n",
      "Training Batch [572/782]: Loss 0.8522728681564331\n",
      "Training Batch [573/782]: Loss 0.38391441106796265\n",
      "Training Batch [574/782]: Loss 0.6331017017364502\n",
      "Training Batch [575/782]: Loss 0.4945976436138153\n",
      "Training Batch [576/782]: Loss 0.8663994073867798\n",
      "Training Batch [577/782]: Loss 0.5318976044654846\n",
      "Training Batch [578/782]: Loss 0.6523274779319763\n",
      "Training Batch [579/782]: Loss 0.4270462989807129\n",
      "Training Batch [580/782]: Loss 0.5658648610115051\n",
      "Training Batch [581/782]: Loss 0.629777193069458\n",
      "Training Batch [582/782]: Loss 0.40158388018608093\n",
      "Training Batch [583/782]: Loss 0.4625706970691681\n",
      "Training Batch [584/782]: Loss 0.8078281283378601\n",
      "Training Batch [585/782]: Loss 0.6450608372688293\n",
      "Training Batch [586/782]: Loss 0.55999755859375\n",
      "Training Batch [587/782]: Loss 0.6642608642578125\n",
      "Training Batch [588/782]: Loss 0.7033683061599731\n",
      "Training Batch [589/782]: Loss 0.7502390742301941\n",
      "Training Batch [590/782]: Loss 0.5535529851913452\n",
      "Training Batch [591/782]: Loss 0.5764877796173096\n",
      "Training Batch [592/782]: Loss 0.4308837652206421\n",
      "Training Batch [593/782]: Loss 0.528762936592102\n",
      "Training Batch [594/782]: Loss 0.6793755888938904\n",
      "Training Batch [595/782]: Loss 0.5301544666290283\n",
      "Training Batch [596/782]: Loss 0.6111500263214111\n",
      "Training Batch [597/782]: Loss 0.4944094717502594\n",
      "Training Batch [598/782]: Loss 0.6660012006759644\n",
      "Training Batch [599/782]: Loss 0.6620296239852905\n",
      "Training Batch [600/782]: Loss 0.6923952102661133\n",
      "Training Batch [601/782]: Loss 0.573604941368103\n",
      "Training Batch [602/782]: Loss 0.4381048083305359\n",
      "Training Batch [603/782]: Loss 0.8044607043266296\n",
      "Training Batch [604/782]: Loss 0.81202632188797\n",
      "Training Batch [605/782]: Loss 0.633953869342804\n",
      "Training Batch [606/782]: Loss 0.6684961318969727\n",
      "Training Batch [607/782]: Loss 0.6677544713020325\n",
      "Training Batch [608/782]: Loss 0.49084532260894775\n",
      "Training Batch [609/782]: Loss 0.7079402208328247\n",
      "Training Batch [610/782]: Loss 0.6836066842079163\n",
      "Training Batch [611/782]: Loss 0.5057370066642761\n",
      "Training Batch [612/782]: Loss 0.644472062587738\n",
      "Training Batch [613/782]: Loss 0.7558109760284424\n",
      "Training Batch [614/782]: Loss 0.5816826820373535\n",
      "Training Batch [615/782]: Loss 0.3817661702632904\n",
      "Training Batch [616/782]: Loss 0.9218265414237976\n",
      "Training Batch [617/782]: Loss 0.5627360343933105\n",
      "Training Batch [618/782]: Loss 0.5947746634483337\n",
      "Training Batch [619/782]: Loss 0.6043717861175537\n",
      "Training Batch [620/782]: Loss 0.795312762260437\n",
      "Training Batch [621/782]: Loss 0.5968277454376221\n",
      "Training Batch [622/782]: Loss 0.46355903148651123\n",
      "Training Batch [623/782]: Loss 0.7028816342353821\n",
      "Training Batch [624/782]: Loss 0.5563166737556458\n",
      "Training Batch [625/782]: Loss 0.3874419033527374\n",
      "Training Batch [626/782]: Loss 0.36275768280029297\n",
      "Training Batch [627/782]: Loss 0.6418888568878174\n",
      "Training Batch [628/782]: Loss 0.4973430037498474\n",
      "Training Batch [629/782]: Loss 0.5574213266372681\n",
      "Training Batch [630/782]: Loss 0.8171422481536865\n",
      "Training Batch [631/782]: Loss 0.5895832777023315\n",
      "Training Batch [632/782]: Loss 0.5071826577186584\n",
      "Training Batch [633/782]: Loss 0.46734166145324707\n",
      "Training Batch [634/782]: Loss 0.46330180764198303\n",
      "Training Batch [635/782]: Loss 0.6759510636329651\n",
      "Training Batch [636/782]: Loss 0.49117541313171387\n",
      "Training Batch [637/782]: Loss 0.748320460319519\n",
      "Training Batch [638/782]: Loss 0.5584837198257446\n",
      "Training Batch [639/782]: Loss 0.5015021562576294\n",
      "Training Batch [640/782]: Loss 0.8233556747436523\n",
      "Training Batch [641/782]: Loss 0.48885488510131836\n",
      "Training Batch [642/782]: Loss 0.414817750453949\n",
      "Training Batch [643/782]: Loss 0.5999020934104919\n",
      "Training Batch [644/782]: Loss 0.39877647161483765\n",
      "Training Batch [645/782]: Loss 0.4484003186225891\n",
      "Training Batch [646/782]: Loss 0.6394298076629639\n",
      "Training Batch [647/782]: Loss 0.7174970507621765\n",
      "Training Batch [648/782]: Loss 0.5210030674934387\n",
      "Training Batch [649/782]: Loss 0.5426693558692932\n",
      "Training Batch [650/782]: Loss 0.6017376184463501\n",
      "Training Batch [651/782]: Loss 0.493874192237854\n",
      "Training Batch [652/782]: Loss 0.38080909848213196\n",
      "Training Batch [653/782]: Loss 0.4137951731681824\n",
      "Training Batch [654/782]: Loss 0.7828572392463684\n",
      "Training Batch [655/782]: Loss 0.5858956575393677\n",
      "Training Batch [656/782]: Loss 0.5290603637695312\n",
      "Training Batch [657/782]: Loss 0.6526886820793152\n",
      "Training Batch [658/782]: Loss 0.4827013909816742\n",
      "Training Batch [659/782]: Loss 0.46102914214134216\n",
      "Training Batch [660/782]: Loss 0.6038467884063721\n",
      "Training Batch [661/782]: Loss 0.5999683141708374\n",
      "Training Batch [662/782]: Loss 0.6196604371070862\n",
      "Training Batch [663/782]: Loss 0.47709590196609497\n",
      "Training Batch [664/782]: Loss 0.6942441463470459\n",
      "Training Batch [665/782]: Loss 0.6635014414787292\n",
      "Training Batch [666/782]: Loss 0.35871925950050354\n",
      "Training Batch [667/782]: Loss 0.5534191727638245\n",
      "Training Batch [668/782]: Loss 0.5363940000534058\n",
      "Training Batch [669/782]: Loss 0.5855457186698914\n",
      "Training Batch [670/782]: Loss 0.851766049861908\n",
      "Training Batch [671/782]: Loss 0.6102914810180664\n",
      "Training Batch [672/782]: Loss 0.6848439574241638\n",
      "Training Batch [673/782]: Loss 0.6179217100143433\n",
      "Training Batch [674/782]: Loss 0.45994243025779724\n",
      "Training Batch [675/782]: Loss 0.6100379228591919\n",
      "Training Batch [676/782]: Loss 0.7508412003517151\n",
      "Training Batch [677/782]: Loss 0.5505616068840027\n",
      "Training Batch [678/782]: Loss 0.636368453502655\n",
      "Training Batch [679/782]: Loss 0.6134451627731323\n",
      "Training Batch [680/782]: Loss 0.6279090642929077\n",
      "Training Batch [681/782]: Loss 0.45270487666130066\n",
      "Training Batch [682/782]: Loss 0.4700593054294586\n",
      "Training Batch [683/782]: Loss 0.502575159072876\n",
      "Training Batch [684/782]: Loss 0.7402868270874023\n",
      "Training Batch [685/782]: Loss 0.6555315256118774\n",
      "Training Batch [686/782]: Loss 0.5844151377677917\n",
      "Training Batch [687/782]: Loss 0.6751227378845215\n",
      "Training Batch [688/782]: Loss 0.7378361225128174\n",
      "Training Batch [689/782]: Loss 0.6529754996299744\n",
      "Training Batch [690/782]: Loss 0.6469409465789795\n",
      "Training Batch [691/782]: Loss 0.809323787689209\n",
      "Training Batch [692/782]: Loss 0.5719485282897949\n",
      "Training Batch [693/782]: Loss 0.5243633389472961\n",
      "Training Batch [694/782]: Loss 0.3725864887237549\n",
      "Training Batch [695/782]: Loss 0.5503670573234558\n",
      "Training Batch [696/782]: Loss 0.5231695771217346\n",
      "Training Batch [697/782]: Loss 0.4721859097480774\n",
      "Training Batch [698/782]: Loss 0.5746111273765564\n",
      "Training Batch [699/782]: Loss 0.5809359550476074\n",
      "Training Batch [700/782]: Loss 0.8877169489860535\n",
      "Training Batch [701/782]: Loss 0.5523403286933899\n",
      "Training Batch [702/782]: Loss 0.39310064911842346\n",
      "Training Batch [703/782]: Loss 0.46052470803260803\n",
      "Training Batch [704/782]: Loss 0.7648658752441406\n",
      "Training Batch [705/782]: Loss 0.5293170213699341\n",
      "Training Batch [706/782]: Loss 0.5653864145278931\n",
      "Training Batch [707/782]: Loss 0.8339746594429016\n",
      "Training Batch [708/782]: Loss 0.696223795413971\n",
      "Training Batch [709/782]: Loss 0.4711756110191345\n",
      "Training Batch [710/782]: Loss 0.36411404609680176\n",
      "Training Batch [711/782]: Loss 0.4894198775291443\n",
      "Training Batch [712/782]: Loss 0.6064969897270203\n",
      "Training Batch [713/782]: Loss 0.35711732506752014\n",
      "Training Batch [714/782]: Loss 0.5331531763076782\n",
      "Training Batch [715/782]: Loss 0.5087096095085144\n",
      "Training Batch [716/782]: Loss 0.6085174679756165\n",
      "Training Batch [717/782]: Loss 0.4970959722995758\n",
      "Training Batch [718/782]: Loss 0.6945762634277344\n",
      "Training Batch [719/782]: Loss 0.44036194682121277\n",
      "Training Batch [720/782]: Loss 0.6671735048294067\n",
      "Training Batch [721/782]: Loss 0.690072238445282\n",
      "Training Batch [722/782]: Loss 0.6439706087112427\n",
      "Training Batch [723/782]: Loss 0.44463515281677246\n",
      "Training Batch [724/782]: Loss 0.6433640122413635\n",
      "Training Batch [725/782]: Loss 0.49617254734039307\n",
      "Training Batch [726/782]: Loss 0.6854041218757629\n",
      "Training Batch [727/782]: Loss 0.6055919528007507\n",
      "Training Batch [728/782]: Loss 0.5262702703475952\n",
      "Training Batch [729/782]: Loss 0.5329365134239197\n",
      "Training Batch [730/782]: Loss 0.5520395040512085\n",
      "Training Batch [731/782]: Loss 0.47813892364501953\n",
      "Training Batch [732/782]: Loss 0.683188259601593\n",
      "Training Batch [733/782]: Loss 0.6537930369377136\n",
      "Training Batch [734/782]: Loss 0.5081086158752441\n",
      "Training Batch [735/782]: Loss 0.38522109389305115\n",
      "Training Batch [736/782]: Loss 0.6307329535484314\n",
      "Training Batch [737/782]: Loss 0.6455351114273071\n",
      "Training Batch [738/782]: Loss 0.6375910043716431\n",
      "Training Batch [739/782]: Loss 0.41752395033836365\n",
      "Training Batch [740/782]: Loss 0.5318167209625244\n",
      "Training Batch [741/782]: Loss 0.4627296030521393\n",
      "Training Batch [742/782]: Loss 0.6654158234596252\n",
      "Training Batch [743/782]: Loss 0.6019085645675659\n",
      "Training Batch [744/782]: Loss 0.6556443572044373\n",
      "Training Batch [745/782]: Loss 0.5520197749137878\n",
      "Training Batch [746/782]: Loss 0.41022518277168274\n",
      "Training Batch [747/782]: Loss 0.5103635191917419\n",
      "Training Batch [748/782]: Loss 0.729301393032074\n",
      "Training Batch [749/782]: Loss 0.5558067560195923\n",
      "Training Batch [750/782]: Loss 0.4730977714061737\n",
      "Training Batch [751/782]: Loss 0.5981015563011169\n",
      "Training Batch [752/782]: Loss 0.615313708782196\n",
      "Training Batch [753/782]: Loss 0.6595445871353149\n",
      "Training Batch [754/782]: Loss 0.5254955291748047\n",
      "Training Batch [755/782]: Loss 0.7080482840538025\n",
      "Training Batch [756/782]: Loss 0.5247133374214172\n",
      "Training Batch [757/782]: Loss 0.612320065498352\n",
      "Training Batch [758/782]: Loss 0.717317521572113\n",
      "Training Batch [759/782]: Loss 0.47338783740997314\n",
      "Training Batch [760/782]: Loss 0.30783772468566895\n",
      "Training Batch [761/782]: Loss 0.6476095914840698\n",
      "Training Batch [762/782]: Loss 0.5535944700241089\n",
      "Training Batch [763/782]: Loss 0.7576448321342468\n",
      "Training Batch [764/782]: Loss 0.5560870170593262\n",
      "Training Batch [765/782]: Loss 0.4239429533481598\n",
      "Training Batch [766/782]: Loss 0.5496934652328491\n",
      "Training Batch [767/782]: Loss 0.563123345375061\n",
      "Training Batch [768/782]: Loss 0.5047720670700073\n",
      "Training Batch [769/782]: Loss 0.6712481379508972\n",
      "Training Batch [770/782]: Loss 0.6851370334625244\n",
      "Training Batch [771/782]: Loss 0.5919752717018127\n",
      "Training Batch [772/782]: Loss 0.6319238543510437\n",
      "Training Batch [773/782]: Loss 0.5730338096618652\n",
      "Training Batch [774/782]: Loss 0.47199830412864685\n",
      "Training Batch [775/782]: Loss 0.5615832805633545\n",
      "Training Batch [776/782]: Loss 0.5561901926994324\n",
      "Training Batch [777/782]: Loss 0.6049452424049377\n",
      "Training Batch [778/782]: Loss 0.6328599452972412\n",
      "Training Batch [779/782]: Loss 0.584050714969635\n",
      "Training Batch [780/782]: Loss 0.38451632857322693\n",
      "Training Batch [781/782]: Loss 0.4536822736263275\n",
      "Training Batch [782/782]: Loss 0.7014047503471375\n",
      "Epoch 7 - Train Loss: 0.5370\n",
      "*********  Epoch 8/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.3888770341873169\n",
      "Training Batch [2/782]: Loss 0.3780551254749298\n",
      "Training Batch [3/782]: Loss 0.32549288868904114\n",
      "Training Batch [4/782]: Loss 0.35650989413261414\n",
      "Training Batch [5/782]: Loss 0.2742386758327484\n",
      "Training Batch [6/782]: Loss 0.3521635830402374\n",
      "Training Batch [7/782]: Loss 0.31611740589141846\n",
      "Training Batch [8/782]: Loss 0.46598124504089355\n",
      "Training Batch [9/782]: Loss 0.2293248027563095\n",
      "Training Batch [10/782]: Loss 0.22848905622959137\n",
      "Training Batch [11/782]: Loss 0.2941232919692993\n",
      "Training Batch [12/782]: Loss 0.45411354303359985\n",
      "Training Batch [13/782]: Loss 0.2710525393486023\n",
      "Training Batch [14/782]: Loss 0.40785959362983704\n",
      "Training Batch [15/782]: Loss 0.28833386301994324\n",
      "Training Batch [16/782]: Loss 0.3556647002696991\n",
      "Training Batch [17/782]: Loss 0.37524110078811646\n",
      "Training Batch [18/782]: Loss 0.2588706314563751\n",
      "Training Batch [19/782]: Loss 0.35034671425819397\n",
      "Training Batch [20/782]: Loss 0.3325197994709015\n",
      "Training Batch [21/782]: Loss 0.35606521368026733\n",
      "Training Batch [22/782]: Loss 0.2647766172885895\n",
      "Training Batch [23/782]: Loss 0.35602688789367676\n",
      "Training Batch [24/782]: Loss 0.21805934607982635\n",
      "Training Batch [25/782]: Loss 0.31155696511268616\n",
      "Training Batch [26/782]: Loss 0.32333648204803467\n",
      "Training Batch [27/782]: Loss 0.3053657114505768\n",
      "Training Batch [28/782]: Loss 0.5125857591629028\n",
      "Training Batch [29/782]: Loss 0.2950000464916229\n",
      "Training Batch [30/782]: Loss 0.2531394958496094\n",
      "Training Batch [31/782]: Loss 0.25374364852905273\n",
      "Training Batch [32/782]: Loss 0.2702188193798065\n",
      "Training Batch [33/782]: Loss 0.269889771938324\n",
      "Training Batch [34/782]: Loss 0.27513939142227173\n",
      "Training Batch [35/782]: Loss 0.1867678016424179\n",
      "Training Batch [36/782]: Loss 0.3707098364830017\n",
      "Training Batch [37/782]: Loss 0.26001912355422974\n",
      "Training Batch [38/782]: Loss 0.3794163167476654\n",
      "Training Batch [39/782]: Loss 0.2031613290309906\n",
      "Training Batch [40/782]: Loss 0.2904602885246277\n",
      "Training Batch [41/782]: Loss 0.2262345552444458\n",
      "Training Batch [42/782]: Loss 0.342600017786026\n",
      "Training Batch [43/782]: Loss 0.20767953991889954\n",
      "Training Batch [44/782]: Loss 0.22646066546440125\n",
      "Training Batch [45/782]: Loss 0.2019624561071396\n",
      "Training Batch [46/782]: Loss 0.16265928745269775\n",
      "Training Batch [47/782]: Loss 0.3537420630455017\n",
      "Training Batch [48/782]: Loss 0.2525698244571686\n",
      "Training Batch [49/782]: Loss 0.20108389854431152\n",
      "Training Batch [50/782]: Loss 0.3057319223880768\n",
      "Training Batch [51/782]: Loss 0.2456883192062378\n",
      "Training Batch [52/782]: Loss 0.2746492624282837\n",
      "Training Batch [53/782]: Loss 0.22777193784713745\n",
      "Training Batch [54/782]: Loss 0.19064903259277344\n",
      "Training Batch [55/782]: Loss 0.2783198058605194\n",
      "Training Batch [56/782]: Loss 0.37610965967178345\n",
      "Training Batch [57/782]: Loss 0.3203999400138855\n",
      "Training Batch [58/782]: Loss 0.23308464884757996\n",
      "Training Batch [59/782]: Loss 0.2235335409641266\n",
      "Training Batch [60/782]: Loss 0.28862103819847107\n",
      "Training Batch [61/782]: Loss 0.24324177205562592\n",
      "Training Batch [62/782]: Loss 0.15197421610355377\n",
      "Training Batch [63/782]: Loss 0.16587331891059875\n",
      "Training Batch [64/782]: Loss 0.18022364377975464\n",
      "Training Batch [65/782]: Loss 0.24661347270011902\n",
      "Training Batch [66/782]: Loss 0.2214505821466446\n",
      "Training Batch [67/782]: Loss 0.20784857869148254\n",
      "Training Batch [68/782]: Loss 0.34951624274253845\n",
      "Training Batch [69/782]: Loss 0.2255149781703949\n",
      "Training Batch [70/782]: Loss 0.18133419752120972\n",
      "Training Batch [71/782]: Loss 0.25327226519584656\n",
      "Training Batch [72/782]: Loss 0.34429505467414856\n",
      "Training Batch [73/782]: Loss 0.28283411264419556\n",
      "Training Batch [74/782]: Loss 0.2978760600090027\n",
      "Training Batch [75/782]: Loss 0.26434144377708435\n",
      "Training Batch [76/782]: Loss 0.19817936420440674\n",
      "Training Batch [77/782]: Loss 0.20578345656394958\n",
      "Training Batch [78/782]: Loss 0.12778930366039276\n",
      "Training Batch [79/782]: Loss 0.30190372467041016\n",
      "Training Batch [80/782]: Loss 0.20114101469516754\n",
      "Training Batch [81/782]: Loss 0.20164725184440613\n",
      "Training Batch [82/782]: Loss 0.20768465101718903\n",
      "Training Batch [83/782]: Loss 0.4332369863986969\n",
      "Training Batch [84/782]: Loss 0.3136020004749298\n",
      "Training Batch [85/782]: Loss 0.26476284861564636\n",
      "Training Batch [86/782]: Loss 0.3403646945953369\n",
      "Training Batch [87/782]: Loss 0.35356640815734863\n",
      "Training Batch [88/782]: Loss 0.3331812024116516\n",
      "Training Batch [89/782]: Loss 0.1593659520149231\n",
      "Training Batch [90/782]: Loss 0.35594165325164795\n",
      "Training Batch [91/782]: Loss 0.24510878324508667\n",
      "Training Batch [92/782]: Loss 0.3634553849697113\n",
      "Training Batch [93/782]: Loss 0.3886582851409912\n",
      "Training Batch [94/782]: Loss 0.30963432788848877\n",
      "Training Batch [95/782]: Loss 0.27892005443573\n",
      "Training Batch [96/782]: Loss 0.23482954502105713\n",
      "Training Batch [97/782]: Loss 0.20781277120113373\n",
      "Training Batch [98/782]: Loss 0.33978578448295593\n",
      "Training Batch [99/782]: Loss 0.3782770335674286\n",
      "Training Batch [100/782]: Loss 0.4320642054080963\n",
      "Training Batch [101/782]: Loss 0.2073802351951599\n",
      "Training Batch [102/782]: Loss 0.41079181432724\n",
      "Training Batch [103/782]: Loss 0.26737430691719055\n",
      "Training Batch [104/782]: Loss 0.3059803545475006\n",
      "Training Batch [105/782]: Loss 0.37282595038414\n",
      "Training Batch [106/782]: Loss 0.24725820124149323\n",
      "Training Batch [107/782]: Loss 0.23207691311836243\n",
      "Training Batch [108/782]: Loss 0.35112082958221436\n",
      "Training Batch [109/782]: Loss 0.30736225843429565\n",
      "Training Batch [110/782]: Loss 0.28703567385673523\n",
      "Training Batch [111/782]: Loss 0.2388675957918167\n",
      "Training Batch [112/782]: Loss 0.2414904236793518\n",
      "Training Batch [113/782]: Loss 0.25477921962738037\n",
      "Training Batch [114/782]: Loss 0.1544126272201538\n",
      "Training Batch [115/782]: Loss 0.20963460206985474\n",
      "Training Batch [116/782]: Loss 0.21182569861412048\n",
      "Training Batch [117/782]: Loss 0.3025892674922943\n",
      "Training Batch [118/782]: Loss 0.19458024203777313\n",
      "Training Batch [119/782]: Loss 0.25490424036979675\n",
      "Training Batch [120/782]: Loss 0.2998194098472595\n",
      "Training Batch [121/782]: Loss 0.21991300582885742\n",
      "Training Batch [122/782]: Loss 0.18249480426311493\n",
      "Training Batch [123/782]: Loss 0.22266621887683868\n",
      "Training Batch [124/782]: Loss 0.16150255501270294\n",
      "Training Batch [125/782]: Loss 0.2441464066505432\n",
      "Training Batch [126/782]: Loss 0.14952746033668518\n",
      "Training Batch [127/782]: Loss 0.26685431599617004\n",
      "Training Batch [128/782]: Loss 0.24976767599582672\n",
      "Training Batch [129/782]: Loss 0.22904162108898163\n",
      "Training Batch [130/782]: Loss 0.22394675016403198\n",
      "Training Batch [131/782]: Loss 0.2228674739599228\n",
      "Training Batch [132/782]: Loss 0.23640629649162292\n",
      "Training Batch [133/782]: Loss 0.24788498878479004\n",
      "Training Batch [134/782]: Loss 0.24063898622989655\n",
      "Training Batch [135/782]: Loss 0.23655098676681519\n",
      "Training Batch [136/782]: Loss 0.21782374382019043\n",
      "Training Batch [137/782]: Loss 0.3020181655883789\n",
      "Training Batch [138/782]: Loss 0.20466352999210358\n",
      "Training Batch [139/782]: Loss 0.2388627529144287\n",
      "Training Batch [140/782]: Loss 0.27401667833328247\n",
      "Training Batch [141/782]: Loss 0.29939988255500793\n",
      "Training Batch [142/782]: Loss 0.41166210174560547\n",
      "Training Batch [143/782]: Loss 0.20476068556308746\n",
      "Training Batch [144/782]: Loss 0.21978263556957245\n",
      "Training Batch [145/782]: Loss 0.20961090922355652\n",
      "Training Batch [146/782]: Loss 0.26732340455055237\n",
      "Training Batch [147/782]: Loss 0.19450275599956512\n",
      "Training Batch [148/782]: Loss 0.28523334860801697\n",
      "Training Batch [149/782]: Loss 0.19136430323123932\n",
      "Training Batch [150/782]: Loss 0.28749170899391174\n",
      "Training Batch [151/782]: Loss 0.2377791404724121\n",
      "Training Batch [152/782]: Loss 0.23021316528320312\n",
      "Training Batch [153/782]: Loss 0.2068658471107483\n",
      "Training Batch [154/782]: Loss 0.4320582151412964\n",
      "Training Batch [155/782]: Loss 0.26531627774238586\n",
      "Training Batch [156/782]: Loss 0.19702860713005066\n",
      "Training Batch [157/782]: Loss 0.19588106870651245\n",
      "Training Batch [158/782]: Loss 0.13727393746376038\n",
      "Training Batch [159/782]: Loss 0.2694914638996124\n",
      "Training Batch [160/782]: Loss 0.2702185809612274\n",
      "Training Batch [161/782]: Loss 0.2787713408470154\n",
      "Training Batch [162/782]: Loss 0.2848678231239319\n",
      "Training Batch [163/782]: Loss 0.3046647310256958\n",
      "Training Batch [164/782]: Loss 0.4126564562320709\n",
      "Training Batch [165/782]: Loss 0.15027515590190887\n",
      "Training Batch [166/782]: Loss 0.27455082535743713\n",
      "Training Batch [167/782]: Loss 0.36874884366989136\n",
      "Training Batch [168/782]: Loss 0.2647961378097534\n",
      "Training Batch [169/782]: Loss 0.27909231185913086\n",
      "Training Batch [170/782]: Loss 0.4040270447731018\n",
      "Training Batch [171/782]: Loss 0.19751770794391632\n",
      "Training Batch [172/782]: Loss 0.24940168857574463\n",
      "Training Batch [173/782]: Loss 0.23559357225894928\n",
      "Training Batch [174/782]: Loss 0.23504212498664856\n",
      "Training Batch [175/782]: Loss 0.22904135286808014\n",
      "Training Batch [176/782]: Loss 0.3171249032020569\n",
      "Training Batch [177/782]: Loss 0.18943284451961517\n",
      "Training Batch [178/782]: Loss 0.24541746079921722\n",
      "Training Batch [179/782]: Loss 0.15167762339115143\n",
      "Training Batch [180/782]: Loss 0.14959019422531128\n",
      "Training Batch [181/782]: Loss 0.26223355531692505\n",
      "Training Batch [182/782]: Loss 0.21632879972457886\n",
      "Training Batch [183/782]: Loss 0.48038870096206665\n",
      "Training Batch [184/782]: Loss 0.24806757271289825\n",
      "Training Batch [185/782]: Loss 0.26854780316352844\n",
      "Training Batch [186/782]: Loss 0.3146655261516571\n",
      "Training Batch [187/782]: Loss 0.28847038745880127\n",
      "Training Batch [188/782]: Loss 0.22345609962940216\n",
      "Training Batch [189/782]: Loss 0.45515263080596924\n",
      "Training Batch [190/782]: Loss 0.3180024027824402\n",
      "Training Batch [191/782]: Loss 0.3296818435192108\n",
      "Training Batch [192/782]: Loss 0.23279806971549988\n",
      "Training Batch [193/782]: Loss 0.19618958234786987\n",
      "Training Batch [194/782]: Loss 0.2704280614852905\n",
      "Training Batch [195/782]: Loss 0.3038921058177948\n",
      "Training Batch [196/782]: Loss 0.30128714442253113\n",
      "Training Batch [197/782]: Loss 0.20763526856899261\n",
      "Training Batch [198/782]: Loss 0.2601719796657562\n",
      "Training Batch [199/782]: Loss 0.33812400698661804\n",
      "Training Batch [200/782]: Loss 0.2643897831439972\n",
      "Training Batch [201/782]: Loss 0.4126138389110565\n",
      "Training Batch [202/782]: Loss 0.30471619963645935\n",
      "Training Batch [203/782]: Loss 0.41349518299102783\n",
      "Training Batch [204/782]: Loss 0.2900288999080658\n",
      "Training Batch [205/782]: Loss 0.28906333446502686\n",
      "Training Batch [206/782]: Loss 0.4327029585838318\n",
      "Training Batch [207/782]: Loss 0.2246226966381073\n",
      "Training Batch [208/782]: Loss 0.24413004517555237\n",
      "Training Batch [209/782]: Loss 0.4178532361984253\n",
      "Training Batch [210/782]: Loss 0.23616693913936615\n",
      "Training Batch [211/782]: Loss 0.3622194230556488\n",
      "Training Batch [212/782]: Loss 0.39894330501556396\n",
      "Training Batch [213/782]: Loss 0.3458472788333893\n",
      "Training Batch [214/782]: Loss 0.17287619411945343\n",
      "Training Batch [215/782]: Loss 0.27661222219467163\n",
      "Training Batch [216/782]: Loss 0.2558344602584839\n",
      "Training Batch [217/782]: Loss 0.2977454364299774\n",
      "Training Batch [218/782]: Loss 0.24044546484947205\n",
      "Training Batch [219/782]: Loss 0.3092513084411621\n",
      "Training Batch [220/782]: Loss 0.42596203088760376\n",
      "Training Batch [221/782]: Loss 0.17676568031311035\n",
      "Training Batch [222/782]: Loss 0.3322778344154358\n",
      "Training Batch [223/782]: Loss 0.1894138604402542\n",
      "Training Batch [224/782]: Loss 0.3344865143299103\n",
      "Training Batch [225/782]: Loss 0.4426456689834595\n",
      "Training Batch [226/782]: Loss 0.1401091367006302\n",
      "Training Batch [227/782]: Loss 0.2876169681549072\n",
      "Training Batch [228/782]: Loss 0.41458791494369507\n",
      "Training Batch [229/782]: Loss 0.30948132276535034\n",
      "Training Batch [230/782]: Loss 0.27656927704811096\n",
      "Training Batch [231/782]: Loss 0.4048806428909302\n",
      "Training Batch [232/782]: Loss 0.2214440107345581\n",
      "Training Batch [233/782]: Loss 0.34967344999313354\n",
      "Training Batch [234/782]: Loss 0.15288154780864716\n",
      "Training Batch [235/782]: Loss 0.20710249245166779\n",
      "Training Batch [236/782]: Loss 0.26731252670288086\n",
      "Training Batch [237/782]: Loss 0.3220992386341095\n",
      "Training Batch [238/782]: Loss 0.16848991811275482\n",
      "Training Batch [239/782]: Loss 0.38005873560905457\n",
      "Training Batch [240/782]: Loss 0.34731343388557434\n",
      "Training Batch [241/782]: Loss 0.27071020007133484\n",
      "Training Batch [242/782]: Loss 0.37739959359169006\n",
      "Training Batch [243/782]: Loss 0.19715739786624908\n",
      "Training Batch [244/782]: Loss 0.19704198837280273\n",
      "Training Batch [245/782]: Loss 0.14382736384868622\n",
      "Training Batch [246/782]: Loss 0.25261375308036804\n",
      "Training Batch [247/782]: Loss 0.3977835476398468\n",
      "Training Batch [248/782]: Loss 0.19027605652809143\n",
      "Training Batch [249/782]: Loss 0.20590806007385254\n",
      "Training Batch [250/782]: Loss 0.21140216290950775\n",
      "Training Batch [251/782]: Loss 0.24615582823753357\n",
      "Training Batch [252/782]: Loss 0.4949096739292145\n",
      "Training Batch [253/782]: Loss 0.352949321269989\n",
      "Training Batch [254/782]: Loss 0.3877566158771515\n",
      "Training Batch [255/782]: Loss 0.32645148038864136\n",
      "Training Batch [256/782]: Loss 0.24336299300193787\n",
      "Training Batch [257/782]: Loss 0.3491404354572296\n",
      "Training Batch [258/782]: Loss 0.4555983543395996\n",
      "Training Batch [259/782]: Loss 0.1951913833618164\n",
      "Training Batch [260/782]: Loss 0.4434361159801483\n",
      "Training Batch [261/782]: Loss 0.3115512728691101\n",
      "Training Batch [262/782]: Loss 0.2748959958553314\n",
      "Training Batch [263/782]: Loss 0.23325924575328827\n",
      "Training Batch [264/782]: Loss 0.19190239906311035\n",
      "Training Batch [265/782]: Loss 0.3478458821773529\n",
      "Training Batch [266/782]: Loss 0.3698823153972626\n",
      "Training Batch [267/782]: Loss 0.23652324080467224\n",
      "Training Batch [268/782]: Loss 0.34516239166259766\n",
      "Training Batch [269/782]: Loss 0.25542494654655457\n",
      "Training Batch [270/782]: Loss 0.2832297384738922\n",
      "Training Batch [271/782]: Loss 0.4715053141117096\n",
      "Training Batch [272/782]: Loss 0.16262078285217285\n",
      "Training Batch [273/782]: Loss 0.4266859292984009\n",
      "Training Batch [274/782]: Loss 0.2083640992641449\n",
      "Training Batch [275/782]: Loss 0.3053723871707916\n",
      "Training Batch [276/782]: Loss 0.3427491784095764\n",
      "Training Batch [277/782]: Loss 0.22264118492603302\n",
      "Training Batch [278/782]: Loss 0.29982778429985046\n",
      "Training Batch [279/782]: Loss 0.3264293968677521\n",
      "Training Batch [280/782]: Loss 0.3382107615470886\n",
      "Training Batch [281/782]: Loss 0.31406721472740173\n",
      "Training Batch [282/782]: Loss 0.35194066166877747\n",
      "Training Batch [283/782]: Loss 0.4081847369670868\n",
      "Training Batch [284/782]: Loss 0.3925580084323883\n",
      "Training Batch [285/782]: Loss 0.2587502598762512\n",
      "Training Batch [286/782]: Loss 0.3932294249534607\n",
      "Training Batch [287/782]: Loss 0.3664667308330536\n",
      "Training Batch [288/782]: Loss 0.22205625474452972\n",
      "Training Batch [289/782]: Loss 0.29109033942222595\n",
      "Training Batch [290/782]: Loss 0.2362946718931198\n",
      "Training Batch [291/782]: Loss 0.18565711379051208\n",
      "Training Batch [292/782]: Loss 0.33890125155448914\n",
      "Training Batch [293/782]: Loss 0.4183162748813629\n",
      "Training Batch [294/782]: Loss 0.17520159482955933\n",
      "Training Batch [295/782]: Loss 0.2415246218442917\n",
      "Training Batch [296/782]: Loss 0.2697927951812744\n",
      "Training Batch [297/782]: Loss 0.2849162518978119\n",
      "Training Batch [298/782]: Loss 0.3179491460323334\n",
      "Training Batch [299/782]: Loss 0.3964715600013733\n",
      "Training Batch [300/782]: Loss 0.25533246994018555\n",
      "Training Batch [301/782]: Loss 0.2878451943397522\n",
      "Training Batch [302/782]: Loss 0.30672571063041687\n",
      "Training Batch [303/782]: Loss 0.2606722116470337\n",
      "Training Batch [304/782]: Loss 0.42624446749687195\n",
      "Training Batch [305/782]: Loss 0.352023720741272\n",
      "Training Batch [306/782]: Loss 0.3200787901878357\n",
      "Training Batch [307/782]: Loss 0.21078109741210938\n",
      "Training Batch [308/782]: Loss 0.23302015662193298\n",
      "Training Batch [309/782]: Loss 0.36499637365341187\n",
      "Training Batch [310/782]: Loss 0.30007076263427734\n",
      "Training Batch [311/782]: Loss 0.31281688809394836\n",
      "Training Batch [312/782]: Loss 0.17315958440303802\n",
      "Training Batch [313/782]: Loss 0.15473908185958862\n",
      "Training Batch [314/782]: Loss 0.32452237606048584\n",
      "Training Batch [315/782]: Loss 0.22745740413665771\n",
      "Training Batch [316/782]: Loss 0.3086824417114258\n",
      "Training Batch [317/782]: Loss 0.24761806428432465\n",
      "Training Batch [318/782]: Loss 0.23359225690364838\n",
      "Training Batch [319/782]: Loss 0.31916213035583496\n",
      "Training Batch [320/782]: Loss 0.33846408128738403\n",
      "Training Batch [321/782]: Loss 0.3378560245037079\n",
      "Training Batch [322/782]: Loss 0.1519881635904312\n",
      "Training Batch [323/782]: Loss 0.43192920088768005\n",
      "Training Batch [324/782]: Loss 0.26122283935546875\n",
      "Training Batch [325/782]: Loss 0.3718802034854889\n",
      "Training Batch [326/782]: Loss 0.337524950504303\n",
      "Training Batch [327/782]: Loss 0.2603810131549835\n",
      "Training Batch [328/782]: Loss 0.37818947434425354\n",
      "Training Batch [329/782]: Loss 0.2705591320991516\n",
      "Training Batch [330/782]: Loss 0.2694817781448364\n",
      "Training Batch [331/782]: Loss 0.22200465202331543\n",
      "Training Batch [332/782]: Loss 0.29774153232574463\n",
      "Training Batch [333/782]: Loss 0.3165208399295807\n",
      "Training Batch [334/782]: Loss 0.25215908885002136\n",
      "Training Batch [335/782]: Loss 0.22043253481388092\n",
      "Training Batch [336/782]: Loss 0.2708725929260254\n",
      "Training Batch [337/782]: Loss 0.2446756809949875\n",
      "Training Batch [338/782]: Loss 0.34223228693008423\n",
      "Training Batch [339/782]: Loss 0.33972781896591187\n",
      "Training Batch [340/782]: Loss 0.23845286667346954\n",
      "Training Batch [341/782]: Loss 0.3603989779949188\n",
      "Training Batch [342/782]: Loss 0.378145307302475\n",
      "Training Batch [343/782]: Loss 0.23600594699382782\n",
      "Training Batch [344/782]: Loss 0.2732861638069153\n",
      "Training Batch [345/782]: Loss 0.23973749577999115\n",
      "Training Batch [346/782]: Loss 0.2890644371509552\n",
      "Training Batch [347/782]: Loss 0.3487299978733063\n",
      "Training Batch [348/782]: Loss 0.26250970363616943\n",
      "Training Batch [349/782]: Loss 0.3014841675758362\n",
      "Training Batch [350/782]: Loss 0.18346038460731506\n",
      "Training Batch [351/782]: Loss 0.3439911901950836\n",
      "Training Batch [352/782]: Loss 0.29962629079818726\n",
      "Training Batch [353/782]: Loss 0.2814784348011017\n",
      "Training Batch [354/782]: Loss 0.24631257355213165\n",
      "Training Batch [355/782]: Loss 0.39780867099761963\n",
      "Training Batch [356/782]: Loss 0.2999419867992401\n",
      "Training Batch [357/782]: Loss 0.42362895607948303\n",
      "Training Batch [358/782]: Loss 0.2648944854736328\n",
      "Training Batch [359/782]: Loss 0.38969674706459045\n",
      "Training Batch [360/782]: Loss 0.2252843827009201\n",
      "Training Batch [361/782]: Loss 0.24692301452159882\n",
      "Training Batch [362/782]: Loss 0.4068688154220581\n",
      "Training Batch [363/782]: Loss 0.4786102771759033\n",
      "Training Batch [364/782]: Loss 0.2252005785703659\n",
      "Training Batch [365/782]: Loss 0.2372041493654251\n",
      "Training Batch [366/782]: Loss 0.34191569685935974\n",
      "Training Batch [367/782]: Loss 0.32050254940986633\n",
      "Training Batch [368/782]: Loss 0.2665841281414032\n",
      "Training Batch [369/782]: Loss 0.2918769419193268\n",
      "Training Batch [370/782]: Loss 0.34199535846710205\n",
      "Training Batch [371/782]: Loss 0.2994665801525116\n",
      "Training Batch [372/782]: Loss 0.3133932054042816\n",
      "Training Batch [373/782]: Loss 0.26935046911239624\n",
      "Training Batch [374/782]: Loss 0.32152193784713745\n",
      "Training Batch [375/782]: Loss 0.4459473788738251\n",
      "Training Batch [376/782]: Loss 0.2802905738353729\n",
      "Training Batch [377/782]: Loss 0.4420834183692932\n",
      "Training Batch [378/782]: Loss 0.23730771243572235\n",
      "Training Batch [379/782]: Loss 0.22180134057998657\n",
      "Training Batch [380/782]: Loss 0.2660793662071228\n",
      "Training Batch [381/782]: Loss 0.2626805901527405\n",
      "Training Batch [382/782]: Loss 0.2811235189437866\n",
      "Training Batch [383/782]: Loss 0.3622964024543762\n",
      "Training Batch [384/782]: Loss 0.32979825139045715\n",
      "Training Batch [385/782]: Loss 0.2748992145061493\n",
      "Training Batch [386/782]: Loss 0.38731834292411804\n",
      "Training Batch [387/782]: Loss 0.3026440739631653\n",
      "Training Batch [388/782]: Loss 0.45178645849227905\n",
      "Training Batch [389/782]: Loss 0.20757164061069489\n",
      "Training Batch [390/782]: Loss 0.4316258132457733\n",
      "Training Batch [391/782]: Loss 0.2521304190158844\n",
      "Training Batch [392/782]: Loss 0.1990799456834793\n",
      "Training Batch [393/782]: Loss 0.27799463272094727\n",
      "Training Batch [394/782]: Loss 0.32275626063346863\n",
      "Training Batch [395/782]: Loss 0.30368825793266296\n",
      "Training Batch [396/782]: Loss 0.416282057762146\n",
      "Training Batch [397/782]: Loss 0.27156978845596313\n",
      "Training Batch [398/782]: Loss 0.2648123800754547\n",
      "Training Batch [399/782]: Loss 0.29360395669937134\n",
      "Training Batch [400/782]: Loss 0.3616247773170471\n",
      "Training Batch [401/782]: Loss 0.23565390706062317\n",
      "Training Batch [402/782]: Loss 0.2237040251493454\n",
      "Training Batch [403/782]: Loss 0.32648247480392456\n",
      "Training Batch [404/782]: Loss 0.3251475393772125\n",
      "Training Batch [405/782]: Loss 0.3724079430103302\n",
      "Training Batch [406/782]: Loss 0.30857568979263306\n",
      "Training Batch [407/782]: Loss 0.3806314468383789\n",
      "Training Batch [408/782]: Loss 0.18711377680301666\n",
      "Training Batch [409/782]: Loss 0.3839189410209656\n",
      "Training Batch [410/782]: Loss 0.4346296191215515\n",
      "Training Batch [411/782]: Loss 0.3139273226261139\n",
      "Training Batch [412/782]: Loss 0.2821732461452484\n",
      "Training Batch [413/782]: Loss 0.5061565637588501\n",
      "Training Batch [414/782]: Loss 0.41149234771728516\n",
      "Training Batch [415/782]: Loss 0.35295844078063965\n",
      "Training Batch [416/782]: Loss 0.35209161043167114\n",
      "Training Batch [417/782]: Loss 0.3396809697151184\n",
      "Training Batch [418/782]: Loss 0.25519001483917236\n",
      "Training Batch [419/782]: Loss 0.2682855725288391\n",
      "Training Batch [420/782]: Loss 0.5409825444221497\n",
      "Training Batch [421/782]: Loss 0.3178514540195465\n",
      "Training Batch [422/782]: Loss 0.23976996541023254\n",
      "Training Batch [423/782]: Loss 0.3802608847618103\n",
      "Training Batch [424/782]: Loss 0.47919899225234985\n",
      "Training Batch [425/782]: Loss 0.2583595812320709\n",
      "Training Batch [426/782]: Loss 0.3472553789615631\n",
      "Training Batch [427/782]: Loss 0.28564757108688354\n",
      "Training Batch [428/782]: Loss 0.2125641107559204\n",
      "Training Batch [429/782]: Loss 0.44551917910575867\n",
      "Training Batch [430/782]: Loss 0.37372836470603943\n",
      "Training Batch [431/782]: Loss 0.20754697918891907\n",
      "Training Batch [432/782]: Loss 0.20548206567764282\n",
      "Training Batch [433/782]: Loss 0.31733208894729614\n",
      "Training Batch [434/782]: Loss 0.4866228997707367\n",
      "Training Batch [435/782]: Loss 0.542851984500885\n",
      "Training Batch [436/782]: Loss 0.4738005995750427\n",
      "Training Batch [437/782]: Loss 0.3468349575996399\n",
      "Training Batch [438/782]: Loss 0.3869968056678772\n",
      "Training Batch [439/782]: Loss 0.38393524289131165\n",
      "Training Batch [440/782]: Loss 0.2190771996974945\n",
      "Training Batch [441/782]: Loss 0.3256600499153137\n",
      "Training Batch [442/782]: Loss 0.32216086983680725\n",
      "Training Batch [443/782]: Loss 0.2788078188896179\n",
      "Training Batch [444/782]: Loss 0.172512486577034\n",
      "Training Batch [445/782]: Loss 0.35899609327316284\n",
      "Training Batch [446/782]: Loss 0.33889007568359375\n",
      "Training Batch [447/782]: Loss 0.25622832775115967\n",
      "Training Batch [448/782]: Loss 0.48491519689559937\n",
      "Training Batch [449/782]: Loss 0.32984212040901184\n",
      "Training Batch [450/782]: Loss 0.35548290610313416\n",
      "Training Batch [451/782]: Loss 0.5521961450576782\n",
      "Training Batch [452/782]: Loss 0.5387449264526367\n",
      "Training Batch [453/782]: Loss 0.4227471351623535\n",
      "Training Batch [454/782]: Loss 0.2849544286727905\n",
      "Training Batch [455/782]: Loss 0.20969055593013763\n",
      "Training Batch [456/782]: Loss 0.4416327178478241\n",
      "Training Batch [457/782]: Loss 0.23743028938770294\n",
      "Training Batch [458/782]: Loss 0.37442857027053833\n",
      "Training Batch [459/782]: Loss 0.33593204617500305\n",
      "Training Batch [460/782]: Loss 0.3667776584625244\n",
      "Training Batch [461/782]: Loss 0.28766360878944397\n",
      "Training Batch [462/782]: Loss 0.32483208179473877\n",
      "Training Batch [463/782]: Loss 0.22345587611198425\n",
      "Training Batch [464/782]: Loss 0.38852596282958984\n",
      "Training Batch [465/782]: Loss 0.32953107357025146\n",
      "Training Batch [466/782]: Loss 0.18932068347930908\n",
      "Training Batch [467/782]: Loss 0.35873648524284363\n",
      "Training Batch [468/782]: Loss 0.3825813829898834\n",
      "Training Batch [469/782]: Loss 0.293157160282135\n",
      "Training Batch [470/782]: Loss 0.3201307952404022\n",
      "Training Batch [471/782]: Loss 0.5039118528366089\n",
      "Training Batch [472/782]: Loss 0.24015092849731445\n",
      "Training Batch [473/782]: Loss 0.32474708557128906\n",
      "Training Batch [474/782]: Loss 0.25601670145988464\n",
      "Training Batch [475/782]: Loss 0.3576791286468506\n",
      "Training Batch [476/782]: Loss 0.317258358001709\n",
      "Training Batch [477/782]: Loss 0.2604435980319977\n",
      "Training Batch [478/782]: Loss 0.2543811500072479\n",
      "Training Batch [479/782]: Loss 0.5027554035186768\n",
      "Training Batch [480/782]: Loss 0.3567027747631073\n",
      "Training Batch [481/782]: Loss 0.31101518869400024\n",
      "Training Batch [482/782]: Loss 0.3597152829170227\n",
      "Training Batch [483/782]: Loss 0.34405574202537537\n",
      "Training Batch [484/782]: Loss 0.27134084701538086\n",
      "Training Batch [485/782]: Loss 0.37279343605041504\n",
      "Training Batch [486/782]: Loss 0.5138099789619446\n",
      "Training Batch [487/782]: Loss 0.3915157616138458\n",
      "Training Batch [488/782]: Loss 0.3989575505256653\n",
      "Training Batch [489/782]: Loss 0.23199014365673065\n",
      "Training Batch [490/782]: Loss 0.23440341651439667\n",
      "Training Batch [491/782]: Loss 0.44588741660118103\n",
      "Training Batch [492/782]: Loss 0.5203816294670105\n",
      "Training Batch [493/782]: Loss 0.1867910474538803\n",
      "Training Batch [494/782]: Loss 0.33314061164855957\n",
      "Training Batch [495/782]: Loss 0.29241886734962463\n",
      "Training Batch [496/782]: Loss 0.37293899059295654\n",
      "Training Batch [497/782]: Loss 0.250699520111084\n",
      "Training Batch [498/782]: Loss 0.3069704473018646\n",
      "Training Batch [499/782]: Loss 0.37964770197868347\n",
      "Training Batch [500/782]: Loss 0.3310776352882385\n",
      "Training Batch [501/782]: Loss 0.413401335477829\n",
      "Training Batch [502/782]: Loss 0.37030839920043945\n",
      "Training Batch [503/782]: Loss 0.30248016119003296\n",
      "Training Batch [504/782]: Loss 0.4620649516582489\n",
      "Training Batch [505/782]: Loss 0.32243815064430237\n",
      "Training Batch [506/782]: Loss 0.37400931119918823\n",
      "Training Batch [507/782]: Loss 0.30390530824661255\n",
      "Training Batch [508/782]: Loss 0.43193817138671875\n",
      "Training Batch [509/782]: Loss 0.39388617873191833\n",
      "Training Batch [510/782]: Loss 0.3280825912952423\n",
      "Training Batch [511/782]: Loss 0.3748548626899719\n",
      "Training Batch [512/782]: Loss 0.4693996012210846\n",
      "Training Batch [513/782]: Loss 0.3871394693851471\n",
      "Training Batch [514/782]: Loss 0.26700669527053833\n",
      "Training Batch [515/782]: Loss 0.4717656672000885\n",
      "Training Batch [516/782]: Loss 0.35019439458847046\n",
      "Training Batch [517/782]: Loss 0.31150031089782715\n",
      "Training Batch [518/782]: Loss 0.34206292033195496\n",
      "Training Batch [519/782]: Loss 0.31609469652175903\n",
      "Training Batch [520/782]: Loss 0.3253895342350006\n",
      "Training Batch [521/782]: Loss 0.315153568983078\n",
      "Training Batch [522/782]: Loss 0.2832412123680115\n",
      "Training Batch [523/782]: Loss 0.2628386914730072\n",
      "Training Batch [524/782]: Loss 0.3656301200389862\n",
      "Training Batch [525/782]: Loss 0.2332465797662735\n",
      "Training Batch [526/782]: Loss 0.6151904463768005\n",
      "Training Batch [527/782]: Loss 0.28971269726753235\n",
      "Training Batch [528/782]: Loss 0.4398927688598633\n",
      "Training Batch [529/782]: Loss 0.24223896861076355\n",
      "Training Batch [530/782]: Loss 0.4380585551261902\n",
      "Training Batch [531/782]: Loss 0.3701673448085785\n",
      "Training Batch [532/782]: Loss 0.326410710811615\n",
      "Training Batch [533/782]: Loss 0.32844653725624084\n",
      "Training Batch [534/782]: Loss 0.31095007061958313\n",
      "Training Batch [535/782]: Loss 0.28562378883361816\n",
      "Training Batch [536/782]: Loss 0.3528236448764801\n",
      "Training Batch [537/782]: Loss 0.2804940938949585\n",
      "Training Batch [538/782]: Loss 0.2964981198310852\n",
      "Training Batch [539/782]: Loss 0.3286330997943878\n",
      "Training Batch [540/782]: Loss 0.2750687003135681\n",
      "Training Batch [541/782]: Loss 0.2470131367444992\n",
      "Training Batch [542/782]: Loss 0.2469005137681961\n",
      "Training Batch [543/782]: Loss 0.33427244424819946\n",
      "Training Batch [544/782]: Loss 0.3115120232105255\n",
      "Training Batch [545/782]: Loss 0.3318208158016205\n",
      "Training Batch [546/782]: Loss 0.5988180041313171\n",
      "Training Batch [547/782]: Loss 0.39403167366981506\n",
      "Training Batch [548/782]: Loss 0.348302960395813\n",
      "Training Batch [549/782]: Loss 0.3955114483833313\n",
      "Training Batch [550/782]: Loss 0.44556617736816406\n",
      "Training Batch [551/782]: Loss 0.29176634550094604\n",
      "Training Batch [552/782]: Loss 0.31264033913612366\n",
      "Training Batch [553/782]: Loss 0.29870423674583435\n",
      "Training Batch [554/782]: Loss 0.27712127566337585\n",
      "Training Batch [555/782]: Loss 0.35445961356163025\n",
      "Training Batch [556/782]: Loss 0.39985284209251404\n",
      "Training Batch [557/782]: Loss 0.4780559241771698\n",
      "Training Batch [558/782]: Loss 0.2885732352733612\n",
      "Training Batch [559/782]: Loss 0.23438386619091034\n",
      "Training Batch [560/782]: Loss 0.395888090133667\n",
      "Training Batch [561/782]: Loss 0.2094895839691162\n",
      "Training Batch [562/782]: Loss 0.41935402154922485\n",
      "Training Batch [563/782]: Loss 0.25796234607696533\n",
      "Training Batch [564/782]: Loss 0.29667866230010986\n",
      "Training Batch [565/782]: Loss 0.3073769211769104\n",
      "Training Batch [566/782]: Loss 0.3650478720664978\n",
      "Training Batch [567/782]: Loss 0.4241463840007782\n",
      "Training Batch [568/782]: Loss 0.2168472856283188\n",
      "Training Batch [569/782]: Loss 0.30265358090400696\n",
      "Training Batch [570/782]: Loss 0.29932165145874023\n",
      "Training Batch [571/782]: Loss 0.3071005940437317\n",
      "Training Batch [572/782]: Loss 0.2804572880268097\n",
      "Training Batch [573/782]: Loss 0.451419472694397\n",
      "Training Batch [574/782]: Loss 0.4285559356212616\n",
      "Training Batch [575/782]: Loss 0.4027595818042755\n",
      "Training Batch [576/782]: Loss 0.35372498631477356\n",
      "Training Batch [577/782]: Loss 0.49176451563835144\n",
      "Training Batch [578/782]: Loss 0.2094828486442566\n",
      "Training Batch [579/782]: Loss 0.3827562630176544\n",
      "Training Batch [580/782]: Loss 0.5958889722824097\n",
      "Training Batch [581/782]: Loss 0.29757311940193176\n",
      "Training Batch [582/782]: Loss 0.5420590043067932\n",
      "Training Batch [583/782]: Loss 0.4288334846496582\n",
      "Training Batch [584/782]: Loss 0.4302278757095337\n",
      "Training Batch [585/782]: Loss 0.45088180899620056\n",
      "Training Batch [586/782]: Loss 0.32001978158950806\n",
      "Training Batch [587/782]: Loss 0.4120144248008728\n",
      "Training Batch [588/782]: Loss 0.45543840527534485\n",
      "Training Batch [589/782]: Loss 0.39113256335258484\n",
      "Training Batch [590/782]: Loss 0.28935766220092773\n",
      "Training Batch [591/782]: Loss 0.1840801239013672\n",
      "Training Batch [592/782]: Loss 0.4579203426837921\n",
      "Training Batch [593/782]: Loss 0.40646934509277344\n",
      "Training Batch [594/782]: Loss 0.37918514013290405\n",
      "Training Batch [595/782]: Loss 0.3405292332172394\n",
      "Training Batch [596/782]: Loss 0.37600061297416687\n",
      "Training Batch [597/782]: Loss 0.3919548988342285\n",
      "Training Batch [598/782]: Loss 0.2985856235027313\n",
      "Training Batch [599/782]: Loss 0.5597134232521057\n",
      "Training Batch [600/782]: Loss 0.28825679421424866\n",
      "Training Batch [601/782]: Loss 0.5906490683555603\n",
      "Training Batch [602/782]: Loss 0.6703989505767822\n",
      "Training Batch [603/782]: Loss 0.3750994801521301\n",
      "Training Batch [604/782]: Loss 0.3583860397338867\n",
      "Training Batch [605/782]: Loss 0.34201672673225403\n",
      "Training Batch [606/782]: Loss 0.29171979427337646\n",
      "Training Batch [607/782]: Loss 0.5114845633506775\n",
      "Training Batch [608/782]: Loss 0.3163846731185913\n",
      "Training Batch [609/782]: Loss 0.30778768658638\n",
      "Training Batch [610/782]: Loss 0.3141234219074249\n",
      "Training Batch [611/782]: Loss 0.2704375386238098\n",
      "Training Batch [612/782]: Loss 0.3984197974205017\n",
      "Training Batch [613/782]: Loss 0.2884436249732971\n",
      "Training Batch [614/782]: Loss 0.3349495828151703\n",
      "Training Batch [615/782]: Loss 0.3918822407722473\n",
      "Training Batch [616/782]: Loss 0.3835892975330353\n",
      "Training Batch [617/782]: Loss 0.35851290822029114\n",
      "Training Batch [618/782]: Loss 0.2537974715232849\n",
      "Training Batch [619/782]: Loss 0.2288307100534439\n",
      "Training Batch [620/782]: Loss 0.2935636341571808\n",
      "Training Batch [621/782]: Loss 0.40457063913345337\n",
      "Training Batch [622/782]: Loss 0.4360441565513611\n",
      "Training Batch [623/782]: Loss 0.2572951912879944\n",
      "Training Batch [624/782]: Loss 0.5675041675567627\n",
      "Training Batch [625/782]: Loss 0.3417470157146454\n",
      "Training Batch [626/782]: Loss 0.47554072737693787\n",
      "Training Batch [627/782]: Loss 0.33715003728866577\n",
      "Training Batch [628/782]: Loss 0.72843337059021\n",
      "Training Batch [629/782]: Loss 0.2698715627193451\n",
      "Training Batch [630/782]: Loss 0.45557212829589844\n",
      "Training Batch [631/782]: Loss 0.4272807240486145\n",
      "Training Batch [632/782]: Loss 0.43615201115608215\n",
      "Training Batch [633/782]: Loss 0.49371469020843506\n",
      "Training Batch [634/782]: Loss 0.2787465453147888\n",
      "Training Batch [635/782]: Loss 0.436459481716156\n",
      "Training Batch [636/782]: Loss 0.2646699547767639\n",
      "Training Batch [637/782]: Loss 0.42129161953926086\n",
      "Training Batch [638/782]: Loss 0.525428056716919\n",
      "Training Batch [639/782]: Loss 0.43269068002700806\n",
      "Training Batch [640/782]: Loss 0.42689791321754456\n",
      "Training Batch [641/782]: Loss 0.23887953162193298\n",
      "Training Batch [642/782]: Loss 0.4727439284324646\n",
      "Training Batch [643/782]: Loss 0.2854667603969574\n",
      "Training Batch [644/782]: Loss 0.23147283494472504\n",
      "Training Batch [645/782]: Loss 0.2765454649925232\n",
      "Training Batch [646/782]: Loss 0.43037137389183044\n",
      "Training Batch [647/782]: Loss 0.44753772020339966\n",
      "Training Batch [648/782]: Loss 0.4301662743091583\n",
      "Training Batch [649/782]: Loss 0.3472192883491516\n",
      "Training Batch [650/782]: Loss 0.5191070437431335\n",
      "Training Batch [651/782]: Loss 0.4467620849609375\n",
      "Training Batch [652/782]: Loss 0.3087705373764038\n",
      "Training Batch [653/782]: Loss 0.4045800566673279\n",
      "Training Batch [654/782]: Loss 0.3152647912502289\n",
      "Training Batch [655/782]: Loss 0.4074060320854187\n",
      "Training Batch [656/782]: Loss 0.3623805344104767\n",
      "Training Batch [657/782]: Loss 0.5049588680267334\n",
      "Training Batch [658/782]: Loss 0.3815891146659851\n",
      "Training Batch [659/782]: Loss 0.38406676054000854\n",
      "Training Batch [660/782]: Loss 0.45652270317077637\n",
      "Training Batch [661/782]: Loss 0.42176178097724915\n",
      "Training Batch [662/782]: Loss 0.5544601678848267\n",
      "Training Batch [663/782]: Loss 0.3808838427066803\n",
      "Training Batch [664/782]: Loss 0.31932058930397034\n",
      "Training Batch [665/782]: Loss 0.4956544041633606\n",
      "Training Batch [666/782]: Loss 0.44807660579681396\n",
      "Training Batch [667/782]: Loss 0.34729716181755066\n",
      "Training Batch [668/782]: Loss 0.6456030607223511\n",
      "Training Batch [669/782]: Loss 0.3472195565700531\n",
      "Training Batch [670/782]: Loss 0.35926544666290283\n",
      "Training Batch [671/782]: Loss 0.5625374913215637\n",
      "Training Batch [672/782]: Loss 0.5025193095207214\n",
      "Training Batch [673/782]: Loss 0.5000048279762268\n",
      "Training Batch [674/782]: Loss 0.3383205235004425\n",
      "Training Batch [675/782]: Loss 0.21640166640281677\n",
      "Training Batch [676/782]: Loss 0.5093711614608765\n",
      "Training Batch [677/782]: Loss 0.2792147696018219\n",
      "Training Batch [678/782]: Loss 0.17440161108970642\n",
      "Training Batch [679/782]: Loss 0.5017431378364563\n",
      "Training Batch [680/782]: Loss 0.3926860988140106\n",
      "Training Batch [681/782]: Loss 0.32625800371170044\n",
      "Training Batch [682/782]: Loss 0.3385072350502014\n",
      "Training Batch [683/782]: Loss 0.3961191177368164\n",
      "Training Batch [684/782]: Loss 0.3543689250946045\n",
      "Training Batch [685/782]: Loss 0.25977495312690735\n",
      "Training Batch [686/782]: Loss 0.35954636335372925\n",
      "Training Batch [687/782]: Loss 0.4898693859577179\n",
      "Training Batch [688/782]: Loss 0.5064473748207092\n",
      "Training Batch [689/782]: Loss 0.323052316904068\n",
      "Training Batch [690/782]: Loss 0.3941049575805664\n",
      "Training Batch [691/782]: Loss 0.5457326173782349\n",
      "Training Batch [692/782]: Loss 0.3756408989429474\n",
      "Training Batch [693/782]: Loss 0.1756487339735031\n",
      "Training Batch [694/782]: Loss 0.4225897789001465\n",
      "Training Batch [695/782]: Loss 0.2562324106693268\n",
      "Training Batch [696/782]: Loss 0.41468748450279236\n",
      "Training Batch [697/782]: Loss 0.4961835741996765\n",
      "Training Batch [698/782]: Loss 0.4829690754413605\n",
      "Training Batch [699/782]: Loss 0.5595659017562866\n",
      "Training Batch [700/782]: Loss 0.22088950872421265\n",
      "Training Batch [701/782]: Loss 0.3634401559829712\n",
      "Training Batch [702/782]: Loss 0.3711174726486206\n",
      "Training Batch [703/782]: Loss 0.4114842116832733\n",
      "Training Batch [704/782]: Loss 0.5587881803512573\n",
      "Training Batch [705/782]: Loss 0.23510155081748962\n",
      "Training Batch [706/782]: Loss 0.36187559366226196\n",
      "Training Batch [707/782]: Loss 0.360496461391449\n",
      "Training Batch [708/782]: Loss 0.44688186049461365\n",
      "Training Batch [709/782]: Loss 0.2655286490917206\n",
      "Training Batch [710/782]: Loss 0.21279984712600708\n",
      "Training Batch [711/782]: Loss 0.4131425619125366\n",
      "Training Batch [712/782]: Loss 0.4156384766101837\n",
      "Training Batch [713/782]: Loss 0.32240670919418335\n",
      "Training Batch [714/782]: Loss 0.42228612303733826\n",
      "Training Batch [715/782]: Loss 0.35213297605514526\n",
      "Training Batch [716/782]: Loss 0.34910160303115845\n",
      "Training Batch [717/782]: Loss 0.35214781761169434\n",
      "Training Batch [718/782]: Loss 0.37654101848602295\n",
      "Training Batch [719/782]: Loss 0.5281258225440979\n",
      "Training Batch [720/782]: Loss 0.38204556703567505\n",
      "Training Batch [721/782]: Loss 0.2887989282608032\n",
      "Training Batch [722/782]: Loss 0.34444090723991394\n",
      "Training Batch [723/782]: Loss 0.45783644914627075\n",
      "Training Batch [724/782]: Loss 0.3243842124938965\n",
      "Training Batch [725/782]: Loss 0.46186044812202454\n",
      "Training Batch [726/782]: Loss 0.4031692445278168\n",
      "Training Batch [727/782]: Loss 0.467224657535553\n",
      "Training Batch [728/782]: Loss 0.52642822265625\n",
      "Training Batch [729/782]: Loss 0.5954173803329468\n",
      "Training Batch [730/782]: Loss 0.26919320225715637\n",
      "Training Batch [731/782]: Loss 0.3186052739620209\n",
      "Training Batch [732/782]: Loss 0.3537886440753937\n",
      "Training Batch [733/782]: Loss 0.45443177223205566\n",
      "Training Batch [734/782]: Loss 0.2733114957809448\n",
      "Training Batch [735/782]: Loss 0.19326095283031464\n",
      "Training Batch [736/782]: Loss 0.4810839593410492\n",
      "Training Batch [737/782]: Loss 0.38336455821990967\n",
      "Training Batch [738/782]: Loss 0.228027805685997\n",
      "Training Batch [739/782]: Loss 0.4795284867286682\n",
      "Training Batch [740/782]: Loss 0.49190372228622437\n",
      "Training Batch [741/782]: Loss 0.34041881561279297\n",
      "Training Batch [742/782]: Loss 0.517453670501709\n",
      "Training Batch [743/782]: Loss 0.20130449533462524\n",
      "Training Batch [744/782]: Loss 0.37199512124061584\n",
      "Training Batch [745/782]: Loss 0.5541182160377502\n",
      "Training Batch [746/782]: Loss 0.3953792154788971\n",
      "Training Batch [747/782]: Loss 0.38372230529785156\n",
      "Training Batch [748/782]: Loss 0.44011107087135315\n",
      "Training Batch [749/782]: Loss 0.2999405562877655\n",
      "Training Batch [750/782]: Loss 0.3912360966205597\n",
      "Training Batch [751/782]: Loss 0.3015322983264923\n",
      "Training Batch [752/782]: Loss 0.43796709179878235\n",
      "Training Batch [753/782]: Loss 0.6028113961219788\n",
      "Training Batch [754/782]: Loss 0.4235613942146301\n",
      "Training Batch [755/782]: Loss 0.3224583864212036\n",
      "Training Batch [756/782]: Loss 0.3926507234573364\n",
      "Training Batch [757/782]: Loss 0.300384521484375\n",
      "Training Batch [758/782]: Loss 0.46970856189727783\n",
      "Training Batch [759/782]: Loss 0.38186338543891907\n",
      "Training Batch [760/782]: Loss 0.32451850175857544\n",
      "Training Batch [761/782]: Loss 0.2693672478199005\n",
      "Training Batch [762/782]: Loss 0.439359188079834\n",
      "Training Batch [763/782]: Loss 0.27921319007873535\n",
      "Training Batch [764/782]: Loss 0.31321123242378235\n",
      "Training Batch [765/782]: Loss 0.4355861246585846\n",
      "Training Batch [766/782]: Loss 0.23287959396839142\n",
      "Training Batch [767/782]: Loss 0.3570697605609894\n",
      "Training Batch [768/782]: Loss 0.4012162685394287\n",
      "Training Batch [769/782]: Loss 0.563377320766449\n",
      "Training Batch [770/782]: Loss 0.38012832403182983\n",
      "Training Batch [771/782]: Loss 0.40067028999328613\n",
      "Training Batch [772/782]: Loss 0.21705982089042664\n",
      "Training Batch [773/782]: Loss 0.30616050958633423\n",
      "Training Batch [774/782]: Loss 0.36909019947052\n",
      "Training Batch [775/782]: Loss 0.40028882026672363\n",
      "Training Batch [776/782]: Loss 0.4163818955421448\n",
      "Training Batch [777/782]: Loss 0.47000187635421753\n",
      "Training Batch [778/782]: Loss 0.515536904335022\n",
      "Training Batch [779/782]: Loss 0.5308154821395874\n",
      "Training Batch [780/782]: Loss 0.30284860730171204\n",
      "Training Batch [781/782]: Loss 0.31284916400909424\n",
      "Training Batch [782/782]: Loss 0.5598278641700745\n",
      "Epoch 8 - Train Loss: 0.3265\n",
      "*********  Epoch 9/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.2580684423446655\n",
      "Training Batch [2/782]: Loss 0.22649222612380981\n",
      "Training Batch [3/782]: Loss 0.21889297664165497\n",
      "Training Batch [4/782]: Loss 0.08862173557281494\n",
      "Training Batch [5/782]: Loss 0.3013336658477783\n",
      "Training Batch [6/782]: Loss 0.2594263553619385\n",
      "Training Batch [7/782]: Loss 0.2108449786901474\n",
      "Training Batch [8/782]: Loss 0.23601190745830536\n",
      "Training Batch [9/782]: Loss 0.26226580142974854\n",
      "Training Batch [10/782]: Loss 0.13616134226322174\n",
      "Training Batch [11/782]: Loss 0.20891638100147247\n",
      "Training Batch [12/782]: Loss 0.28534018993377686\n",
      "Training Batch [13/782]: Loss 0.1896534413099289\n",
      "Training Batch [14/782]: Loss 0.24473443627357483\n",
      "Training Batch [15/782]: Loss 0.2521587014198303\n",
      "Training Batch [16/782]: Loss 0.27980977296829224\n",
      "Training Batch [17/782]: Loss 0.11889413744211197\n",
      "Training Batch [18/782]: Loss 0.2879861295223236\n",
      "Training Batch [19/782]: Loss 0.16926459968090057\n",
      "Training Batch [20/782]: Loss 0.14066125452518463\n",
      "Training Batch [21/782]: Loss 0.26568323373794556\n",
      "Training Batch [22/782]: Loss 0.17171074450016022\n",
      "Training Batch [23/782]: Loss 0.12179164588451385\n",
      "Training Batch [24/782]: Loss 0.1954314410686493\n",
      "Training Batch [25/782]: Loss 0.23708891868591309\n",
      "Training Batch [26/782]: Loss 0.12123451381921768\n",
      "Training Batch [27/782]: Loss 0.1252550184726715\n",
      "Training Batch [28/782]: Loss 0.27439454197883606\n",
      "Training Batch [29/782]: Loss 0.2630167603492737\n",
      "Training Batch [30/782]: Loss 0.12142027169466019\n",
      "Training Batch [31/782]: Loss 0.1221061423420906\n",
      "Training Batch [32/782]: Loss 0.167817160487175\n",
      "Training Batch [33/782]: Loss 0.3613849878311157\n",
      "Training Batch [34/782]: Loss 0.12902745604515076\n",
      "Training Batch [35/782]: Loss 0.3189084827899933\n",
      "Training Batch [36/782]: Loss 0.15736837685108185\n",
      "Training Batch [37/782]: Loss 0.2688639760017395\n",
      "Training Batch [38/782]: Loss 0.08577848970890045\n",
      "Training Batch [39/782]: Loss 0.1023118868470192\n",
      "Training Batch [40/782]: Loss 0.15144766867160797\n",
      "Training Batch [41/782]: Loss 0.10994860529899597\n",
      "Training Batch [42/782]: Loss 0.23524542152881622\n",
      "Training Batch [43/782]: Loss 0.12749361991882324\n",
      "Training Batch [44/782]: Loss 0.20458297431468964\n",
      "Training Batch [45/782]: Loss 0.1596890240907669\n",
      "Training Batch [46/782]: Loss 0.15612110495567322\n",
      "Training Batch [47/782]: Loss 0.1370406299829483\n",
      "Training Batch [48/782]: Loss 0.10090718418359756\n",
      "Training Batch [49/782]: Loss 0.16222794353961945\n",
      "Training Batch [50/782]: Loss 0.21368926763534546\n",
      "Training Batch [51/782]: Loss 0.22592537105083466\n",
      "Training Batch [52/782]: Loss 0.14954861998558044\n",
      "Training Batch [53/782]: Loss 0.11879555135965347\n",
      "Training Batch [54/782]: Loss 0.15919092297554016\n",
      "Training Batch [55/782]: Loss 0.22944188117980957\n",
      "Training Batch [56/782]: Loss 0.30613985657691956\n",
      "Training Batch [57/782]: Loss 0.19544152915477753\n",
      "Training Batch [58/782]: Loss 0.20779769122600555\n",
      "Training Batch [59/782]: Loss 0.17592918872833252\n",
      "Training Batch [60/782]: Loss 0.1466250717639923\n",
      "Training Batch [61/782]: Loss 0.13915099203586578\n",
      "Training Batch [62/782]: Loss 0.13893349468708038\n",
      "Training Batch [63/782]: Loss 0.2006428986787796\n",
      "Training Batch [64/782]: Loss 0.15019188821315765\n",
      "Training Batch [65/782]: Loss 0.10357395559549332\n",
      "Training Batch [66/782]: Loss 0.08979996293783188\n",
      "Training Batch [67/782]: Loss 0.09334396570920944\n",
      "Training Batch [68/782]: Loss 0.12397883087396622\n",
      "Training Batch [69/782]: Loss 0.18237058818340302\n",
      "Training Batch [70/782]: Loss 0.08867161720991135\n",
      "Training Batch [71/782]: Loss 0.12486355006694794\n",
      "Training Batch [72/782]: Loss 0.22352957725524902\n",
      "Training Batch [73/782]: Loss 0.15165673196315765\n",
      "Training Batch [74/782]: Loss 0.14010968804359436\n",
      "Training Batch [75/782]: Loss 0.1129423975944519\n",
      "Training Batch [76/782]: Loss 0.23601998388767242\n",
      "Training Batch [77/782]: Loss 0.15023022890090942\n",
      "Training Batch [78/782]: Loss 0.19766280055046082\n",
      "Training Batch [79/782]: Loss 0.15882785618305206\n",
      "Training Batch [80/782]: Loss 0.09417188912630081\n",
      "Training Batch [81/782]: Loss 0.09686329215765\n",
      "Training Batch [82/782]: Loss 0.12964703142642975\n",
      "Training Batch [83/782]: Loss 0.13303855061531067\n",
      "Training Batch [84/782]: Loss 0.18270854651927948\n",
      "Training Batch [85/782]: Loss 0.1523696631193161\n",
      "Training Batch [86/782]: Loss 0.1539348065853119\n",
      "Training Batch [87/782]: Loss 0.11914433538913727\n",
      "Training Batch [88/782]: Loss 0.09010826051235199\n",
      "Training Batch [89/782]: Loss 0.18222691118717194\n",
      "Training Batch [90/782]: Loss 0.15576134622097015\n",
      "Training Batch [91/782]: Loss 0.09445124864578247\n",
      "Training Batch [92/782]: Loss 0.08857416361570358\n",
      "Training Batch [93/782]: Loss 0.14086931943893433\n",
      "Training Batch [94/782]: Loss 0.17058837413787842\n",
      "Training Batch [95/782]: Loss 0.14777661859989166\n",
      "Training Batch [96/782]: Loss 0.14573094248771667\n",
      "Training Batch [97/782]: Loss 0.09803052991628647\n",
      "Training Batch [98/782]: Loss 0.16297855973243713\n",
      "Training Batch [99/782]: Loss 0.09682682156562805\n",
      "Training Batch [100/782]: Loss 0.10852789878845215\n",
      "Training Batch [101/782]: Loss 0.09866002202033997\n",
      "Training Batch [102/782]: Loss 0.2518581748008728\n",
      "Training Batch [103/782]: Loss 0.197391539812088\n",
      "Training Batch [104/782]: Loss 0.04880312457680702\n",
      "Training Batch [105/782]: Loss 0.11014652997255325\n",
      "Training Batch [106/782]: Loss 0.12566803395748138\n",
      "Training Batch [107/782]: Loss 0.14556889235973358\n",
      "Training Batch [108/782]: Loss 0.13414481282234192\n",
      "Training Batch [109/782]: Loss 0.1232261061668396\n",
      "Training Batch [110/782]: Loss 0.3016417920589447\n",
      "Training Batch [111/782]: Loss 0.24934419989585876\n",
      "Training Batch [112/782]: Loss 0.12911035120487213\n",
      "Training Batch [113/782]: Loss 0.1301743984222412\n",
      "Training Batch [114/782]: Loss 0.1461474746465683\n",
      "Training Batch [115/782]: Loss 0.18203647434711456\n",
      "Training Batch [116/782]: Loss 0.21816939115524292\n",
      "Training Batch [117/782]: Loss 0.1321113407611847\n",
      "Training Batch [118/782]: Loss 0.15970343351364136\n",
      "Training Batch [119/782]: Loss 0.10242153704166412\n",
      "Training Batch [120/782]: Loss 0.11794394999742508\n",
      "Training Batch [121/782]: Loss 0.07325264811515808\n",
      "Training Batch [122/782]: Loss 0.06797847896814346\n",
      "Training Batch [123/782]: Loss 0.16330163180828094\n",
      "Training Batch [124/782]: Loss 0.10852614045143127\n",
      "Training Batch [125/782]: Loss 0.08394269645214081\n",
      "Training Batch [126/782]: Loss 0.04948965460062027\n",
      "Training Batch [127/782]: Loss 0.07628504186868668\n",
      "Training Batch [128/782]: Loss 0.19204017519950867\n",
      "Training Batch [129/782]: Loss 0.07117383927106857\n",
      "Training Batch [130/782]: Loss 0.11542171984910965\n",
      "Training Batch [131/782]: Loss 0.1290486752986908\n",
      "Training Batch [132/782]: Loss 0.16012492775917053\n",
      "Training Batch [133/782]: Loss 0.15444183349609375\n",
      "Training Batch [134/782]: Loss 0.14378774166107178\n",
      "Training Batch [135/782]: Loss 0.17983761429786682\n",
      "Training Batch [136/782]: Loss 0.15733514726161957\n",
      "Training Batch [137/782]: Loss 0.14876385033130646\n",
      "Training Batch [138/782]: Loss 0.19993437826633453\n",
      "Training Batch [139/782]: Loss 0.18059831857681274\n",
      "Training Batch [140/782]: Loss 0.12458834052085876\n",
      "Training Batch [141/782]: Loss 0.20136825740337372\n",
      "Training Batch [142/782]: Loss 0.22246380150318146\n",
      "Training Batch [143/782]: Loss 0.1481415033340454\n",
      "Training Batch [144/782]: Loss 0.10954848676919937\n",
      "Training Batch [145/782]: Loss 0.17475679516792297\n",
      "Training Batch [146/782]: Loss 0.12237277626991272\n",
      "Training Batch [147/782]: Loss 0.19939255714416504\n",
      "Training Batch [148/782]: Loss 0.1998734176158905\n",
      "Training Batch [149/782]: Loss 0.31384146213531494\n",
      "Training Batch [150/782]: Loss 0.13971468806266785\n",
      "Training Batch [151/782]: Loss 0.13012778759002686\n",
      "Training Batch [152/782]: Loss 0.13087093830108643\n",
      "Training Batch [153/782]: Loss 0.15376876294612885\n",
      "Training Batch [154/782]: Loss 0.23210620880126953\n",
      "Training Batch [155/782]: Loss 0.1221722811460495\n",
      "Training Batch [156/782]: Loss 0.14321842789649963\n",
      "Training Batch [157/782]: Loss 0.24761933088302612\n",
      "Training Batch [158/782]: Loss 0.13096798956394196\n",
      "Training Batch [159/782]: Loss 0.1579904556274414\n",
      "Training Batch [160/782]: Loss 0.11092505604028702\n",
      "Training Batch [161/782]: Loss 0.1896226853132248\n",
      "Training Batch [162/782]: Loss 0.11200479418039322\n",
      "Training Batch [163/782]: Loss 0.1320812851190567\n",
      "Training Batch [164/782]: Loss 0.17058025300502777\n",
      "Training Batch [165/782]: Loss 0.1741340160369873\n",
      "Training Batch [166/782]: Loss 0.11602392047643661\n",
      "Training Batch [167/782]: Loss 0.27036699652671814\n",
      "Training Batch [168/782]: Loss 0.16885249316692352\n",
      "Training Batch [169/782]: Loss 0.1619328260421753\n",
      "Training Batch [170/782]: Loss 0.13300485908985138\n",
      "Training Batch [171/782]: Loss 0.07853728532791138\n",
      "Training Batch [172/782]: Loss 0.1427571028470993\n",
      "Training Batch [173/782]: Loss 0.3203352093696594\n",
      "Training Batch [174/782]: Loss 0.12836883962154388\n",
      "Training Batch [175/782]: Loss 0.08845993876457214\n",
      "Training Batch [176/782]: Loss 0.2036573588848114\n",
      "Training Batch [177/782]: Loss 0.08145016431808472\n",
      "Training Batch [178/782]: Loss 0.10050591826438904\n",
      "Training Batch [179/782]: Loss 0.12358729541301727\n",
      "Training Batch [180/782]: Loss 0.08930577337741852\n",
      "Training Batch [181/782]: Loss 0.1302552968263626\n",
      "Training Batch [182/782]: Loss 0.20818692445755005\n",
      "Training Batch [183/782]: Loss 0.1533350795507431\n",
      "Training Batch [184/782]: Loss 0.22749562561511993\n",
      "Training Batch [185/782]: Loss 0.10501673072576523\n",
      "Training Batch [186/782]: Loss 0.10257626324892044\n",
      "Training Batch [187/782]: Loss 0.12184768915176392\n",
      "Training Batch [188/782]: Loss 0.12993596494197845\n",
      "Training Batch [189/782]: Loss 0.15334677696228027\n",
      "Training Batch [190/782]: Loss 0.11877672374248505\n",
      "Training Batch [191/782]: Loss 0.07735650986433029\n",
      "Training Batch [192/782]: Loss 0.20329394936561584\n",
      "Training Batch [193/782]: Loss 0.13886061310768127\n",
      "Training Batch [194/782]: Loss 0.15050847828388214\n",
      "Training Batch [195/782]: Loss 0.1281096190214157\n",
      "Training Batch [196/782]: Loss 0.25267454981803894\n",
      "Training Batch [197/782]: Loss 0.11037799715995789\n",
      "Training Batch [198/782]: Loss 0.25326159596443176\n",
      "Training Batch [199/782]: Loss 0.13475745916366577\n",
      "Training Batch [200/782]: Loss 0.1856744885444641\n",
      "Training Batch [201/782]: Loss 0.09890621155500412\n",
      "Training Batch [202/782]: Loss 0.2604028284549713\n",
      "Training Batch [203/782]: Loss 0.15633517503738403\n",
      "Training Batch [204/782]: Loss 0.09444018453359604\n",
      "Training Batch [205/782]: Loss 0.164470374584198\n",
      "Training Batch [206/782]: Loss 0.19657130539417267\n",
      "Training Batch [207/782]: Loss 0.15527376532554626\n",
      "Training Batch [208/782]: Loss 0.10315520316362381\n",
      "Training Batch [209/782]: Loss 0.14695043861865997\n",
      "Training Batch [210/782]: Loss 0.20656374096870422\n",
      "Training Batch [211/782]: Loss 0.13820692896842957\n",
      "Training Batch [212/782]: Loss 0.1992851346731186\n",
      "Training Batch [213/782]: Loss 0.10311002284288406\n",
      "Training Batch [214/782]: Loss 0.17608676850795746\n",
      "Training Batch [215/782]: Loss 0.08819599449634552\n",
      "Training Batch [216/782]: Loss 0.11441401392221451\n",
      "Training Batch [217/782]: Loss 0.10579610615968704\n",
      "Training Batch [218/782]: Loss 0.1354520618915558\n",
      "Training Batch [219/782]: Loss 0.2772941589355469\n",
      "Training Batch [220/782]: Loss 0.1261099874973297\n",
      "Training Batch [221/782]: Loss 0.16701607406139374\n",
      "Training Batch [222/782]: Loss 0.11760121583938599\n",
      "Training Batch [223/782]: Loss 0.11244305968284607\n",
      "Training Batch [224/782]: Loss 0.1659574955701828\n",
      "Training Batch [225/782]: Loss 0.14425255358219147\n",
      "Training Batch [226/782]: Loss 0.11270739138126373\n",
      "Training Batch [227/782]: Loss 0.14011214673519135\n",
      "Training Batch [228/782]: Loss 0.1457671970129013\n",
      "Training Batch [229/782]: Loss 0.11293355375528336\n",
      "Training Batch [230/782]: Loss 0.11129075288772583\n",
      "Training Batch [231/782]: Loss 0.0954236164689064\n",
      "Training Batch [232/782]: Loss 0.13084527850151062\n",
      "Training Batch [233/782]: Loss 0.23437288403511047\n",
      "Training Batch [234/782]: Loss 0.25007936358451843\n",
      "Training Batch [235/782]: Loss 0.14400960505008698\n",
      "Training Batch [236/782]: Loss 0.1489991843700409\n",
      "Training Batch [237/782]: Loss 0.1832510083913803\n",
      "Training Batch [238/782]: Loss 0.12573778629302979\n",
      "Training Batch [239/782]: Loss 0.1328064352273941\n",
      "Training Batch [240/782]: Loss 0.18516045808792114\n",
      "Training Batch [241/782]: Loss 0.08097406476736069\n",
      "Training Batch [242/782]: Loss 0.1310536116361618\n",
      "Training Batch [243/782]: Loss 0.07621943950653076\n",
      "Training Batch [244/782]: Loss 0.14707805216312408\n",
      "Training Batch [245/782]: Loss 0.13346979022026062\n",
      "Training Batch [246/782]: Loss 0.159366637468338\n",
      "Training Batch [247/782]: Loss 0.13794812560081482\n",
      "Training Batch [248/782]: Loss 0.18234208226203918\n",
      "Training Batch [249/782]: Loss 0.1631700098514557\n",
      "Training Batch [250/782]: Loss 0.08511361479759216\n",
      "Training Batch [251/782]: Loss 0.14202184975147247\n",
      "Training Batch [252/782]: Loss 0.07564260065555573\n",
      "Training Batch [253/782]: Loss 0.18294619023799896\n",
      "Training Batch [254/782]: Loss 0.10054811835289001\n",
      "Training Batch [255/782]: Loss 0.18696850538253784\n",
      "Training Batch [256/782]: Loss 0.2671763598918915\n",
      "Training Batch [257/782]: Loss 0.07637175172567368\n",
      "Training Batch [258/782]: Loss 0.22764651477336884\n",
      "Training Batch [259/782]: Loss 0.15437081456184387\n",
      "Training Batch [260/782]: Loss 0.14870776236057281\n",
      "Training Batch [261/782]: Loss 0.11333364993333817\n",
      "Training Batch [262/782]: Loss 0.1280258595943451\n",
      "Training Batch [263/782]: Loss 0.2052491009235382\n",
      "Training Batch [264/782]: Loss 0.08969593048095703\n",
      "Training Batch [265/782]: Loss 0.14886203408241272\n",
      "Training Batch [266/782]: Loss 0.16686145961284637\n",
      "Training Batch [267/782]: Loss 0.1554175466299057\n",
      "Training Batch [268/782]: Loss 0.1927139312028885\n",
      "Training Batch [269/782]: Loss 0.13204124569892883\n",
      "Training Batch [270/782]: Loss 0.1416013538837433\n",
      "Training Batch [271/782]: Loss 0.179398313164711\n",
      "Training Batch [272/782]: Loss 0.1213454008102417\n",
      "Training Batch [273/782]: Loss 0.0739256963133812\n",
      "Training Batch [274/782]: Loss 0.10285894572734833\n",
      "Training Batch [275/782]: Loss 0.06546825915575027\n",
      "Training Batch [276/782]: Loss 0.09948857128620148\n",
      "Training Batch [277/782]: Loss 0.2222730815410614\n",
      "Training Batch [278/782]: Loss 0.1706380993127823\n",
      "Training Batch [279/782]: Loss 0.21572618186473846\n",
      "Training Batch [280/782]: Loss 0.12365853041410446\n",
      "Training Batch [281/782]: Loss 0.07087866961956024\n",
      "Training Batch [282/782]: Loss 0.15225696563720703\n",
      "Training Batch [283/782]: Loss 0.11309794336557388\n",
      "Training Batch [284/782]: Loss 0.17324531078338623\n",
      "Training Batch [285/782]: Loss 0.1283954530954361\n",
      "Training Batch [286/782]: Loss 0.11726648360490799\n",
      "Training Batch [287/782]: Loss 0.22919072210788727\n",
      "Training Batch [288/782]: Loss 0.1627524197101593\n",
      "Training Batch [289/782]: Loss 0.28172358870506287\n",
      "Training Batch [290/782]: Loss 0.06005221977829933\n",
      "Training Batch [291/782]: Loss 0.17324045300483704\n",
      "Training Batch [292/782]: Loss 0.101376973092556\n",
      "Training Batch [293/782]: Loss 0.1538507342338562\n",
      "Training Batch [294/782]: Loss 0.37904563546180725\n",
      "Training Batch [295/782]: Loss 0.3194364607334137\n",
      "Training Batch [296/782]: Loss 0.10526613891124725\n",
      "Training Batch [297/782]: Loss 0.1385354995727539\n",
      "Training Batch [298/782]: Loss 0.14231714606285095\n",
      "Training Batch [299/782]: Loss 0.09318706393241882\n",
      "Training Batch [300/782]: Loss 0.1256140023469925\n",
      "Training Batch [301/782]: Loss 0.2261139303445816\n",
      "Training Batch [302/782]: Loss 0.20371092855930328\n",
      "Training Batch [303/782]: Loss 0.22874058783054352\n",
      "Training Batch [304/782]: Loss 0.1240239068865776\n",
      "Training Batch [305/782]: Loss 0.08977387845516205\n",
      "Training Batch [306/782]: Loss 0.1802849918603897\n",
      "Training Batch [307/782]: Loss 0.1791270226240158\n",
      "Training Batch [308/782]: Loss 0.2079123556613922\n",
      "Training Batch [309/782]: Loss 0.1425151824951172\n",
      "Training Batch [310/782]: Loss 0.085256427526474\n",
      "Training Batch [311/782]: Loss 0.13296158611774445\n",
      "Training Batch [312/782]: Loss 0.19018509984016418\n",
      "Training Batch [313/782]: Loss 0.22167757153511047\n",
      "Training Batch [314/782]: Loss 0.08279494941234589\n",
      "Training Batch [315/782]: Loss 0.1657065898180008\n",
      "Training Batch [316/782]: Loss 0.19235451519489288\n",
      "Training Batch [317/782]: Loss 0.2355174720287323\n",
      "Training Batch [318/782]: Loss 0.22012358903884888\n",
      "Training Batch [319/782]: Loss 0.16176572442054749\n",
      "Training Batch [320/782]: Loss 0.08817707747220993\n",
      "Training Batch [321/782]: Loss 0.1656239628791809\n",
      "Training Batch [322/782]: Loss 0.1422082781791687\n",
      "Training Batch [323/782]: Loss 0.2027183622121811\n",
      "Training Batch [324/782]: Loss 0.11950346827507019\n",
      "Training Batch [325/782]: Loss 0.3091621994972229\n",
      "Training Batch [326/782]: Loss 0.19657590985298157\n",
      "Training Batch [327/782]: Loss 0.17923055589199066\n",
      "Training Batch [328/782]: Loss 0.153753861784935\n",
      "Training Batch [329/782]: Loss 0.24617476761341095\n",
      "Training Batch [330/782]: Loss 0.2311737984418869\n",
      "Training Batch [331/782]: Loss 0.17676234245300293\n",
      "Training Batch [332/782]: Loss 0.34633269906044006\n",
      "Training Batch [333/782]: Loss 0.2502096891403198\n",
      "Training Batch [334/782]: Loss 0.2717442512512207\n",
      "Training Batch [335/782]: Loss 0.17584630846977234\n",
      "Training Batch [336/782]: Loss 0.18548054993152618\n",
      "Training Batch [337/782]: Loss 0.23260101675987244\n",
      "Training Batch [338/782]: Loss 0.17411069571971893\n",
      "Training Batch [339/782]: Loss 0.19576221704483032\n",
      "Training Batch [340/782]: Loss 0.3132641613483429\n",
      "Training Batch [341/782]: Loss 0.20035548508167267\n",
      "Training Batch [342/782]: Loss 0.12381576746702194\n",
      "Training Batch [343/782]: Loss 0.17709816992282867\n",
      "Training Batch [344/782]: Loss 0.23709677159786224\n",
      "Training Batch [345/782]: Loss 0.194279283285141\n",
      "Training Batch [346/782]: Loss 0.15208394825458527\n",
      "Training Batch [347/782]: Loss 0.2413274645805359\n",
      "Training Batch [348/782]: Loss 0.20295076072216034\n",
      "Training Batch [349/782]: Loss 0.2592627704143524\n",
      "Training Batch [350/782]: Loss 0.09773023426532745\n",
      "Training Batch [351/782]: Loss 0.14083746075630188\n",
      "Training Batch [352/782]: Loss 0.23995238542556763\n",
      "Training Batch [353/782]: Loss 0.14714165031909943\n",
      "Training Batch [354/782]: Loss 0.13466927409172058\n",
      "Training Batch [355/782]: Loss 0.19442857801914215\n",
      "Training Batch [356/782]: Loss 0.13471288979053497\n",
      "Training Batch [357/782]: Loss 0.1946430802345276\n",
      "Training Batch [358/782]: Loss 0.20955726504325867\n",
      "Training Batch [359/782]: Loss 0.15780040621757507\n",
      "Training Batch [360/782]: Loss 0.18395867943763733\n",
      "Training Batch [361/782]: Loss 0.09255876392126083\n",
      "Training Batch [362/782]: Loss 0.1790701299905777\n",
      "Training Batch [363/782]: Loss 0.2728109061717987\n",
      "Training Batch [364/782]: Loss 0.168294295668602\n",
      "Training Batch [365/782]: Loss 0.0987536832690239\n",
      "Training Batch [366/782]: Loss 0.15739582479000092\n",
      "Training Batch [367/782]: Loss 0.2634356617927551\n",
      "Training Batch [368/782]: Loss 0.09323700517416\n",
      "Training Batch [369/782]: Loss 0.1978926956653595\n",
      "Training Batch [370/782]: Loss 0.44865578413009644\n",
      "Training Batch [371/782]: Loss 0.28946810960769653\n",
      "Training Batch [372/782]: Loss 0.19289876520633698\n",
      "Training Batch [373/782]: Loss 0.28359362483024597\n",
      "Training Batch [374/782]: Loss 0.12212497740983963\n",
      "Training Batch [375/782]: Loss 0.05677211657166481\n",
      "Training Batch [376/782]: Loss 0.41825222969055176\n",
      "Training Batch [377/782]: Loss 0.20075112581253052\n",
      "Training Batch [378/782]: Loss 0.23950035870075226\n",
      "Training Batch [379/782]: Loss 0.2407037764787674\n",
      "Training Batch [380/782]: Loss 0.20912539958953857\n",
      "Training Batch [381/782]: Loss 0.27704742550849915\n",
      "Training Batch [382/782]: Loss 0.19363026320934296\n",
      "Training Batch [383/782]: Loss 0.18657606840133667\n",
      "Training Batch [384/782]: Loss 0.2749994695186615\n",
      "Training Batch [385/782]: Loss 0.22108885645866394\n",
      "Training Batch [386/782]: Loss 0.15959946811199188\n",
      "Training Batch [387/782]: Loss 0.1419200599193573\n",
      "Training Batch [388/782]: Loss 0.22952976822853088\n",
      "Training Batch [389/782]: Loss 0.1830388754606247\n",
      "Training Batch [390/782]: Loss 0.24401505291461945\n",
      "Training Batch [391/782]: Loss 0.3684556186199188\n",
      "Training Batch [392/782]: Loss 0.15763172507286072\n",
      "Training Batch [393/782]: Loss 0.3634089529514313\n",
      "Training Batch [394/782]: Loss 0.274372398853302\n",
      "Training Batch [395/782]: Loss 0.15614762902259827\n",
      "Training Batch [396/782]: Loss 0.2233663648366928\n",
      "Training Batch [397/782]: Loss 0.21702829003334045\n",
      "Training Batch [398/782]: Loss 0.2286350429058075\n",
      "Training Batch [399/782]: Loss 0.2833139896392822\n",
      "Training Batch [400/782]: Loss 0.19443336129188538\n",
      "Training Batch [401/782]: Loss 0.2755289375782013\n",
      "Training Batch [402/782]: Loss 0.17409469187259674\n",
      "Training Batch [403/782]: Loss 0.2534647583961487\n",
      "Training Batch [404/782]: Loss 0.3038042187690735\n",
      "Training Batch [405/782]: Loss 0.15540814399719238\n",
      "Training Batch [406/782]: Loss 0.2514451742172241\n",
      "Training Batch [407/782]: Loss 0.0889538899064064\n",
      "Training Batch [408/782]: Loss 0.2163379043340683\n",
      "Training Batch [409/782]: Loss 0.10354412347078323\n",
      "Training Batch [410/782]: Loss 0.15164309740066528\n",
      "Training Batch [411/782]: Loss 0.2554464042186737\n",
      "Training Batch [412/782]: Loss 0.12165781855583191\n",
      "Training Batch [413/782]: Loss 0.36276331543922424\n",
      "Training Batch [414/782]: Loss 0.18486393988132477\n",
      "Training Batch [415/782]: Loss 0.24632994830608368\n",
      "Training Batch [416/782]: Loss 0.1450985223054886\n",
      "Training Batch [417/782]: Loss 0.16098599135875702\n",
      "Training Batch [418/782]: Loss 0.10893433541059494\n",
      "Training Batch [419/782]: Loss 0.1657458394765854\n",
      "Training Batch [420/782]: Loss 0.1303396075963974\n",
      "Training Batch [421/782]: Loss 0.20095181465148926\n",
      "Training Batch [422/782]: Loss 0.19739341735839844\n",
      "Training Batch [423/782]: Loss 0.15862491726875305\n",
      "Training Batch [424/782]: Loss 0.25513792037963867\n",
      "Training Batch [425/782]: Loss 0.17671160399913788\n",
      "Training Batch [426/782]: Loss 0.14862191677093506\n",
      "Training Batch [427/782]: Loss 0.36723145842552185\n",
      "Training Batch [428/782]: Loss 0.3077749013900757\n",
      "Training Batch [429/782]: Loss 0.24086551368236542\n",
      "Training Batch [430/782]: Loss 0.12827086448669434\n",
      "Training Batch [431/782]: Loss 0.18354789912700653\n",
      "Training Batch [432/782]: Loss 0.1935192048549652\n",
      "Training Batch [433/782]: Loss 0.24180951714515686\n",
      "Training Batch [434/782]: Loss 0.20473845303058624\n",
      "Training Batch [435/782]: Loss 0.303849458694458\n",
      "Training Batch [436/782]: Loss 0.10196124017238617\n",
      "Training Batch [437/782]: Loss 0.27795225381851196\n",
      "Training Batch [438/782]: Loss 0.23114584386348724\n",
      "Training Batch [439/782]: Loss 0.1754656285047531\n",
      "Training Batch [440/782]: Loss 0.32167181372642517\n",
      "Training Batch [441/782]: Loss 0.16488488018512726\n",
      "Training Batch [442/782]: Loss 0.27628955245018005\n",
      "Training Batch [443/782]: Loss 0.16865500807762146\n",
      "Training Batch [444/782]: Loss 0.24937909841537476\n",
      "Training Batch [445/782]: Loss 0.12966586649417877\n",
      "Training Batch [446/782]: Loss 0.11446301639080048\n",
      "Training Batch [447/782]: Loss 0.28591760993003845\n",
      "Training Batch [448/782]: Loss 0.24399954080581665\n",
      "Training Batch [449/782]: Loss 0.18093915283679962\n",
      "Training Batch [450/782]: Loss 0.20693862438201904\n",
      "Training Batch [451/782]: Loss 0.17784607410430908\n",
      "Training Batch [452/782]: Loss 0.13971911370754242\n",
      "Training Batch [453/782]: Loss 0.23183804750442505\n",
      "Training Batch [454/782]: Loss 0.2504112422466278\n",
      "Training Batch [455/782]: Loss 0.2310716211795807\n",
      "Training Batch [456/782]: Loss 0.20113185048103333\n",
      "Training Batch [457/782]: Loss 0.2615683078765869\n",
      "Training Batch [458/782]: Loss 0.21754276752471924\n",
      "Training Batch [459/782]: Loss 0.1884055733680725\n",
      "Training Batch [460/782]: Loss 0.13730891048908234\n",
      "Training Batch [461/782]: Loss 0.1726648360490799\n",
      "Training Batch [462/782]: Loss 0.17286503314971924\n",
      "Training Batch [463/782]: Loss 0.2877483069896698\n",
      "Training Batch [464/782]: Loss 0.1895264834165573\n",
      "Training Batch [465/782]: Loss 0.16833427548408508\n",
      "Training Batch [466/782]: Loss 0.19406820833683014\n",
      "Training Batch [467/782]: Loss 0.13658975064754486\n",
      "Training Batch [468/782]: Loss 0.20765812695026398\n",
      "Training Batch [469/782]: Loss 0.15339402854442596\n",
      "Training Batch [470/782]: Loss 0.2200784683227539\n",
      "Training Batch [471/782]: Loss 0.20850104093551636\n",
      "Training Batch [472/782]: Loss 0.30401283502578735\n",
      "Training Batch [473/782]: Loss 0.1448448896408081\n",
      "Training Batch [474/782]: Loss 0.08484235405921936\n",
      "Training Batch [475/782]: Loss 0.2339007556438446\n",
      "Training Batch [476/782]: Loss 0.4255850315093994\n",
      "Training Batch [477/782]: Loss 0.20823192596435547\n",
      "Training Batch [478/782]: Loss 0.18424133956432343\n",
      "Training Batch [479/782]: Loss 0.20848190784454346\n",
      "Training Batch [480/782]: Loss 0.22089968621730804\n",
      "Training Batch [481/782]: Loss 0.3047561049461365\n",
      "Training Batch [482/782]: Loss 0.21268479526042938\n",
      "Training Batch [483/782]: Loss 0.22919254004955292\n",
      "Training Batch [484/782]: Loss 0.30078819394111633\n",
      "Training Batch [485/782]: Loss 0.24119868874549866\n",
      "Training Batch [486/782]: Loss 0.14959250390529633\n",
      "Training Batch [487/782]: Loss 0.1758405864238739\n",
      "Training Batch [488/782]: Loss 0.3457318842411041\n",
      "Training Batch [489/782]: Loss 0.3956700563430786\n",
      "Training Batch [490/782]: Loss 0.196830615401268\n",
      "Training Batch [491/782]: Loss 0.2483643889427185\n",
      "Training Batch [492/782]: Loss 0.16749051213264465\n",
      "Training Batch [493/782]: Loss 0.29911062121391296\n",
      "Training Batch [494/782]: Loss 0.3234463036060333\n",
      "Training Batch [495/782]: Loss 0.22416538000106812\n",
      "Training Batch [496/782]: Loss 0.1842624545097351\n",
      "Training Batch [497/782]: Loss 0.12361907958984375\n",
      "Training Batch [498/782]: Loss 0.13772089779376984\n",
      "Training Batch [499/782]: Loss 0.16373838484287262\n",
      "Training Batch [500/782]: Loss 0.11331348866224289\n",
      "Training Batch [501/782]: Loss 0.14558285474777222\n",
      "Training Batch [502/782]: Loss 0.17257043719291687\n",
      "Training Batch [503/782]: Loss 0.1392444670200348\n",
      "Training Batch [504/782]: Loss 0.1706250011920929\n",
      "Training Batch [505/782]: Loss 0.19627518951892853\n",
      "Training Batch [506/782]: Loss 0.24271710216999054\n",
      "Training Batch [507/782]: Loss 0.2674415111541748\n",
      "Training Batch [508/782]: Loss 0.20971731841564178\n",
      "Training Batch [509/782]: Loss 0.21996057033538818\n",
      "Training Batch [510/782]: Loss 0.23027487099170685\n",
      "Training Batch [511/782]: Loss 0.21400120854377747\n",
      "Training Batch [512/782]: Loss 0.14620374143123627\n",
      "Training Batch [513/782]: Loss 0.21560581028461456\n",
      "Training Batch [514/782]: Loss 0.28142425417900085\n",
      "Training Batch [515/782]: Loss 0.2883620262145996\n",
      "Training Batch [516/782]: Loss 0.2285837084054947\n",
      "Training Batch [517/782]: Loss 0.4507400691509247\n",
      "Training Batch [518/782]: Loss 0.08474605530500412\n",
      "Training Batch [519/782]: Loss 0.19369223713874817\n",
      "Training Batch [520/782]: Loss 0.25033581256866455\n",
      "Training Batch [521/782]: Loss 0.22862312197685242\n",
      "Training Batch [522/782]: Loss 0.22105944156646729\n",
      "Training Batch [523/782]: Loss 0.2711470425128937\n",
      "Training Batch [524/782]: Loss 0.20331376791000366\n",
      "Training Batch [525/782]: Loss 0.22832287847995758\n",
      "Training Batch [526/782]: Loss 0.10435011982917786\n",
      "Training Batch [527/782]: Loss 0.2506861388683319\n",
      "Training Batch [528/782]: Loss 0.26069265604019165\n",
      "Training Batch [529/782]: Loss 0.293862521648407\n",
      "Training Batch [530/782]: Loss 0.2939291298389435\n",
      "Training Batch [531/782]: Loss 0.31542590260505676\n",
      "Training Batch [532/782]: Loss 0.21455660462379456\n",
      "Training Batch [533/782]: Loss 0.18364353477954865\n",
      "Training Batch [534/782]: Loss 0.14210858941078186\n",
      "Training Batch [535/782]: Loss 0.24752280116081238\n",
      "Training Batch [536/782]: Loss 0.30204617977142334\n",
      "Training Batch [537/782]: Loss 0.22631186246871948\n",
      "Training Batch [538/782]: Loss 0.3195333480834961\n",
      "Training Batch [539/782]: Loss 0.15779893100261688\n",
      "Training Batch [540/782]: Loss 0.17646348476409912\n",
      "Training Batch [541/782]: Loss 0.15505826473236084\n",
      "Training Batch [542/782]: Loss 0.4368630349636078\n",
      "Training Batch [543/782]: Loss 0.2013612985610962\n",
      "Training Batch [544/782]: Loss 0.24601401388645172\n",
      "Training Batch [545/782]: Loss 0.1193050667643547\n",
      "Training Batch [546/782]: Loss 0.1780446469783783\n",
      "Training Batch [547/782]: Loss 0.26395493745803833\n",
      "Training Batch [548/782]: Loss 0.15489338338375092\n",
      "Training Batch [549/782]: Loss 0.21537348628044128\n",
      "Training Batch [550/782]: Loss 0.34514862298965454\n",
      "Training Batch [551/782]: Loss 0.23076137900352478\n",
      "Training Batch [552/782]: Loss 0.10966567695140839\n",
      "Training Batch [553/782]: Loss 0.21232104301452637\n",
      "Training Batch [554/782]: Loss 0.20808403193950653\n",
      "Training Batch [555/782]: Loss 0.20664021372795105\n",
      "Training Batch [556/782]: Loss 0.30969756841659546\n",
      "Training Batch [557/782]: Loss 0.2550121247768402\n",
      "Training Batch [558/782]: Loss 0.11942226439714432\n",
      "Training Batch [559/782]: Loss 0.35618212819099426\n",
      "Training Batch [560/782]: Loss 0.1866912841796875\n",
      "Training Batch [561/782]: Loss 0.24181778728961945\n",
      "Training Batch [562/782]: Loss 0.1893288940191269\n",
      "Training Batch [563/782]: Loss 0.29197466373443604\n",
      "Training Batch [564/782]: Loss 0.4683695435523987\n",
      "Training Batch [565/782]: Loss 0.14698123931884766\n",
      "Training Batch [566/782]: Loss 0.24114421010017395\n",
      "Training Batch [567/782]: Loss 0.1914905309677124\n",
      "Training Batch [568/782]: Loss 0.3517969846725464\n",
      "Training Batch [569/782]: Loss 0.1712729036808014\n",
      "Training Batch [570/782]: Loss 0.18763363361358643\n",
      "Training Batch [571/782]: Loss 0.24460695683956146\n",
      "Training Batch [572/782]: Loss 0.33428752422332764\n",
      "Training Batch [573/782]: Loss 0.2648908197879791\n",
      "Training Batch [574/782]: Loss 0.16925513744354248\n",
      "Training Batch [575/782]: Loss 0.1392500251531601\n",
      "Training Batch [576/782]: Loss 0.26159149408340454\n",
      "Training Batch [577/782]: Loss 0.11108050495386124\n",
      "Training Batch [578/782]: Loss 0.32257869839668274\n",
      "Training Batch [579/782]: Loss 0.1293753683567047\n",
      "Training Batch [580/782]: Loss 0.1731596142053604\n",
      "Training Batch [581/782]: Loss 0.275556355714798\n",
      "Training Batch [582/782]: Loss 0.20737889409065247\n",
      "Training Batch [583/782]: Loss 0.5453432202339172\n",
      "Training Batch [584/782]: Loss 0.1793048232793808\n",
      "Training Batch [585/782]: Loss 0.3435136377811432\n",
      "Training Batch [586/782]: Loss 0.33302006125450134\n",
      "Training Batch [587/782]: Loss 0.1889176368713379\n",
      "Training Batch [588/782]: Loss 0.3423958718776703\n",
      "Training Batch [589/782]: Loss 0.19012035429477692\n",
      "Training Batch [590/782]: Loss 0.33133408427238464\n",
      "Training Batch [591/782]: Loss 0.25353944301605225\n",
      "Training Batch [592/782]: Loss 0.5131776332855225\n",
      "Training Batch [593/782]: Loss 0.17775696516036987\n",
      "Training Batch [594/782]: Loss 0.2352021038532257\n",
      "Training Batch [595/782]: Loss 0.1697944849729538\n",
      "Training Batch [596/782]: Loss 0.24117426574230194\n",
      "Training Batch [597/782]: Loss 0.2404552400112152\n",
      "Training Batch [598/782]: Loss 0.2366034835577011\n",
      "Training Batch [599/782]: Loss 0.404782235622406\n",
      "Training Batch [600/782]: Loss 0.22859464585781097\n",
      "Training Batch [601/782]: Loss 0.13708795607089996\n",
      "Training Batch [602/782]: Loss 0.2893408238887787\n",
      "Training Batch [603/782]: Loss 0.2487402856349945\n",
      "Training Batch [604/782]: Loss 0.2032558172941208\n",
      "Training Batch [605/782]: Loss 0.18306584656238556\n",
      "Training Batch [606/782]: Loss 0.28363776206970215\n",
      "Training Batch [607/782]: Loss 0.17395785450935364\n",
      "Training Batch [608/782]: Loss 0.13585156202316284\n",
      "Training Batch [609/782]: Loss 0.4983740448951721\n",
      "Training Batch [610/782]: Loss 0.21540822088718414\n",
      "Training Batch [611/782]: Loss 0.21357712149620056\n",
      "Training Batch [612/782]: Loss 0.1751265674829483\n",
      "Training Batch [613/782]: Loss 0.2845767140388489\n",
      "Training Batch [614/782]: Loss 0.26399704813957214\n",
      "Training Batch [615/782]: Loss 0.29942587018013\n",
      "Training Batch [616/782]: Loss 0.20845544338226318\n",
      "Training Batch [617/782]: Loss 0.2346494197845459\n",
      "Training Batch [618/782]: Loss 0.25756460428237915\n",
      "Training Batch [619/782]: Loss 0.33636030554771423\n",
      "Training Batch [620/782]: Loss 0.2139410376548767\n",
      "Training Batch [621/782]: Loss 0.18167847394943237\n",
      "Training Batch [622/782]: Loss 0.1801360696554184\n",
      "Training Batch [623/782]: Loss 0.4079160690307617\n",
      "Training Batch [624/782]: Loss 0.3524497449398041\n",
      "Training Batch [625/782]: Loss 0.3245488107204437\n",
      "Training Batch [626/782]: Loss 0.2538128197193146\n",
      "Training Batch [627/782]: Loss 0.23670876026153564\n",
      "Training Batch [628/782]: Loss 0.3630366325378418\n",
      "Training Batch [629/782]: Loss 0.2786186635494232\n",
      "Training Batch [630/782]: Loss 0.16991031169891357\n",
      "Training Batch [631/782]: Loss 0.2702435851097107\n",
      "Training Batch [632/782]: Loss 0.47516125440597534\n",
      "Training Batch [633/782]: Loss 0.22701624035835266\n",
      "Training Batch [634/782]: Loss 0.2816738486289978\n",
      "Training Batch [635/782]: Loss 0.1728735715150833\n",
      "Training Batch [636/782]: Loss 0.1415758579969406\n",
      "Training Batch [637/782]: Loss 0.3122238516807556\n",
      "Training Batch [638/782]: Loss 0.32121163606643677\n",
      "Training Batch [639/782]: Loss 0.17454536259174347\n",
      "Training Batch [640/782]: Loss 0.21092207729816437\n",
      "Training Batch [641/782]: Loss 0.24197427928447723\n",
      "Training Batch [642/782]: Loss 0.27609866857528687\n",
      "Training Batch [643/782]: Loss 0.3777346611022949\n",
      "Training Batch [644/782]: Loss 0.09645570814609528\n",
      "Training Batch [645/782]: Loss 0.11131066083908081\n",
      "Training Batch [646/782]: Loss 0.19363409280776978\n",
      "Training Batch [647/782]: Loss 0.1624801605939865\n",
      "Training Batch [648/782]: Loss 0.30648618936538696\n",
      "Training Batch [649/782]: Loss 0.25177091360092163\n",
      "Training Batch [650/782]: Loss 0.233033686876297\n",
      "Training Batch [651/782]: Loss 0.26003333926200867\n",
      "Training Batch [652/782]: Loss 0.33984488248825073\n",
      "Training Batch [653/782]: Loss 0.11890488117933273\n",
      "Training Batch [654/782]: Loss 0.22856733202934265\n",
      "Training Batch [655/782]: Loss 0.26621025800704956\n",
      "Training Batch [656/782]: Loss 0.4104444682598114\n",
      "Training Batch [657/782]: Loss 0.28261855244636536\n",
      "Training Batch [658/782]: Loss 0.20397765934467316\n",
      "Training Batch [659/782]: Loss 0.21008101105690002\n",
      "Training Batch [660/782]: Loss 0.2239873856306076\n",
      "Training Batch [661/782]: Loss 0.27582669258117676\n",
      "Training Batch [662/782]: Loss 0.26864296197891235\n",
      "Training Batch [663/782]: Loss 0.19290706515312195\n",
      "Training Batch [664/782]: Loss 0.20680205523967743\n",
      "Training Batch [665/782]: Loss 0.15049491822719574\n",
      "Training Batch [666/782]: Loss 0.14153823256492615\n",
      "Training Batch [667/782]: Loss 0.21619364619255066\n",
      "Training Batch [668/782]: Loss 0.26659318804740906\n",
      "Training Batch [669/782]: Loss 0.45595306158065796\n",
      "Training Batch [670/782]: Loss 0.21253710985183716\n",
      "Training Batch [671/782]: Loss 0.16534169018268585\n",
      "Training Batch [672/782]: Loss 0.13701635599136353\n",
      "Training Batch [673/782]: Loss 0.16425298154354095\n",
      "Training Batch [674/782]: Loss 0.1635153591632843\n",
      "Training Batch [675/782]: Loss 0.33066025376319885\n",
      "Training Batch [676/782]: Loss 0.25637200474739075\n",
      "Training Batch [677/782]: Loss 0.3660559058189392\n",
      "Training Batch [678/782]: Loss 0.36977341771125793\n",
      "Training Batch [679/782]: Loss 0.38329577445983887\n",
      "Training Batch [680/782]: Loss 0.21539194881916046\n",
      "Training Batch [681/782]: Loss 0.14344088733196259\n",
      "Training Batch [682/782]: Loss 0.18042567372322083\n",
      "Training Batch [683/782]: Loss 0.2716813087463379\n",
      "Training Batch [684/782]: Loss 0.1693316549062729\n",
      "Training Batch [685/782]: Loss 0.20977012813091278\n",
      "Training Batch [686/782]: Loss 0.17603212594985962\n",
      "Training Batch [687/782]: Loss 0.2759028971195221\n",
      "Training Batch [688/782]: Loss 0.21317638456821442\n",
      "Training Batch [689/782]: Loss 0.275010883808136\n",
      "Training Batch [690/782]: Loss 0.1650051474571228\n",
      "Training Batch [691/782]: Loss 0.19237372279167175\n",
      "Training Batch [692/782]: Loss 0.34433358907699585\n",
      "Training Batch [693/782]: Loss 0.4714512825012207\n",
      "Training Batch [694/782]: Loss 0.29250970482826233\n",
      "Training Batch [695/782]: Loss 0.16401103138923645\n",
      "Training Batch [696/782]: Loss 0.2998926043510437\n",
      "Training Batch [697/782]: Loss 0.2109968066215515\n",
      "Training Batch [698/782]: Loss 0.31468117237091064\n",
      "Training Batch [699/782]: Loss 0.24168817698955536\n",
      "Training Batch [700/782]: Loss 0.3371381163597107\n",
      "Training Batch [701/782]: Loss 0.2022545337677002\n",
      "Training Batch [702/782]: Loss 0.27665844559669495\n",
      "Training Batch [703/782]: Loss 0.22811655700206757\n",
      "Training Batch [704/782]: Loss 0.308220237493515\n",
      "Training Batch [705/782]: Loss 0.29181569814682007\n",
      "Training Batch [706/782]: Loss 0.199104905128479\n",
      "Training Batch [707/782]: Loss 0.1940324455499649\n",
      "Training Batch [708/782]: Loss 0.314520001411438\n",
      "Training Batch [709/782]: Loss 0.20069941878318787\n",
      "Training Batch [710/782]: Loss 0.3759816586971283\n",
      "Training Batch [711/782]: Loss 0.27971071004867554\n",
      "Training Batch [712/782]: Loss 0.24670712649822235\n",
      "Training Batch [713/782]: Loss 0.30941933393478394\n",
      "Training Batch [714/782]: Loss 0.3070075809955597\n",
      "Training Batch [715/782]: Loss 0.2990640699863434\n",
      "Training Batch [716/782]: Loss 0.29939594864845276\n",
      "Training Batch [717/782]: Loss 0.36017951369285583\n",
      "Training Batch [718/782]: Loss 0.2928025424480438\n",
      "Training Batch [719/782]: Loss 0.23689644038677216\n",
      "Training Batch [720/782]: Loss 0.2893356382846832\n",
      "Training Batch [721/782]: Loss 0.2961166501045227\n",
      "Training Batch [722/782]: Loss 0.18277469277381897\n",
      "Training Batch [723/782]: Loss 0.26361894607543945\n",
      "Training Batch [724/782]: Loss 0.19069696962833405\n",
      "Training Batch [725/782]: Loss 0.2810303866863251\n",
      "Training Batch [726/782]: Loss 0.36168748140335083\n",
      "Training Batch [727/782]: Loss 0.34549644589424133\n",
      "Training Batch [728/782]: Loss 0.3376328647136688\n",
      "Training Batch [729/782]: Loss 0.14303039014339447\n",
      "Training Batch [730/782]: Loss 0.2231535166501999\n",
      "Training Batch [731/782]: Loss 0.4548925757408142\n",
      "Training Batch [732/782]: Loss 0.2445751130580902\n",
      "Training Batch [733/782]: Loss 0.3698285222053528\n",
      "Training Batch [734/782]: Loss 0.4599204361438751\n",
      "Training Batch [735/782]: Loss 0.35342299938201904\n",
      "Training Batch [736/782]: Loss 0.2880966067314148\n",
      "Training Batch [737/782]: Loss 0.33154547214508057\n",
      "Training Batch [738/782]: Loss 0.05269215628504753\n",
      "Training Batch [739/782]: Loss 0.21984100341796875\n",
      "Training Batch [740/782]: Loss 0.22696565091609955\n",
      "Training Batch [741/782]: Loss 0.3601544499397278\n",
      "Training Batch [742/782]: Loss 0.17093412578105927\n",
      "Training Batch [743/782]: Loss 0.23614202439785004\n",
      "Training Batch [744/782]: Loss 0.29778748750686646\n",
      "Training Batch [745/782]: Loss 0.17700257897377014\n",
      "Training Batch [746/782]: Loss 0.306112676858902\n",
      "Training Batch [747/782]: Loss 0.21123838424682617\n",
      "Training Batch [748/782]: Loss 0.3302546441555023\n",
      "Training Batch [749/782]: Loss 0.29967519640922546\n",
      "Training Batch [750/782]: Loss 0.32044360041618347\n",
      "Training Batch [751/782]: Loss 0.4282369315624237\n",
      "Training Batch [752/782]: Loss 0.253273606300354\n",
      "Training Batch [753/782]: Loss 0.23845772445201874\n",
      "Training Batch [754/782]: Loss 0.2564941644668579\n",
      "Training Batch [755/782]: Loss 0.18334583938121796\n",
      "Training Batch [756/782]: Loss 0.26186755299568176\n",
      "Training Batch [757/782]: Loss 0.3329249918460846\n",
      "Training Batch [758/782]: Loss 0.2677103579044342\n",
      "Training Batch [759/782]: Loss 0.39134931564331055\n",
      "Training Batch [760/782]: Loss 0.533222496509552\n",
      "Training Batch [761/782]: Loss 0.22947555780410767\n",
      "Training Batch [762/782]: Loss 0.2577219009399414\n",
      "Training Batch [763/782]: Loss 0.462484747171402\n",
      "Training Batch [764/782]: Loss 0.22632044553756714\n",
      "Training Batch [765/782]: Loss 0.3135700523853302\n",
      "Training Batch [766/782]: Loss 0.34596431255340576\n",
      "Training Batch [767/782]: Loss 0.2011314481496811\n",
      "Training Batch [768/782]: Loss 0.47568225860595703\n",
      "Training Batch [769/782]: Loss 0.2632125914096832\n",
      "Training Batch [770/782]: Loss 0.3046863079071045\n",
      "Training Batch [771/782]: Loss 0.29283851385116577\n",
      "Training Batch [772/782]: Loss 0.1978643238544464\n",
      "Training Batch [773/782]: Loss 0.26211246848106384\n",
      "Training Batch [774/782]: Loss 0.2076960802078247\n",
      "Training Batch [775/782]: Loss 0.3486371636390686\n",
      "Training Batch [776/782]: Loss 0.3375929594039917\n",
      "Training Batch [777/782]: Loss 0.1721121072769165\n",
      "Training Batch [778/782]: Loss 0.19908025860786438\n",
      "Training Batch [779/782]: Loss 0.26828673481941223\n",
      "Training Batch [780/782]: Loss 0.365800678730011\n",
      "Training Batch [781/782]: Loss 0.31597617268562317\n",
      "Training Batch [782/782]: Loss 0.40748265385627747\n",
      "Epoch 9 - Train Loss: 0.2052\n",
      "*********  Epoch 10/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.14455266296863556\n",
      "Training Batch [2/782]: Loss 0.22968460619449615\n",
      "Training Batch [3/782]: Loss 0.2775844931602478\n",
      "Training Batch [4/782]: Loss 0.16728192567825317\n",
      "Training Batch [5/782]: Loss 0.18872641026973724\n",
      "Training Batch [6/782]: Loss 0.3518689274787903\n",
      "Training Batch [7/782]: Loss 0.15881818532943726\n",
      "Training Batch [8/782]: Loss 0.1082465648651123\n",
      "Training Batch [9/782]: Loss 0.1427977830171585\n",
      "Training Batch [10/782]: Loss 0.17939862608909607\n",
      "Training Batch [11/782]: Loss 0.18351204693317413\n",
      "Training Batch [12/782]: Loss 0.12560150027275085\n",
      "Training Batch [13/782]: Loss 0.2880468964576721\n",
      "Training Batch [14/782]: Loss 0.21281898021697998\n",
      "Training Batch [15/782]: Loss 0.15562893450260162\n",
      "Training Batch [16/782]: Loss 0.13997882604599\n",
      "Training Batch [17/782]: Loss 0.2650398015975952\n",
      "Training Batch [18/782]: Loss 0.2743776738643646\n",
      "Training Batch [19/782]: Loss 0.3021382987499237\n",
      "Training Batch [20/782]: Loss 0.23222041130065918\n",
      "Training Batch [21/782]: Loss 0.1696311980485916\n",
      "Training Batch [22/782]: Loss 0.26314494013786316\n",
      "Training Batch [23/782]: Loss 0.17390377819538116\n",
      "Training Batch [24/782]: Loss 0.3168501555919647\n",
      "Training Batch [25/782]: Loss 0.2842784523963928\n",
      "Training Batch [26/782]: Loss 0.15473860502243042\n",
      "Training Batch [27/782]: Loss 0.19499070942401886\n",
      "Training Batch [28/782]: Loss 0.22616630792617798\n",
      "Training Batch [29/782]: Loss 0.29568570852279663\n",
      "Training Batch [30/782]: Loss 0.3011232614517212\n",
      "Training Batch [31/782]: Loss 0.18554019927978516\n",
      "Training Batch [32/782]: Loss 0.1166120246052742\n",
      "Training Batch [33/782]: Loss 0.09122937172651291\n",
      "Training Batch [34/782]: Loss 0.16853980720043182\n",
      "Training Batch [35/782]: Loss 0.11482342332601547\n",
      "Training Batch [36/782]: Loss 0.0920993760228157\n",
      "Training Batch [37/782]: Loss 0.1504122018814087\n",
      "Training Batch [38/782]: Loss 0.26644957065582275\n",
      "Training Batch [39/782]: Loss 0.15003859996795654\n",
      "Training Batch [40/782]: Loss 0.19825588166713715\n",
      "Training Batch [41/782]: Loss 0.14898781478405\n",
      "Training Batch [42/782]: Loss 0.08048655092716217\n",
      "Training Batch [43/782]: Loss 0.1963653713464737\n",
      "Training Batch [44/782]: Loss 0.20196810364723206\n",
      "Training Batch [45/782]: Loss 0.10425417870283127\n",
      "Training Batch [46/782]: Loss 0.16833381354808807\n",
      "Training Batch [47/782]: Loss 0.13332098722457886\n",
      "Training Batch [48/782]: Loss 0.17828254401683807\n",
      "Training Batch [49/782]: Loss 0.295207679271698\n",
      "Training Batch [50/782]: Loss 0.15358829498291016\n",
      "Training Batch [51/782]: Loss 0.13447286188602448\n",
      "Training Batch [52/782]: Loss 0.14343078434467316\n",
      "Training Batch [53/782]: Loss 0.1761234700679779\n",
      "Training Batch [54/782]: Loss 0.08726023137569427\n",
      "Training Batch [55/782]: Loss 0.19104544818401337\n",
      "Training Batch [56/782]: Loss 0.1490415632724762\n",
      "Training Batch [57/782]: Loss 0.06133342161774635\n",
      "Training Batch [58/782]: Loss 0.1465117335319519\n",
      "Training Batch [59/782]: Loss 0.15251153707504272\n",
      "Training Batch [60/782]: Loss 0.18082663416862488\n",
      "Training Batch [61/782]: Loss 0.05825483426451683\n",
      "Training Batch [62/782]: Loss 0.14447419345378876\n",
      "Training Batch [63/782]: Loss 0.220610111951828\n",
      "Training Batch [64/782]: Loss 0.2838546633720398\n",
      "Training Batch [65/782]: Loss 0.07238494604825974\n",
      "Training Batch [66/782]: Loss 0.11895927786827087\n",
      "Training Batch [67/782]: Loss 0.2173759490251541\n",
      "Training Batch [68/782]: Loss 0.08478882163763046\n",
      "Training Batch [69/782]: Loss 0.09368736296892166\n",
      "Training Batch [70/782]: Loss 0.047381024807691574\n",
      "Training Batch [71/782]: Loss 0.019950129091739655\n",
      "Training Batch [72/782]: Loss 0.15576866269111633\n",
      "Training Batch [73/782]: Loss 0.31641969084739685\n",
      "Training Batch [74/782]: Loss 0.27706271409988403\n",
      "Training Batch [75/782]: Loss 0.07884753495454788\n",
      "Training Batch [76/782]: Loss 0.08597765117883682\n",
      "Training Batch [77/782]: Loss 0.11165795475244522\n",
      "Training Batch [78/782]: Loss 0.17396628856658936\n",
      "Training Batch [79/782]: Loss 0.13775768876075745\n",
      "Training Batch [80/782]: Loss 0.14353208243846893\n",
      "Training Batch [81/782]: Loss 0.12345350533723831\n",
      "Training Batch [82/782]: Loss 0.10114564746618271\n",
      "Training Batch [83/782]: Loss 0.15906144678592682\n",
      "Training Batch [84/782]: Loss 0.14801140129566193\n",
      "Training Batch [85/782]: Loss 0.13171784579753876\n",
      "Training Batch [86/782]: Loss 0.11032020300626755\n",
      "Training Batch [87/782]: Loss 0.04984065890312195\n",
      "Training Batch [88/782]: Loss 0.06729665398597717\n",
      "Training Batch [89/782]: Loss 0.19652216136455536\n",
      "Training Batch [90/782]: Loss 0.08774828165769577\n",
      "Training Batch [91/782]: Loss 0.07154536247253418\n",
      "Training Batch [92/782]: Loss 0.1375209391117096\n",
      "Training Batch [93/782]: Loss 0.1971661001443863\n",
      "Training Batch [94/782]: Loss 0.07612427324056625\n",
      "Training Batch [95/782]: Loss 0.06651975214481354\n",
      "Training Batch [96/782]: Loss 0.09854881465435028\n",
      "Training Batch [97/782]: Loss 0.10715562850236893\n",
      "Training Batch [98/782]: Loss 0.08170730620622635\n",
      "Training Batch [99/782]: Loss 0.23819144070148468\n",
      "Training Batch [100/782]: Loss 0.08280228078365326\n",
      "Training Batch [101/782]: Loss 0.21401765942573547\n",
      "Training Batch [102/782]: Loss 0.08254996687173843\n",
      "Training Batch [103/782]: Loss 0.05507894605398178\n",
      "Training Batch [104/782]: Loss 0.07641386985778809\n",
      "Training Batch [105/782]: Loss 0.1035197451710701\n",
      "Training Batch [106/782]: Loss 0.11259467154741287\n",
      "Training Batch [107/782]: Loss 0.12669271230697632\n",
      "Training Batch [108/782]: Loss 0.06219872087240219\n",
      "Training Batch [109/782]: Loss 0.19513635337352753\n",
      "Training Batch [110/782]: Loss 0.10023275762796402\n",
      "Training Batch [111/782]: Loss 0.15160010755062103\n",
      "Training Batch [112/782]: Loss 0.05265220254659653\n",
      "Training Batch [113/782]: Loss 0.16477367281913757\n",
      "Training Batch [114/782]: Loss 0.12388908863067627\n",
      "Training Batch [115/782]: Loss 0.09516265988349915\n",
      "Training Batch [116/782]: Loss 0.15463317930698395\n",
      "Training Batch [117/782]: Loss 0.13603171706199646\n",
      "Training Batch [118/782]: Loss 0.08993979543447495\n",
      "Training Batch [119/782]: Loss 0.12348488718271255\n",
      "Training Batch [120/782]: Loss 0.20566394925117493\n",
      "Training Batch [121/782]: Loss 0.07651102542877197\n",
      "Training Batch [122/782]: Loss 0.21640188992023468\n",
      "Training Batch [123/782]: Loss 0.11742869019508362\n",
      "Training Batch [124/782]: Loss 0.11086803674697876\n",
      "Training Batch [125/782]: Loss 0.10139408707618713\n",
      "Training Batch [126/782]: Loss 0.08132628351449966\n",
      "Training Batch [127/782]: Loss 0.15280798077583313\n",
      "Training Batch [128/782]: Loss 0.09168935567140579\n",
      "Training Batch [129/782]: Loss 0.08807475119829178\n",
      "Training Batch [130/782]: Loss 0.16078412532806396\n",
      "Training Batch [131/782]: Loss 0.1239694207906723\n",
      "Training Batch [132/782]: Loss 0.06858351826667786\n",
      "Training Batch [133/782]: Loss 0.2591840922832489\n",
      "Training Batch [134/782]: Loss 0.10397583991289139\n",
      "Training Batch [135/782]: Loss 0.08789610862731934\n",
      "Training Batch [136/782]: Loss 0.0677977204322815\n",
      "Training Batch [137/782]: Loss 0.10860687494277954\n",
      "Training Batch [138/782]: Loss 0.10534205287694931\n",
      "Training Batch [139/782]: Loss 0.1074865534901619\n",
      "Training Batch [140/782]: Loss 0.10235625505447388\n",
      "Training Batch [141/782]: Loss 0.14442814886569977\n",
      "Training Batch [142/782]: Loss 0.09122934937477112\n",
      "Training Batch [143/782]: Loss 0.1716492623090744\n",
      "Training Batch [144/782]: Loss 0.06991816312074661\n",
      "Training Batch [145/782]: Loss 0.06737181544303894\n",
      "Training Batch [146/782]: Loss 0.17771980166435242\n",
      "Training Batch [147/782]: Loss 0.06936631351709366\n",
      "Training Batch [148/782]: Loss 0.08425415307283401\n",
      "Training Batch [149/782]: Loss 0.08221461623907089\n",
      "Training Batch [150/782]: Loss 0.09467346221208572\n",
      "Training Batch [151/782]: Loss 0.07824932038784027\n",
      "Training Batch [152/782]: Loss 0.08877521008253098\n",
      "Training Batch [153/782]: Loss 0.08784343302249908\n",
      "Training Batch [154/782]: Loss 0.20142731070518494\n",
      "Training Batch [155/782]: Loss 0.13787275552749634\n",
      "Training Batch [156/782]: Loss 0.08101453632116318\n",
      "Training Batch [157/782]: Loss 0.059752412140369415\n",
      "Training Batch [158/782]: Loss 0.06228993460536003\n",
      "Training Batch [159/782]: Loss 0.09784775227308273\n",
      "Training Batch [160/782]: Loss 0.07411440461874008\n",
      "Training Batch [161/782]: Loss 0.037647806107997894\n",
      "Training Batch [162/782]: Loss 0.2070905864238739\n",
      "Training Batch [163/782]: Loss 0.10007299482822418\n",
      "Training Batch [164/782]: Loss 0.0507245697081089\n",
      "Training Batch [165/782]: Loss 0.10670621693134308\n",
      "Training Batch [166/782]: Loss 0.15926973521709442\n",
      "Training Batch [167/782]: Loss 0.12497120350599289\n",
      "Training Batch [168/782]: Loss 0.11734475940465927\n",
      "Training Batch [169/782]: Loss 0.14966478943824768\n",
      "Training Batch [170/782]: Loss 0.11788969486951828\n",
      "Training Batch [171/782]: Loss 0.06037979573011398\n",
      "Training Batch [172/782]: Loss 0.19839388132095337\n",
      "Training Batch [173/782]: Loss 0.12469542026519775\n",
      "Training Batch [174/782]: Loss 0.1236928403377533\n",
      "Training Batch [175/782]: Loss 0.050047509372234344\n",
      "Training Batch [176/782]: Loss 0.053711168467998505\n",
      "Training Batch [177/782]: Loss 0.18047037720680237\n",
      "Training Batch [178/782]: Loss 0.13367213308811188\n",
      "Training Batch [179/782]: Loss 0.03269249200820923\n",
      "Training Batch [180/782]: Loss 0.18357355892658234\n",
      "Training Batch [181/782]: Loss 0.16487087309360504\n",
      "Training Batch [182/782]: Loss 0.08127914369106293\n",
      "Training Batch [183/782]: Loss 0.04286397993564606\n",
      "Training Batch [184/782]: Loss 0.08663728088140488\n",
      "Training Batch [185/782]: Loss 0.13205856084823608\n",
      "Training Batch [186/782]: Loss 0.1643146574497223\n",
      "Training Batch [187/782]: Loss 0.09483416378498077\n",
      "Training Batch [188/782]: Loss 0.1118587777018547\n",
      "Training Batch [189/782]: Loss 0.121186263859272\n",
      "Training Batch [190/782]: Loss 0.08184757083654404\n",
      "Training Batch [191/782]: Loss 0.0799902155995369\n",
      "Training Batch [192/782]: Loss 0.06739432364702225\n",
      "Training Batch [193/782]: Loss 0.10944756120443344\n",
      "Training Batch [194/782]: Loss 0.045507632195949554\n",
      "Training Batch [195/782]: Loss 0.05504186078906059\n",
      "Training Batch [196/782]: Loss 0.10738522559404373\n",
      "Training Batch [197/782]: Loss 0.04617125168442726\n",
      "Training Batch [198/782]: Loss 0.15820229053497314\n",
      "Training Batch [199/782]: Loss 0.11323573440313339\n",
      "Training Batch [200/782]: Loss 0.07154154777526855\n",
      "Training Batch [201/782]: Loss 0.04798474535346031\n",
      "Training Batch [202/782]: Loss 0.1673186719417572\n",
      "Training Batch [203/782]: Loss 0.10408848524093628\n",
      "Training Batch [204/782]: Loss 0.08932624757289886\n",
      "Training Batch [205/782]: Loss 0.06572099775075912\n",
      "Training Batch [206/782]: Loss 0.06660749018192291\n",
      "Training Batch [207/782]: Loss 0.1507929414510727\n",
      "Training Batch [208/782]: Loss 0.1708241105079651\n",
      "Training Batch [209/782]: Loss 0.12493103742599487\n",
      "Training Batch [210/782]: Loss 0.09484213590621948\n",
      "Training Batch [211/782]: Loss 0.16068026423454285\n",
      "Training Batch [212/782]: Loss 0.06299372762441635\n",
      "Training Batch [213/782]: Loss 0.12198426574468613\n",
      "Training Batch [214/782]: Loss 0.16804872453212738\n",
      "Training Batch [215/782]: Loss 0.10967031121253967\n",
      "Training Batch [216/782]: Loss 0.10856811702251434\n",
      "Training Batch [217/782]: Loss 0.0622904971241951\n",
      "Training Batch [218/782]: Loss 0.04075101390480995\n",
      "Training Batch [219/782]: Loss 0.06702592223882675\n",
      "Training Batch [220/782]: Loss 0.13173845410346985\n",
      "Training Batch [221/782]: Loss 0.13220657408237457\n",
      "Training Batch [222/782]: Loss 0.09623134136199951\n",
      "Training Batch [223/782]: Loss 0.12895260751247406\n",
      "Training Batch [224/782]: Loss 0.07937964797019958\n",
      "Training Batch [225/782]: Loss 0.11807308346033096\n",
      "Training Batch [226/782]: Loss 0.09539978206157684\n",
      "Training Batch [227/782]: Loss 0.062434740364551544\n",
      "Training Batch [228/782]: Loss 0.09307312220335007\n",
      "Training Batch [229/782]: Loss 0.12161372601985931\n",
      "Training Batch [230/782]: Loss 0.2289418876171112\n",
      "Training Batch [231/782]: Loss 0.09688424319028854\n",
      "Training Batch [232/782]: Loss 0.08688832074403763\n",
      "Training Batch [233/782]: Loss 0.07347695529460907\n",
      "Training Batch [234/782]: Loss 0.09535183757543564\n",
      "Training Batch [235/782]: Loss 0.06490582227706909\n",
      "Training Batch [236/782]: Loss 0.07598961889743805\n",
      "Training Batch [237/782]: Loss 0.09336031973361969\n",
      "Training Batch [238/782]: Loss 0.16001412272453308\n",
      "Training Batch [239/782]: Loss 0.14023618400096893\n",
      "Training Batch [240/782]: Loss 0.20122867822647095\n",
      "Training Batch [241/782]: Loss 0.10576675832271576\n",
      "Training Batch [242/782]: Loss 0.0731230229139328\n",
      "Training Batch [243/782]: Loss 0.06772807240486145\n",
      "Training Batch [244/782]: Loss 0.14957711100578308\n",
      "Training Batch [245/782]: Loss 0.14438605308532715\n",
      "Training Batch [246/782]: Loss 0.0634286031126976\n",
      "Training Batch [247/782]: Loss 0.20774805545806885\n",
      "Training Batch [248/782]: Loss 0.09459926933050156\n",
      "Training Batch [249/782]: Loss 0.12152659893035889\n",
      "Training Batch [250/782]: Loss 0.0917142927646637\n",
      "Training Batch [251/782]: Loss 0.07063870877027512\n",
      "Training Batch [252/782]: Loss 0.12749162316322327\n",
      "Training Batch [253/782]: Loss 0.10657769441604614\n",
      "Training Batch [254/782]: Loss 0.09471742063760757\n",
      "Training Batch [255/782]: Loss 0.08130195736885071\n",
      "Training Batch [256/782]: Loss 0.06543802469968796\n",
      "Training Batch [257/782]: Loss 0.07400701940059662\n",
      "Training Batch [258/782]: Loss 0.07032982259988785\n",
      "Training Batch [259/782]: Loss 0.07348085194826126\n",
      "Training Batch [260/782]: Loss 0.073358453810215\n",
      "Training Batch [261/782]: Loss 0.05584685504436493\n",
      "Training Batch [262/782]: Loss 0.17463327944278717\n",
      "Training Batch [263/782]: Loss 0.14156286418437958\n",
      "Training Batch [264/782]: Loss 0.06741972267627716\n",
      "Training Batch [265/782]: Loss 0.15468639135360718\n",
      "Training Batch [266/782]: Loss 0.0760578066110611\n",
      "Training Batch [267/782]: Loss 0.16548116505146027\n",
      "Training Batch [268/782]: Loss 0.10169138014316559\n",
      "Training Batch [269/782]: Loss 0.1939847767353058\n",
      "Training Batch [270/782]: Loss 0.1853565275669098\n",
      "Training Batch [271/782]: Loss 0.11652158200740814\n",
      "Training Batch [272/782]: Loss 0.20365270972251892\n",
      "Training Batch [273/782]: Loss 0.16982696950435638\n",
      "Training Batch [274/782]: Loss 0.09964395314455032\n",
      "Training Batch [275/782]: Loss 0.09203808009624481\n",
      "Training Batch [276/782]: Loss 0.19958101212978363\n",
      "Training Batch [277/782]: Loss 0.05466136708855629\n",
      "Training Batch [278/782]: Loss 0.07422758638858795\n",
      "Training Batch [279/782]: Loss 0.08052173256874084\n",
      "Training Batch [280/782]: Loss 0.1276451200246811\n",
      "Training Batch [281/782]: Loss 0.0688505619764328\n",
      "Training Batch [282/782]: Loss 0.1398301124572754\n",
      "Training Batch [283/782]: Loss 0.1315087229013443\n",
      "Training Batch [284/782]: Loss 0.11205508559942245\n",
      "Training Batch [285/782]: Loss 0.17353904247283936\n",
      "Training Batch [286/782]: Loss 0.17089441418647766\n",
      "Training Batch [287/782]: Loss 0.21874290704727173\n",
      "Training Batch [288/782]: Loss 0.08201032131910324\n",
      "Training Batch [289/782]: Loss 0.1686064749956131\n",
      "Training Batch [290/782]: Loss 0.11494342982769012\n",
      "Training Batch [291/782]: Loss 0.13646385073661804\n",
      "Training Batch [292/782]: Loss 0.08946336805820465\n",
      "Training Batch [293/782]: Loss 0.14413270354270935\n",
      "Training Batch [294/782]: Loss 0.18858394026756287\n",
      "Training Batch [295/782]: Loss 0.2118118554353714\n",
      "Training Batch [296/782]: Loss 0.0829869881272316\n",
      "Training Batch [297/782]: Loss 0.12430840730667114\n",
      "Training Batch [298/782]: Loss 0.11174117028713226\n",
      "Training Batch [299/782]: Loss 0.14462873339653015\n",
      "Training Batch [300/782]: Loss 0.04836365953087807\n",
      "Training Batch [301/782]: Loss 0.1268003284931183\n",
      "Training Batch [302/782]: Loss 0.1571441888809204\n",
      "Training Batch [303/782]: Loss 0.1542910784482956\n",
      "Training Batch [304/782]: Loss 0.1951364278793335\n",
      "Training Batch [305/782]: Loss 0.21009615063667297\n",
      "Training Batch [306/782]: Loss 0.17322103679180145\n",
      "Training Batch [307/782]: Loss 0.11559267342090607\n",
      "Training Batch [308/782]: Loss 0.07549377530813217\n",
      "Training Batch [309/782]: Loss 0.13776795566082\n",
      "Training Batch [310/782]: Loss 0.07306373864412308\n",
      "Training Batch [311/782]: Loss 0.15574169158935547\n",
      "Training Batch [312/782]: Loss 0.2087264508008957\n",
      "Training Batch [313/782]: Loss 0.16618356108665466\n",
      "Training Batch [314/782]: Loss 0.08195023983716965\n",
      "Training Batch [315/782]: Loss 0.0743441954255104\n",
      "Training Batch [316/782]: Loss 0.11725477129220963\n",
      "Training Batch [317/782]: Loss 0.07055694609880447\n",
      "Training Batch [318/782]: Loss 0.074185311794281\n",
      "Training Batch [319/782]: Loss 0.08491399884223938\n",
      "Training Batch [320/782]: Loss 0.22225287556648254\n",
      "Training Batch [321/782]: Loss 0.12425549328327179\n",
      "Training Batch [322/782]: Loss 0.052947141230106354\n",
      "Training Batch [323/782]: Loss 0.08110359311103821\n",
      "Training Batch [324/782]: Loss 0.1410013735294342\n",
      "Training Batch [325/782]: Loss 0.18406477570533752\n",
      "Training Batch [326/782]: Loss 0.1250142902135849\n",
      "Training Batch [327/782]: Loss 0.20584498345851898\n",
      "Training Batch [328/782]: Loss 0.05919560045003891\n",
      "Training Batch [329/782]: Loss 0.08847158402204514\n",
      "Training Batch [330/782]: Loss 0.20978102087974548\n",
      "Training Batch [331/782]: Loss 0.057583555579185486\n",
      "Training Batch [332/782]: Loss 0.1406790018081665\n",
      "Training Batch [333/782]: Loss 0.08899759501218796\n",
      "Training Batch [334/782]: Loss 0.10989192128181458\n",
      "Training Batch [335/782]: Loss 0.07954797148704529\n",
      "Training Batch [336/782]: Loss 0.23910364508628845\n",
      "Training Batch [337/782]: Loss 0.15827490389347076\n",
      "Training Batch [338/782]: Loss 0.30776044726371765\n",
      "Training Batch [339/782]: Loss 0.11015550792217255\n",
      "Training Batch [340/782]: Loss 0.2478826344013214\n",
      "Training Batch [341/782]: Loss 0.13649903237819672\n",
      "Training Batch [342/782]: Loss 0.10486382991075516\n",
      "Training Batch [343/782]: Loss 0.19060859084129333\n",
      "Training Batch [344/782]: Loss 0.14030244946479797\n",
      "Training Batch [345/782]: Loss 0.321737140417099\n",
      "Training Batch [346/782]: Loss 0.16422909498214722\n",
      "Training Batch [347/782]: Loss 0.0685684010386467\n",
      "Training Batch [348/782]: Loss 0.1481519192457199\n",
      "Training Batch [349/782]: Loss 0.20155692100524902\n",
      "Training Batch [350/782]: Loss 0.08939416706562042\n",
      "Training Batch [351/782]: Loss 0.21394339203834534\n",
      "Training Batch [352/782]: Loss 0.07261437922716141\n",
      "Training Batch [353/782]: Loss 0.13565880060195923\n",
      "Training Batch [354/782]: Loss 0.1208469346165657\n",
      "Training Batch [355/782]: Loss 0.10936053842306137\n",
      "Training Batch [356/782]: Loss 0.08739474415779114\n",
      "Training Batch [357/782]: Loss 0.18297044932842255\n",
      "Training Batch [358/782]: Loss 0.1385463923215866\n",
      "Training Batch [359/782]: Loss 0.10092916339635849\n",
      "Training Batch [360/782]: Loss 0.07496576756238937\n",
      "Training Batch [361/782]: Loss 0.13302423059940338\n",
      "Training Batch [362/782]: Loss 0.15077762305736542\n",
      "Training Batch [363/782]: Loss 0.12789562344551086\n",
      "Training Batch [364/782]: Loss 0.13125234842300415\n",
      "Training Batch [365/782]: Loss 0.10188797861337662\n",
      "Training Batch [366/782]: Loss 0.09605658799409866\n",
      "Training Batch [367/782]: Loss 0.18377350270748138\n",
      "Training Batch [368/782]: Loss 0.05127023532986641\n",
      "Training Batch [369/782]: Loss 0.1660003364086151\n",
      "Training Batch [370/782]: Loss 0.1822064369916916\n",
      "Training Batch [371/782]: Loss 0.15454550087451935\n",
      "Training Batch [372/782]: Loss 0.12008752673864365\n",
      "Training Batch [373/782]: Loss 0.10812198370695114\n",
      "Training Batch [374/782]: Loss 0.10063087195158005\n",
      "Training Batch [375/782]: Loss 0.08348274230957031\n",
      "Training Batch [376/782]: Loss 0.09005221724510193\n",
      "Training Batch [377/782]: Loss 0.10593602061271667\n",
      "Training Batch [378/782]: Loss 0.15941458940505981\n",
      "Training Batch [379/782]: Loss 0.14311730861663818\n",
      "Training Batch [380/782]: Loss 0.23950500786304474\n",
      "Training Batch [381/782]: Loss 0.10945697128772736\n",
      "Training Batch [382/782]: Loss 0.09226389229297638\n",
      "Training Batch [383/782]: Loss 0.08589733392000198\n",
      "Training Batch [384/782]: Loss 0.16332721710205078\n",
      "Training Batch [385/782]: Loss 0.09881501644849777\n",
      "Training Batch [386/782]: Loss 0.1418626755475998\n",
      "Training Batch [387/782]: Loss 0.13490425050258636\n",
      "Training Batch [388/782]: Loss 0.10829531401395798\n",
      "Training Batch [389/782]: Loss 0.1963038444519043\n",
      "Training Batch [390/782]: Loss 0.20801401138305664\n",
      "Training Batch [391/782]: Loss 0.11342815309762955\n",
      "Training Batch [392/782]: Loss 0.04938816279172897\n",
      "Training Batch [393/782]: Loss 0.07439008355140686\n",
      "Training Batch [394/782]: Loss 0.12689729034900665\n",
      "Training Batch [395/782]: Loss 0.06479067355394363\n",
      "Training Batch [396/782]: Loss 0.08809144049882889\n",
      "Training Batch [397/782]: Loss 0.1294603943824768\n",
      "Training Batch [398/782]: Loss 0.14325524866580963\n",
      "Training Batch [399/782]: Loss 0.1941205859184265\n",
      "Training Batch [400/782]: Loss 0.09408446401357651\n",
      "Training Batch [401/782]: Loss 0.1090574637055397\n",
      "Training Batch [402/782]: Loss 0.14547666907310486\n",
      "Training Batch [403/782]: Loss 0.13381041586399078\n",
      "Training Batch [404/782]: Loss 0.14080998301506042\n",
      "Training Batch [405/782]: Loss 0.10836707800626755\n",
      "Training Batch [406/782]: Loss 0.09122386574745178\n",
      "Training Batch [407/782]: Loss 0.23741567134857178\n",
      "Training Batch [408/782]: Loss 0.05841388553380966\n",
      "Training Batch [409/782]: Loss 0.12067839503288269\n",
      "Training Batch [410/782]: Loss 0.21397803723812103\n",
      "Training Batch [411/782]: Loss 0.09647675603628159\n",
      "Training Batch [412/782]: Loss 0.15622961521148682\n",
      "Training Batch [413/782]: Loss 0.048139750957489014\n",
      "Training Batch [414/782]: Loss 0.11509430408477783\n",
      "Training Batch [415/782]: Loss 0.06721124798059464\n",
      "Training Batch [416/782]: Loss 0.08716070652008057\n",
      "Training Batch [417/782]: Loss 0.1815967708826065\n",
      "Training Batch [418/782]: Loss 0.051280420273542404\n",
      "Training Batch [419/782]: Loss 0.14725583791732788\n",
      "Training Batch [420/782]: Loss 0.084053173661232\n",
      "Training Batch [421/782]: Loss 0.10020068287849426\n",
      "Training Batch [422/782]: Loss 0.3022661507129669\n",
      "Training Batch [423/782]: Loss 0.07574494928121567\n",
      "Training Batch [424/782]: Loss 0.12813085317611694\n",
      "Training Batch [425/782]: Loss 0.07395743578672409\n",
      "Training Batch [426/782]: Loss 0.2152029275894165\n",
      "Training Batch [427/782]: Loss 0.08120469748973846\n",
      "Training Batch [428/782]: Loss 0.06271066516637802\n",
      "Training Batch [429/782]: Loss 0.2441650778055191\n",
      "Training Batch [430/782]: Loss 0.08415434509515762\n",
      "Training Batch [431/782]: Loss 0.1720881313085556\n",
      "Training Batch [432/782]: Loss 0.13009525835514069\n",
      "Training Batch [433/782]: Loss 0.09165807068347931\n",
      "Training Batch [434/782]: Loss 0.19186781346797943\n",
      "Training Batch [435/782]: Loss 0.11826290935277939\n",
      "Training Batch [436/782]: Loss 0.11845478415489197\n",
      "Training Batch [437/782]: Loss 0.12017226219177246\n",
      "Training Batch [438/782]: Loss 0.05795278027653694\n",
      "Training Batch [439/782]: Loss 0.11885299533605576\n",
      "Training Batch [440/782]: Loss 0.1132560595870018\n",
      "Training Batch [441/782]: Loss 0.24230045080184937\n",
      "Training Batch [442/782]: Loss 0.11409201472997665\n",
      "Training Batch [443/782]: Loss 0.17206013202667236\n",
      "Training Batch [444/782]: Loss 0.0806187316775322\n",
      "Training Batch [445/782]: Loss 0.08707978576421738\n",
      "Training Batch [446/782]: Loss 0.13654401898384094\n",
      "Training Batch [447/782]: Loss 0.04889459162950516\n",
      "Training Batch [448/782]: Loss 0.1694069504737854\n",
      "Training Batch [449/782]: Loss 0.1729584038257599\n",
      "Training Batch [450/782]: Loss 0.16012504696846008\n",
      "Training Batch [451/782]: Loss 0.22399775683879852\n",
      "Training Batch [452/782]: Loss 0.21183940768241882\n",
      "Training Batch [453/782]: Loss 0.08080647140741348\n",
      "Training Batch [454/782]: Loss 0.09332109987735748\n",
      "Training Batch [455/782]: Loss 0.09044109284877777\n",
      "Training Batch [456/782]: Loss 0.16524384915828705\n",
      "Training Batch [457/782]: Loss 0.06414887309074402\n",
      "Training Batch [458/782]: Loss 0.07414016872644424\n",
      "Training Batch [459/782]: Loss 0.1971105933189392\n",
      "Training Batch [460/782]: Loss 0.1292901188135147\n",
      "Training Batch [461/782]: Loss 0.12799593806266785\n",
      "Training Batch [462/782]: Loss 0.259958952665329\n",
      "Training Batch [463/782]: Loss 0.14881104230880737\n",
      "Training Batch [464/782]: Loss 0.1419699788093567\n",
      "Training Batch [465/782]: Loss 0.12524883449077606\n",
      "Training Batch [466/782]: Loss 0.1983930915594101\n",
      "Training Batch [467/782]: Loss 0.11268658190965652\n",
      "Training Batch [468/782]: Loss 0.21647517383098602\n",
      "Training Batch [469/782]: Loss 0.06653687357902527\n",
      "Training Batch [470/782]: Loss 0.09247364103794098\n",
      "Training Batch [471/782]: Loss 0.09469738602638245\n",
      "Training Batch [472/782]: Loss 0.16062846779823303\n",
      "Training Batch [473/782]: Loss 0.14880487322807312\n",
      "Training Batch [474/782]: Loss 0.06501804292201996\n",
      "Training Batch [475/782]: Loss 0.15687482059001923\n",
      "Training Batch [476/782]: Loss 0.19833612442016602\n",
      "Training Batch [477/782]: Loss 0.11526037752628326\n",
      "Training Batch [478/782]: Loss 0.1286948025226593\n",
      "Training Batch [479/782]: Loss 0.073504239320755\n",
      "Training Batch [480/782]: Loss 0.1547798365354538\n",
      "Training Batch [481/782]: Loss 0.06770528107881546\n",
      "Training Batch [482/782]: Loss 0.17225052416324615\n",
      "Training Batch [483/782]: Loss 0.14684975147247314\n",
      "Training Batch [484/782]: Loss 0.14216497540473938\n",
      "Training Batch [485/782]: Loss 0.05195268988609314\n",
      "Training Batch [486/782]: Loss 0.08473405987024307\n",
      "Training Batch [487/782]: Loss 0.1590244472026825\n",
      "Training Batch [488/782]: Loss 0.1109769195318222\n",
      "Training Batch [489/782]: Loss 0.1063750758767128\n",
      "Training Batch [490/782]: Loss 0.09151609241962433\n",
      "Training Batch [491/782]: Loss 0.10230684280395508\n",
      "Training Batch [492/782]: Loss 0.14067243039608002\n",
      "Training Batch [493/782]: Loss 0.11703837662935257\n",
      "Training Batch [494/782]: Loss 0.09226974844932556\n",
      "Training Batch [495/782]: Loss 0.18387843668460846\n",
      "Training Batch [496/782]: Loss 0.1144043579697609\n",
      "Training Batch [497/782]: Loss 0.09620141983032227\n",
      "Training Batch [498/782]: Loss 0.14149107038974762\n",
      "Training Batch [499/782]: Loss 0.12868604063987732\n",
      "Training Batch [500/782]: Loss 0.15555955469608307\n",
      "Training Batch [501/782]: Loss 0.13038572669029236\n",
      "Training Batch [502/782]: Loss 0.0958670973777771\n",
      "Training Batch [503/782]: Loss 0.1132020428776741\n",
      "Training Batch [504/782]: Loss 0.08445490151643753\n",
      "Training Batch [505/782]: Loss 0.057447243481874466\n",
      "Training Batch [506/782]: Loss 0.14923393726348877\n",
      "Training Batch [507/782]: Loss 0.16747231781482697\n",
      "Training Batch [508/782]: Loss 0.08273051679134369\n",
      "Training Batch [509/782]: Loss 0.07330445200204849\n",
      "Training Batch [510/782]: Loss 0.07991787791252136\n",
      "Training Batch [511/782]: Loss 0.16086405515670776\n",
      "Training Batch [512/782]: Loss 0.16270087659358978\n",
      "Training Batch [513/782]: Loss 0.09950944036245346\n",
      "Training Batch [514/782]: Loss 0.06052282080054283\n",
      "Training Batch [515/782]: Loss 0.057510774582624435\n",
      "Training Batch [516/782]: Loss 0.21669353544712067\n",
      "Training Batch [517/782]: Loss 0.06653767824172974\n",
      "Training Batch [518/782]: Loss 0.1226402223110199\n",
      "Training Batch [519/782]: Loss 0.08318234980106354\n",
      "Training Batch [520/782]: Loss 0.2855788469314575\n",
      "Training Batch [521/782]: Loss 0.07987219095230103\n",
      "Training Batch [522/782]: Loss 0.16681821644306183\n",
      "Training Batch [523/782]: Loss 0.12500683963298798\n",
      "Training Batch [524/782]: Loss 0.12006319314241409\n",
      "Training Batch [525/782]: Loss 0.13334240019321442\n",
      "Training Batch [526/782]: Loss 0.13961239159107208\n",
      "Training Batch [527/782]: Loss 0.1135735958814621\n",
      "Training Batch [528/782]: Loss 0.16623632609844208\n",
      "Training Batch [529/782]: Loss 0.15448124706745148\n",
      "Training Batch [530/782]: Loss 0.11683616787195206\n",
      "Training Batch [531/782]: Loss 0.13336017727851868\n",
      "Training Batch [532/782]: Loss 0.09370840340852737\n",
      "Training Batch [533/782]: Loss 0.059455521404743195\n",
      "Training Batch [534/782]: Loss 0.15526323020458221\n",
      "Training Batch [535/782]: Loss 0.16375328600406647\n",
      "Training Batch [536/782]: Loss 0.15096482634544373\n",
      "Training Batch [537/782]: Loss 0.1557963788509369\n",
      "Training Batch [538/782]: Loss 0.10070707648992538\n",
      "Training Batch [539/782]: Loss 0.09622389823198318\n",
      "Training Batch [540/782]: Loss 0.14470379054546356\n",
      "Training Batch [541/782]: Loss 0.12253782153129578\n",
      "Training Batch [542/782]: Loss 0.2442343384027481\n",
      "Training Batch [543/782]: Loss 0.08407765626907349\n",
      "Training Batch [544/782]: Loss 0.13902492821216583\n",
      "Training Batch [545/782]: Loss 0.1611182689666748\n",
      "Training Batch [546/782]: Loss 0.17584899067878723\n",
      "Training Batch [547/782]: Loss 0.05685587227344513\n",
      "Training Batch [548/782]: Loss 0.22698929905891418\n",
      "Training Batch [549/782]: Loss 0.13292968273162842\n",
      "Training Batch [550/782]: Loss 0.056019917130470276\n",
      "Training Batch [551/782]: Loss 0.13902252912521362\n",
      "Training Batch [552/782]: Loss 0.1992359459400177\n",
      "Training Batch [553/782]: Loss 0.1268955022096634\n",
      "Training Batch [554/782]: Loss 0.15892809629440308\n",
      "Training Batch [555/782]: Loss 0.08424337208271027\n",
      "Training Batch [556/782]: Loss 0.15530696511268616\n",
      "Training Batch [557/782]: Loss 0.10252179205417633\n",
      "Training Batch [558/782]: Loss 0.1719818413257599\n",
      "Training Batch [559/782]: Loss 0.08442308753728867\n",
      "Training Batch [560/782]: Loss 0.10856327414512634\n",
      "Training Batch [561/782]: Loss 0.14380668103694916\n",
      "Training Batch [562/782]: Loss 0.12117470800876617\n",
      "Training Batch [563/782]: Loss 0.11428916454315186\n",
      "Training Batch [564/782]: Loss 0.050235897302627563\n",
      "Training Batch [565/782]: Loss 0.2550647258758545\n",
      "Training Batch [566/782]: Loss 0.17513200640678406\n",
      "Training Batch [567/782]: Loss 0.13181665539741516\n",
      "Training Batch [568/782]: Loss 0.13652154803276062\n",
      "Training Batch [569/782]: Loss 0.05787910521030426\n",
      "Training Batch [570/782]: Loss 0.055834535509347916\n",
      "Training Batch [571/782]: Loss 0.12725679576396942\n",
      "Training Batch [572/782]: Loss 0.06784781813621521\n",
      "Training Batch [573/782]: Loss 0.07579071074724197\n",
      "Training Batch [574/782]: Loss 0.15441864728927612\n",
      "Training Batch [575/782]: Loss 0.23305287957191467\n",
      "Training Batch [576/782]: Loss 0.23540306091308594\n",
      "Training Batch [577/782]: Loss 0.10222528874874115\n",
      "Training Batch [578/782]: Loss 0.09385879337787628\n",
      "Training Batch [579/782]: Loss 0.1388002336025238\n",
      "Training Batch [580/782]: Loss 0.06657787412405014\n",
      "Training Batch [581/782]: Loss 0.10147672146558762\n",
      "Training Batch [582/782]: Loss 0.10689929127693176\n",
      "Training Batch [583/782]: Loss 0.23581336438655853\n",
      "Training Batch [584/782]: Loss 0.1040041446685791\n",
      "Training Batch [585/782]: Loss 0.14156179130077362\n",
      "Training Batch [586/782]: Loss 0.3161896765232086\n",
      "Training Batch [587/782]: Loss 0.15478578209877014\n",
      "Training Batch [588/782]: Loss 0.12694813311100006\n",
      "Training Batch [589/782]: Loss 0.18016879260540009\n",
      "Training Batch [590/782]: Loss 0.13722801208496094\n",
      "Training Batch [591/782]: Loss 0.35834434628486633\n",
      "Training Batch [592/782]: Loss 0.15364320576190948\n",
      "Training Batch [593/782]: Loss 0.18729659914970398\n",
      "Training Batch [594/782]: Loss 0.2359454333782196\n",
      "Training Batch [595/782]: Loss 0.07740186154842377\n",
      "Training Batch [596/782]: Loss 0.3107955753803253\n",
      "Training Batch [597/782]: Loss 0.06939496845006943\n",
      "Training Batch [598/782]: Loss 0.17520226538181305\n",
      "Training Batch [599/782]: Loss 0.09225195646286011\n",
      "Training Batch [600/782]: Loss 0.17013026773929596\n",
      "Training Batch [601/782]: Loss 0.06277855485677719\n",
      "Training Batch [602/782]: Loss 0.23232784867286682\n",
      "Training Batch [603/782]: Loss 0.19289670884609222\n",
      "Training Batch [604/782]: Loss 0.23915995657444\n",
      "Training Batch [605/782]: Loss 0.1970163881778717\n",
      "Training Batch [606/782]: Loss 0.24842242896556854\n",
      "Training Batch [607/782]: Loss 0.25472375750541687\n",
      "Training Batch [608/782]: Loss 0.347067266702652\n",
      "Training Batch [609/782]: Loss 0.08966031670570374\n",
      "Training Batch [610/782]: Loss 0.11700817197561264\n",
      "Training Batch [611/782]: Loss 0.0738837718963623\n",
      "Training Batch [612/782]: Loss 0.1966865211725235\n",
      "Training Batch [613/782]: Loss 0.14514312148094177\n",
      "Training Batch [614/782]: Loss 0.13308052718639374\n",
      "Training Batch [615/782]: Loss 0.08576999604701996\n",
      "Training Batch [616/782]: Loss 0.17985351383686066\n",
      "Training Batch [617/782]: Loss 0.10132334381341934\n",
      "Training Batch [618/782]: Loss 0.23187246918678284\n",
      "Training Batch [619/782]: Loss 0.22461099922657013\n",
      "Training Batch [620/782]: Loss 0.13113076984882355\n",
      "Training Batch [621/782]: Loss 0.21670475602149963\n",
      "Training Batch [622/782]: Loss 0.21588626503944397\n",
      "Training Batch [623/782]: Loss 0.1400413066148758\n",
      "Training Batch [624/782]: Loss 0.25395187735557556\n",
      "Training Batch [625/782]: Loss 0.3628792464733124\n",
      "Training Batch [626/782]: Loss 0.16530741751194\n",
      "Training Batch [627/782]: Loss 0.20216330885887146\n",
      "Training Batch [628/782]: Loss 0.08890631049871445\n",
      "Training Batch [629/782]: Loss 0.10669679939746857\n",
      "Training Batch [630/782]: Loss 0.1678951233625412\n",
      "Training Batch [631/782]: Loss 0.1633816510438919\n",
      "Training Batch [632/782]: Loss 0.19625763595104218\n",
      "Training Batch [633/782]: Loss 0.2520791292190552\n",
      "Training Batch [634/782]: Loss 0.24956759810447693\n",
      "Training Batch [635/782]: Loss 0.2983298897743225\n",
      "Training Batch [636/782]: Loss 0.1554383486509323\n",
      "Training Batch [637/782]: Loss 0.2062106728553772\n",
      "Training Batch [638/782]: Loss 0.16672420501708984\n",
      "Training Batch [639/782]: Loss 0.11270780861377716\n",
      "Training Batch [640/782]: Loss 0.23864874243736267\n",
      "Training Batch [641/782]: Loss 0.18969930708408356\n",
      "Training Batch [642/782]: Loss 0.21007175743579865\n",
      "Training Batch [643/782]: Loss 0.1983540654182434\n",
      "Training Batch [644/782]: Loss 0.17113633453845978\n",
      "Training Batch [645/782]: Loss 0.09460607171058655\n",
      "Training Batch [646/782]: Loss 0.05139262601733208\n",
      "Training Batch [647/782]: Loss 0.24891138076782227\n",
      "Training Batch [648/782]: Loss 0.19316744804382324\n",
      "Training Batch [649/782]: Loss 0.1301189512014389\n",
      "Training Batch [650/782]: Loss 0.09392204135656357\n",
      "Training Batch [651/782]: Loss 0.14029835164546967\n",
      "Training Batch [652/782]: Loss 0.07008787244558334\n",
      "Training Batch [653/782]: Loss 0.16188190877437592\n",
      "Training Batch [654/782]: Loss 0.10802336782217026\n",
      "Training Batch [655/782]: Loss 0.18795369565486908\n",
      "Training Batch [656/782]: Loss 0.1376078575849533\n",
      "Training Batch [657/782]: Loss 0.11123314499855042\n",
      "Training Batch [658/782]: Loss 0.23726581037044525\n",
      "Training Batch [659/782]: Loss 0.19191937148571014\n",
      "Training Batch [660/782]: Loss 0.19690553843975067\n",
      "Training Batch [661/782]: Loss 0.06505799293518066\n",
      "Training Batch [662/782]: Loss 0.1603471338748932\n",
      "Training Batch [663/782]: Loss 0.2081436663866043\n",
      "Training Batch [664/782]: Loss 0.09261056780815125\n",
      "Training Batch [665/782]: Loss 0.18827509880065918\n",
      "Training Batch [666/782]: Loss 0.12886668741703033\n",
      "Training Batch [667/782]: Loss 0.11558164656162262\n",
      "Training Batch [668/782]: Loss 0.16902321577072144\n",
      "Training Batch [669/782]: Loss 0.17793160676956177\n",
      "Training Batch [670/782]: Loss 0.12958979606628418\n",
      "Training Batch [671/782]: Loss 0.08923496305942535\n",
      "Training Batch [672/782]: Loss 0.2189660221338272\n",
      "Training Batch [673/782]: Loss 0.185784712433815\n",
      "Training Batch [674/782]: Loss 0.2144038826227188\n",
      "Training Batch [675/782]: Loss 0.28850120306015015\n",
      "Training Batch [676/782]: Loss 0.23844438791275024\n",
      "Training Batch [677/782]: Loss 0.19207237660884857\n",
      "Training Batch [678/782]: Loss 0.1990332156419754\n",
      "Training Batch [679/782]: Loss 0.13242071866989136\n",
      "Training Batch [680/782]: Loss 0.09988628327846527\n",
      "Training Batch [681/782]: Loss 0.14209862053394318\n",
      "Training Batch [682/782]: Loss 0.22189639508724213\n",
      "Training Batch [683/782]: Loss 0.1108529269695282\n",
      "Training Batch [684/782]: Loss 0.07417827099561691\n",
      "Training Batch [685/782]: Loss 0.16973824799060822\n",
      "Training Batch [686/782]: Loss 0.3587799668312073\n",
      "Training Batch [687/782]: Loss 0.137308269739151\n",
      "Training Batch [688/782]: Loss 0.2065008282661438\n",
      "Training Batch [689/782]: Loss 0.1523810476064682\n",
      "Training Batch [690/782]: Loss 0.2620619535446167\n",
      "Training Batch [691/782]: Loss 0.2789687514305115\n",
      "Training Batch [692/782]: Loss 0.1808183491230011\n",
      "Training Batch [693/782]: Loss 0.14569111168384552\n",
      "Training Batch [694/782]: Loss 0.18919731676578522\n",
      "Training Batch [695/782]: Loss 0.3497827649116516\n",
      "Training Batch [696/782]: Loss 0.30605125427246094\n",
      "Training Batch [697/782]: Loss 0.16085578501224518\n",
      "Training Batch [698/782]: Loss 0.18471290171146393\n",
      "Training Batch [699/782]: Loss 0.17513471841812134\n",
      "Training Batch [700/782]: Loss 0.14466547966003418\n",
      "Training Batch [701/782]: Loss 0.08714205771684647\n",
      "Training Batch [702/782]: Loss 0.25110769271850586\n",
      "Training Batch [703/782]: Loss 0.1929374486207962\n",
      "Training Batch [704/782]: Loss 0.1810484528541565\n",
      "Training Batch [705/782]: Loss 0.13228820264339447\n",
      "Training Batch [706/782]: Loss 0.23346957564353943\n",
      "Training Batch [707/782]: Loss 0.1567951887845993\n",
      "Training Batch [708/782]: Loss 0.12285104393959045\n",
      "Training Batch [709/782]: Loss 0.20183949172496796\n",
      "Training Batch [710/782]: Loss 0.13498623669147491\n",
      "Training Batch [711/782]: Loss 0.1954527646303177\n",
      "Training Batch [712/782]: Loss 0.24760203063488007\n",
      "Training Batch [713/782]: Loss 0.13691768050193787\n",
      "Training Batch [714/782]: Loss 0.29402148723602295\n",
      "Training Batch [715/782]: Loss 0.17426776885986328\n",
      "Training Batch [716/782]: Loss 0.18331263959407806\n",
      "Training Batch [717/782]: Loss 0.11873194575309753\n",
      "Training Batch [718/782]: Loss 0.20306342840194702\n",
      "Training Batch [719/782]: Loss 0.1816156953573227\n",
      "Training Batch [720/782]: Loss 0.31236615777015686\n",
      "Training Batch [721/782]: Loss 0.22088810801506042\n",
      "Training Batch [722/782]: Loss 0.24771976470947266\n",
      "Training Batch [723/782]: Loss 0.17012636363506317\n",
      "Training Batch [724/782]: Loss 0.14032849669456482\n",
      "Training Batch [725/782]: Loss 0.20776695013046265\n",
      "Training Batch [726/782]: Loss 0.19204042851924896\n",
      "Training Batch [727/782]: Loss 0.21666750311851501\n",
      "Training Batch [728/782]: Loss 0.2633191645145416\n",
      "Training Batch [729/782]: Loss 0.13092301785945892\n",
      "Training Batch [730/782]: Loss 0.22668930888175964\n",
      "Training Batch [731/782]: Loss 0.17530909180641174\n",
      "Training Batch [732/782]: Loss 0.10198115557432175\n",
      "Training Batch [733/782]: Loss 0.24644853174686432\n",
      "Training Batch [734/782]: Loss 0.1780284345149994\n",
      "Training Batch [735/782]: Loss 0.20135438442230225\n",
      "Training Batch [736/782]: Loss 0.20832538604736328\n",
      "Training Batch [737/782]: Loss 0.3274584710597992\n",
      "Training Batch [738/782]: Loss 0.1471027433872223\n",
      "Training Batch [739/782]: Loss 0.24469734728336334\n",
      "Training Batch [740/782]: Loss 0.11121643334627151\n",
      "Training Batch [741/782]: Loss 0.17653125524520874\n",
      "Training Batch [742/782]: Loss 0.05156029760837555\n",
      "Training Batch [743/782]: Loss 0.1854977309703827\n",
      "Training Batch [744/782]: Loss 0.18549910187721252\n",
      "Training Batch [745/782]: Loss 0.08970357477664948\n",
      "Training Batch [746/782]: Loss 0.13273876905441284\n",
      "Training Batch [747/782]: Loss 0.20624738931655884\n",
      "Training Batch [748/782]: Loss 0.13941693305969238\n",
      "Training Batch [749/782]: Loss 0.3830128014087677\n",
      "Training Batch [750/782]: Loss 0.08157118409872055\n",
      "Training Batch [751/782]: Loss 0.08943875879049301\n",
      "Training Batch [752/782]: Loss 0.25030317902565\n",
      "Training Batch [753/782]: Loss 0.33340832591056824\n",
      "Training Batch [754/782]: Loss 0.11444154381752014\n",
      "Training Batch [755/782]: Loss 0.4193040728569031\n",
      "Training Batch [756/782]: Loss 0.2328236699104309\n",
      "Training Batch [757/782]: Loss 0.16133436560630798\n",
      "Training Batch [758/782]: Loss 0.09853625297546387\n",
      "Training Batch [759/782]: Loss 0.09688819944858551\n",
      "Training Batch [760/782]: Loss 0.27063679695129395\n",
      "Training Batch [761/782]: Loss 0.17291903495788574\n",
      "Training Batch [762/782]: Loss 0.23855435848236084\n",
      "Training Batch [763/782]: Loss 0.156916543841362\n",
      "Training Batch [764/782]: Loss 0.2208343744277954\n",
      "Training Batch [765/782]: Loss 0.13239139318466187\n",
      "Training Batch [766/782]: Loss 0.1587640792131424\n",
      "Training Batch [767/782]: Loss 0.21934790909290314\n",
      "Training Batch [768/782]: Loss 0.29140546917915344\n",
      "Training Batch [769/782]: Loss 0.19120675325393677\n",
      "Training Batch [770/782]: Loss 0.16515667736530304\n",
      "Training Batch [771/782]: Loss 0.1897001564502716\n",
      "Training Batch [772/782]: Loss 0.24785083532333374\n",
      "Training Batch [773/782]: Loss 0.11117365211248398\n",
      "Training Batch [774/782]: Loss 0.14696064591407776\n",
      "Training Batch [775/782]: Loss 0.11228589713573456\n",
      "Training Batch [776/782]: Loss 0.20231641829013824\n",
      "Training Batch [777/782]: Loss 0.10823093354701996\n",
      "Training Batch [778/782]: Loss 0.20345468819141388\n",
      "Training Batch [779/782]: Loss 0.274417519569397\n",
      "Training Batch [780/782]: Loss 0.12924286723136902\n",
      "Training Batch [781/782]: Loss 0.148321270942688\n",
      "Training Batch [782/782]: Loss 0.13381755352020264\n",
      "Epoch 10 - Train Loss: 0.1431\n",
      "*********  Epoch 11/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.043554846197366714\n",
      "Training Batch [2/782]: Loss 0.030814532190561295\n",
      "Training Batch [3/782]: Loss 0.07860065996646881\n",
      "Training Batch [4/782]: Loss 0.18500584363937378\n",
      "Training Batch [5/782]: Loss 0.08874477446079254\n",
      "Training Batch [6/782]: Loss 0.08258090168237686\n",
      "Training Batch [7/782]: Loss 0.04602144658565521\n",
      "Training Batch [8/782]: Loss 0.0513836033642292\n",
      "Training Batch [9/782]: Loss 0.19135676324367523\n",
      "Training Batch [10/782]: Loss 0.13076087832450867\n",
      "Training Batch [11/782]: Loss 0.06902613490819931\n",
      "Training Batch [12/782]: Loss 0.08911758661270142\n",
      "Training Batch [13/782]: Loss 0.08191109448671341\n",
      "Training Batch [14/782]: Loss 0.1109248697757721\n",
      "Training Batch [15/782]: Loss 0.14542634785175323\n",
      "Training Batch [16/782]: Loss 0.14650900661945343\n",
      "Training Batch [17/782]: Loss 0.13903333246707916\n",
      "Training Batch [18/782]: Loss 0.05074549466371536\n",
      "Training Batch [19/782]: Loss 0.027035975828766823\n",
      "Training Batch [20/782]: Loss 0.07263390719890594\n",
      "Training Batch [21/782]: Loss 0.1416187584400177\n",
      "Training Batch [22/782]: Loss 0.08920398354530334\n",
      "Training Batch [23/782]: Loss 0.15395914018154144\n",
      "Training Batch [24/782]: Loss 0.10544128715991974\n",
      "Training Batch [25/782]: Loss 0.03314768150448799\n",
      "Training Batch [26/782]: Loss 0.014129729010164738\n",
      "Training Batch [27/782]: Loss 0.05403366684913635\n",
      "Training Batch [28/782]: Loss 0.14616602659225464\n",
      "Training Batch [29/782]: Loss 0.10013487935066223\n",
      "Training Batch [30/782]: Loss 0.15724527835845947\n",
      "Training Batch [31/782]: Loss 0.04421965032815933\n",
      "Training Batch [32/782]: Loss 0.07616899907588959\n",
      "Training Batch [33/782]: Loss 0.09682469069957733\n",
      "Training Batch [34/782]: Loss 0.09783727675676346\n",
      "Training Batch [35/782]: Loss 0.04986632242798805\n",
      "Training Batch [36/782]: Loss 0.04916355386376381\n",
      "Training Batch [37/782]: Loss 0.09111707657575607\n",
      "Training Batch [38/782]: Loss 0.1624915450811386\n",
      "Training Batch [39/782]: Loss 0.07480522245168686\n",
      "Training Batch [40/782]: Loss 0.18187011778354645\n",
      "Training Batch [41/782]: Loss 0.031619567424058914\n",
      "Training Batch [42/782]: Loss 0.06132061034440994\n",
      "Training Batch [43/782]: Loss 0.12192904949188232\n",
      "Training Batch [44/782]: Loss 0.037055857479572296\n",
      "Training Batch [45/782]: Loss 0.12026258558034897\n",
      "Training Batch [46/782]: Loss 0.0991344004869461\n",
      "Training Batch [47/782]: Loss 0.11803435534238815\n",
      "Training Batch [48/782]: Loss 0.08913833647966385\n",
      "Training Batch [49/782]: Loss 0.10784345865249634\n",
      "Training Batch [50/782]: Loss 0.17214703559875488\n",
      "Training Batch [51/782]: Loss 0.16407403349876404\n",
      "Training Batch [52/782]: Loss 0.10409602522850037\n",
      "Training Batch [53/782]: Loss 0.06913561373949051\n",
      "Training Batch [54/782]: Loss 0.03594525530934334\n",
      "Training Batch [55/782]: Loss 0.02217179909348488\n",
      "Training Batch [56/782]: Loss 0.02873379737138748\n",
      "Training Batch [57/782]: Loss 0.17167766392230988\n",
      "Training Batch [58/782]: Loss 0.05089687556028366\n",
      "Training Batch [59/782]: Loss 0.05000518634915352\n",
      "Training Batch [60/782]: Loss 0.09045964479446411\n",
      "Training Batch [61/782]: Loss 0.11037033796310425\n",
      "Training Batch [62/782]: Loss 0.11394631117582321\n",
      "Training Batch [63/782]: Loss 0.05294645577669144\n",
      "Training Batch [64/782]: Loss 0.06419778615236282\n",
      "Training Batch [65/782]: Loss 0.036976028233766556\n",
      "Training Batch [66/782]: Loss 0.0947398841381073\n",
      "Training Batch [67/782]: Loss 0.07867264002561569\n",
      "Training Batch [68/782]: Loss 0.051936183124780655\n",
      "Training Batch [69/782]: Loss 0.039136748760938644\n",
      "Training Batch [70/782]: Loss 0.16626732051372528\n",
      "Training Batch [71/782]: Loss 0.05244818702340126\n",
      "Training Batch [72/782]: Loss 0.0772298127412796\n",
      "Training Batch [73/782]: Loss 0.1260042041540146\n",
      "Training Batch [74/782]: Loss 0.07238998264074326\n",
      "Training Batch [75/782]: Loss 0.12704546749591827\n",
      "Training Batch [76/782]: Loss 0.07810516655445099\n",
      "Training Batch [77/782]: Loss 0.19840608537197113\n",
      "Training Batch [78/782]: Loss 0.04446617513895035\n",
      "Training Batch [79/782]: Loss 0.0766923651099205\n",
      "Training Batch [80/782]: Loss 0.0469537116587162\n",
      "Training Batch [81/782]: Loss 0.04583046957850456\n",
      "Training Batch [82/782]: Loss 0.04648670554161072\n",
      "Training Batch [83/782]: Loss 0.030452469363808632\n",
      "Training Batch [84/782]: Loss 0.14410024881362915\n",
      "Training Batch [85/782]: Loss 0.06838075071573257\n",
      "Training Batch [86/782]: Loss 0.031602296978235245\n",
      "Training Batch [87/782]: Loss 0.05790458247065544\n",
      "Training Batch [88/782]: Loss 0.04966675490140915\n",
      "Training Batch [89/782]: Loss 0.03583995997905731\n",
      "Training Batch [90/782]: Loss 0.053886886686086655\n",
      "Training Batch [91/782]: Loss 0.10994422435760498\n",
      "Training Batch [92/782]: Loss 0.08818963170051575\n",
      "Training Batch [93/782]: Loss 0.062190618366003036\n",
      "Training Batch [94/782]: Loss 0.0429961197078228\n",
      "Training Batch [95/782]: Loss 0.08675793558359146\n",
      "Training Batch [96/782]: Loss 0.02240539714694023\n",
      "Training Batch [97/782]: Loss 0.07886949926614761\n",
      "Training Batch [98/782]: Loss 0.04450175538659096\n",
      "Training Batch [99/782]: Loss 0.08972916007041931\n",
      "Training Batch [100/782]: Loss 0.050305500626564026\n",
      "Training Batch [101/782]: Loss 0.04763001576066017\n",
      "Training Batch [102/782]: Loss 0.05143103748559952\n",
      "Training Batch [103/782]: Loss 0.018154563382267952\n",
      "Training Batch [104/782]: Loss 0.03380585089325905\n",
      "Training Batch [105/782]: Loss 0.048565108329057693\n",
      "Training Batch [106/782]: Loss 0.05997025594115257\n",
      "Training Batch [107/782]: Loss 0.04981323704123497\n",
      "Training Batch [108/782]: Loss 0.07843400537967682\n",
      "Training Batch [109/782]: Loss 0.0703311562538147\n",
      "Training Batch [110/782]: Loss 0.0576762892305851\n",
      "Training Batch [111/782]: Loss 0.1053747683763504\n",
      "Training Batch [112/782]: Loss 0.06656817346811295\n",
      "Training Batch [113/782]: Loss 0.07669532299041748\n",
      "Training Batch [114/782]: Loss 0.13939979672431946\n",
      "Training Batch [115/782]: Loss 0.07275321334600449\n",
      "Training Batch [116/782]: Loss 0.02767978422343731\n",
      "Training Batch [117/782]: Loss 0.05765818804502487\n",
      "Training Batch [118/782]: Loss 0.07194990664720535\n",
      "Training Batch [119/782]: Loss 0.042787835001945496\n",
      "Training Batch [120/782]: Loss 0.040808238089084625\n",
      "Training Batch [121/782]: Loss 0.022075647488236427\n",
      "Training Batch [122/782]: Loss 0.013883362524211407\n",
      "Training Batch [123/782]: Loss 0.061942268162965775\n",
      "Training Batch [124/782]: Loss 0.05420176684856415\n",
      "Training Batch [125/782]: Loss 0.055390506982803345\n",
      "Training Batch [126/782]: Loss 0.02926524728536606\n",
      "Training Batch [127/782]: Loss 0.09014642238616943\n",
      "Training Batch [128/782]: Loss 0.024258915334939957\n",
      "Training Batch [129/782]: Loss 0.052866023033857346\n",
      "Training Batch [130/782]: Loss 0.08583857119083405\n",
      "Training Batch [131/782]: Loss 0.028134910389780998\n",
      "Training Batch [132/782]: Loss 0.017790205776691437\n",
      "Training Batch [133/782]: Loss 0.041291043162345886\n",
      "Training Batch [134/782]: Loss 0.009026647545397282\n",
      "Training Batch [135/782]: Loss 0.0645391121506691\n",
      "Training Batch [136/782]: Loss 0.07135797291994095\n",
      "Training Batch [137/782]: Loss 0.05293454974889755\n",
      "Training Batch [138/782]: Loss 0.04071608558297157\n",
      "Training Batch [139/782]: Loss 0.07420119643211365\n",
      "Training Batch [140/782]: Loss 0.176569402217865\n",
      "Training Batch [141/782]: Loss 0.040864426642656326\n",
      "Training Batch [142/782]: Loss 0.059120502322912216\n",
      "Training Batch [143/782]: Loss 0.04658712074160576\n",
      "Training Batch [144/782]: Loss 0.09128255397081375\n",
      "Training Batch [145/782]: Loss 0.018193697556853294\n",
      "Training Batch [146/782]: Loss 0.04141552746295929\n",
      "Training Batch [147/782]: Loss 0.06526767462491989\n",
      "Training Batch [148/782]: Loss 0.07300522178411484\n",
      "Training Batch [149/782]: Loss 0.03768586367368698\n",
      "Training Batch [150/782]: Loss 0.08034001290798187\n",
      "Training Batch [151/782]: Loss 0.02381480298936367\n",
      "Training Batch [152/782]: Loss 0.0694829672574997\n",
      "Training Batch [153/782]: Loss 0.10275939106941223\n",
      "Training Batch [154/782]: Loss 0.1695403754711151\n",
      "Training Batch [155/782]: Loss 0.0703873485326767\n",
      "Training Batch [156/782]: Loss 0.061835043132305145\n",
      "Training Batch [157/782]: Loss 0.10806868970394135\n",
      "Training Batch [158/782]: Loss 0.10874011367559433\n",
      "Training Batch [159/782]: Loss 0.029292823746800423\n",
      "Training Batch [160/782]: Loss 0.09315846860408783\n",
      "Training Batch [161/782]: Loss 0.05770886689424515\n",
      "Training Batch [162/782]: Loss 0.035522833466529846\n",
      "Training Batch [163/782]: Loss 0.08779725432395935\n",
      "Training Batch [164/782]: Loss 0.11448587477207184\n",
      "Training Batch [165/782]: Loss 0.14011922478675842\n",
      "Training Batch [166/782]: Loss 0.09437461197376251\n",
      "Training Batch [167/782]: Loss 0.0822024792432785\n",
      "Training Batch [168/782]: Loss 0.026713330298662186\n",
      "Training Batch [169/782]: Loss 0.06584426760673523\n",
      "Training Batch [170/782]: Loss 0.18949130177497864\n",
      "Training Batch [171/782]: Loss 0.04447814077138901\n",
      "Training Batch [172/782]: Loss 0.09658005088567734\n",
      "Training Batch [173/782]: Loss 0.013737183064222336\n",
      "Training Batch [174/782]: Loss 0.05358705669641495\n",
      "Training Batch [175/782]: Loss 0.08470992743968964\n",
      "Training Batch [176/782]: Loss 0.14821657538414001\n",
      "Training Batch [177/782]: Loss 0.07506274431943893\n",
      "Training Batch [178/782]: Loss 0.14686773717403412\n",
      "Training Batch [179/782]: Loss 0.05636703968048096\n",
      "Training Batch [180/782]: Loss 0.05509089305996895\n",
      "Training Batch [181/782]: Loss 0.037710875272750854\n",
      "Training Batch [182/782]: Loss 0.2121516615152359\n",
      "Training Batch [183/782]: Loss 0.06195395067334175\n",
      "Training Batch [184/782]: Loss 0.09461914002895355\n",
      "Training Batch [185/782]: Loss 0.11460022628307343\n",
      "Training Batch [186/782]: Loss 0.03265273943543434\n",
      "Training Batch [187/782]: Loss 0.09109701216220856\n",
      "Training Batch [188/782]: Loss 0.11315134167671204\n",
      "Training Batch [189/782]: Loss 0.12325496971607208\n",
      "Training Batch [190/782]: Loss 0.03611680120229721\n",
      "Training Batch [191/782]: Loss 0.06609047949314117\n",
      "Training Batch [192/782]: Loss 0.056810516864061356\n",
      "Training Batch [193/782]: Loss 0.06270791590213776\n",
      "Training Batch [194/782]: Loss 0.03675651177763939\n",
      "Training Batch [195/782]: Loss 0.07365007698535919\n",
      "Training Batch [196/782]: Loss 0.0867028459906578\n",
      "Training Batch [197/782]: Loss 0.0628456175327301\n",
      "Training Batch [198/782]: Loss 0.044194117188453674\n",
      "Training Batch [199/782]: Loss 0.034776173532009125\n",
      "Training Batch [200/782]: Loss 0.09463097900152206\n",
      "Training Batch [201/782]: Loss 0.11680494248867035\n",
      "Training Batch [202/782]: Loss 0.034511156380176544\n",
      "Training Batch [203/782]: Loss 0.10370317846536636\n",
      "Training Batch [204/782]: Loss 0.07100095599889755\n",
      "Training Batch [205/782]: Loss 0.09995146840810776\n",
      "Training Batch [206/782]: Loss 0.09445697069168091\n",
      "Training Batch [207/782]: Loss 0.13046589493751526\n",
      "Training Batch [208/782]: Loss 0.10386263579130173\n",
      "Training Batch [209/782]: Loss 0.02828257530927658\n",
      "Training Batch [210/782]: Loss 0.03150060772895813\n",
      "Training Batch [211/782]: Loss 0.025971518829464912\n",
      "Training Batch [212/782]: Loss 0.022570883855223656\n",
      "Training Batch [213/782]: Loss 0.05408478528261185\n",
      "Training Batch [214/782]: Loss 0.08352242410182953\n",
      "Training Batch [215/782]: Loss 0.05482559651136398\n",
      "Training Batch [216/782]: Loss 0.08811870962381363\n",
      "Training Batch [217/782]: Loss 0.09941388666629791\n",
      "Training Batch [218/782]: Loss 0.052646756172180176\n",
      "Training Batch [219/782]: Loss 0.07175810635089874\n",
      "Training Batch [220/782]: Loss 0.0916563868522644\n",
      "Training Batch [221/782]: Loss 0.017561696469783783\n",
      "Training Batch [222/782]: Loss 0.05323304980993271\n",
      "Training Batch [223/782]: Loss 0.058657918125391006\n",
      "Training Batch [224/782]: Loss 0.08442961424589157\n",
      "Training Batch [225/782]: Loss 0.12766602635383606\n",
      "Training Batch [226/782]: Loss 0.024118101224303246\n",
      "Training Batch [227/782]: Loss 0.07103925198316574\n",
      "Training Batch [228/782]: Loss 0.03431029990315437\n",
      "Training Batch [229/782]: Loss 0.08028627932071686\n",
      "Training Batch [230/782]: Loss 0.14379511773586273\n",
      "Training Batch [231/782]: Loss 0.062144748866558075\n",
      "Training Batch [232/782]: Loss 0.08008606731891632\n",
      "Training Batch [233/782]: Loss 0.06714148074388504\n",
      "Training Batch [234/782]: Loss 0.05353190749883652\n",
      "Training Batch [235/782]: Loss 0.09634890407323837\n",
      "Training Batch [236/782]: Loss 0.0495428591966629\n",
      "Training Batch [237/782]: Loss 0.09060279279947281\n",
      "Training Batch [238/782]: Loss 0.021198077127337456\n",
      "Training Batch [239/782]: Loss 0.025870274752378464\n",
      "Training Batch [240/782]: Loss 0.10201523452997208\n",
      "Training Batch [241/782]: Loss 0.1254652589559555\n",
      "Training Batch [242/782]: Loss 0.04346903786063194\n",
      "Training Batch [243/782]: Loss 0.052458759397268295\n",
      "Training Batch [244/782]: Loss 0.05374063178896904\n",
      "Training Batch [245/782]: Loss 0.13495832681655884\n",
      "Training Batch [246/782]: Loss 0.05335640534758568\n",
      "Training Batch [247/782]: Loss 0.08532839268445969\n",
      "Training Batch [248/782]: Loss 0.10284874588251114\n",
      "Training Batch [249/782]: Loss 0.11098313331604004\n",
      "Training Batch [250/782]: Loss 0.2611564099788666\n",
      "Training Batch [251/782]: Loss 0.04703424870967865\n",
      "Training Batch [252/782]: Loss 0.1041763573884964\n",
      "Training Batch [253/782]: Loss 0.05142108350992203\n",
      "Training Batch [254/782]: Loss 0.0455053336918354\n",
      "Training Batch [255/782]: Loss 0.0429018959403038\n",
      "Training Batch [256/782]: Loss 0.03331736475229263\n",
      "Training Batch [257/782]: Loss 0.040372639894485474\n",
      "Training Batch [258/782]: Loss 0.038721855729818344\n",
      "Training Batch [259/782]: Loss 0.05549108237028122\n",
      "Training Batch [260/782]: Loss 0.0665922462940216\n",
      "Training Batch [261/782]: Loss 0.06854749470949173\n",
      "Training Batch [262/782]: Loss 0.1392984241247177\n",
      "Training Batch [263/782]: Loss 0.02620733715593815\n",
      "Training Batch [264/782]: Loss 0.028070839121937752\n",
      "Training Batch [265/782]: Loss 0.07667699456214905\n",
      "Training Batch [266/782]: Loss 0.12979695200920105\n",
      "Training Batch [267/782]: Loss 0.08353381603956223\n",
      "Training Batch [268/782]: Loss 0.1255197525024414\n",
      "Training Batch [269/782]: Loss 0.1539565920829773\n",
      "Training Batch [270/782]: Loss 0.0859452560544014\n",
      "Training Batch [271/782]: Loss 0.0689370334148407\n",
      "Training Batch [272/782]: Loss 0.07983256131410599\n",
      "Training Batch [273/782]: Loss 0.05523533746600151\n",
      "Training Batch [274/782]: Loss 0.08474647998809814\n",
      "Training Batch [275/782]: Loss 0.06438206136226654\n",
      "Training Batch [276/782]: Loss 0.1206626445055008\n",
      "Training Batch [277/782]: Loss 0.12764842808246613\n",
      "Training Batch [278/782]: Loss 0.05566643923521042\n",
      "Training Batch [279/782]: Loss 0.05994899570941925\n",
      "Training Batch [280/782]: Loss 0.16946083307266235\n",
      "Training Batch [281/782]: Loss 0.14309033751487732\n",
      "Training Batch [282/782]: Loss 0.14930202066898346\n",
      "Training Batch [283/782]: Loss 0.06669045984745026\n",
      "Training Batch [284/782]: Loss 0.02046641893684864\n",
      "Training Batch [285/782]: Loss 0.05077105760574341\n",
      "Training Batch [286/782]: Loss 0.07336670160293579\n",
      "Training Batch [287/782]: Loss 0.1175987496972084\n",
      "Training Batch [288/782]: Loss 0.0809984877705574\n",
      "Training Batch [289/782]: Loss 0.12976421415805817\n",
      "Training Batch [290/782]: Loss 0.12693867087364197\n",
      "Training Batch [291/782]: Loss 0.12039642781019211\n",
      "Training Batch [292/782]: Loss 0.08183930069208145\n",
      "Training Batch [293/782]: Loss 0.14498886466026306\n",
      "Training Batch [294/782]: Loss 0.12413644790649414\n",
      "Training Batch [295/782]: Loss 0.05147548019886017\n",
      "Training Batch [296/782]: Loss 0.08973078429698944\n",
      "Training Batch [297/782]: Loss 0.04667995870113373\n",
      "Training Batch [298/782]: Loss 0.14427229762077332\n",
      "Training Batch [299/782]: Loss 0.13870885968208313\n",
      "Training Batch [300/782]: Loss 0.08321326225996017\n",
      "Training Batch [301/782]: Loss 0.18981195986270905\n",
      "Training Batch [302/782]: Loss 0.11131913959980011\n",
      "Training Batch [303/782]: Loss 0.06390797346830368\n",
      "Training Batch [304/782]: Loss 0.06793063133955002\n",
      "Training Batch [305/782]: Loss 0.146083801984787\n",
      "Training Batch [306/782]: Loss 0.08768097311258316\n",
      "Training Batch [307/782]: Loss 0.04615186154842377\n",
      "Training Batch [308/782]: Loss 0.05067456513643265\n",
      "Training Batch [309/782]: Loss 0.06138380244374275\n",
      "Training Batch [310/782]: Loss 0.05943991616368294\n",
      "Training Batch [311/782]: Loss 0.01607651077210903\n",
      "Training Batch [312/782]: Loss 0.031677037477493286\n",
      "Training Batch [313/782]: Loss 0.11227051168680191\n",
      "Training Batch [314/782]: Loss 0.08213964104652405\n",
      "Training Batch [315/782]: Loss 0.17645204067230225\n",
      "Training Batch [316/782]: Loss 0.03033141791820526\n",
      "Training Batch [317/782]: Loss 0.09091614931821823\n",
      "Training Batch [318/782]: Loss 0.16888365149497986\n",
      "Training Batch [319/782]: Loss 0.04951062053442001\n",
      "Training Batch [320/782]: Loss 0.07318855822086334\n",
      "Training Batch [321/782]: Loss 0.09186113625764847\n",
      "Training Batch [322/782]: Loss 0.06174171343445778\n",
      "Training Batch [323/782]: Loss 0.07325665652751923\n",
      "Training Batch [324/782]: Loss 0.09448127448558807\n",
      "Training Batch [325/782]: Loss 0.10226500034332275\n",
      "Training Batch [326/782]: Loss 0.04166993498802185\n",
      "Training Batch [327/782]: Loss 0.04875452071428299\n",
      "Training Batch [328/782]: Loss 0.06860439479351044\n",
      "Training Batch [329/782]: Loss 0.08157604932785034\n",
      "Training Batch [330/782]: Loss 0.20053544640541077\n",
      "Training Batch [331/782]: Loss 0.20569473505020142\n",
      "Training Batch [332/782]: Loss 0.08898025751113892\n",
      "Training Batch [333/782]: Loss 0.13289768993854523\n",
      "Training Batch [334/782]: Loss 0.07272553443908691\n",
      "Training Batch [335/782]: Loss 0.08182711154222488\n",
      "Training Batch [336/782]: Loss 0.07053027302026749\n",
      "Training Batch [337/782]: Loss 0.09957416355609894\n",
      "Training Batch [338/782]: Loss 0.12767015397548676\n",
      "Training Batch [339/782]: Loss 0.10854863375425339\n",
      "Training Batch [340/782]: Loss 0.12112106382846832\n",
      "Training Batch [341/782]: Loss 0.19622769951820374\n",
      "Training Batch [342/782]: Loss 0.042124539613723755\n",
      "Training Batch [343/782]: Loss 0.13197222352027893\n",
      "Training Batch [344/782]: Loss 0.0968666672706604\n",
      "Training Batch [345/782]: Loss 0.059422045946121216\n",
      "Training Batch [346/782]: Loss 0.12192589789628983\n",
      "Training Batch [347/782]: Loss 0.07554306834936142\n",
      "Training Batch [348/782]: Loss 0.028556067496538162\n",
      "Training Batch [349/782]: Loss 0.04090391471982002\n",
      "Training Batch [350/782]: Loss 0.12697719037532806\n",
      "Training Batch [351/782]: Loss 0.17009848356246948\n",
      "Training Batch [352/782]: Loss 0.04099777713418007\n",
      "Training Batch [353/782]: Loss 0.0627264678478241\n",
      "Training Batch [354/782]: Loss 0.127029150724411\n",
      "Training Batch [355/782]: Loss 0.1381325125694275\n",
      "Training Batch [356/782]: Loss 0.10772489756345749\n",
      "Training Batch [357/782]: Loss 0.1259871870279312\n",
      "Training Batch [358/782]: Loss 0.06939554959535599\n",
      "Training Batch [359/782]: Loss 0.21614113450050354\n",
      "Training Batch [360/782]: Loss 0.24111953377723694\n",
      "Training Batch [361/782]: Loss 0.1773194670677185\n",
      "Training Batch [362/782]: Loss 0.1183590516448021\n",
      "Training Batch [363/782]: Loss 0.1549198031425476\n",
      "Training Batch [364/782]: Loss 0.1348922699689865\n",
      "Training Batch [365/782]: Loss 0.15753093361854553\n",
      "Training Batch [366/782]: Loss 0.2023383229970932\n",
      "Training Batch [367/782]: Loss 0.13295215368270874\n",
      "Training Batch [368/782]: Loss 0.13145087659358978\n",
      "Training Batch [369/782]: Loss 0.06499525159597397\n",
      "Training Batch [370/782]: Loss 0.06857699900865555\n",
      "Training Batch [371/782]: Loss 0.09833316504955292\n",
      "Training Batch [372/782]: Loss 0.1687326282262802\n",
      "Training Batch [373/782]: Loss 0.12356637418270111\n",
      "Training Batch [374/782]: Loss 0.2633567452430725\n",
      "Training Batch [375/782]: Loss 0.1181892603635788\n",
      "Training Batch [376/782]: Loss 0.06445567309856415\n",
      "Training Batch [377/782]: Loss 0.1483931690454483\n",
      "Training Batch [378/782]: Loss 0.18558819591999054\n",
      "Training Batch [379/782]: Loss 0.1473686844110489\n",
      "Training Batch [380/782]: Loss 0.050951533019542694\n",
      "Training Batch [381/782]: Loss 0.0829148069024086\n",
      "Training Batch [382/782]: Loss 0.14791999757289886\n",
      "Training Batch [383/782]: Loss 0.1407403200864792\n",
      "Training Batch [384/782]: Loss 0.09624620527029037\n",
      "Training Batch [385/782]: Loss 0.06296750903129578\n",
      "Training Batch [386/782]: Loss 0.13948103785514832\n",
      "Training Batch [387/782]: Loss 0.1404748558998108\n",
      "Training Batch [388/782]: Loss 0.07256215810775757\n",
      "Training Batch [389/782]: Loss 0.14968034625053406\n",
      "Training Batch [390/782]: Loss 0.1859414428472519\n",
      "Training Batch [391/782]: Loss 0.10491479188203812\n",
      "Training Batch [392/782]: Loss 0.2010781168937683\n",
      "Training Batch [393/782]: Loss 0.04372558742761612\n",
      "Training Batch [394/782]: Loss 0.183747798204422\n",
      "Training Batch [395/782]: Loss 0.13076560199260712\n",
      "Training Batch [396/782]: Loss 0.07526883482933044\n",
      "Training Batch [397/782]: Loss 0.10721075534820557\n",
      "Training Batch [398/782]: Loss 0.12366906553506851\n",
      "Training Batch [399/782]: Loss 0.11937113106250763\n",
      "Training Batch [400/782]: Loss 0.11548050493001938\n",
      "Training Batch [401/782]: Loss 0.047541022300720215\n",
      "Training Batch [402/782]: Loss 0.13730615377426147\n",
      "Training Batch [403/782]: Loss 0.3054119050502777\n",
      "Training Batch [404/782]: Loss 0.28588923811912537\n",
      "Training Batch [405/782]: Loss 0.16266991198062897\n",
      "Training Batch [406/782]: Loss 0.05125487223267555\n",
      "Training Batch [407/782]: Loss 0.22661466896533966\n",
      "Training Batch [408/782]: Loss 0.07571910321712494\n",
      "Training Batch [409/782]: Loss 0.103045254945755\n",
      "Training Batch [410/782]: Loss 0.1365094631910324\n",
      "Training Batch [411/782]: Loss 0.1184639036655426\n",
      "Training Batch [412/782]: Loss 0.09632137417793274\n",
      "Training Batch [413/782]: Loss 0.14416125416755676\n",
      "Training Batch [414/782]: Loss 0.15634675323963165\n",
      "Training Batch [415/782]: Loss 0.13463060557842255\n",
      "Training Batch [416/782]: Loss 0.19007980823516846\n",
      "Training Batch [417/782]: Loss 0.19641262292861938\n",
      "Training Batch [418/782]: Loss 0.06534748524427414\n",
      "Training Batch [419/782]: Loss 0.06977827101945877\n",
      "Training Batch [420/782]: Loss 0.2495540976524353\n",
      "Training Batch [421/782]: Loss 0.1346476525068283\n",
      "Training Batch [422/782]: Loss 0.1397378146648407\n",
      "Training Batch [423/782]: Loss 0.1496889293193817\n",
      "Training Batch [424/782]: Loss 0.2112615555524826\n",
      "Training Batch [425/782]: Loss 0.15399356186389923\n",
      "Training Batch [426/782]: Loss 0.17760515213012695\n",
      "Training Batch [427/782]: Loss 0.06648384034633636\n",
      "Training Batch [428/782]: Loss 0.215570867061615\n",
      "Training Batch [429/782]: Loss 0.19382785260677338\n",
      "Training Batch [430/782]: Loss 0.18199075758457184\n",
      "Training Batch [431/782]: Loss 0.2691642940044403\n",
      "Training Batch [432/782]: Loss 0.03005254827439785\n",
      "Training Batch [433/782]: Loss 0.1117897555232048\n",
      "Training Batch [434/782]: Loss 0.27914220094680786\n",
      "Training Batch [435/782]: Loss 0.0535397008061409\n",
      "Training Batch [436/782]: Loss 0.10837477445602417\n",
      "Training Batch [437/782]: Loss 0.11948490142822266\n",
      "Training Batch [438/782]: Loss 0.11180143058300018\n",
      "Training Batch [439/782]: Loss 0.13494771718978882\n",
      "Training Batch [440/782]: Loss 0.10125136375427246\n",
      "Training Batch [441/782]: Loss 0.13343095779418945\n",
      "Training Batch [442/782]: Loss 0.2378312647342682\n",
      "Training Batch [443/782]: Loss 0.10665924102067947\n",
      "Training Batch [444/782]: Loss 0.08502829074859619\n",
      "Training Batch [445/782]: Loss 0.2461702525615692\n",
      "Training Batch [446/782]: Loss 0.13648441433906555\n",
      "Training Batch [447/782]: Loss 0.11925653368234634\n",
      "Training Batch [448/782]: Loss 0.1301521211862564\n",
      "Training Batch [449/782]: Loss 0.13023126125335693\n",
      "Training Batch [450/782]: Loss 0.16069601476192474\n",
      "Training Batch [451/782]: Loss 0.09475939720869064\n",
      "Training Batch [452/782]: Loss 0.1387416124343872\n",
      "Training Batch [453/782]: Loss 0.17109252512454987\n",
      "Training Batch [454/782]: Loss 0.21317994594573975\n",
      "Training Batch [455/782]: Loss 0.06861806660890579\n",
      "Training Batch [456/782]: Loss 0.10682322084903717\n",
      "Training Batch [457/782]: Loss 0.11815409362316132\n",
      "Training Batch [458/782]: Loss 0.11500443518161774\n",
      "Training Batch [459/782]: Loss 0.18849101662635803\n",
      "Training Batch [460/782]: Loss 0.08636192977428436\n",
      "Training Batch [461/782]: Loss 0.12098181247711182\n",
      "Training Batch [462/782]: Loss 0.18063583970069885\n",
      "Training Batch [463/782]: Loss 0.17952074110507965\n",
      "Training Batch [464/782]: Loss 0.16519232094287872\n",
      "Training Batch [465/782]: Loss 0.09765885025262833\n",
      "Training Batch [466/782]: Loss 0.08626549690961838\n",
      "Training Batch [467/782]: Loss 0.16290070116519928\n",
      "Training Batch [468/782]: Loss 0.26811516284942627\n",
      "Training Batch [469/782]: Loss 0.07187088578939438\n",
      "Training Batch [470/782]: Loss 0.19156137108802795\n",
      "Training Batch [471/782]: Loss 0.09087206423282623\n",
      "Training Batch [472/782]: Loss 0.0893387421965599\n",
      "Training Batch [473/782]: Loss 0.1456129550933838\n",
      "Training Batch [474/782]: Loss 0.33306512236595154\n",
      "Training Batch [475/782]: Loss 0.15598836541175842\n",
      "Training Batch [476/782]: Loss 0.16695430874824524\n",
      "Training Batch [477/782]: Loss 0.16855992376804352\n",
      "Training Batch [478/782]: Loss 0.06041250377893448\n",
      "Training Batch [479/782]: Loss 0.2584682106971741\n",
      "Training Batch [480/782]: Loss 0.055866509675979614\n",
      "Training Batch [481/782]: Loss 0.14498308300971985\n",
      "Training Batch [482/782]: Loss 0.24000650644302368\n",
      "Training Batch [483/782]: Loss 0.07290476560592651\n",
      "Training Batch [484/782]: Loss 0.10766610503196716\n",
      "Training Batch [485/782]: Loss 0.19312536716461182\n",
      "Training Batch [486/782]: Loss 0.18345709145069122\n",
      "Training Batch [487/782]: Loss 0.35158446431159973\n",
      "Training Batch [488/782]: Loss 0.12284479290246964\n",
      "Training Batch [489/782]: Loss 0.18796919286251068\n",
      "Training Batch [490/782]: Loss 0.18432119488716125\n",
      "Training Batch [491/782]: Loss 0.23572847247123718\n",
      "Training Batch [492/782]: Loss 0.16318078339099884\n",
      "Training Batch [493/782]: Loss 0.1002219095826149\n",
      "Training Batch [494/782]: Loss 0.20264004170894623\n",
      "Training Batch [495/782]: Loss 0.1321711391210556\n",
      "Training Batch [496/782]: Loss 0.18801410496234894\n",
      "Training Batch [497/782]: Loss 0.06558911502361298\n",
      "Training Batch [498/782]: Loss 0.10855051130056381\n",
      "Training Batch [499/782]: Loss 0.19466163218021393\n",
      "Training Batch [500/782]: Loss 0.2055375576019287\n",
      "Training Batch [501/782]: Loss 0.11739181727170944\n",
      "Training Batch [502/782]: Loss 0.19471383094787598\n",
      "Training Batch [503/782]: Loss 0.07963764667510986\n",
      "Training Batch [504/782]: Loss 0.1993299126625061\n",
      "Training Batch [505/782]: Loss 0.11975034326314926\n",
      "Training Batch [506/782]: Loss 0.1377566009759903\n",
      "Training Batch [507/782]: Loss 0.14644871652126312\n",
      "Training Batch [508/782]: Loss 0.2626127600669861\n",
      "Training Batch [509/782]: Loss 0.14980530738830566\n",
      "Training Batch [510/782]: Loss 0.1151205450296402\n",
      "Training Batch [511/782]: Loss 0.13584499061107635\n",
      "Training Batch [512/782]: Loss 0.13763192296028137\n",
      "Training Batch [513/782]: Loss 0.19374652206897736\n",
      "Training Batch [514/782]: Loss 0.2231372892856598\n",
      "Training Batch [515/782]: Loss 0.15800124406814575\n",
      "Training Batch [516/782]: Loss 0.24813541769981384\n",
      "Training Batch [517/782]: Loss 0.2321307361125946\n",
      "Training Batch [518/782]: Loss 0.08393891155719757\n",
      "Training Batch [519/782]: Loss 0.2242870032787323\n",
      "Training Batch [520/782]: Loss 0.08136796951293945\n",
      "Training Batch [521/782]: Loss 0.12601272761821747\n",
      "Training Batch [522/782]: Loss 0.18089039623737335\n",
      "Training Batch [523/782]: Loss 0.08694054931402206\n",
      "Training Batch [524/782]: Loss 0.057649560272693634\n",
      "Training Batch [525/782]: Loss 0.14950990676879883\n",
      "Training Batch [526/782]: Loss 0.22819940745830536\n",
      "Training Batch [527/782]: Loss 0.13791771233081818\n",
      "Training Batch [528/782]: Loss 0.13628342747688293\n",
      "Training Batch [529/782]: Loss 0.10018289089202881\n",
      "Training Batch [530/782]: Loss 0.04426177591085434\n",
      "Training Batch [531/782]: Loss 0.12528462707996368\n",
      "Training Batch [532/782]: Loss 0.17926567792892456\n",
      "Training Batch [533/782]: Loss 0.15445129573345184\n",
      "Training Batch [534/782]: Loss 0.19389072060585022\n",
      "Training Batch [535/782]: Loss 0.1687205582857132\n",
      "Training Batch [536/782]: Loss 0.1168603003025055\n",
      "Training Batch [537/782]: Loss 0.056897491216659546\n",
      "Training Batch [538/782]: Loss 0.22745461761951447\n",
      "Training Batch [539/782]: Loss 0.13481022417545319\n",
      "Training Batch [540/782]: Loss 0.14560125768184662\n",
      "Training Batch [541/782]: Loss 0.13672377169132233\n",
      "Training Batch [542/782]: Loss 0.20911218225955963\n",
      "Training Batch [543/782]: Loss 0.0379490852355957\n",
      "Training Batch [544/782]: Loss 0.21015529334545135\n",
      "Training Batch [545/782]: Loss 0.0999816283583641\n",
      "Training Batch [546/782]: Loss 0.08967901766300201\n",
      "Training Batch [547/782]: Loss 0.1281917542219162\n",
      "Training Batch [548/782]: Loss 0.13008762896060944\n",
      "Training Batch [549/782]: Loss 0.1755904257297516\n",
      "Training Batch [550/782]: Loss 0.19530922174453735\n",
      "Training Batch [551/782]: Loss 0.21088141202926636\n",
      "Training Batch [552/782]: Loss 0.19838941097259521\n",
      "Training Batch [553/782]: Loss 0.2321075201034546\n",
      "Training Batch [554/782]: Loss 0.20127998292446136\n",
      "Training Batch [555/782]: Loss 0.1770794838666916\n",
      "Training Batch [556/782]: Loss 0.17503595352172852\n",
      "Training Batch [557/782]: Loss 0.095669686794281\n",
      "Training Batch [558/782]: Loss 0.14538507163524628\n",
      "Training Batch [559/782]: Loss 0.26266738772392273\n",
      "Training Batch [560/782]: Loss 0.100969597697258\n",
      "Training Batch [561/782]: Loss 0.3292291760444641\n",
      "Training Batch [562/782]: Loss 0.12330909818410873\n",
      "Training Batch [563/782]: Loss 0.1878044307231903\n",
      "Training Batch [564/782]: Loss 0.08578182756900787\n",
      "Training Batch [565/782]: Loss 0.1630374789237976\n",
      "Training Batch [566/782]: Loss 0.08795475959777832\n",
      "Training Batch [567/782]: Loss 0.31958508491516113\n",
      "Training Batch [568/782]: Loss 0.12967899441719055\n",
      "Training Batch [569/782]: Loss 0.20487003028392792\n",
      "Training Batch [570/782]: Loss 0.11014449596405029\n",
      "Training Batch [571/782]: Loss 0.11767030507326126\n",
      "Training Batch [572/782]: Loss 0.08811060339212418\n",
      "Training Batch [573/782]: Loss 0.13436995446681976\n",
      "Training Batch [574/782]: Loss 0.31180691719055176\n",
      "Training Batch [575/782]: Loss 0.10901341587305069\n",
      "Training Batch [576/782]: Loss 0.21129754185676575\n",
      "Training Batch [577/782]: Loss 0.1417665183544159\n",
      "Training Batch [578/782]: Loss 0.13926389813423157\n",
      "Training Batch [579/782]: Loss 0.22456032037734985\n",
      "Training Batch [580/782]: Loss 0.16042810678482056\n",
      "Training Batch [581/782]: Loss 0.12210927903652191\n",
      "Training Batch [582/782]: Loss 0.16469693183898926\n",
      "Training Batch [583/782]: Loss 0.18933331966400146\n",
      "Training Batch [584/782]: Loss 0.14206744730472565\n",
      "Training Batch [585/782]: Loss 0.08824580907821655\n",
      "Training Batch [586/782]: Loss 0.13625222444534302\n",
      "Training Batch [587/782]: Loss 0.1917213499546051\n",
      "Training Batch [588/782]: Loss 0.244020015001297\n",
      "Training Batch [589/782]: Loss 0.11849883198738098\n",
      "Training Batch [590/782]: Loss 0.07070472836494446\n",
      "Training Batch [591/782]: Loss 0.19846448302268982\n",
      "Training Batch [592/782]: Loss 0.07846910506486893\n",
      "Training Batch [593/782]: Loss 0.10251959413290024\n",
      "Training Batch [594/782]: Loss 0.10901574790477753\n",
      "Training Batch [595/782]: Loss 0.20879031717777252\n",
      "Training Batch [596/782]: Loss 0.2171255499124527\n",
      "Training Batch [597/782]: Loss 0.10176364332437515\n",
      "Training Batch [598/782]: Loss 0.15421628952026367\n",
      "Training Batch [599/782]: Loss 0.05855462700128555\n",
      "Training Batch [600/782]: Loss 0.10601416230201721\n",
      "Training Batch [601/782]: Loss 0.15352559089660645\n",
      "Training Batch [602/782]: Loss 0.06443711370229721\n",
      "Training Batch [603/782]: Loss 0.08284594118595123\n",
      "Training Batch [604/782]: Loss 0.06899825483560562\n",
      "Training Batch [605/782]: Loss 0.08379948884248734\n",
      "Training Batch [606/782]: Loss 0.16909322142601013\n",
      "Training Batch [607/782]: Loss 0.14631447196006775\n",
      "Training Batch [608/782]: Loss 0.15187212824821472\n",
      "Training Batch [609/782]: Loss 0.08847202360630035\n",
      "Training Batch [610/782]: Loss 0.05642980337142944\n",
      "Training Batch [611/782]: Loss 0.06758865714073181\n",
      "Training Batch [612/782]: Loss 0.20409955084323883\n",
      "Training Batch [613/782]: Loss 0.13664855062961578\n",
      "Training Batch [614/782]: Loss 0.23771029710769653\n",
      "Training Batch [615/782]: Loss 0.04988032951951027\n",
      "Training Batch [616/782]: Loss 0.05135317146778107\n",
      "Training Batch [617/782]: Loss 0.17894849181175232\n",
      "Training Batch [618/782]: Loss 0.08379849791526794\n",
      "Training Batch [619/782]: Loss 0.20770534873008728\n",
      "Training Batch [620/782]: Loss 0.3259519934654236\n",
      "Training Batch [621/782]: Loss 0.22306692600250244\n",
      "Training Batch [622/782]: Loss 0.09368828684091568\n",
      "Training Batch [623/782]: Loss 0.14333339035511017\n",
      "Training Batch [624/782]: Loss 0.11598420143127441\n",
      "Training Batch [625/782]: Loss 0.09667981415987015\n",
      "Training Batch [626/782]: Loss 0.11238180845975876\n",
      "Training Batch [627/782]: Loss 0.2535152733325958\n",
      "Training Batch [628/782]: Loss 0.18162837624549866\n",
      "Training Batch [629/782]: Loss 0.15001916885375977\n",
      "Training Batch [630/782]: Loss 0.08174639195203781\n",
      "Training Batch [631/782]: Loss 0.10956882685422897\n",
      "Training Batch [632/782]: Loss 0.1445680856704712\n",
      "Training Batch [633/782]: Loss 0.16757303476333618\n",
      "Training Batch [634/782]: Loss 0.22528447210788727\n",
      "Training Batch [635/782]: Loss 0.12634308636188507\n",
      "Training Batch [636/782]: Loss 0.2102537900209427\n",
      "Training Batch [637/782]: Loss 0.07635974884033203\n",
      "Training Batch [638/782]: Loss 0.08719315379858017\n",
      "Training Batch [639/782]: Loss 0.0892687514424324\n",
      "Training Batch [640/782]: Loss 0.1701132208108902\n",
      "Training Batch [641/782]: Loss 0.0793587863445282\n",
      "Training Batch [642/782]: Loss 0.16544003784656525\n",
      "Training Batch [643/782]: Loss 0.17097119987010956\n",
      "Training Batch [644/782]: Loss 0.11378227174282074\n",
      "Training Batch [645/782]: Loss 0.11858662217855453\n",
      "Training Batch [646/782]: Loss 0.14354458451271057\n",
      "Training Batch [647/782]: Loss 0.0967429056763649\n",
      "Training Batch [648/782]: Loss 0.16350939869880676\n",
      "Training Batch [649/782]: Loss 0.07716406136751175\n",
      "Training Batch [650/782]: Loss 0.18258392810821533\n",
      "Training Batch [651/782]: Loss 0.22251379489898682\n",
      "Training Batch [652/782]: Loss 0.2273624688386917\n",
      "Training Batch [653/782]: Loss 0.04408355802297592\n",
      "Training Batch [654/782]: Loss 0.15007027983665466\n",
      "Training Batch [655/782]: Loss 0.20633652806282043\n",
      "Training Batch [656/782]: Loss 0.12136120349168777\n",
      "Training Batch [657/782]: Loss 0.16290436685085297\n",
      "Training Batch [658/782]: Loss 0.19414831697940826\n",
      "Training Batch [659/782]: Loss 0.16106751561164856\n",
      "Training Batch [660/782]: Loss 0.1610652357339859\n",
      "Training Batch [661/782]: Loss 0.21275369822978973\n",
      "Training Batch [662/782]: Loss 0.36313045024871826\n",
      "Training Batch [663/782]: Loss 0.10495594888925552\n",
      "Training Batch [664/782]: Loss 0.2273256480693817\n",
      "Training Batch [665/782]: Loss 0.14784130454063416\n",
      "Training Batch [666/782]: Loss 0.11509093642234802\n",
      "Training Batch [667/782]: Loss 0.2001924067735672\n",
      "Training Batch [668/782]: Loss 0.15960882604122162\n",
      "Training Batch [669/782]: Loss 0.13882730901241302\n",
      "Training Batch [670/782]: Loss 0.11058048903942108\n",
      "Training Batch [671/782]: Loss 0.1725105196237564\n",
      "Training Batch [672/782]: Loss 0.2551005482673645\n",
      "Training Batch [673/782]: Loss 0.18861402571201324\n",
      "Training Batch [674/782]: Loss 0.1685328483581543\n",
      "Training Batch [675/782]: Loss 0.17157025635242462\n",
      "Training Batch [676/782]: Loss 0.10373319685459137\n",
      "Training Batch [677/782]: Loss 0.08781544864177704\n",
      "Training Batch [678/782]: Loss 0.08486711233854294\n",
      "Training Batch [679/782]: Loss 0.09087106585502625\n",
      "Training Batch [680/782]: Loss 0.09418565779924393\n",
      "Training Batch [681/782]: Loss 0.12365365028381348\n",
      "Training Batch [682/782]: Loss 0.15676839649677277\n",
      "Training Batch [683/782]: Loss 0.17119506001472473\n",
      "Training Batch [684/782]: Loss 0.16429954767227173\n",
      "Training Batch [685/782]: Loss 0.12497565150260925\n",
      "Training Batch [686/782]: Loss 0.07194872945547104\n",
      "Training Batch [687/782]: Loss 0.17049120366573334\n",
      "Training Batch [688/782]: Loss 0.10056651383638382\n",
      "Training Batch [689/782]: Loss 0.284962922334671\n",
      "Training Batch [690/782]: Loss 0.17715120315551758\n",
      "Training Batch [691/782]: Loss 0.30958789587020874\n",
      "Training Batch [692/782]: Loss 0.10658138990402222\n",
      "Training Batch [693/782]: Loss 0.21086932718753815\n",
      "Training Batch [694/782]: Loss 0.19368010759353638\n",
      "Training Batch [695/782]: Loss 0.15673673152923584\n",
      "Training Batch [696/782]: Loss 0.2133374810218811\n",
      "Training Batch [697/782]: Loss 0.20484168827533722\n",
      "Training Batch [698/782]: Loss 0.1032780185341835\n",
      "Training Batch [699/782]: Loss 0.2105628401041031\n",
      "Training Batch [700/782]: Loss 0.22428745031356812\n",
      "Training Batch [701/782]: Loss 0.2656193673610687\n",
      "Training Batch [702/782]: Loss 0.11802031844854355\n",
      "Training Batch [703/782]: Loss 0.19445885717868805\n",
      "Training Batch [704/782]: Loss 0.194157674908638\n",
      "Training Batch [705/782]: Loss 0.09720297902822495\n",
      "Training Batch [706/782]: Loss 0.13909243047237396\n",
      "Training Batch [707/782]: Loss 0.16569828987121582\n",
      "Training Batch [708/782]: Loss 0.07705074548721313\n",
      "Training Batch [709/782]: Loss 0.24861329793930054\n",
      "Training Batch [710/782]: Loss 0.41609981656074524\n",
      "Training Batch [711/782]: Loss 0.16189488768577576\n",
      "Training Batch [712/782]: Loss 0.17703767120838165\n",
      "Training Batch [713/782]: Loss 0.10894879698753357\n",
      "Training Batch [714/782]: Loss 0.11367195099592209\n",
      "Training Batch [715/782]: Loss 0.1538987010717392\n",
      "Training Batch [716/782]: Loss 0.12187478691339493\n",
      "Training Batch [717/782]: Loss 0.1836189329624176\n",
      "Training Batch [718/782]: Loss 0.20346374809741974\n",
      "Training Batch [719/782]: Loss 0.1192542016506195\n",
      "Training Batch [720/782]: Loss 0.28779515624046326\n",
      "Training Batch [721/782]: Loss 0.22366029024124146\n",
      "Training Batch [722/782]: Loss 0.21923118829727173\n",
      "Training Batch [723/782]: Loss 0.29557132720947266\n",
      "Training Batch [724/782]: Loss 0.16935265064239502\n",
      "Training Batch [725/782]: Loss 0.1787065863609314\n",
      "Training Batch [726/782]: Loss 0.09140798449516296\n",
      "Training Batch [727/782]: Loss 0.19116146862506866\n",
      "Training Batch [728/782]: Loss 0.14525023102760315\n",
      "Training Batch [729/782]: Loss 0.2724531888961792\n",
      "Training Batch [730/782]: Loss 0.19474899768829346\n",
      "Training Batch [731/782]: Loss 0.11876466870307922\n",
      "Training Batch [732/782]: Loss 0.18119953572750092\n",
      "Training Batch [733/782]: Loss 0.08257343620061874\n",
      "Training Batch [734/782]: Loss 0.12349741160869598\n",
      "Training Batch [735/782]: Loss 0.2375764548778534\n",
      "Training Batch [736/782]: Loss 0.0685005709528923\n",
      "Training Batch [737/782]: Loss 0.13881903886795044\n",
      "Training Batch [738/782]: Loss 0.1728469282388687\n",
      "Training Batch [739/782]: Loss 0.16500136256217957\n",
      "Training Batch [740/782]: Loss 0.1269567310810089\n",
      "Training Batch [741/782]: Loss 0.16435779631137848\n",
      "Training Batch [742/782]: Loss 0.12769673764705658\n",
      "Training Batch [743/782]: Loss 0.14332930743694305\n",
      "Training Batch [744/782]: Loss 0.07501789182424545\n",
      "Training Batch [745/782]: Loss 0.15548570454120636\n",
      "Training Batch [746/782]: Loss 0.23681290447711945\n",
      "Training Batch [747/782]: Loss 0.0734434425830841\n",
      "Training Batch [748/782]: Loss 0.1950569897890091\n",
      "Training Batch [749/782]: Loss 0.12283430993556976\n",
      "Training Batch [750/782]: Loss 0.14345815777778625\n",
      "Training Batch [751/782]: Loss 0.08464609831571579\n",
      "Training Batch [752/782]: Loss 0.15388630330562592\n",
      "Training Batch [753/782]: Loss 0.1635710746049881\n",
      "Training Batch [754/782]: Loss 0.14712220430374146\n",
      "Training Batch [755/782]: Loss 0.10225671529769897\n",
      "Training Batch [756/782]: Loss 0.23699358105659485\n",
      "Training Batch [757/782]: Loss 0.16193395853042603\n",
      "Training Batch [758/782]: Loss 0.1853889524936676\n",
      "Training Batch [759/782]: Loss 0.08222342282533646\n",
      "Training Batch [760/782]: Loss 0.2073143720626831\n",
      "Training Batch [761/782]: Loss 0.3092164099216461\n",
      "Training Batch [762/782]: Loss 0.1314621865749359\n",
      "Training Batch [763/782]: Loss 0.1962006390094757\n",
      "Training Batch [764/782]: Loss 0.06777499616146088\n",
      "Training Batch [765/782]: Loss 0.05552833899855614\n",
      "Training Batch [766/782]: Loss 0.12174350768327713\n",
      "Training Batch [767/782]: Loss 0.1882759928703308\n",
      "Training Batch [768/782]: Loss 0.2019473910331726\n",
      "Training Batch [769/782]: Loss 0.2067049741744995\n",
      "Training Batch [770/782]: Loss 0.2193831205368042\n",
      "Training Batch [771/782]: Loss 0.14906831085681915\n",
      "Training Batch [772/782]: Loss 0.17871423065662384\n",
      "Training Batch [773/782]: Loss 0.22085356712341309\n",
      "Training Batch [774/782]: Loss 0.13394176959991455\n",
      "Training Batch [775/782]: Loss 0.10042741894721985\n",
      "Training Batch [776/782]: Loss 0.1335902065038681\n",
      "Training Batch [777/782]: Loss 0.09708444774150848\n",
      "Training Batch [778/782]: Loss 0.09408815205097198\n",
      "Training Batch [779/782]: Loss 0.10359517484903336\n",
      "Training Batch [780/782]: Loss 0.10849910974502563\n",
      "Training Batch [781/782]: Loss 0.21030470728874207\n",
      "Training Batch [782/782]: Loss 0.11828191578388214\n",
      "Epoch 11 - Train Loss: 0.1195\n",
      "*********  Epoch 12/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.06832349300384521\n",
      "Training Batch [2/782]: Loss 0.06892449408769608\n",
      "Training Batch [3/782]: Loss 0.042390551418066025\n",
      "Training Batch [4/782]: Loss 0.09605948626995087\n",
      "Training Batch [5/782]: Loss 0.09851905703544617\n",
      "Training Batch [6/782]: Loss 0.09339132905006409\n",
      "Training Batch [7/782]: Loss 0.06153939291834831\n",
      "Training Batch [8/782]: Loss 0.031825147569179535\n",
      "Training Batch [9/782]: Loss 0.17220863699913025\n",
      "Training Batch [10/782]: Loss 0.07596659660339355\n",
      "Training Batch [11/782]: Loss 0.08619154989719391\n",
      "Training Batch [12/782]: Loss 0.2024409919977188\n",
      "Training Batch [13/782]: Loss 0.1291334629058838\n",
      "Training Batch [14/782]: Loss 0.04119311645627022\n",
      "Training Batch [15/782]: Loss 0.06841946393251419\n",
      "Training Batch [16/782]: Loss 0.12631110846996307\n",
      "Training Batch [17/782]: Loss 0.10545986145734787\n",
      "Training Batch [18/782]: Loss 0.13405443727970123\n",
      "Training Batch [19/782]: Loss 0.20501422882080078\n",
      "Training Batch [20/782]: Loss 0.12265895307064056\n",
      "Training Batch [21/782]: Loss 0.03701544180512428\n",
      "Training Batch [22/782]: Loss 0.04995626211166382\n",
      "Training Batch [23/782]: Loss 0.12379469722509384\n",
      "Training Batch [24/782]: Loss 0.03149120509624481\n",
      "Training Batch [25/782]: Loss 0.16111890971660614\n",
      "Training Batch [26/782]: Loss 0.10055188089609146\n",
      "Training Batch [27/782]: Loss 0.06069816276431084\n",
      "Training Batch [28/782]: Loss 0.0929030328989029\n",
      "Training Batch [29/782]: Loss 0.03547517955303192\n",
      "Training Batch [30/782]: Loss 0.08639045059680939\n",
      "Training Batch [31/782]: Loss 0.08447346836328506\n",
      "Training Batch [32/782]: Loss 0.04432695731520653\n",
      "Training Batch [33/782]: Loss 0.07167154550552368\n",
      "Training Batch [34/782]: Loss 0.050544705241918564\n",
      "Training Batch [35/782]: Loss 0.02622724138200283\n",
      "Training Batch [36/782]: Loss 0.07034901529550552\n",
      "Training Batch [37/782]: Loss 0.06395190209150314\n",
      "Training Batch [38/782]: Loss 0.06431116163730621\n",
      "Training Batch [39/782]: Loss 0.10244831442832947\n",
      "Training Batch [40/782]: Loss 0.07394757866859436\n",
      "Training Batch [41/782]: Loss 0.04900216683745384\n",
      "Training Batch [42/782]: Loss 0.11043534427881241\n",
      "Training Batch [43/782]: Loss 0.05020859092473984\n",
      "Training Batch [44/782]: Loss 0.11588580161333084\n",
      "Training Batch [45/782]: Loss 0.08774362504482269\n",
      "Training Batch [46/782]: Loss 0.10233951359987259\n",
      "Training Batch [47/782]: Loss 0.03480762988328934\n",
      "Training Batch [48/782]: Loss 0.12938494980335236\n",
      "Training Batch [49/782]: Loss 0.06724311411380768\n",
      "Training Batch [50/782]: Loss 0.07125405222177505\n",
      "Training Batch [51/782]: Loss 0.08828844875097275\n",
      "Training Batch [52/782]: Loss 0.051349155604839325\n",
      "Training Batch [53/782]: Loss 0.1406293660402298\n",
      "Training Batch [54/782]: Loss 0.07628139108419418\n",
      "Training Batch [55/782]: Loss 0.07652994245290756\n",
      "Training Batch [56/782]: Loss 0.07762619853019714\n",
      "Training Batch [57/782]: Loss 0.08881423622369766\n",
      "Training Batch [58/782]: Loss 0.1478653848171234\n",
      "Training Batch [59/782]: Loss 0.12606796622276306\n",
      "Training Batch [60/782]: Loss 0.12779009342193604\n",
      "Training Batch [61/782]: Loss 0.03654797747731209\n",
      "Training Batch [62/782]: Loss 0.034021250903606415\n",
      "Training Batch [63/782]: Loss 0.056256089359521866\n",
      "Training Batch [64/782]: Loss 0.03068610467016697\n",
      "Training Batch [65/782]: Loss 0.09844140708446503\n",
      "Training Batch [66/782]: Loss 0.08786726742982864\n",
      "Training Batch [67/782]: Loss 0.052998367697000504\n",
      "Training Batch [68/782]: Loss 0.049068037420511246\n",
      "Training Batch [69/782]: Loss 0.14460842311382294\n",
      "Training Batch [70/782]: Loss 0.08813091367483139\n",
      "Training Batch [71/782]: Loss 0.07170045375823975\n",
      "Training Batch [72/782]: Loss 0.09163864701986313\n",
      "Training Batch [73/782]: Loss 0.07797467708587646\n",
      "Training Batch [74/782]: Loss 0.11576897650957108\n",
      "Training Batch [75/782]: Loss 0.0470837727189064\n",
      "Training Batch [76/782]: Loss 0.11116451770067215\n",
      "Training Batch [77/782]: Loss 0.10755839198827744\n",
      "Training Batch [78/782]: Loss 0.13496121764183044\n",
      "Training Batch [79/782]: Loss 0.07676097750663757\n",
      "Training Batch [80/782]: Loss 0.06628062576055527\n",
      "Training Batch [81/782]: Loss 0.12942850589752197\n",
      "Training Batch [82/782]: Loss 0.08914454281330109\n",
      "Training Batch [83/782]: Loss 0.0963529646396637\n",
      "Training Batch [84/782]: Loss 0.050426121801137924\n",
      "Training Batch [85/782]: Loss 0.1307280957698822\n",
      "Training Batch [86/782]: Loss 0.10540337860584259\n",
      "Training Batch [87/782]: Loss 0.09167607873678207\n",
      "Training Batch [88/782]: Loss 0.05783126875758171\n",
      "Training Batch [89/782]: Loss 0.1280217170715332\n",
      "Training Batch [90/782]: Loss 0.08622007817029953\n",
      "Training Batch [91/782]: Loss 0.0660654753446579\n",
      "Training Batch [92/782]: Loss 0.055170442909002304\n",
      "Training Batch [93/782]: Loss 0.08786999434232712\n",
      "Training Batch [94/782]: Loss 0.06594038754701614\n",
      "Training Batch [95/782]: Loss 0.13979879021644592\n",
      "Training Batch [96/782]: Loss 0.046007800847291946\n",
      "Training Batch [97/782]: Loss 0.0618908628821373\n",
      "Training Batch [98/782]: Loss 0.06075439229607582\n",
      "Training Batch [99/782]: Loss 0.027841711416840553\n",
      "Training Batch [100/782]: Loss 0.0790371373295784\n",
      "Training Batch [101/782]: Loss 0.09952320903539658\n",
      "Training Batch [102/782]: Loss 0.041903045028448105\n",
      "Training Batch [103/782]: Loss 0.03858638182282448\n",
      "Training Batch [104/782]: Loss 0.02481033466756344\n",
      "Training Batch [105/782]: Loss 0.028393801301717758\n",
      "Training Batch [106/782]: Loss 0.0676485151052475\n",
      "Training Batch [107/782]: Loss 0.11202287673950195\n",
      "Training Batch [108/782]: Loss 0.04071692004799843\n",
      "Training Batch [109/782]: Loss 0.09877919405698776\n",
      "Training Batch [110/782]: Loss 0.05680730193853378\n",
      "Training Batch [111/782]: Loss 0.08459117263555527\n",
      "Training Batch [112/782]: Loss 0.07811203598976135\n",
      "Training Batch [113/782]: Loss 0.07945762574672699\n",
      "Training Batch [114/782]: Loss 0.05845022201538086\n",
      "Training Batch [115/782]: Loss 0.10023406893014908\n",
      "Training Batch [116/782]: Loss 0.07199860364198685\n",
      "Training Batch [117/782]: Loss 0.13176068663597107\n",
      "Training Batch [118/782]: Loss 0.03336817026138306\n",
      "Training Batch [119/782]: Loss 0.02667354978621006\n",
      "Training Batch [120/782]: Loss 0.05293022096157074\n",
      "Training Batch [121/782]: Loss 0.04070151969790459\n",
      "Training Batch [122/782]: Loss 0.0765037089586258\n",
      "Training Batch [123/782]: Loss 0.1416904330253601\n",
      "Training Batch [124/782]: Loss 0.15219879150390625\n",
      "Training Batch [125/782]: Loss 0.12358567118644714\n",
      "Training Batch [126/782]: Loss 0.06761687248945236\n",
      "Training Batch [127/782]: Loss 0.056592509150505066\n",
      "Training Batch [128/782]: Loss 0.06520958989858627\n",
      "Training Batch [129/782]: Loss 0.08050713688135147\n",
      "Training Batch [130/782]: Loss 0.13690504431724548\n",
      "Training Batch [131/782]: Loss 0.0677797943353653\n",
      "Training Batch [132/782]: Loss 0.1648014783859253\n",
      "Training Batch [133/782]: Loss 0.1068093329668045\n",
      "Training Batch [134/782]: Loss 0.07885321229696274\n",
      "Training Batch [135/782]: Loss 0.05901111289858818\n",
      "Training Batch [136/782]: Loss 0.05333523452281952\n",
      "Training Batch [137/782]: Loss 0.01922418177127838\n",
      "Training Batch [138/782]: Loss 0.08373234421014786\n",
      "Training Batch [139/782]: Loss 0.05140078812837601\n",
      "Training Batch [140/782]: Loss 0.05399036407470703\n",
      "Training Batch [141/782]: Loss 0.08050137758255005\n",
      "Training Batch [142/782]: Loss 0.06207703799009323\n",
      "Training Batch [143/782]: Loss 0.06644721329212189\n",
      "Training Batch [144/782]: Loss 0.03887564688920975\n",
      "Training Batch [145/782]: Loss 0.07852254807949066\n",
      "Training Batch [146/782]: Loss 0.022635983303189278\n",
      "Training Batch [147/782]: Loss 0.03071698546409607\n",
      "Training Batch [148/782]: Loss 0.11120341718196869\n",
      "Training Batch [149/782]: Loss 0.07250203937292099\n",
      "Training Batch [150/782]: Loss 0.0845954567193985\n",
      "Training Batch [151/782]: Loss 0.11906275153160095\n",
      "Training Batch [152/782]: Loss 0.02533538267016411\n",
      "Training Batch [153/782]: Loss 0.20600053668022156\n",
      "Training Batch [154/782]: Loss 0.02244219183921814\n",
      "Training Batch [155/782]: Loss 0.08270809799432755\n",
      "Training Batch [156/782]: Loss 0.11835271120071411\n",
      "Training Batch [157/782]: Loss 0.07252096384763718\n",
      "Training Batch [158/782]: Loss 0.08011757582426071\n",
      "Training Batch [159/782]: Loss 0.06353684514760971\n",
      "Training Batch [160/782]: Loss 0.1780572384595871\n",
      "Training Batch [161/782]: Loss 0.16293500363826752\n",
      "Training Batch [162/782]: Loss 0.08368037641048431\n",
      "Training Batch [163/782]: Loss 0.03671972453594208\n",
      "Training Batch [164/782]: Loss 0.21915468573570251\n",
      "Training Batch [165/782]: Loss 0.12089468538761139\n",
      "Training Batch [166/782]: Loss 0.10368674993515015\n",
      "Training Batch [167/782]: Loss 0.11718545109033585\n",
      "Training Batch [168/782]: Loss 0.022397661581635475\n",
      "Training Batch [169/782]: Loss 0.0914868712425232\n",
      "Training Batch [170/782]: Loss 0.04422009736299515\n",
      "Training Batch [171/782]: Loss 0.1026027575135231\n",
      "Training Batch [172/782]: Loss 0.07072282582521439\n",
      "Training Batch [173/782]: Loss 0.2903209924697876\n",
      "Training Batch [174/782]: Loss 0.037857040762901306\n",
      "Training Batch [175/782]: Loss 0.10534042865037918\n",
      "Training Batch [176/782]: Loss 0.02235538139939308\n",
      "Training Batch [177/782]: Loss 0.1546604484319687\n",
      "Training Batch [178/782]: Loss 0.11063279211521149\n",
      "Training Batch [179/782]: Loss 0.06373614072799683\n",
      "Training Batch [180/782]: Loss 0.10172712802886963\n",
      "Training Batch [181/782]: Loss 0.07544011622667313\n",
      "Training Batch [182/782]: Loss 0.10848093032836914\n",
      "Training Batch [183/782]: Loss 0.08454296737909317\n",
      "Training Batch [184/782]: Loss 0.082170769572258\n",
      "Training Batch [185/782]: Loss 0.02799317054450512\n",
      "Training Batch [186/782]: Loss 0.06852705031633377\n",
      "Training Batch [187/782]: Loss 0.10975879430770874\n",
      "Training Batch [188/782]: Loss 0.018968796357512474\n",
      "Training Batch [189/782]: Loss 0.12264073640108109\n",
      "Training Batch [190/782]: Loss 0.06825001537799835\n",
      "Training Batch [191/782]: Loss 0.043767210096120834\n",
      "Training Batch [192/782]: Loss 0.058040447533130646\n",
      "Training Batch [193/782]: Loss 0.11942152678966522\n",
      "Training Batch [194/782]: Loss 0.07526891678571701\n",
      "Training Batch [195/782]: Loss 0.13067969679832458\n",
      "Training Batch [196/782]: Loss 0.08406995981931686\n",
      "Training Batch [197/782]: Loss 0.08508966118097305\n",
      "Training Batch [198/782]: Loss 0.09424395114183426\n",
      "Training Batch [199/782]: Loss 0.05419309064745903\n",
      "Training Batch [200/782]: Loss 0.09057190269231796\n",
      "Training Batch [201/782]: Loss 0.07187299430370331\n",
      "Training Batch [202/782]: Loss 0.08857879042625427\n",
      "Training Batch [203/782]: Loss 0.03009396232664585\n",
      "Training Batch [204/782]: Loss 0.06096756458282471\n",
      "Training Batch [205/782]: Loss 0.05948773771524429\n",
      "Training Batch [206/782]: Loss 0.07522690296173096\n",
      "Training Batch [207/782]: Loss 0.10079464316368103\n",
      "Training Batch [208/782]: Loss 0.009230933152139187\n",
      "Training Batch [209/782]: Loss 0.11382193863391876\n",
      "Training Batch [210/782]: Loss 0.13605259358882904\n",
      "Training Batch [211/782]: Loss 0.028672806918621063\n",
      "Training Batch [212/782]: Loss 0.17386086285114288\n",
      "Training Batch [213/782]: Loss 0.045073024928569794\n",
      "Training Batch [214/782]: Loss 0.09813165664672852\n",
      "Training Batch [215/782]: Loss 0.11105233430862427\n",
      "Training Batch [216/782]: Loss 0.07146625220775604\n",
      "Training Batch [217/782]: Loss 0.056877292692661285\n",
      "Training Batch [218/782]: Loss 0.2070702761411667\n",
      "Training Batch [219/782]: Loss 0.08299484848976135\n",
      "Training Batch [220/782]: Loss 0.05460705608129501\n",
      "Training Batch [221/782]: Loss 0.11278154700994492\n",
      "Training Batch [222/782]: Loss 0.09733891487121582\n",
      "Training Batch [223/782]: Loss 0.11803974211215973\n",
      "Training Batch [224/782]: Loss 0.11471009999513626\n",
      "Training Batch [225/782]: Loss 0.06395793706178665\n",
      "Training Batch [226/782]: Loss 0.05881168693304062\n",
      "Training Batch [227/782]: Loss 0.12206975370645523\n",
      "Training Batch [228/782]: Loss 0.04099177196621895\n",
      "Training Batch [229/782]: Loss 0.08614791184663773\n",
      "Training Batch [230/782]: Loss 0.11148760467767715\n",
      "Training Batch [231/782]: Loss 0.22403496503829956\n",
      "Training Batch [232/782]: Loss 0.14030489325523376\n",
      "Training Batch [233/782]: Loss 0.09203595668077469\n",
      "Training Batch [234/782]: Loss 0.10774030536413193\n",
      "Training Batch [235/782]: Loss 0.17853417992591858\n",
      "Training Batch [236/782]: Loss 0.14595533907413483\n",
      "Training Batch [237/782]: Loss 0.04713422805070877\n",
      "Training Batch [238/782]: Loss 0.11545408517122269\n",
      "Training Batch [239/782]: Loss 0.08115661889314651\n",
      "Training Batch [240/782]: Loss 0.1327720582485199\n",
      "Training Batch [241/782]: Loss 0.08676367998123169\n",
      "Training Batch [242/782]: Loss 0.1188337653875351\n",
      "Training Batch [243/782]: Loss 0.10082051157951355\n",
      "Training Batch [244/782]: Loss 0.21543458104133606\n",
      "Training Batch [245/782]: Loss 0.09101416170597076\n",
      "Training Batch [246/782]: Loss 0.08760704100131989\n",
      "Training Batch [247/782]: Loss 0.19984585046768188\n",
      "Training Batch [248/782]: Loss 0.1257791519165039\n",
      "Training Batch [249/782]: Loss 0.07726984471082687\n",
      "Training Batch [250/782]: Loss 0.04240627959370613\n",
      "Training Batch [251/782]: Loss 0.1016763299703598\n",
      "Training Batch [252/782]: Loss 0.12285492569208145\n",
      "Training Batch [253/782]: Loss 0.20882968604564667\n",
      "Training Batch [254/782]: Loss 0.058251362293958664\n",
      "Training Batch [255/782]: Loss 0.04938484728336334\n",
      "Training Batch [256/782]: Loss 0.0571322925388813\n",
      "Training Batch [257/782]: Loss 0.05414484441280365\n",
      "Training Batch [258/782]: Loss 0.06035728007555008\n",
      "Training Batch [259/782]: Loss 0.039951764047145844\n",
      "Training Batch [260/782]: Loss 0.08752366900444031\n",
      "Training Batch [261/782]: Loss 0.18463566899299622\n",
      "Training Batch [262/782]: Loss 0.10535376518964767\n",
      "Training Batch [263/782]: Loss 0.08672317862510681\n",
      "Training Batch [264/782]: Loss 0.12008434534072876\n",
      "Training Batch [265/782]: Loss 0.10021302849054337\n",
      "Training Batch [266/782]: Loss 0.05687706172466278\n",
      "Training Batch [267/782]: Loss 0.13875684142112732\n",
      "Training Batch [268/782]: Loss 0.07411663979291916\n",
      "Training Batch [269/782]: Loss 0.14024941623210907\n",
      "Training Batch [270/782]: Loss 0.16674822568893433\n",
      "Training Batch [271/782]: Loss 0.1125761866569519\n",
      "Training Batch [272/782]: Loss 0.06002897024154663\n",
      "Training Batch [273/782]: Loss 0.11145088076591492\n",
      "Training Batch [274/782]: Loss 0.06405679881572723\n",
      "Training Batch [275/782]: Loss 0.22063316404819489\n",
      "Training Batch [276/782]: Loss 0.10659290105104446\n",
      "Training Batch [277/782]: Loss 0.20800171792507172\n",
      "Training Batch [278/782]: Loss 0.08355288952589035\n",
      "Training Batch [279/782]: Loss 0.04504014551639557\n",
      "Training Batch [280/782]: Loss 0.12755118310451508\n",
      "Training Batch [281/782]: Loss 0.06186222657561302\n",
      "Training Batch [282/782]: Loss 0.06195630878210068\n",
      "Training Batch [283/782]: Loss 0.1918703317642212\n",
      "Training Batch [284/782]: Loss 0.04092507064342499\n",
      "Training Batch [285/782]: Loss 0.13328877091407776\n",
      "Training Batch [286/782]: Loss 0.11877474188804626\n",
      "Training Batch [287/782]: Loss 0.11701279133558273\n",
      "Training Batch [288/782]: Loss 0.12396102398633957\n",
      "Training Batch [289/782]: Loss 0.1568608582019806\n",
      "Training Batch [290/782]: Loss 0.07234417647123337\n",
      "Training Batch [291/782]: Loss 0.13970878720283508\n",
      "Training Batch [292/782]: Loss 0.10665461421012878\n",
      "Training Batch [293/782]: Loss 0.08013364672660828\n",
      "Training Batch [294/782]: Loss 0.040543656796216965\n",
      "Training Batch [295/782]: Loss 0.06004892289638519\n",
      "Training Batch [296/782]: Loss 0.06671261042356491\n",
      "Training Batch [297/782]: Loss 0.09146647900342941\n",
      "Training Batch [298/782]: Loss 0.06681922823190689\n",
      "Training Batch [299/782]: Loss 0.1369163691997528\n",
      "Training Batch [300/782]: Loss 0.0790741965174675\n",
      "Training Batch [301/782]: Loss 0.14934366941452026\n",
      "Training Batch [302/782]: Loss 0.13642269372940063\n",
      "Training Batch [303/782]: Loss 0.09658735990524292\n",
      "Training Batch [304/782]: Loss 0.08948148787021637\n",
      "Training Batch [305/782]: Loss 0.04581639915704727\n",
      "Training Batch [306/782]: Loss 0.05178358405828476\n",
      "Training Batch [307/782]: Loss 0.12068358063697815\n",
      "Training Batch [308/782]: Loss 0.09640910476446152\n",
      "Training Batch [309/782]: Loss 0.11005301773548126\n",
      "Training Batch [310/782]: Loss 0.14480344951152802\n",
      "Training Batch [311/782]: Loss 0.15127119421958923\n",
      "Training Batch [312/782]: Loss 0.12152902036905289\n",
      "Training Batch [313/782]: Loss 0.08107493817806244\n",
      "Training Batch [314/782]: Loss 0.08089805394411087\n",
      "Training Batch [315/782]: Loss 0.05361245572566986\n",
      "Training Batch [316/782]: Loss 0.031150082126259804\n",
      "Training Batch [317/782]: Loss 0.06474068015813828\n",
      "Training Batch [318/782]: Loss 0.04111752659082413\n",
      "Training Batch [319/782]: Loss 0.05628879740834236\n",
      "Training Batch [320/782]: Loss 0.08995024859905243\n",
      "Training Batch [321/782]: Loss 0.06896810233592987\n",
      "Training Batch [322/782]: Loss 0.08431416749954224\n",
      "Training Batch [323/782]: Loss 0.1353520005941391\n",
      "Training Batch [324/782]: Loss 0.06392118334770203\n",
      "Training Batch [325/782]: Loss 0.053940631449222565\n",
      "Training Batch [326/782]: Loss 0.04302120953798294\n",
      "Training Batch [327/782]: Loss 0.2007652074098587\n",
      "Training Batch [328/782]: Loss 0.04985736683011055\n",
      "Training Batch [329/782]: Loss 0.09543601423501968\n",
      "Training Batch [330/782]: Loss 0.042158350348472595\n",
      "Training Batch [331/782]: Loss 0.03870748355984688\n",
      "Training Batch [332/782]: Loss 0.07099895924329758\n",
      "Training Batch [333/782]: Loss 0.06518106162548065\n",
      "Training Batch [334/782]: Loss 0.08568096905946732\n",
      "Training Batch [335/782]: Loss 0.0034507415257394314\n",
      "Training Batch [336/782]: Loss 0.07224909961223602\n",
      "Training Batch [337/782]: Loss 0.08435515314340591\n",
      "Training Batch [338/782]: Loss 0.13522875308990479\n",
      "Training Batch [339/782]: Loss 0.052466440945863724\n",
      "Training Batch [340/782]: Loss 0.057479873299598694\n",
      "Training Batch [341/782]: Loss 0.07156427204608917\n",
      "Training Batch [342/782]: Loss 0.10156675428152084\n",
      "Training Batch [343/782]: Loss 0.04394130781292915\n",
      "Training Batch [344/782]: Loss 0.0902971625328064\n",
      "Training Batch [345/782]: Loss 0.04706912115216255\n",
      "Training Batch [346/782]: Loss 0.11330438405275345\n",
      "Training Batch [347/782]: Loss 0.11820012331008911\n",
      "Training Batch [348/782]: Loss 0.03828304633498192\n",
      "Training Batch [349/782]: Loss 0.0693483054637909\n",
      "Training Batch [350/782]: Loss 0.13122519850730896\n",
      "Training Batch [351/782]: Loss 0.07973591238260269\n",
      "Training Batch [352/782]: Loss 0.04408612474799156\n",
      "Training Batch [353/782]: Loss 0.1394960880279541\n",
      "Training Batch [354/782]: Loss 0.04058544710278511\n",
      "Training Batch [355/782]: Loss 0.19579429924488068\n",
      "Training Batch [356/782]: Loss 0.05137740448117256\n",
      "Training Batch [357/782]: Loss 0.20207931101322174\n",
      "Training Batch [358/782]: Loss 0.06933223456144333\n",
      "Training Batch [359/782]: Loss 0.09268978983163834\n",
      "Training Batch [360/782]: Loss 0.175322026014328\n",
      "Training Batch [361/782]: Loss 0.11428713798522949\n",
      "Training Batch [362/782]: Loss 0.07176473736763\n",
      "Training Batch [363/782]: Loss 0.07119382917881012\n",
      "Training Batch [364/782]: Loss 0.05283692479133606\n",
      "Training Batch [365/782]: Loss 0.15018151700496674\n",
      "Training Batch [366/782]: Loss 0.05584012717008591\n",
      "Training Batch [367/782]: Loss 0.05056914687156677\n",
      "Training Batch [368/782]: Loss 0.09496897459030151\n",
      "Training Batch [369/782]: Loss 0.11396923661231995\n",
      "Training Batch [370/782]: Loss 0.07494792342185974\n",
      "Training Batch [371/782]: Loss 0.029748879373073578\n",
      "Training Batch [372/782]: Loss 0.043371398001909256\n",
      "Training Batch [373/782]: Loss 0.10653849691152573\n",
      "Training Batch [374/782]: Loss 0.09333834052085876\n",
      "Training Batch [375/782]: Loss 0.09211523085832596\n",
      "Training Batch [376/782]: Loss 0.07675498723983765\n",
      "Training Batch [377/782]: Loss 0.12945964932441711\n",
      "Training Batch [378/782]: Loss 0.25913214683532715\n",
      "Training Batch [379/782]: Loss 0.18249724805355072\n",
      "Training Batch [380/782]: Loss 0.05977225303649902\n",
      "Training Batch [381/782]: Loss 0.10567311942577362\n",
      "Training Batch [382/782]: Loss 0.035263560712337494\n",
      "Training Batch [383/782]: Loss 0.07229402661323547\n",
      "Training Batch [384/782]: Loss 0.026093125343322754\n",
      "Training Batch [385/782]: Loss 0.08093801140785217\n",
      "Training Batch [386/782]: Loss 0.16540943086147308\n",
      "Training Batch [387/782]: Loss 0.21630671620368958\n",
      "Training Batch [388/782]: Loss 0.08880865573883057\n",
      "Training Batch [389/782]: Loss 0.1449965089559555\n",
      "Training Batch [390/782]: Loss 0.11687089502811432\n",
      "Training Batch [391/782]: Loss 0.05837571993470192\n",
      "Training Batch [392/782]: Loss 0.1153506189584732\n",
      "Training Batch [393/782]: Loss 0.09131437540054321\n",
      "Training Batch [394/782]: Loss 0.06503795087337494\n",
      "Training Batch [395/782]: Loss 0.08171630650758743\n",
      "Training Batch [396/782]: Loss 0.07290488481521606\n",
      "Training Batch [397/782]: Loss 0.12907463312149048\n",
      "Training Batch [398/782]: Loss 0.09576031565666199\n",
      "Training Batch [399/782]: Loss 0.042947571724653244\n",
      "Training Batch [400/782]: Loss 0.04317525401711464\n",
      "Training Batch [401/782]: Loss 0.03913181275129318\n",
      "Training Batch [402/782]: Loss 0.1027161031961441\n",
      "Training Batch [403/782]: Loss 0.049327220767736435\n",
      "Training Batch [404/782]: Loss 0.1736375093460083\n",
      "Training Batch [405/782]: Loss 0.08291247487068176\n",
      "Training Batch [406/782]: Loss 0.12398729473352432\n",
      "Training Batch [407/782]: Loss 0.03279738873243332\n",
      "Training Batch [408/782]: Loss 0.13622480630874634\n",
      "Training Batch [409/782]: Loss 0.04644033685326576\n",
      "Training Batch [410/782]: Loss 0.12846463918685913\n",
      "Training Batch [411/782]: Loss 0.06848746538162231\n",
      "Training Batch [412/782]: Loss 0.12245789170265198\n",
      "Training Batch [413/782]: Loss 0.15395045280456543\n",
      "Training Batch [414/782]: Loss 0.10192962735891342\n",
      "Training Batch [415/782]: Loss 0.05392539128661156\n",
      "Training Batch [416/782]: Loss 0.11836261302232742\n",
      "Training Batch [417/782]: Loss 0.08331169188022614\n",
      "Training Batch [418/782]: Loss 0.11237245798110962\n",
      "Training Batch [419/782]: Loss 0.02652127668261528\n",
      "Training Batch [420/782]: Loss 0.0907813087105751\n",
      "Training Batch [421/782]: Loss 0.08677416294813156\n",
      "Training Batch [422/782]: Loss 0.060600779950618744\n",
      "Training Batch [423/782]: Loss 0.1003921627998352\n",
      "Training Batch [424/782]: Loss 0.014260893687605858\n",
      "Training Batch [425/782]: Loss 0.08809319883584976\n",
      "Training Batch [426/782]: Loss 0.07375282049179077\n",
      "Training Batch [427/782]: Loss 0.07140098512172699\n",
      "Training Batch [428/782]: Loss 0.0901075154542923\n",
      "Training Batch [429/782]: Loss 0.06349868327379227\n",
      "Training Batch [430/782]: Loss 0.02752014994621277\n",
      "Training Batch [431/782]: Loss 0.1009351834654808\n",
      "Training Batch [432/782]: Loss 0.19582274556159973\n",
      "Training Batch [433/782]: Loss 0.19370503723621368\n",
      "Training Batch [434/782]: Loss 0.054885100573301315\n",
      "Training Batch [435/782]: Loss 0.18646159768104553\n",
      "Training Batch [436/782]: Loss 0.028927773237228394\n",
      "Training Batch [437/782]: Loss 0.05549871549010277\n",
      "Training Batch [438/782]: Loss 0.20301729440689087\n",
      "Training Batch [439/782]: Loss 0.06679927557706833\n",
      "Training Batch [440/782]: Loss 0.06635680794715881\n",
      "Training Batch [441/782]: Loss 0.09139811992645264\n",
      "Training Batch [442/782]: Loss 0.07685810327529907\n",
      "Training Batch [443/782]: Loss 0.1037275493144989\n",
      "Training Batch [444/782]: Loss 0.052848394960165024\n",
      "Training Batch [445/782]: Loss 0.09184785932302475\n",
      "Training Batch [446/782]: Loss 0.05196159705519676\n",
      "Training Batch [447/782]: Loss 0.2078629732131958\n",
      "Training Batch [448/782]: Loss 0.17904213070869446\n",
      "Training Batch [449/782]: Loss 0.23004232347011566\n",
      "Training Batch [450/782]: Loss 0.1515277922153473\n",
      "Training Batch [451/782]: Loss 0.052086472511291504\n",
      "Training Batch [452/782]: Loss 0.12280560284852982\n",
      "Training Batch [453/782]: Loss 0.1410757154226303\n",
      "Training Batch [454/782]: Loss 0.1320139467716217\n",
      "Training Batch [455/782]: Loss 0.06437751650810242\n",
      "Training Batch [456/782]: Loss 0.1922086775302887\n",
      "Training Batch [457/782]: Loss 0.1978185474872589\n",
      "Training Batch [458/782]: Loss 0.028210077434778214\n",
      "Training Batch [459/782]: Loss 0.12755174934864044\n",
      "Training Batch [460/782]: Loss 0.23717430233955383\n",
      "Training Batch [461/782]: Loss 0.17703458666801453\n",
      "Training Batch [462/782]: Loss 0.26330697536468506\n",
      "Training Batch [463/782]: Loss 0.10855529457330704\n",
      "Training Batch [464/782]: Loss 0.19272789359092712\n",
      "Training Batch [465/782]: Loss 0.14226652681827545\n",
      "Training Batch [466/782]: Loss 0.04290790110826492\n",
      "Training Batch [467/782]: Loss 0.13721787929534912\n",
      "Training Batch [468/782]: Loss 0.19620104134082794\n",
      "Training Batch [469/782]: Loss 0.11337916553020477\n",
      "Training Batch [470/782]: Loss 0.14060257375240326\n",
      "Training Batch [471/782]: Loss 0.06563670188188553\n",
      "Training Batch [472/782]: Loss 0.26507073640823364\n",
      "Training Batch [473/782]: Loss 0.09568621218204498\n",
      "Training Batch [474/782]: Loss 0.05337366461753845\n",
      "Training Batch [475/782]: Loss 0.22941166162490845\n",
      "Training Batch [476/782]: Loss 0.12385380268096924\n",
      "Training Batch [477/782]: Loss 0.09133518487215042\n",
      "Training Batch [478/782]: Loss 0.08236396312713623\n",
      "Training Batch [479/782]: Loss 0.26096147298812866\n",
      "Training Batch [480/782]: Loss 0.06538388878107071\n",
      "Training Batch [481/782]: Loss 0.18796300888061523\n",
      "Training Batch [482/782]: Loss 0.19546698033809662\n",
      "Training Batch [483/782]: Loss 0.04671841859817505\n",
      "Training Batch [484/782]: Loss 0.11713414639234543\n",
      "Training Batch [485/782]: Loss 0.19484785199165344\n",
      "Training Batch [486/782]: Loss 0.2188868671655655\n",
      "Training Batch [487/782]: Loss 0.11804850399494171\n",
      "Training Batch [488/782]: Loss 0.1785229742527008\n",
      "Training Batch [489/782]: Loss 0.177609384059906\n",
      "Training Batch [490/782]: Loss 0.03807215020060539\n",
      "Training Batch [491/782]: Loss 0.14600422978401184\n",
      "Training Batch [492/782]: Loss 0.1561216413974762\n",
      "Training Batch [493/782]: Loss 0.05809847638010979\n",
      "Training Batch [494/782]: Loss 0.11649104207754135\n",
      "Training Batch [495/782]: Loss 0.08420538902282715\n",
      "Training Batch [496/782]: Loss 0.06788617372512817\n",
      "Training Batch [497/782]: Loss 0.12563666701316833\n",
      "Training Batch [498/782]: Loss 0.080429807305336\n",
      "Training Batch [499/782]: Loss 0.08398165553808212\n",
      "Training Batch [500/782]: Loss 0.09185101091861725\n",
      "Training Batch [501/782]: Loss 0.0946710929274559\n",
      "Training Batch [502/782]: Loss 0.035865239799022675\n",
      "Training Batch [503/782]: Loss 0.11968471109867096\n",
      "Training Batch [504/782]: Loss 0.08960512280464172\n",
      "Training Batch [505/782]: Loss 0.13288649916648865\n",
      "Training Batch [506/782]: Loss 0.09832829982042313\n",
      "Training Batch [507/782]: Loss 0.07433811575174332\n",
      "Training Batch [508/782]: Loss 0.13498052954673767\n",
      "Training Batch [509/782]: Loss 0.09059606492519379\n",
      "Training Batch [510/782]: Loss 0.09565848112106323\n",
      "Training Batch [511/782]: Loss 0.08133608102798462\n",
      "Training Batch [512/782]: Loss 0.11856044828891754\n",
      "Training Batch [513/782]: Loss 0.07660476118326187\n",
      "Training Batch [514/782]: Loss 0.18533481657505035\n",
      "Training Batch [515/782]: Loss 0.08519647270441055\n",
      "Training Batch [516/782]: Loss 0.030280346050858498\n",
      "Training Batch [517/782]: Loss 0.07242381572723389\n",
      "Training Batch [518/782]: Loss 0.09648097306489944\n",
      "Training Batch [519/782]: Loss 0.32674649357795715\n",
      "Training Batch [520/782]: Loss 0.10222890228033066\n",
      "Training Batch [521/782]: Loss 0.12016072869300842\n",
      "Training Batch [522/782]: Loss 0.04995233938097954\n",
      "Training Batch [523/782]: Loss 0.030665788799524307\n",
      "Training Batch [524/782]: Loss 0.0351007804274559\n",
      "Training Batch [525/782]: Loss 0.14241141080856323\n",
      "Training Batch [526/782]: Loss 0.10339099168777466\n",
      "Training Batch [527/782]: Loss 0.08852405101060867\n",
      "Training Batch [528/782]: Loss 0.18541526794433594\n",
      "Training Batch [529/782]: Loss 0.03001401573419571\n",
      "Training Batch [530/782]: Loss 0.13171979784965515\n",
      "Training Batch [531/782]: Loss 0.16706061363220215\n",
      "Training Batch [532/782]: Loss 0.12458319216966629\n",
      "Training Batch [533/782]: Loss 0.23846407234668732\n",
      "Training Batch [534/782]: Loss 0.19105207920074463\n",
      "Training Batch [535/782]: Loss 0.12335824221372604\n",
      "Training Batch [536/782]: Loss 0.2387397438287735\n",
      "Training Batch [537/782]: Loss 0.07340313494205475\n",
      "Training Batch [538/782]: Loss 0.1731353998184204\n",
      "Training Batch [539/782]: Loss 0.06922154873609543\n",
      "Training Batch [540/782]: Loss 0.06115305796265602\n",
      "Training Batch [541/782]: Loss 0.06395623087882996\n",
      "Training Batch [542/782]: Loss 0.0757552981376648\n",
      "Training Batch [543/782]: Loss 0.1026625707745552\n",
      "Training Batch [544/782]: Loss 0.10047806799411774\n",
      "Training Batch [545/782]: Loss 0.22268754243850708\n",
      "Training Batch [546/782]: Loss 0.16206607222557068\n",
      "Training Batch [547/782]: Loss 0.06346932798624039\n",
      "Training Batch [548/782]: Loss 0.11284641176462173\n",
      "Training Batch [549/782]: Loss 0.11924920231103897\n",
      "Training Batch [550/782]: Loss 0.08795250952243805\n",
      "Training Batch [551/782]: Loss 0.03220812603831291\n",
      "Training Batch [552/782]: Loss 0.10373559594154358\n",
      "Training Batch [553/782]: Loss 0.22824297845363617\n",
      "Training Batch [554/782]: Loss 0.17095297574996948\n",
      "Training Batch [555/782]: Loss 0.13676032423973083\n",
      "Training Batch [556/782]: Loss 0.1206110417842865\n",
      "Training Batch [557/782]: Loss 0.1782505065202713\n",
      "Training Batch [558/782]: Loss 0.09642544388771057\n",
      "Training Batch [559/782]: Loss 0.027198560535907745\n",
      "Training Batch [560/782]: Loss 0.11499308794736862\n",
      "Training Batch [561/782]: Loss 0.11067180335521698\n",
      "Training Batch [562/782]: Loss 0.09352701902389526\n",
      "Training Batch [563/782]: Loss 0.07048732042312622\n",
      "Training Batch [564/782]: Loss 0.10456588119268417\n",
      "Training Batch [565/782]: Loss 0.07241623103618622\n",
      "Training Batch [566/782]: Loss 0.11904017627239227\n",
      "Training Batch [567/782]: Loss 0.14450500905513763\n",
      "Training Batch [568/782]: Loss 0.16044676303863525\n",
      "Training Batch [569/782]: Loss 0.06059334799647331\n",
      "Training Batch [570/782]: Loss 0.07974985986948013\n",
      "Training Batch [571/782]: Loss 0.2117408663034439\n",
      "Training Batch [572/782]: Loss 0.11218591779470444\n",
      "Training Batch [573/782]: Loss 0.05030761659145355\n",
      "Training Batch [574/782]: Loss 0.18070779740810394\n",
      "Training Batch [575/782]: Loss 0.11969072371721268\n",
      "Training Batch [576/782]: Loss 0.2605035603046417\n",
      "Training Batch [577/782]: Loss 0.1870524138212204\n",
      "Training Batch [578/782]: Loss 0.13625508546829224\n",
      "Training Batch [579/782]: Loss 0.157057985663414\n",
      "Training Batch [580/782]: Loss 0.10053327679634094\n",
      "Training Batch [581/782]: Loss 0.05846211686730385\n",
      "Training Batch [582/782]: Loss 0.09844046831130981\n",
      "Training Batch [583/782]: Loss 0.11018973588943481\n",
      "Training Batch [584/782]: Loss 0.10152433067560196\n",
      "Training Batch [585/782]: Loss 0.2003730982542038\n",
      "Training Batch [586/782]: Loss 0.24369588494300842\n",
      "Training Batch [587/782]: Loss 0.12155923247337341\n",
      "Training Batch [588/782]: Loss 0.1720314770936966\n",
      "Training Batch [589/782]: Loss 0.19062396883964539\n",
      "Training Batch [590/782]: Loss 0.1532159000635147\n",
      "Training Batch [591/782]: Loss 0.16040048003196716\n",
      "Training Batch [592/782]: Loss 0.07860525697469711\n",
      "Training Batch [593/782]: Loss 0.2421763837337494\n",
      "Training Batch [594/782]: Loss 0.12414634227752686\n",
      "Training Batch [595/782]: Loss 0.1531267762184143\n",
      "Training Batch [596/782]: Loss 0.16444484889507294\n",
      "Training Batch [597/782]: Loss 0.03902365639805794\n",
      "Training Batch [598/782]: Loss 0.08180151879787445\n",
      "Training Batch [599/782]: Loss 0.2073521763086319\n",
      "Training Batch [600/782]: Loss 0.0926302969455719\n",
      "Training Batch [601/782]: Loss 0.11829761415719986\n",
      "Training Batch [602/782]: Loss 0.11872026324272156\n",
      "Training Batch [603/782]: Loss 0.06699290871620178\n",
      "Training Batch [604/782]: Loss 0.0745195522904396\n",
      "Training Batch [605/782]: Loss 0.16483016312122345\n",
      "Training Batch [606/782]: Loss 0.08218933641910553\n",
      "Training Batch [607/782]: Loss 0.0727577954530716\n",
      "Training Batch [608/782]: Loss 0.16709782183170319\n",
      "Training Batch [609/782]: Loss 0.13928206264972687\n",
      "Training Batch [610/782]: Loss 0.14502522349357605\n",
      "Training Batch [611/782]: Loss 0.26553383469581604\n",
      "Training Batch [612/782]: Loss 0.12981905043125153\n",
      "Training Batch [613/782]: Loss 0.11752763390541077\n",
      "Training Batch [614/782]: Loss 0.20676597952842712\n",
      "Training Batch [615/782]: Loss 0.29326775670051575\n",
      "Training Batch [616/782]: Loss 0.2345433384180069\n",
      "Training Batch [617/782]: Loss 0.10411910712718964\n",
      "Training Batch [618/782]: Loss 0.09877578169107437\n",
      "Training Batch [619/782]: Loss 0.1221868172287941\n",
      "Training Batch [620/782]: Loss 0.046597402542829514\n",
      "Training Batch [621/782]: Loss 0.09605178236961365\n",
      "Training Batch [622/782]: Loss 0.22806167602539062\n",
      "Training Batch [623/782]: Loss 0.05223745107650757\n",
      "Training Batch [624/782]: Loss 0.21349720656871796\n",
      "Training Batch [625/782]: Loss 0.13617822527885437\n",
      "Training Batch [626/782]: Loss 0.24213474988937378\n",
      "Training Batch [627/782]: Loss 0.14072372019290924\n",
      "Training Batch [628/782]: Loss 0.14057059586048126\n",
      "Training Batch [629/782]: Loss 0.1095210537314415\n",
      "Training Batch [630/782]: Loss 0.13236172497272491\n",
      "Training Batch [631/782]: Loss 0.27842283248901367\n",
      "Training Batch [632/782]: Loss 0.14966121315956116\n",
      "Training Batch [633/782]: Loss 0.291761189699173\n",
      "Training Batch [634/782]: Loss 0.08852724730968475\n",
      "Training Batch [635/782]: Loss 0.06253155320882797\n",
      "Training Batch [636/782]: Loss 0.04956868290901184\n",
      "Training Batch [637/782]: Loss 0.15992222726345062\n",
      "Training Batch [638/782]: Loss 0.16011466085910797\n",
      "Training Batch [639/782]: Loss 0.18889670073986053\n",
      "Training Batch [640/782]: Loss 0.19593678414821625\n",
      "Training Batch [641/782]: Loss 0.07736102491617203\n",
      "Training Batch [642/782]: Loss 0.23830179870128632\n",
      "Training Batch [643/782]: Loss 0.1806681901216507\n",
      "Training Batch [644/782]: Loss 0.1032671332359314\n",
      "Training Batch [645/782]: Loss 0.19039560854434967\n",
      "Training Batch [646/782]: Loss 0.14593489468097687\n",
      "Training Batch [647/782]: Loss 0.16294731199741364\n",
      "Training Batch [648/782]: Loss 0.06714699417352676\n",
      "Training Batch [649/782]: Loss 0.19725652039051056\n",
      "Training Batch [650/782]: Loss 0.05795112997293472\n",
      "Training Batch [651/782]: Loss 0.26975223422050476\n",
      "Training Batch [652/782]: Loss 0.15128420293331146\n",
      "Training Batch [653/782]: Loss 0.13082653284072876\n",
      "Training Batch [654/782]: Loss 0.10239486396312714\n",
      "Training Batch [655/782]: Loss 0.11915821582078934\n",
      "Training Batch [656/782]: Loss 0.08153975754976273\n",
      "Training Batch [657/782]: Loss 0.07533615827560425\n",
      "Training Batch [658/782]: Loss 0.15711922943592072\n",
      "Training Batch [659/782]: Loss 0.07626410573720932\n",
      "Training Batch [660/782]: Loss 0.10240262746810913\n",
      "Training Batch [661/782]: Loss 0.10263514518737793\n",
      "Training Batch [662/782]: Loss 0.038116756826639175\n",
      "Training Batch [663/782]: Loss 0.08566424995660782\n",
      "Training Batch [664/782]: Loss 0.1646137684583664\n",
      "Training Batch [665/782]: Loss 0.03673550486564636\n",
      "Training Batch [666/782]: Loss 0.01365677546709776\n",
      "Training Batch [667/782]: Loss 0.0706150010228157\n",
      "Training Batch [668/782]: Loss 0.35054856538772583\n",
      "Training Batch [669/782]: Loss 0.1465153843164444\n",
      "Training Batch [670/782]: Loss 0.11462065577507019\n",
      "Training Batch [671/782]: Loss 0.1132892593741417\n",
      "Training Batch [672/782]: Loss 0.10132349282503128\n",
      "Training Batch [673/782]: Loss 0.11704853177070618\n",
      "Training Batch [674/782]: Loss 0.07866337895393372\n",
      "Training Batch [675/782]: Loss 0.17270904779434204\n",
      "Training Batch [676/782]: Loss 0.0804663673043251\n",
      "Training Batch [677/782]: Loss 0.059505246579647064\n",
      "Training Batch [678/782]: Loss 0.18426351249217987\n",
      "Training Batch [679/782]: Loss 0.1411670595407486\n",
      "Training Batch [680/782]: Loss 0.10164415836334229\n",
      "Training Batch [681/782]: Loss 0.2220461070537567\n",
      "Training Batch [682/782]: Loss 0.06440095603466034\n",
      "Training Batch [683/782]: Loss 0.12322618067264557\n",
      "Training Batch [684/782]: Loss 0.10766094923019409\n",
      "Training Batch [685/782]: Loss 0.08634572476148605\n",
      "Training Batch [686/782]: Loss 0.09327381104230881\n",
      "Training Batch [687/782]: Loss 0.223488911986351\n",
      "Training Batch [688/782]: Loss 0.07732417434453964\n",
      "Training Batch [689/782]: Loss 0.13041095435619354\n",
      "Training Batch [690/782]: Loss 0.05896662175655365\n",
      "Training Batch [691/782]: Loss 0.06363487243652344\n",
      "Training Batch [692/782]: Loss 0.07823160290718079\n",
      "Training Batch [693/782]: Loss 0.14618772268295288\n",
      "Training Batch [694/782]: Loss 0.07232537865638733\n",
      "Training Batch [695/782]: Loss 0.09909483790397644\n",
      "Training Batch [696/782]: Loss 0.06778784841299057\n",
      "Training Batch [697/782]: Loss 0.17935970425605774\n",
      "Training Batch [698/782]: Loss 0.11006897687911987\n",
      "Training Batch [699/782]: Loss 0.14844653010368347\n",
      "Training Batch [700/782]: Loss 0.052339281886816025\n",
      "Training Batch [701/782]: Loss 0.21390485763549805\n",
      "Training Batch [702/782]: Loss 0.1650109589099884\n",
      "Training Batch [703/782]: Loss 0.13495486974716187\n",
      "Training Batch [704/782]: Loss 0.038907624781131744\n",
      "Training Batch [705/782]: Loss 0.09504984319210052\n",
      "Training Batch [706/782]: Loss 0.16966471076011658\n",
      "Training Batch [707/782]: Loss 0.10128533840179443\n",
      "Training Batch [708/782]: Loss 0.19361963868141174\n",
      "Training Batch [709/782]: Loss 0.22512827813625336\n",
      "Training Batch [710/782]: Loss 0.03398241847753525\n",
      "Training Batch [711/782]: Loss 0.20873203873634338\n",
      "Training Batch [712/782]: Loss 0.19307297468185425\n",
      "Training Batch [713/782]: Loss 0.08198732882738113\n",
      "Training Batch [714/782]: Loss 0.11771392077207565\n",
      "Training Batch [715/782]: Loss 0.13249137997627258\n",
      "Training Batch [716/782]: Loss 0.16684356331825256\n",
      "Training Batch [717/782]: Loss 0.11798910796642303\n",
      "Training Batch [718/782]: Loss 0.1331862360239029\n",
      "Training Batch [719/782]: Loss 0.05691279470920563\n",
      "Training Batch [720/782]: Loss 0.04358705133199692\n",
      "Training Batch [721/782]: Loss 0.22269555926322937\n",
      "Training Batch [722/782]: Loss 0.22073028981685638\n",
      "Training Batch [723/782]: Loss 0.1892898976802826\n",
      "Training Batch [724/782]: Loss 0.14306263625621796\n",
      "Training Batch [725/782]: Loss 0.03839348256587982\n",
      "Training Batch [726/782]: Loss 0.187645822763443\n",
      "Training Batch [727/782]: Loss 0.1712692826986313\n",
      "Training Batch [728/782]: Loss 0.058049604296684265\n",
      "Training Batch [729/782]: Loss 0.15993626415729523\n",
      "Training Batch [730/782]: Loss 0.2664704918861389\n",
      "Training Batch [731/782]: Loss 0.14915914833545685\n",
      "Training Batch [732/782]: Loss 0.10162613540887833\n",
      "Training Batch [733/782]: Loss 0.18301253020763397\n",
      "Training Batch [734/782]: Loss 0.061280757188797\n",
      "Training Batch [735/782]: Loss 0.2837193012237549\n",
      "Training Batch [736/782]: Loss 0.13200481235980988\n",
      "Training Batch [737/782]: Loss 0.11386092752218246\n",
      "Training Batch [738/782]: Loss 0.09240079671144485\n",
      "Training Batch [739/782]: Loss 0.1292780488729477\n",
      "Training Batch [740/782]: Loss 0.16001151502132416\n",
      "Training Batch [741/782]: Loss 0.14442677795886993\n",
      "Training Batch [742/782]: Loss 0.09645433723926544\n",
      "Training Batch [743/782]: Loss 0.13234730064868927\n",
      "Training Batch [744/782]: Loss 0.04242206737399101\n",
      "Training Batch [745/782]: Loss 0.19536332786083221\n",
      "Training Batch [746/782]: Loss 0.05279696360230446\n",
      "Training Batch [747/782]: Loss 0.11681538820266724\n",
      "Training Batch [748/782]: Loss 0.18343105912208557\n",
      "Training Batch [749/782]: Loss 0.17350777983665466\n",
      "Training Batch [750/782]: Loss 0.11103929579257965\n",
      "Training Batch [751/782]: Loss 0.07690182328224182\n",
      "Training Batch [752/782]: Loss 0.08484481275081635\n",
      "Training Batch [753/782]: Loss 0.10786332190036774\n",
      "Training Batch [754/782]: Loss 0.05905451253056526\n",
      "Training Batch [755/782]: Loss 0.2578222155570984\n",
      "Training Batch [756/782]: Loss 0.1448579728603363\n",
      "Training Batch [757/782]: Loss 0.0669306144118309\n",
      "Training Batch [758/782]: Loss 0.1518235057592392\n",
      "Training Batch [759/782]: Loss 0.045070189982652664\n",
      "Training Batch [760/782]: Loss 0.15788859128952026\n",
      "Training Batch [761/782]: Loss 0.1530151665210724\n",
      "Training Batch [762/782]: Loss 0.06029527634382248\n",
      "Training Batch [763/782]: Loss 0.11765231192111969\n",
      "Training Batch [764/782]: Loss 0.19942280650138855\n",
      "Training Batch [765/782]: Loss 0.3116234242916107\n",
      "Training Batch [766/782]: Loss 0.18413524329662323\n",
      "Training Batch [767/782]: Loss 0.10408682376146317\n",
      "Training Batch [768/782]: Loss 0.08609863370656967\n",
      "Training Batch [769/782]: Loss 0.15426896512508392\n",
      "Training Batch [770/782]: Loss 0.17258712649345398\n",
      "Training Batch [771/782]: Loss 0.1918618530035019\n",
      "Training Batch [772/782]: Loss 0.43456095457077026\n",
      "Training Batch [773/782]: Loss 0.08238951861858368\n",
      "Training Batch [774/782]: Loss 0.11612609773874283\n",
      "Training Batch [775/782]: Loss 0.14567045867443085\n",
      "Training Batch [776/782]: Loss 0.13010112941265106\n",
      "Training Batch [777/782]: Loss 0.12244413048028946\n",
      "Training Batch [778/782]: Loss 0.29371392726898193\n",
      "Training Batch [779/782]: Loss 0.25148704648017883\n",
      "Training Batch [780/782]: Loss 0.18327999114990234\n",
      "Training Batch [781/782]: Loss 0.09050077199935913\n",
      "Training Batch [782/782]: Loss 0.012953472323715687\n",
      "Epoch 12 - Train Loss: 0.1079\n",
      "*********  Epoch 13/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.062008000910282135\n",
      "Training Batch [2/782]: Loss 0.08718258887529373\n",
      "Training Batch [3/782]: Loss 0.06389284133911133\n",
      "Training Batch [4/782]: Loss 0.12732961773872375\n",
      "Training Batch [5/782]: Loss 0.28981688618659973\n",
      "Training Batch [6/782]: Loss 0.052181605249643326\n",
      "Training Batch [7/782]: Loss 0.013982804492115974\n",
      "Training Batch [8/782]: Loss 0.09944828599691391\n",
      "Training Batch [9/782]: Loss 0.09627041965723038\n",
      "Training Batch [10/782]: Loss 0.07917682081460953\n",
      "Training Batch [11/782]: Loss 0.08774713426828384\n",
      "Training Batch [12/782]: Loss 0.07776996493339539\n",
      "Training Batch [13/782]: Loss 0.13399094343185425\n",
      "Training Batch [14/782]: Loss 0.108158178627491\n",
      "Training Batch [15/782]: Loss 0.026395605877041817\n",
      "Training Batch [16/782]: Loss 0.034544553607702255\n",
      "Training Batch [17/782]: Loss 0.09811799228191376\n",
      "Training Batch [18/782]: Loss 0.03698507696390152\n",
      "Training Batch [19/782]: Loss 0.11251324415206909\n",
      "Training Batch [20/782]: Loss 0.06751641631126404\n",
      "Training Batch [21/782]: Loss 0.17257092893123627\n",
      "Training Batch [22/782]: Loss 0.06027895584702492\n",
      "Training Batch [23/782]: Loss 0.07355903089046478\n",
      "Training Batch [24/782]: Loss 0.09931635111570358\n",
      "Training Batch [25/782]: Loss 0.1668739765882492\n",
      "Training Batch [26/782]: Loss 0.058743178844451904\n",
      "Training Batch [27/782]: Loss 0.054177649319171906\n",
      "Training Batch [28/782]: Loss 0.08344511687755585\n",
      "Training Batch [29/782]: Loss 0.06763308495283127\n",
      "Training Batch [30/782]: Loss 0.03533479571342468\n",
      "Training Batch [31/782]: Loss 0.2741320729255676\n",
      "Training Batch [32/782]: Loss 0.18417298793792725\n",
      "Training Batch [33/782]: Loss 0.1660393923521042\n",
      "Training Batch [34/782]: Loss 0.08312886208295822\n",
      "Training Batch [35/782]: Loss 0.008585195057094097\n",
      "Training Batch [36/782]: Loss 0.09340975433588028\n",
      "Training Batch [37/782]: Loss 0.08886386454105377\n",
      "Training Batch [38/782]: Loss 0.1501890867948532\n",
      "Training Batch [39/782]: Loss 0.0678943321108818\n",
      "Training Batch [40/782]: Loss 0.09825403243303299\n",
      "Training Batch [41/782]: Loss 0.05965577811002731\n",
      "Training Batch [42/782]: Loss 0.08607253432273865\n",
      "Training Batch [43/782]: Loss 0.12029081583023071\n",
      "Training Batch [44/782]: Loss 0.06017573922872543\n",
      "Training Batch [45/782]: Loss 0.05779484286904335\n",
      "Training Batch [46/782]: Loss 0.07983240485191345\n",
      "Training Batch [47/782]: Loss 0.08447162806987762\n",
      "Training Batch [48/782]: Loss 0.06373719871044159\n",
      "Training Batch [49/782]: Loss 0.10975955426692963\n",
      "Training Batch [50/782]: Loss 0.12655465304851532\n",
      "Training Batch [51/782]: Loss 0.09947479516267776\n",
      "Training Batch [52/782]: Loss 0.12589295208454132\n",
      "Training Batch [53/782]: Loss 0.11929842084646225\n",
      "Training Batch [54/782]: Loss 0.051791634410619736\n",
      "Training Batch [55/782]: Loss 0.2077142596244812\n",
      "Training Batch [56/782]: Loss 0.10550179332494736\n",
      "Training Batch [57/782]: Loss 0.07395993173122406\n",
      "Training Batch [58/782]: Loss 0.11055977642536163\n",
      "Training Batch [59/782]: Loss 0.14723125100135803\n",
      "Training Batch [60/782]: Loss 0.1367981731891632\n",
      "Training Batch [61/782]: Loss 0.07506167143583298\n",
      "Training Batch [62/782]: Loss 0.09888876229524612\n",
      "Training Batch [63/782]: Loss 0.09846430271863937\n",
      "Training Batch [64/782]: Loss 0.08894465863704681\n",
      "Training Batch [65/782]: Loss 0.1263655573129654\n",
      "Training Batch [66/782]: Loss 0.0358881801366806\n",
      "Training Batch [67/782]: Loss 0.0588681623339653\n",
      "Training Batch [68/782]: Loss 0.037571582943201065\n",
      "Training Batch [69/782]: Loss 0.10239345580339432\n",
      "Training Batch [70/782]: Loss 0.07172032445669174\n",
      "Training Batch [71/782]: Loss 0.07542881369590759\n",
      "Training Batch [72/782]: Loss 0.03845342621207237\n",
      "Training Batch [73/782]: Loss 0.18223950266838074\n",
      "Training Batch [74/782]: Loss 0.045853663235902786\n",
      "Training Batch [75/782]: Loss 0.02451441064476967\n",
      "Training Batch [76/782]: Loss 0.06324728578329086\n",
      "Training Batch [77/782]: Loss 0.028462445363402367\n",
      "Training Batch [78/782]: Loss 0.08916179835796356\n",
      "Training Batch [79/782]: Loss 0.05185089260339737\n",
      "Training Batch [80/782]: Loss 0.028713611885905266\n",
      "Training Batch [81/782]: Loss 0.1514778435230255\n",
      "Training Batch [82/782]: Loss 0.040751829743385315\n",
      "Training Batch [83/782]: Loss 0.0808538943529129\n",
      "Training Batch [84/782]: Loss 0.026003435254096985\n",
      "Training Batch [85/782]: Loss 0.13056635856628418\n",
      "Training Batch [86/782]: Loss 0.07297533005475998\n",
      "Training Batch [87/782]: Loss 0.08989646285772324\n",
      "Training Batch [88/782]: Loss 0.05437754467129707\n",
      "Training Batch [89/782]: Loss 0.13860970735549927\n",
      "Training Batch [90/782]: Loss 0.13683025538921356\n",
      "Training Batch [91/782]: Loss 0.08468299359083176\n",
      "Training Batch [92/782]: Loss 0.03121797926723957\n",
      "Training Batch [93/782]: Loss 0.07208181917667389\n",
      "Training Batch [94/782]: Loss 0.1081666424870491\n",
      "Training Batch [95/782]: Loss 0.05864967405796051\n",
      "Training Batch [96/782]: Loss 0.034826889634132385\n",
      "Training Batch [97/782]: Loss 0.03650609403848648\n",
      "Training Batch [98/782]: Loss 0.04762060567736626\n",
      "Training Batch [99/782]: Loss 0.0695454552769661\n",
      "Training Batch [100/782]: Loss 0.10028883814811707\n",
      "Training Batch [101/782]: Loss 0.043069176375865936\n",
      "Training Batch [102/782]: Loss 0.10514663904905319\n",
      "Training Batch [103/782]: Loss 0.06937309354543686\n",
      "Training Batch [104/782]: Loss 0.02421426773071289\n",
      "Training Batch [105/782]: Loss 0.033504411578178406\n",
      "Training Batch [106/782]: Loss 0.12848210334777832\n",
      "Training Batch [107/782]: Loss 0.11247149109840393\n",
      "Training Batch [108/782]: Loss 0.0912557989358902\n",
      "Training Batch [109/782]: Loss 0.04361385852098465\n",
      "Training Batch [110/782]: Loss 0.09044879674911499\n",
      "Training Batch [111/782]: Loss 0.032204240560531616\n",
      "Training Batch [112/782]: Loss 0.01193848717957735\n",
      "Training Batch [113/782]: Loss 0.0329967625439167\n",
      "Training Batch [114/782]: Loss 0.04253543168306351\n",
      "Training Batch [115/782]: Loss 0.037189919501543045\n",
      "Training Batch [116/782]: Loss 0.022824641317129135\n",
      "Training Batch [117/782]: Loss 0.1014106273651123\n",
      "Training Batch [118/782]: Loss 0.11482201516628265\n",
      "Training Batch [119/782]: Loss 0.059168800711631775\n",
      "Training Batch [120/782]: Loss 0.04105166345834732\n",
      "Training Batch [121/782]: Loss 0.03864666074514389\n",
      "Training Batch [122/782]: Loss 0.026424836367368698\n",
      "Training Batch [123/782]: Loss 0.04596628621220589\n",
      "Training Batch [124/782]: Loss 0.03942706808447838\n",
      "Training Batch [125/782]: Loss 0.026362456381320953\n",
      "Training Batch [126/782]: Loss 0.04245683178305626\n",
      "Training Batch [127/782]: Loss 0.09883931279182434\n",
      "Training Batch [128/782]: Loss 0.04216594994068146\n",
      "Training Batch [129/782]: Loss 0.05036262422800064\n",
      "Training Batch [130/782]: Loss 0.021332694217562675\n",
      "Training Batch [131/782]: Loss 0.039847880601882935\n",
      "Training Batch [132/782]: Loss 0.05071204528212547\n",
      "Training Batch [133/782]: Loss 0.03642798960208893\n",
      "Training Batch [134/782]: Loss 0.033516861498355865\n",
      "Training Batch [135/782]: Loss 0.07601367682218552\n",
      "Training Batch [136/782]: Loss 0.06758203357458115\n",
      "Training Batch [137/782]: Loss 0.008690348826348782\n",
      "Training Batch [138/782]: Loss 0.022453702986240387\n",
      "Training Batch [139/782]: Loss 0.022740473970770836\n",
      "Training Batch [140/782]: Loss 0.028571806848049164\n",
      "Training Batch [141/782]: Loss 0.027406753972172737\n",
      "Training Batch [142/782]: Loss 0.15338565409183502\n",
      "Training Batch [143/782]: Loss 0.03673477843403816\n",
      "Training Batch [144/782]: Loss 0.024863626807928085\n",
      "Training Batch [145/782]: Loss 0.11902859807014465\n",
      "Training Batch [146/782]: Loss 0.02502228505909443\n",
      "Training Batch [147/782]: Loss 0.02318452298641205\n",
      "Training Batch [148/782]: Loss 0.07413004338741302\n",
      "Training Batch [149/782]: Loss 0.029000675305724144\n",
      "Training Batch [150/782]: Loss 0.09766504168510437\n",
      "Training Batch [151/782]: Loss 0.06001780182123184\n",
      "Training Batch [152/782]: Loss 0.06881003826856613\n",
      "Training Batch [153/782]: Loss 0.08666371554136276\n",
      "Training Batch [154/782]: Loss 0.07478464394807816\n",
      "Training Batch [155/782]: Loss 0.053062859922647476\n",
      "Training Batch [156/782]: Loss 0.009289338253438473\n",
      "Training Batch [157/782]: Loss 0.019991623237729073\n",
      "Training Batch [158/782]: Loss 0.04142335057258606\n",
      "Training Batch [159/782]: Loss 0.13979105651378632\n",
      "Training Batch [160/782]: Loss 0.070468969643116\n",
      "Training Batch [161/782]: Loss 0.016843123361468315\n",
      "Training Batch [162/782]: Loss 0.06259593367576599\n",
      "Training Batch [163/782]: Loss 0.05735866352915764\n",
      "Training Batch [164/782]: Loss 0.03906022757291794\n",
      "Training Batch [165/782]: Loss 0.03363463282585144\n",
      "Training Batch [166/782]: Loss 0.0299224890768528\n",
      "Training Batch [167/782]: Loss 0.020289577543735504\n",
      "Training Batch [168/782]: Loss 0.13291288912296295\n",
      "Training Batch [169/782]: Loss 0.014680680818855762\n",
      "Training Batch [170/782]: Loss 0.17745457589626312\n",
      "Training Batch [171/782]: Loss 0.07200387865304947\n",
      "Training Batch [172/782]: Loss 0.014655955135822296\n",
      "Training Batch [173/782]: Loss 0.02186470851302147\n",
      "Training Batch [174/782]: Loss 0.05989415571093559\n",
      "Training Batch [175/782]: Loss 0.03851225972175598\n",
      "Training Batch [176/782]: Loss 0.11224038153886795\n",
      "Training Batch [177/782]: Loss 0.03563863784074783\n",
      "Training Batch [178/782]: Loss 0.05253048613667488\n",
      "Training Batch [179/782]: Loss 0.0542433001101017\n",
      "Training Batch [180/782]: Loss 0.01340098213404417\n",
      "Training Batch [181/782]: Loss 0.09228578954935074\n",
      "Training Batch [182/782]: Loss 0.02183460257947445\n",
      "Training Batch [183/782]: Loss 0.019716652110219002\n",
      "Training Batch [184/782]: Loss 0.015944771468639374\n",
      "Training Batch [185/782]: Loss 0.05516817048192024\n",
      "Training Batch [186/782]: Loss 0.026998689398169518\n",
      "Training Batch [187/782]: Loss 0.020893536508083344\n",
      "Training Batch [188/782]: Loss 0.047100335359573364\n",
      "Training Batch [189/782]: Loss 0.028144339099526405\n",
      "Training Batch [190/782]: Loss 0.04906677082180977\n",
      "Training Batch [191/782]: Loss 0.09851713478565216\n",
      "Training Batch [192/782]: Loss 0.08613823354244232\n",
      "Training Batch [193/782]: Loss 0.05371912196278572\n",
      "Training Batch [194/782]: Loss 0.01985943131148815\n",
      "Training Batch [195/782]: Loss 0.028068475425243378\n",
      "Training Batch [196/782]: Loss 0.051457103341817856\n",
      "Training Batch [197/782]: Loss 0.045876000076532364\n",
      "Training Batch [198/782]: Loss 0.03563844412565231\n",
      "Training Batch [199/782]: Loss 0.04728199541568756\n",
      "Training Batch [200/782]: Loss 0.07635091245174408\n",
      "Training Batch [201/782]: Loss 0.07380568981170654\n",
      "Training Batch [202/782]: Loss 0.12479609996080399\n",
      "Training Batch [203/782]: Loss 0.09719794988632202\n",
      "Training Batch [204/782]: Loss 0.04729583114385605\n",
      "Training Batch [205/782]: Loss 0.07782361656427383\n",
      "Training Batch [206/782]: Loss 0.1431886851787567\n",
      "Training Batch [207/782]: Loss 0.08643357455730438\n",
      "Training Batch [208/782]: Loss 0.0558609701693058\n",
      "Training Batch [209/782]: Loss 0.11197897046804428\n",
      "Training Batch [210/782]: Loss 0.06726329773664474\n",
      "Training Batch [211/782]: Loss 0.039791595190763474\n",
      "Training Batch [212/782]: Loss 0.066314198076725\n",
      "Training Batch [213/782]: Loss 0.09026476740837097\n",
      "Training Batch [214/782]: Loss 0.03563990443944931\n",
      "Training Batch [215/782]: Loss 0.053205594420433044\n",
      "Training Batch [216/782]: Loss 0.02789168991148472\n",
      "Training Batch [217/782]: Loss 0.0765073299407959\n",
      "Training Batch [218/782]: Loss 0.06620459258556366\n",
      "Training Batch [219/782]: Loss 0.07677103579044342\n",
      "Training Batch [220/782]: Loss 0.06219605356454849\n",
      "Training Batch [221/782]: Loss 0.017758946865797043\n",
      "Training Batch [222/782]: Loss 0.14227531850337982\n",
      "Training Batch [223/782]: Loss 0.03075387328863144\n",
      "Training Batch [224/782]: Loss 0.06626290082931519\n",
      "Training Batch [225/782]: Loss 0.015164809301495552\n",
      "Training Batch [226/782]: Loss 0.026102298870682716\n",
      "Training Batch [227/782]: Loss 0.16105572879314423\n",
      "Training Batch [228/782]: Loss 0.059104759246110916\n",
      "Training Batch [229/782]: Loss 0.014944416470825672\n",
      "Training Batch [230/782]: Loss 0.10106595605611801\n",
      "Training Batch [231/782]: Loss 0.04773763567209244\n",
      "Training Batch [232/782]: Loss 0.05281323567032814\n",
      "Training Batch [233/782]: Loss 0.09482303261756897\n",
      "Training Batch [234/782]: Loss 0.02954898029565811\n",
      "Training Batch [235/782]: Loss 0.055519234389066696\n",
      "Training Batch [236/782]: Loss 0.02105625718832016\n",
      "Training Batch [237/782]: Loss 0.08545507490634918\n",
      "Training Batch [238/782]: Loss 0.012560654431581497\n",
      "Training Batch [239/782]: Loss 0.10759974271059036\n",
      "Training Batch [240/782]: Loss 0.0161553006619215\n",
      "Training Batch [241/782]: Loss 0.0915263444185257\n",
      "Training Batch [242/782]: Loss 0.029128383845090866\n",
      "Training Batch [243/782]: Loss 0.18095657229423523\n",
      "Training Batch [244/782]: Loss 0.0858888104557991\n",
      "Training Batch [245/782]: Loss 0.026873981580138206\n",
      "Training Batch [246/782]: Loss 0.03856709972023964\n",
      "Training Batch [247/782]: Loss 0.08509974181652069\n",
      "Training Batch [248/782]: Loss 0.10557522624731064\n",
      "Training Batch [249/782]: Loss 0.050020698457956314\n",
      "Training Batch [250/782]: Loss 0.19084614515304565\n",
      "Training Batch [251/782]: Loss 0.08623796701431274\n",
      "Training Batch [252/782]: Loss 0.07251898944377899\n",
      "Training Batch [253/782]: Loss 0.04798811674118042\n",
      "Training Batch [254/782]: Loss 0.056133486330509186\n",
      "Training Batch [255/782]: Loss 0.06690168380737305\n",
      "Training Batch [256/782]: Loss 0.0653185248374939\n",
      "Training Batch [257/782]: Loss 0.06255335360765457\n",
      "Training Batch [258/782]: Loss 0.021343549713492393\n",
      "Training Batch [259/782]: Loss 0.02833118475973606\n",
      "Training Batch [260/782]: Loss 0.06345482170581818\n",
      "Training Batch [261/782]: Loss 0.022824976593255997\n",
      "Training Batch [262/782]: Loss 0.05370837077498436\n",
      "Training Batch [263/782]: Loss 0.019832583144307137\n",
      "Training Batch [264/782]: Loss 0.07890103757381439\n",
      "Training Batch [265/782]: Loss 0.09474722295999527\n",
      "Training Batch [266/782]: Loss 0.059258606284856796\n",
      "Training Batch [267/782]: Loss 0.1380728781223297\n",
      "Training Batch [268/782]: Loss 0.07766225188970566\n",
      "Training Batch [269/782]: Loss 0.038552187383174896\n",
      "Training Batch [270/782]: Loss 0.05601426959037781\n",
      "Training Batch [271/782]: Loss 0.07535386085510254\n",
      "Training Batch [272/782]: Loss 0.13888749480247498\n",
      "Training Batch [273/782]: Loss 0.05787467956542969\n",
      "Training Batch [274/782]: Loss 0.14991362392902374\n",
      "Training Batch [275/782]: Loss 0.07336710393428802\n",
      "Training Batch [276/782]: Loss 0.19447126984596252\n",
      "Training Batch [277/782]: Loss 0.08357493579387665\n",
      "Training Batch [278/782]: Loss 0.08385570347309113\n",
      "Training Batch [279/782]: Loss 0.055517688393592834\n",
      "Training Batch [280/782]: Loss 0.0799885168671608\n",
      "Training Batch [281/782]: Loss 0.06593374162912369\n",
      "Training Batch [282/782]: Loss 0.08052415400743484\n",
      "Training Batch [283/782]: Loss 0.07623143494129181\n",
      "Training Batch [284/782]: Loss 0.039903197437524796\n",
      "Training Batch [285/782]: Loss 0.06754100322723389\n",
      "Training Batch [286/782]: Loss 0.03765536844730377\n",
      "Training Batch [287/782]: Loss 0.03921840712428093\n",
      "Training Batch [288/782]: Loss 0.05871361866593361\n",
      "Training Batch [289/782]: Loss 0.15679259598255157\n",
      "Training Batch [290/782]: Loss 0.09605131298303604\n",
      "Training Batch [291/782]: Loss 0.040424712002277374\n",
      "Training Batch [292/782]: Loss 0.026275336742401123\n",
      "Training Batch [293/782]: Loss 0.0859604924917221\n",
      "Training Batch [294/782]: Loss 0.04311477392911911\n",
      "Training Batch [295/782]: Loss 0.03536958619952202\n",
      "Training Batch [296/782]: Loss 0.03774210810661316\n",
      "Training Batch [297/782]: Loss 0.11145174503326416\n",
      "Training Batch [298/782]: Loss 0.10264813154935837\n",
      "Training Batch [299/782]: Loss 0.06426696479320526\n",
      "Training Batch [300/782]: Loss 0.06789147853851318\n",
      "Training Batch [301/782]: Loss 0.04508081451058388\n",
      "Training Batch [302/782]: Loss 0.021776564419269562\n",
      "Training Batch [303/782]: Loss 0.08201482892036438\n",
      "Training Batch [304/782]: Loss 0.06836917251348495\n",
      "Training Batch [305/782]: Loss 0.13655222952365875\n",
      "Training Batch [306/782]: Loss 0.10931809991598129\n",
      "Training Batch [307/782]: Loss 0.03588796406984329\n",
      "Training Batch [308/782]: Loss 0.01882295310497284\n",
      "Training Batch [309/782]: Loss 0.02956228330731392\n",
      "Training Batch [310/782]: Loss 0.2840171754360199\n",
      "Training Batch [311/782]: Loss 0.03551311418414116\n",
      "Training Batch [312/782]: Loss 0.04231322929263115\n",
      "Training Batch [313/782]: Loss 0.06702033430337906\n",
      "Training Batch [314/782]: Loss 0.01702454499900341\n",
      "Training Batch [315/782]: Loss 0.05683589726686478\n",
      "Training Batch [316/782]: Loss 0.08265063166618347\n",
      "Training Batch [317/782]: Loss 0.09667996317148209\n",
      "Training Batch [318/782]: Loss 0.050061386078596115\n",
      "Training Batch [319/782]: Loss 0.06549842655658722\n",
      "Training Batch [320/782]: Loss 0.10784809291362762\n",
      "Training Batch [321/782]: Loss 0.020191220566630363\n",
      "Training Batch [322/782]: Loss 0.04177917167544365\n",
      "Training Batch [323/782]: Loss 0.029422389343380928\n",
      "Training Batch [324/782]: Loss 0.07333709299564362\n",
      "Training Batch [325/782]: Loss 0.11251285672187805\n",
      "Training Batch [326/782]: Loss 0.06148583069443703\n",
      "Training Batch [327/782]: Loss 0.04961639270186424\n",
      "Training Batch [328/782]: Loss 0.0795806497335434\n",
      "Training Batch [329/782]: Loss 0.14118561148643494\n",
      "Training Batch [330/782]: Loss 0.049156125634908676\n",
      "Training Batch [331/782]: Loss 0.04975469410419464\n",
      "Training Batch [332/782]: Loss 0.03129325062036514\n",
      "Training Batch [333/782]: Loss 0.0326499417424202\n",
      "Training Batch [334/782]: Loss 0.053667012602090836\n",
      "Training Batch [335/782]: Loss 0.07408700883388519\n",
      "Training Batch [336/782]: Loss 0.06398626416921616\n",
      "Training Batch [337/782]: Loss 0.022736575454473495\n",
      "Training Batch [338/782]: Loss 0.07871439307928085\n",
      "Training Batch [339/782]: Loss 0.14032095670700073\n",
      "Training Batch [340/782]: Loss 0.09094908088445663\n",
      "Training Batch [341/782]: Loss 0.048969533294439316\n",
      "Training Batch [342/782]: Loss 0.026556842029094696\n",
      "Training Batch [343/782]: Loss 0.1409035325050354\n",
      "Training Batch [344/782]: Loss 0.014230309054255486\n",
      "Training Batch [345/782]: Loss 0.03323110565543175\n",
      "Training Batch [346/782]: Loss 0.06501943618059158\n",
      "Training Batch [347/782]: Loss 0.0164941493421793\n",
      "Training Batch [348/782]: Loss 0.02362518385052681\n",
      "Training Batch [349/782]: Loss 0.04538973048329353\n",
      "Training Batch [350/782]: Loss 0.20578116178512573\n",
      "Training Batch [351/782]: Loss 0.10859346389770508\n",
      "Training Batch [352/782]: Loss 0.035747919231653214\n",
      "Training Batch [353/782]: Loss 0.09382743388414383\n",
      "Training Batch [354/782]: Loss 0.06928089261054993\n",
      "Training Batch [355/782]: Loss 0.25100260972976685\n",
      "Training Batch [356/782]: Loss 0.071922168135643\n",
      "Training Batch [357/782]: Loss 0.17143979668617249\n",
      "Training Batch [358/782]: Loss 0.09743641316890717\n",
      "Training Batch [359/782]: Loss 0.13231316208839417\n",
      "Training Batch [360/782]: Loss 0.20823869109153748\n",
      "Training Batch [361/782]: Loss 0.048474062234163284\n",
      "Training Batch [362/782]: Loss 0.023741986602544785\n",
      "Training Batch [363/782]: Loss 0.07250268012285233\n",
      "Training Batch [364/782]: Loss 0.04473024606704712\n",
      "Training Batch [365/782]: Loss 0.07948436588048935\n",
      "Training Batch [366/782]: Loss 0.021022627130150795\n",
      "Training Batch [367/782]: Loss 0.09284964203834534\n",
      "Training Batch [368/782]: Loss 0.12246038019657135\n",
      "Training Batch [369/782]: Loss 0.04371616989374161\n",
      "Training Batch [370/782]: Loss 0.05110448971390724\n",
      "Training Batch [371/782]: Loss 0.19106051325798035\n",
      "Training Batch [372/782]: Loss 0.10026323795318604\n",
      "Training Batch [373/782]: Loss 0.047455642372369766\n",
      "Training Batch [374/782]: Loss 0.12413739413022995\n",
      "Training Batch [375/782]: Loss 0.10097850859165192\n",
      "Training Batch [376/782]: Loss 0.0350487194955349\n",
      "Training Batch [377/782]: Loss 0.05373234301805496\n",
      "Training Batch [378/782]: Loss 0.07736589014530182\n",
      "Training Batch [379/782]: Loss 0.04957713931798935\n",
      "Training Batch [380/782]: Loss 0.08909853547811508\n",
      "Training Batch [381/782]: Loss 0.2380707561969757\n",
      "Training Batch [382/782]: Loss 0.059085678309202194\n",
      "Training Batch [383/782]: Loss 0.06706798076629639\n",
      "Training Batch [384/782]: Loss 0.07676097750663757\n",
      "Training Batch [385/782]: Loss 0.032922860234975815\n",
      "Training Batch [386/782]: Loss 0.07080885767936707\n",
      "Training Batch [387/782]: Loss 0.14753256738185883\n",
      "Training Batch [388/782]: Loss 0.05113168805837631\n",
      "Training Batch [389/782]: Loss 0.11739838868379593\n",
      "Training Batch [390/782]: Loss 0.07537999004125595\n",
      "Training Batch [391/782]: Loss 0.052881453186273575\n",
      "Training Batch [392/782]: Loss 0.06265580654144287\n",
      "Training Batch [393/782]: Loss 0.08904322236776352\n",
      "Training Batch [394/782]: Loss 0.10157062113285065\n",
      "Training Batch [395/782]: Loss 0.015418645925819874\n",
      "Training Batch [396/782]: Loss 0.20920974016189575\n",
      "Training Batch [397/782]: Loss 0.029479680582880974\n",
      "Training Batch [398/782]: Loss 0.08130598068237305\n",
      "Training Batch [399/782]: Loss 0.06699196249246597\n",
      "Training Batch [400/782]: Loss 0.11172166466712952\n",
      "Training Batch [401/782]: Loss 0.1372869610786438\n",
      "Training Batch [402/782]: Loss 0.06034766882658005\n",
      "Training Batch [403/782]: Loss 0.061821065843105316\n",
      "Training Batch [404/782]: Loss 0.08420003950595856\n",
      "Training Batch [405/782]: Loss 0.11325305700302124\n",
      "Training Batch [406/782]: Loss 0.21094506978988647\n",
      "Training Batch [407/782]: Loss 0.08002423495054245\n",
      "Training Batch [408/782]: Loss 0.09890854358673096\n",
      "Training Batch [409/782]: Loss 0.028317494317889214\n",
      "Training Batch [410/782]: Loss 0.08151159435510635\n",
      "Training Batch [411/782]: Loss 0.1621176153421402\n",
      "Training Batch [412/782]: Loss 0.12019745260477066\n",
      "Training Batch [413/782]: Loss 0.08322389423847198\n",
      "Training Batch [414/782]: Loss 0.21763533353805542\n",
      "Training Batch [415/782]: Loss 0.13590165972709656\n",
      "Training Batch [416/782]: Loss 0.08781128376722336\n",
      "Training Batch [417/782]: Loss 0.2677341103553772\n",
      "Training Batch [418/782]: Loss 0.03293103352189064\n",
      "Training Batch [419/782]: Loss 0.0624953955411911\n",
      "Training Batch [420/782]: Loss 0.10406047105789185\n",
      "Training Batch [421/782]: Loss 0.08030145615339279\n",
      "Training Batch [422/782]: Loss 0.17844723165035248\n",
      "Training Batch [423/782]: Loss 0.11020350456237793\n",
      "Training Batch [424/782]: Loss 0.09514009952545166\n",
      "Training Batch [425/782]: Loss 0.2892882525920868\n",
      "Training Batch [426/782]: Loss 0.09890636801719666\n",
      "Training Batch [427/782]: Loss 0.15260855853557587\n",
      "Training Batch [428/782]: Loss 0.06827514618635178\n",
      "Training Batch [429/782]: Loss 0.1527853012084961\n",
      "Training Batch [430/782]: Loss 0.10523808747529984\n",
      "Training Batch [431/782]: Loss 0.08830462396144867\n",
      "Training Batch [432/782]: Loss 0.019316820427775383\n",
      "Training Batch [433/782]: Loss 0.0647357851266861\n",
      "Training Batch [434/782]: Loss 0.09062401950359344\n",
      "Training Batch [435/782]: Loss 0.04333387687802315\n",
      "Training Batch [436/782]: Loss 0.047601159662008286\n",
      "Training Batch [437/782]: Loss 0.09189821779727936\n",
      "Training Batch [438/782]: Loss 0.08433514088392258\n",
      "Training Batch [439/782]: Loss 0.0927538201212883\n",
      "Training Batch [440/782]: Loss 0.14829809963703156\n",
      "Training Batch [441/782]: Loss 0.06945925951004028\n",
      "Training Batch [442/782]: Loss 0.14762118458747864\n",
      "Training Batch [443/782]: Loss 0.10966184735298157\n",
      "Training Batch [444/782]: Loss 0.08821063488721848\n",
      "Training Batch [445/782]: Loss 0.07951036095619202\n",
      "Training Batch [446/782]: Loss 0.09940382838249207\n",
      "Training Batch [447/782]: Loss 0.07258694618940353\n",
      "Training Batch [448/782]: Loss 0.12849093973636627\n",
      "Training Batch [449/782]: Loss 0.06715380400419235\n",
      "Training Batch [450/782]: Loss 0.07331488281488419\n",
      "Training Batch [451/782]: Loss 0.055545829236507416\n",
      "Training Batch [452/782]: Loss 0.07808862626552582\n",
      "Training Batch [453/782]: Loss 0.09057348221540451\n",
      "Training Batch [454/782]: Loss 0.1579635739326477\n",
      "Training Batch [455/782]: Loss 0.15158839523792267\n",
      "Training Batch [456/782]: Loss 0.06395205110311508\n",
      "Training Batch [457/782]: Loss 0.06787953525781631\n",
      "Training Batch [458/782]: Loss 0.05400330200791359\n",
      "Training Batch [459/782]: Loss 0.04826904460787773\n",
      "Training Batch [460/782]: Loss 0.03524841368198395\n",
      "Training Batch [461/782]: Loss 0.04326220974326134\n",
      "Training Batch [462/782]: Loss 0.04801645129919052\n",
      "Training Batch [463/782]: Loss 0.12941880524158478\n",
      "Training Batch [464/782]: Loss 0.052606020122766495\n",
      "Training Batch [465/782]: Loss 0.05930561572313309\n",
      "Training Batch [466/782]: Loss 0.015273896045982838\n",
      "Training Batch [467/782]: Loss 0.040253277868032455\n",
      "Training Batch [468/782]: Loss 0.15759356319904327\n",
      "Training Batch [469/782]: Loss 0.07374507933855057\n",
      "Training Batch [470/782]: Loss 0.03170837089419365\n",
      "Training Batch [471/782]: Loss 0.07371382415294647\n",
      "Training Batch [472/782]: Loss 0.0910990759730339\n",
      "Training Batch [473/782]: Loss 0.1538696438074112\n",
      "Training Batch [474/782]: Loss 0.009054119698703289\n",
      "Training Batch [475/782]: Loss 0.029587628319859505\n",
      "Training Batch [476/782]: Loss 0.020374884828925133\n",
      "Training Batch [477/782]: Loss 0.03448863327503204\n",
      "Training Batch [478/782]: Loss 0.08204098045825958\n",
      "Training Batch [479/782]: Loss 0.09517725557088852\n",
      "Training Batch [480/782]: Loss 0.04204734042286873\n",
      "Training Batch [481/782]: Loss 0.15115150809288025\n",
      "Training Batch [482/782]: Loss 0.07363130152225494\n",
      "Training Batch [483/782]: Loss 0.1118297278881073\n",
      "Training Batch [484/782]: Loss 0.08733421564102173\n",
      "Training Batch [485/782]: Loss 0.20398543775081635\n",
      "Training Batch [486/782]: Loss 0.09752733260393143\n",
      "Training Batch [487/782]: Loss 0.014988517388701439\n",
      "Training Batch [488/782]: Loss 0.07868185639381409\n",
      "Training Batch [489/782]: Loss 0.03897005692124367\n",
      "Training Batch [490/782]: Loss 0.1425466239452362\n",
      "Training Batch [491/782]: Loss 0.19859245419502258\n",
      "Training Batch [492/782]: Loss 0.07641308009624481\n",
      "Training Batch [493/782]: Loss 0.07636525481939316\n",
      "Training Batch [494/782]: Loss 0.11357381939888\n",
      "Training Batch [495/782]: Loss 0.11027085781097412\n",
      "Training Batch [496/782]: Loss 0.07088489830493927\n",
      "Training Batch [497/782]: Loss 0.04556049406528473\n",
      "Training Batch [498/782]: Loss 0.13681752979755402\n",
      "Training Batch [499/782]: Loss 0.09262684732675552\n",
      "Training Batch [500/782]: Loss 0.14148132503032684\n",
      "Training Batch [501/782]: Loss 0.16689947247505188\n",
      "Training Batch [502/782]: Loss 0.05169197544455528\n",
      "Training Batch [503/782]: Loss 0.1125262975692749\n",
      "Training Batch [504/782]: Loss 0.2773258090019226\n",
      "Training Batch [505/782]: Loss 0.04799016937613487\n",
      "Training Batch [506/782]: Loss 0.14906282722949982\n",
      "Training Batch [507/782]: Loss 0.09108473360538483\n",
      "Training Batch [508/782]: Loss 0.12202150374650955\n",
      "Training Batch [509/782]: Loss 0.11989070475101471\n",
      "Training Batch [510/782]: Loss 0.06123114377260208\n",
      "Training Batch [511/782]: Loss 0.06398975849151611\n",
      "Training Batch [512/782]: Loss 0.1094583049416542\n",
      "Training Batch [513/782]: Loss 0.07888432592153549\n",
      "Training Batch [514/782]: Loss 0.1395968645811081\n",
      "Training Batch [515/782]: Loss 0.0638815313577652\n",
      "Training Batch [516/782]: Loss 0.08292640000581741\n",
      "Training Batch [517/782]: Loss 0.24615973234176636\n",
      "Training Batch [518/782]: Loss 0.044905561953783035\n",
      "Training Batch [519/782]: Loss 0.05505218356847763\n",
      "Training Batch [520/782]: Loss 0.07973302900791168\n",
      "Training Batch [521/782]: Loss 0.16803382337093353\n",
      "Training Batch [522/782]: Loss 0.05904865637421608\n",
      "Training Batch [523/782]: Loss 0.15499722957611084\n",
      "Training Batch [524/782]: Loss 0.04260151460766792\n",
      "Training Batch [525/782]: Loss 0.07412943989038467\n",
      "Training Batch [526/782]: Loss 0.031422242522239685\n",
      "Training Batch [527/782]: Loss 0.11157679557800293\n",
      "Training Batch [528/782]: Loss 0.09630794078111649\n",
      "Training Batch [529/782]: Loss 0.20055031776428223\n",
      "Training Batch [530/782]: Loss 0.032005954533815384\n",
      "Training Batch [531/782]: Loss 0.06444796174764633\n",
      "Training Batch [532/782]: Loss 0.16375033557415009\n",
      "Training Batch [533/782]: Loss 0.13932399451732635\n",
      "Training Batch [534/782]: Loss 0.04085827246308327\n",
      "Training Batch [535/782]: Loss 0.11421234160661697\n",
      "Training Batch [536/782]: Loss 0.03605807572603226\n",
      "Training Batch [537/782]: Loss 0.14232510328292847\n",
      "Training Batch [538/782]: Loss 0.11948566138744354\n",
      "Training Batch [539/782]: Loss 0.09802499413490295\n",
      "Training Batch [540/782]: Loss 0.037397969514131546\n",
      "Training Batch [541/782]: Loss 0.08841145783662796\n",
      "Training Batch [542/782]: Loss 0.09121326357126236\n",
      "Training Batch [543/782]: Loss 0.01883772574365139\n",
      "Training Batch [544/782]: Loss 0.080540232360363\n",
      "Training Batch [545/782]: Loss 0.05121050029993057\n",
      "Training Batch [546/782]: Loss 0.035200636833906174\n",
      "Training Batch [547/782]: Loss 0.2074604332447052\n",
      "Training Batch [548/782]: Loss 0.05322415381669998\n",
      "Training Batch [549/782]: Loss 0.15324929356575012\n",
      "Training Batch [550/782]: Loss 0.08273603767156601\n",
      "Training Batch [551/782]: Loss 0.087304025888443\n",
      "Training Batch [552/782]: Loss 0.08208204805850983\n",
      "Training Batch [553/782]: Loss 0.20290279388427734\n",
      "Training Batch [554/782]: Loss 0.20623892545700073\n",
      "Training Batch [555/782]: Loss 0.09438088536262512\n",
      "Training Batch [556/782]: Loss 0.1478593796491623\n",
      "Training Batch [557/782]: Loss 0.07527381181716919\n",
      "Training Batch [558/782]: Loss 0.08500063419342041\n",
      "Training Batch [559/782]: Loss 0.05798932909965515\n",
      "Training Batch [560/782]: Loss 0.08604695647954941\n",
      "Training Batch [561/782]: Loss 0.07651545852422714\n",
      "Training Batch [562/782]: Loss 0.10495156794786453\n",
      "Training Batch [563/782]: Loss 0.0818338692188263\n",
      "Training Batch [564/782]: Loss 0.07477884739637375\n",
      "Training Batch [565/782]: Loss 0.07956603914499283\n",
      "Training Batch [566/782]: Loss 0.04434875398874283\n",
      "Training Batch [567/782]: Loss 0.01234603300690651\n",
      "Training Batch [568/782]: Loss 0.05073171108961105\n",
      "Training Batch [569/782]: Loss 0.03109571523964405\n",
      "Training Batch [570/782]: Loss 0.04405165836215019\n",
      "Training Batch [571/782]: Loss 0.058581817895174026\n",
      "Training Batch [572/782]: Loss 0.11575546115636826\n",
      "Training Batch [573/782]: Loss 0.031532932072877884\n",
      "Training Batch [574/782]: Loss 0.08652320504188538\n",
      "Training Batch [575/782]: Loss 0.10236294567584991\n",
      "Training Batch [576/782]: Loss 0.03400246053934097\n",
      "Training Batch [577/782]: Loss 0.07484140992164612\n",
      "Training Batch [578/782]: Loss 0.0481211356818676\n",
      "Training Batch [579/782]: Loss 0.10906421393156052\n",
      "Training Batch [580/782]: Loss 0.0459776446223259\n",
      "Training Batch [581/782]: Loss 0.10835675150156021\n",
      "Training Batch [582/782]: Loss 0.05622595176100731\n",
      "Training Batch [583/782]: Loss 0.15218354761600494\n",
      "Training Batch [584/782]: Loss 0.049905434250831604\n",
      "Training Batch [585/782]: Loss 0.07482137531042099\n",
      "Training Batch [586/782]: Loss 0.19692859053611755\n",
      "Training Batch [587/782]: Loss 0.022525472566485405\n",
      "Training Batch [588/782]: Loss 0.15358124673366547\n",
      "Training Batch [589/782]: Loss 0.1151682436466217\n",
      "Training Batch [590/782]: Loss 0.09304554015398026\n",
      "Training Batch [591/782]: Loss 0.1150517463684082\n",
      "Training Batch [592/782]: Loss 0.06195167824625969\n",
      "Training Batch [593/782]: Loss 0.04596883803606033\n",
      "Training Batch [594/782]: Loss 0.052254706621170044\n",
      "Training Batch [595/782]: Loss 0.05766027420759201\n",
      "Training Batch [596/782]: Loss 0.09823804348707199\n",
      "Training Batch [597/782]: Loss 0.07090545445680618\n",
      "Training Batch [598/782]: Loss 0.053703173995018005\n",
      "Training Batch [599/782]: Loss 0.12258739769458771\n",
      "Training Batch [600/782]: Loss 0.1225174069404602\n",
      "Training Batch [601/782]: Loss 0.08337248116731644\n",
      "Training Batch [602/782]: Loss 0.029107173904776573\n",
      "Training Batch [603/782]: Loss 0.08034541457891464\n",
      "Training Batch [604/782]: Loss 0.09410369396209717\n",
      "Training Batch [605/782]: Loss 0.02648666314780712\n",
      "Training Batch [606/782]: Loss 0.058429643511772156\n",
      "Training Batch [607/782]: Loss 0.07731250673532486\n",
      "Training Batch [608/782]: Loss 0.11250803619623184\n",
      "Training Batch [609/782]: Loss 0.13451267778873444\n",
      "Training Batch [610/782]: Loss 0.06505362689495087\n",
      "Training Batch [611/782]: Loss 0.03062772937119007\n",
      "Training Batch [612/782]: Loss 0.07410438358783722\n",
      "Training Batch [613/782]: Loss 0.23376482725143433\n",
      "Training Batch [614/782]: Loss 0.12333841621875763\n",
      "Training Batch [615/782]: Loss 0.18576203286647797\n",
      "Training Batch [616/782]: Loss 0.05250902846455574\n",
      "Training Batch [617/782]: Loss 0.044570840895175934\n",
      "Training Batch [618/782]: Loss 0.10987724363803864\n",
      "Training Batch [619/782]: Loss 0.12029369920492172\n",
      "Training Batch [620/782]: Loss 0.15780845284461975\n",
      "Training Batch [621/782]: Loss 0.12236099690198898\n",
      "Training Batch [622/782]: Loss 0.1801440715789795\n",
      "Training Batch [623/782]: Loss 0.16978783905506134\n",
      "Training Batch [624/782]: Loss 0.026030145585536957\n",
      "Training Batch [625/782]: Loss 0.06128481775522232\n",
      "Training Batch [626/782]: Loss 0.09580998122692108\n",
      "Training Batch [627/782]: Loss 0.16441170871257782\n",
      "Training Batch [628/782]: Loss 0.10214663296937943\n",
      "Training Batch [629/782]: Loss 0.1283428817987442\n",
      "Training Batch [630/782]: Loss 0.08530321717262268\n",
      "Training Batch [631/782]: Loss 0.13032087683677673\n",
      "Training Batch [632/782]: Loss 0.027580060064792633\n",
      "Training Batch [633/782]: Loss 0.06945552676916122\n",
      "Training Batch [634/782]: Loss 0.029854927211999893\n",
      "Training Batch [635/782]: Loss 0.03530042991042137\n",
      "Training Batch [636/782]: Loss 0.14922352135181427\n",
      "Training Batch [637/782]: Loss 0.0721694603562355\n",
      "Training Batch [638/782]: Loss 0.15968216955661774\n",
      "Training Batch [639/782]: Loss 0.1128043457865715\n",
      "Training Batch [640/782]: Loss 0.10902746021747589\n",
      "Training Batch [641/782]: Loss 0.04442797228693962\n",
      "Training Batch [642/782]: Loss 0.16220146417617798\n",
      "Training Batch [643/782]: Loss 0.030215011909604073\n",
      "Training Batch [644/782]: Loss 0.03595403954386711\n",
      "Training Batch [645/782]: Loss 0.17274504899978638\n",
      "Training Batch [646/782]: Loss 0.10956262052059174\n",
      "Training Batch [647/782]: Loss 0.06565888226032257\n",
      "Training Batch [648/782]: Loss 0.13601097464561462\n",
      "Training Batch [649/782]: Loss 0.18173439800739288\n",
      "Training Batch [650/782]: Loss 0.156965434551239\n",
      "Training Batch [651/782]: Loss 0.063153475522995\n",
      "Training Batch [652/782]: Loss 0.039863135665655136\n",
      "Training Batch [653/782]: Loss 0.062353335320949554\n",
      "Training Batch [654/782]: Loss 0.25801825523376465\n",
      "Training Batch [655/782]: Loss 0.17188192903995514\n",
      "Training Batch [656/782]: Loss 0.03508682921528816\n",
      "Training Batch [657/782]: Loss 0.2151370793581009\n",
      "Training Batch [658/782]: Loss 0.1561468541622162\n",
      "Training Batch [659/782]: Loss 0.11467089504003525\n",
      "Training Batch [660/782]: Loss 0.043596550822257996\n",
      "Training Batch [661/782]: Loss 0.20115788280963898\n",
      "Training Batch [662/782]: Loss 0.24807529151439667\n",
      "Training Batch [663/782]: Loss 0.14387749135494232\n",
      "Training Batch [664/782]: Loss 0.06093685328960419\n",
      "Training Batch [665/782]: Loss 0.04727432504296303\n",
      "Training Batch [666/782]: Loss 0.16451534628868103\n",
      "Training Batch [667/782]: Loss 0.06480538100004196\n",
      "Training Batch [668/782]: Loss 0.344215989112854\n",
      "Training Batch [669/782]: Loss 0.096577487885952\n",
      "Training Batch [670/782]: Loss 0.2846910059452057\n",
      "Training Batch [671/782]: Loss 0.09900843352079391\n",
      "Training Batch [672/782]: Loss 0.12350298464298248\n",
      "Training Batch [673/782]: Loss 0.10199358314275742\n",
      "Training Batch [674/782]: Loss 0.06937812268733978\n",
      "Training Batch [675/782]: Loss 0.024738440290093422\n",
      "Training Batch [676/782]: Loss 0.06801365315914154\n",
      "Training Batch [677/782]: Loss 0.13144811987876892\n",
      "Training Batch [678/782]: Loss 0.11364494264125824\n",
      "Training Batch [679/782]: Loss 0.09354116767644882\n",
      "Training Batch [680/782]: Loss 0.13936248421669006\n",
      "Training Batch [681/782]: Loss 0.13001137971878052\n",
      "Training Batch [682/782]: Loss 0.09696277230978012\n",
      "Training Batch [683/782]: Loss 0.061210308223962784\n",
      "Training Batch [684/782]: Loss 0.11224973201751709\n",
      "Training Batch [685/782]: Loss 0.03719714283943176\n",
      "Training Batch [686/782]: Loss 0.08774598687887192\n",
      "Training Batch [687/782]: Loss 0.13875867426395416\n",
      "Training Batch [688/782]: Loss 0.17222964763641357\n",
      "Training Batch [689/782]: Loss 0.15991105139255524\n",
      "Training Batch [690/782]: Loss 0.17352105677127838\n",
      "Training Batch [691/782]: Loss 0.10627418756484985\n",
      "Training Batch [692/782]: Loss 0.08435278385877609\n",
      "Training Batch [693/782]: Loss 0.12318503856658936\n",
      "Training Batch [694/782]: Loss 0.05153004080057144\n",
      "Training Batch [695/782]: Loss 0.13078083097934723\n",
      "Training Batch [696/782]: Loss 0.033177539706230164\n",
      "Training Batch [697/782]: Loss 0.23431235551834106\n",
      "Training Batch [698/782]: Loss 0.09391986578702927\n",
      "Training Batch [699/782]: Loss 0.21226681768894196\n",
      "Training Batch [700/782]: Loss 0.1616223007440567\n",
      "Training Batch [701/782]: Loss 0.20272885262966156\n",
      "Training Batch [702/782]: Loss 0.1628691852092743\n",
      "Training Batch [703/782]: Loss 0.1881854236125946\n",
      "Training Batch [704/782]: Loss 0.12881718575954437\n",
      "Training Batch [705/782]: Loss 0.11485378444194794\n",
      "Training Batch [706/782]: Loss 0.22056230902671814\n",
      "Training Batch [707/782]: Loss 0.14393669366836548\n",
      "Training Batch [708/782]: Loss 0.0674641877412796\n",
      "Training Batch [709/782]: Loss 0.18401579558849335\n",
      "Training Batch [710/782]: Loss 0.11564725637435913\n",
      "Training Batch [711/782]: Loss 0.11680133640766144\n",
      "Training Batch [712/782]: Loss 0.03720111399888992\n",
      "Training Batch [713/782]: Loss 0.05305107682943344\n",
      "Training Batch [714/782]: Loss 0.1637512445449829\n",
      "Training Batch [715/782]: Loss 0.16405344009399414\n",
      "Training Batch [716/782]: Loss 0.10210750252008438\n",
      "Training Batch [717/782]: Loss 0.1611829549074173\n",
      "Training Batch [718/782]: Loss 0.3425266742706299\n",
      "Training Batch [719/782]: Loss 0.12282232940196991\n",
      "Training Batch [720/782]: Loss 0.1230660155415535\n",
      "Training Batch [721/782]: Loss 0.11359496414661407\n",
      "Training Batch [722/782]: Loss 0.16912318766117096\n",
      "Training Batch [723/782]: Loss 0.18919910490512848\n",
      "Training Batch [724/782]: Loss 0.0800425186753273\n",
      "Training Batch [725/782]: Loss 0.2897813022136688\n",
      "Training Batch [726/782]: Loss 0.13416387140750885\n",
      "Training Batch [727/782]: Loss 0.030179370194673538\n",
      "Training Batch [728/782]: Loss 0.14430803060531616\n",
      "Training Batch [729/782]: Loss 0.08840355277061462\n",
      "Training Batch [730/782]: Loss 0.13023686408996582\n",
      "Training Batch [731/782]: Loss 0.2641576826572418\n",
      "Training Batch [732/782]: Loss 0.1301719844341278\n",
      "Training Batch [733/782]: Loss 0.14659056067466736\n",
      "Training Batch [734/782]: Loss 0.13045348227024078\n",
      "Training Batch [735/782]: Loss 0.09337172657251358\n",
      "Training Batch [736/782]: Loss 0.17454573512077332\n",
      "Training Batch [737/782]: Loss 0.12079133093357086\n",
      "Training Batch [738/782]: Loss 0.18643805384635925\n",
      "Training Batch [739/782]: Loss 0.19437941908836365\n",
      "Training Batch [740/782]: Loss 0.09661280363798141\n",
      "Training Batch [741/782]: Loss 0.12826508283615112\n",
      "Training Batch [742/782]: Loss 0.19172774255275726\n",
      "Training Batch [743/782]: Loss 0.06188102811574936\n",
      "Training Batch [744/782]: Loss 0.08292596787214279\n",
      "Training Batch [745/782]: Loss 0.09649396687746048\n",
      "Training Batch [746/782]: Loss 0.158468097448349\n",
      "Training Batch [747/782]: Loss 0.13407787680625916\n",
      "Training Batch [748/782]: Loss 0.2551833391189575\n",
      "Training Batch [749/782]: Loss 0.10146747529506683\n",
      "Training Batch [750/782]: Loss 0.0625615194439888\n",
      "Training Batch [751/782]: Loss 0.11580376327037811\n",
      "Training Batch [752/782]: Loss 0.20469051599502563\n",
      "Training Batch [753/782]: Loss 0.1122954711318016\n",
      "Training Batch [754/782]: Loss 0.20125092566013336\n",
      "Training Batch [755/782]: Loss 0.0840727910399437\n",
      "Training Batch [756/782]: Loss 0.05265101417899132\n",
      "Training Batch [757/782]: Loss 0.03556501120328903\n",
      "Training Batch [758/782]: Loss 0.21149782836437225\n",
      "Training Batch [759/782]: Loss 0.10729938000440598\n",
      "Training Batch [760/782]: Loss 0.20978230237960815\n",
      "Training Batch [761/782]: Loss 0.02913510426878929\n",
      "Training Batch [762/782]: Loss 0.04368468001484871\n",
      "Training Batch [763/782]: Loss 0.19657084345817566\n",
      "Training Batch [764/782]: Loss 0.05274714529514313\n",
      "Training Batch [765/782]: Loss 0.04464923217892647\n",
      "Training Batch [766/782]: Loss 0.09384101629257202\n",
      "Training Batch [767/782]: Loss 0.11729733645915985\n",
      "Training Batch [768/782]: Loss 0.21041487157344818\n",
      "Training Batch [769/782]: Loss 0.18782499432563782\n",
      "Training Batch [770/782]: Loss 0.2003590315580368\n",
      "Training Batch [771/782]: Loss 0.11501644551753998\n",
      "Training Batch [772/782]: Loss 0.1473434865474701\n",
      "Training Batch [773/782]: Loss 0.08418144285678864\n",
      "Training Batch [774/782]: Loss 0.09876550734043121\n",
      "Training Batch [775/782]: Loss 0.028663650155067444\n",
      "Training Batch [776/782]: Loss 0.17495952546596527\n",
      "Training Batch [777/782]: Loss 0.15590991079807281\n",
      "Training Batch [778/782]: Loss 0.1563449501991272\n",
      "Training Batch [779/782]: Loss 0.11912646889686584\n",
      "Training Batch [780/782]: Loss 0.14114631712436676\n",
      "Training Batch [781/782]: Loss 0.15035009384155273\n",
      "Training Batch [782/782]: Loss 0.485989511013031\n",
      "Epoch 13 - Train Loss: 0.0899\n",
      "*********  Epoch 14/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.14592775702476501\n",
      "Training Batch [2/782]: Loss 0.25831320881843567\n",
      "Training Batch [3/782]: Loss 0.06616552919149399\n",
      "Training Batch [4/782]: Loss 0.1134720966219902\n",
      "Training Batch [5/782]: Loss 0.020713452249765396\n",
      "Training Batch [6/782]: Loss 0.13476955890655518\n",
      "Training Batch [7/782]: Loss 0.1343328058719635\n",
      "Training Batch [8/782]: Loss 0.1792917400598526\n",
      "Training Batch [9/782]: Loss 0.08746813237667084\n",
      "Training Batch [10/782]: Loss 0.0973295196890831\n",
      "Training Batch [11/782]: Loss 0.11680733412504196\n",
      "Training Batch [12/782]: Loss 0.1701159030199051\n",
      "Training Batch [13/782]: Loss 0.11457806825637817\n",
      "Training Batch [14/782]: Loss 0.19185738265514374\n",
      "Training Batch [15/782]: Loss 0.09391165524721146\n",
      "Training Batch [16/782]: Loss 0.12388303875923157\n",
      "Training Batch [17/782]: Loss 0.2185559868812561\n",
      "Training Batch [18/782]: Loss 0.07617458701133728\n",
      "Training Batch [19/782]: Loss 0.20622961223125458\n",
      "Training Batch [20/782]: Loss 0.04595644399523735\n",
      "Training Batch [21/782]: Loss 0.23153671622276306\n",
      "Training Batch [22/782]: Loss 0.04157308116555214\n",
      "Training Batch [23/782]: Loss 0.11144659668207169\n",
      "Training Batch [24/782]: Loss 0.1746167689561844\n",
      "Training Batch [25/782]: Loss 0.12117981165647507\n",
      "Training Batch [26/782]: Loss 0.15540581941604614\n",
      "Training Batch [27/782]: Loss 0.05493885278701782\n",
      "Training Batch [28/782]: Loss 0.03470510244369507\n",
      "Training Batch [29/782]: Loss 0.12376265972852707\n",
      "Training Batch [30/782]: Loss 0.08862031996250153\n",
      "Training Batch [31/782]: Loss 0.08994723111391068\n",
      "Training Batch [32/782]: Loss 0.08950745314359665\n",
      "Training Batch [33/782]: Loss 0.21550314128398895\n",
      "Training Batch [34/782]: Loss 0.047438204288482666\n",
      "Training Batch [35/782]: Loss 0.08923211693763733\n",
      "Training Batch [36/782]: Loss 0.11732903867959976\n",
      "Training Batch [37/782]: Loss 0.2618180811405182\n",
      "Training Batch [38/782]: Loss 0.12845203280448914\n",
      "Training Batch [39/782]: Loss 0.07724505662918091\n",
      "Training Batch [40/782]: Loss 0.05971464514732361\n",
      "Training Batch [41/782]: Loss 0.06711765378713608\n",
      "Training Batch [42/782]: Loss 0.03171283379197121\n",
      "Training Batch [43/782]: Loss 0.05845112353563309\n",
      "Training Batch [44/782]: Loss 0.09028493613004684\n",
      "Training Batch [45/782]: Loss 0.0751776248216629\n",
      "Training Batch [46/782]: Loss 0.07720492780208588\n",
      "Training Batch [47/782]: Loss 0.12368220835924149\n",
      "Training Batch [48/782]: Loss 0.19450704753398895\n",
      "Training Batch [49/782]: Loss 0.025445792824029922\n",
      "Training Batch [50/782]: Loss 0.050989724695682526\n",
      "Training Batch [51/782]: Loss 0.07804328203201294\n",
      "Training Batch [52/782]: Loss 0.151524156332016\n",
      "Training Batch [53/782]: Loss 0.2308911830186844\n",
      "Training Batch [54/782]: Loss 0.03201989829540253\n",
      "Training Batch [55/782]: Loss 0.16626855731010437\n",
      "Training Batch [56/782]: Loss 0.06796296685934067\n",
      "Training Batch [57/782]: Loss 0.021245697513222694\n",
      "Training Batch [58/782]: Loss 0.09105230122804642\n",
      "Training Batch [59/782]: Loss 0.08573547005653381\n",
      "Training Batch [60/782]: Loss 0.12148967385292053\n",
      "Training Batch [61/782]: Loss 0.06178998947143555\n",
      "Training Batch [62/782]: Loss 0.07588201016187668\n",
      "Training Batch [63/782]: Loss 0.04698517173528671\n",
      "Training Batch [64/782]: Loss 0.08017446845769882\n",
      "Training Batch [65/782]: Loss 0.0105330441147089\n",
      "Training Batch [66/782]: Loss 0.05845453217625618\n",
      "Training Batch [67/782]: Loss 0.10609313100576401\n",
      "Training Batch [68/782]: Loss 0.032059185206890106\n",
      "Training Batch [69/782]: Loss 0.09460243582725525\n",
      "Training Batch [70/782]: Loss 0.10983320325613022\n",
      "Training Batch [71/782]: Loss 0.08131199330091476\n",
      "Training Batch [72/782]: Loss 0.22515743970870972\n",
      "Training Batch [73/782]: Loss 0.03699555993080139\n",
      "Training Batch [74/782]: Loss 0.06391075998544693\n",
      "Training Batch [75/782]: Loss 0.09253261983394623\n",
      "Training Batch [76/782]: Loss 0.061180517077445984\n",
      "Training Batch [77/782]: Loss 0.15789563953876495\n",
      "Training Batch [78/782]: Loss 0.09064308553934097\n",
      "Training Batch [79/782]: Loss 0.15635207295417786\n",
      "Training Batch [80/782]: Loss 0.05176906660199165\n",
      "Training Batch [81/782]: Loss 0.04890473932027817\n",
      "Training Batch [82/782]: Loss 0.2801996171474457\n",
      "Training Batch [83/782]: Loss 0.08810120075941086\n",
      "Training Batch [84/782]: Loss 0.09349635243415833\n",
      "Training Batch [85/782]: Loss 0.03512534499168396\n",
      "Training Batch [86/782]: Loss 0.037233758717775345\n",
      "Training Batch [87/782]: Loss 0.07219142466783524\n",
      "Training Batch [88/782]: Loss 0.06436395645141602\n",
      "Training Batch [89/782]: Loss 0.05171528086066246\n",
      "Training Batch [90/782]: Loss 0.11873598396778107\n",
      "Training Batch [91/782]: Loss 0.03021872229874134\n",
      "Training Batch [92/782]: Loss 0.16213762760162354\n",
      "Training Batch [93/782]: Loss 0.12959080934524536\n",
      "Training Batch [94/782]: Loss 0.1092371717095375\n",
      "Training Batch [95/782]: Loss 0.06269019097089767\n",
      "Training Batch [96/782]: Loss 0.011917251162230968\n",
      "Training Batch [97/782]: Loss 0.10047822445631027\n",
      "Training Batch [98/782]: Loss 0.132241353392601\n",
      "Training Batch [99/782]: Loss 0.20716746151447296\n",
      "Training Batch [100/782]: Loss 0.15217235684394836\n",
      "Training Batch [101/782]: Loss 0.0646137222647667\n",
      "Training Batch [102/782]: Loss 0.03335415571928024\n",
      "Training Batch [103/782]: Loss 0.08843721449375153\n",
      "Training Batch [104/782]: Loss 0.04486033320426941\n",
      "Training Batch [105/782]: Loss 0.1549815535545349\n",
      "Training Batch [106/782]: Loss 0.1563487946987152\n",
      "Training Batch [107/782]: Loss 0.08111552149057388\n",
      "Training Batch [108/782]: Loss 0.05622800067067146\n",
      "Training Batch [109/782]: Loss 0.11875896155834198\n",
      "Training Batch [110/782]: Loss 0.11712100356817245\n",
      "Training Batch [111/782]: Loss 0.11883671581745148\n",
      "Training Batch [112/782]: Loss 0.03372088074684143\n",
      "Training Batch [113/782]: Loss 0.047346632927656174\n",
      "Training Batch [114/782]: Loss 0.045731596648693085\n",
      "Training Batch [115/782]: Loss 0.0463230274617672\n",
      "Training Batch [116/782]: Loss 0.08335597068071365\n",
      "Training Batch [117/782]: Loss 0.08012712746858597\n",
      "Training Batch [118/782]: Loss 0.14865650236606598\n",
      "Training Batch [119/782]: Loss 0.09135075658559799\n",
      "Training Batch [120/782]: Loss 0.09558648616075516\n",
      "Training Batch [121/782]: Loss 0.06407588720321655\n",
      "Training Batch [122/782]: Loss 0.12180747091770172\n",
      "Training Batch [123/782]: Loss 0.0931793749332428\n",
      "Training Batch [124/782]: Loss 0.05451132357120514\n",
      "Training Batch [125/782]: Loss 0.08150745928287506\n",
      "Training Batch [126/782]: Loss 0.04219149053096771\n",
      "Training Batch [127/782]: Loss 0.11045941710472107\n",
      "Training Batch [128/782]: Loss 0.10409369319677353\n",
      "Training Batch [129/782]: Loss 0.14612017571926117\n",
      "Training Batch [130/782]: Loss 0.07793070375919342\n",
      "Training Batch [131/782]: Loss 0.07177950441837311\n",
      "Training Batch [132/782]: Loss 0.022494817152619362\n",
      "Training Batch [133/782]: Loss 0.18161942064762115\n",
      "Training Batch [134/782]: Loss 0.06945370137691498\n",
      "Training Batch [135/782]: Loss 0.06882531940937042\n",
      "Training Batch [136/782]: Loss 0.10636652261018753\n",
      "Training Batch [137/782]: Loss 0.1653868854045868\n",
      "Training Batch [138/782]: Loss 0.060955628752708435\n",
      "Training Batch [139/782]: Loss 0.13962087035179138\n",
      "Training Batch [140/782]: Loss 0.15279057621955872\n",
      "Training Batch [141/782]: Loss 0.058741483837366104\n",
      "Training Batch [142/782]: Loss 0.12337813526391983\n",
      "Training Batch [143/782]: Loss 0.22090771794319153\n",
      "Training Batch [144/782]: Loss 0.10093194246292114\n",
      "Training Batch [145/782]: Loss 0.08755677938461304\n",
      "Training Batch [146/782]: Loss 0.050230760127305984\n",
      "Training Batch [147/782]: Loss 0.09155481308698654\n",
      "Training Batch [148/782]: Loss 0.15295954048633575\n",
      "Training Batch [149/782]: Loss 0.05468842387199402\n",
      "Training Batch [150/782]: Loss 0.08266746252775192\n",
      "Training Batch [151/782]: Loss 0.03766126185655594\n",
      "Training Batch [152/782]: Loss 0.1239955723285675\n",
      "Training Batch [153/782]: Loss 0.04379900172352791\n",
      "Training Batch [154/782]: Loss 0.08466795086860657\n",
      "Training Batch [155/782]: Loss 0.02816423960030079\n",
      "Training Batch [156/782]: Loss 0.041741594672203064\n",
      "Training Batch [157/782]: Loss 0.10299370437860489\n",
      "Training Batch [158/782]: Loss 0.0433032289147377\n",
      "Training Batch [159/782]: Loss 0.15641632676124573\n",
      "Training Batch [160/782]: Loss 0.11145531386137009\n",
      "Training Batch [161/782]: Loss 0.06739199906587601\n",
      "Training Batch [162/782]: Loss 0.09025070816278458\n",
      "Training Batch [163/782]: Loss 0.07944346964359283\n",
      "Training Batch [164/782]: Loss 0.04137920215725899\n",
      "Training Batch [165/782]: Loss 0.036038171499967575\n",
      "Training Batch [166/782]: Loss 0.08311226963996887\n",
      "Training Batch [167/782]: Loss 0.1283639520406723\n",
      "Training Batch [168/782]: Loss 0.07252625375986099\n",
      "Training Batch [169/782]: Loss 0.17544597387313843\n",
      "Training Batch [170/782]: Loss 0.04748113080859184\n",
      "Training Batch [171/782]: Loss 0.09098071604967117\n",
      "Training Batch [172/782]: Loss 0.06880547851324081\n",
      "Training Batch [173/782]: Loss 0.14176633954048157\n",
      "Training Batch [174/782]: Loss 0.01982603594660759\n",
      "Training Batch [175/782]: Loss 0.06712232530117035\n",
      "Training Batch [176/782]: Loss 0.11579150706529617\n",
      "Training Batch [177/782]: Loss 0.03860722854733467\n",
      "Training Batch [178/782]: Loss 0.24756844341754913\n",
      "Training Batch [179/782]: Loss 0.06262674927711487\n",
      "Training Batch [180/782]: Loss 0.03074849396944046\n",
      "Training Batch [181/782]: Loss 0.12260924279689789\n",
      "Training Batch [182/782]: Loss 0.042348627001047134\n",
      "Training Batch [183/782]: Loss 0.06692664325237274\n",
      "Training Batch [184/782]: Loss 0.034295644611120224\n",
      "Training Batch [185/782]: Loss 0.06015660986304283\n",
      "Training Batch [186/782]: Loss 0.1583997756242752\n",
      "Training Batch [187/782]: Loss 0.04635933041572571\n",
      "Training Batch [188/782]: Loss 0.10831030458211899\n",
      "Training Batch [189/782]: Loss 0.06935770809650421\n",
      "Training Batch [190/782]: Loss 0.03584885969758034\n",
      "Training Batch [191/782]: Loss 0.022924304008483887\n",
      "Training Batch [192/782]: Loss 0.0288642980158329\n",
      "Training Batch [193/782]: Loss 0.1262906938791275\n",
      "Training Batch [194/782]: Loss 0.05550139397382736\n",
      "Training Batch [195/782]: Loss 0.12459716200828552\n",
      "Training Batch [196/782]: Loss 0.06673266738653183\n",
      "Training Batch [197/782]: Loss 0.07639013230800629\n",
      "Training Batch [198/782]: Loss 0.06239151582121849\n",
      "Training Batch [199/782]: Loss 0.17953136563301086\n",
      "Training Batch [200/782]: Loss 0.06914256513118744\n",
      "Training Batch [201/782]: Loss 0.028568698093295097\n",
      "Training Batch [202/782]: Loss 0.11906109750270844\n",
      "Training Batch [203/782]: Loss 0.01988363265991211\n",
      "Training Batch [204/782]: Loss 0.0322163961827755\n",
      "Training Batch [205/782]: Loss 0.14755022525787354\n",
      "Training Batch [206/782]: Loss 0.023073479533195496\n",
      "Training Batch [207/782]: Loss 0.054585959762334824\n",
      "Training Batch [208/782]: Loss 0.012898932211101055\n",
      "Training Batch [209/782]: Loss 0.019271397963166237\n",
      "Training Batch [210/782]: Loss 0.04653683304786682\n",
      "Training Batch [211/782]: Loss 0.08112217485904694\n",
      "Training Batch [212/782]: Loss 0.1137494146823883\n",
      "Training Batch [213/782]: Loss 0.11254847049713135\n",
      "Training Batch [214/782]: Loss 0.024195430800318718\n",
      "Training Batch [215/782]: Loss 0.06908254325389862\n",
      "Training Batch [216/782]: Loss 0.03671559318900108\n",
      "Training Batch [217/782]: Loss 0.05638020113110542\n",
      "Training Batch [218/782]: Loss 0.04707511514425278\n",
      "Training Batch [219/782]: Loss 0.018151117488741875\n",
      "Training Batch [220/782]: Loss 0.050031378865242004\n",
      "Training Batch [221/782]: Loss 0.041027553379535675\n",
      "Training Batch [222/782]: Loss 0.04821951687335968\n",
      "Training Batch [223/782]: Loss 0.031632937490940094\n",
      "Training Batch [224/782]: Loss 0.02303234115242958\n",
      "Training Batch [225/782]: Loss 0.06490679085254669\n",
      "Training Batch [226/782]: Loss 0.16289155185222626\n",
      "Training Batch [227/782]: Loss 0.01363708358258009\n",
      "Training Batch [228/782]: Loss 0.03515346720814705\n",
      "Training Batch [229/782]: Loss 0.02161308191716671\n",
      "Training Batch [230/782]: Loss 0.014624767936766148\n",
      "Training Batch [231/782]: Loss 0.04688531905412674\n",
      "Training Batch [232/782]: Loss 0.11658148467540741\n",
      "Training Batch [233/782]: Loss 0.02593991346657276\n",
      "Training Batch [234/782]: Loss 0.03842431679368019\n",
      "Training Batch [235/782]: Loss 0.14337562024593353\n",
      "Training Batch [236/782]: Loss 0.05157265439629555\n",
      "Training Batch [237/782]: Loss 0.1658129096031189\n",
      "Training Batch [238/782]: Loss 0.03339633718132973\n",
      "Training Batch [239/782]: Loss 0.03468424826860428\n",
      "Training Batch [240/782]: Loss 0.11583378165960312\n",
      "Training Batch [241/782]: Loss 0.03166956454515457\n",
      "Training Batch [242/782]: Loss 0.011288746260106564\n",
      "Training Batch [243/782]: Loss 0.03870154172182083\n",
      "Training Batch [244/782]: Loss 0.02658279985189438\n",
      "Training Batch [245/782]: Loss 0.14538496732711792\n",
      "Training Batch [246/782]: Loss 0.03185557574033737\n",
      "Training Batch [247/782]: Loss 0.07829486578702927\n",
      "Training Batch [248/782]: Loss 0.07242963463068008\n",
      "Training Batch [249/782]: Loss 0.03232121467590332\n",
      "Training Batch [250/782]: Loss 0.08007114380598068\n",
      "Training Batch [251/782]: Loss 0.05658522993326187\n",
      "Training Batch [252/782]: Loss 0.16995593905448914\n",
      "Training Batch [253/782]: Loss 0.1343359500169754\n",
      "Training Batch [254/782]: Loss 0.010407094843685627\n",
      "Training Batch [255/782]: Loss 0.17047959566116333\n",
      "Training Batch [256/782]: Loss 0.10875064134597778\n",
      "Training Batch [257/782]: Loss 0.1239480972290039\n",
      "Training Batch [258/782]: Loss 0.11596264690160751\n",
      "Training Batch [259/782]: Loss 0.0997023656964302\n",
      "Training Batch [260/782]: Loss 0.0808030515909195\n",
      "Training Batch [261/782]: Loss 0.08190175145864487\n",
      "Training Batch [262/782]: Loss 0.09452494978904724\n",
      "Training Batch [263/782]: Loss 0.011461488902568817\n",
      "Training Batch [264/782]: Loss 0.10888772457838058\n",
      "Training Batch [265/782]: Loss 0.20901095867156982\n",
      "Training Batch [266/782]: Loss 0.014922074042260647\n",
      "Training Batch [267/782]: Loss 0.10166631639003754\n",
      "Training Batch [268/782]: Loss 0.18944399058818817\n",
      "Training Batch [269/782]: Loss 0.097384013235569\n",
      "Training Batch [270/782]: Loss 0.054693665355443954\n",
      "Training Batch [271/782]: Loss 0.08207543194293976\n",
      "Training Batch [272/782]: Loss 0.11437081545591354\n",
      "Training Batch [273/782]: Loss 0.11731875687837601\n",
      "Training Batch [274/782]: Loss 0.03191683068871498\n",
      "Training Batch [275/782]: Loss 0.059777941554784775\n",
      "Training Batch [276/782]: Loss 0.014801478944718838\n",
      "Training Batch [277/782]: Loss 0.06223559379577637\n",
      "Training Batch [278/782]: Loss 0.09119654446840286\n",
      "Training Batch [279/782]: Loss 0.10567007958889008\n",
      "Training Batch [280/782]: Loss 0.17407932877540588\n",
      "Training Batch [281/782]: Loss 0.03322771564126015\n",
      "Training Batch [282/782]: Loss 0.019341904670000076\n",
      "Training Batch [283/782]: Loss 0.046979714184999466\n",
      "Training Batch [284/782]: Loss 0.03858324885368347\n",
      "Training Batch [285/782]: Loss 0.06886223703622818\n",
      "Training Batch [286/782]: Loss 0.06058541685342789\n",
      "Training Batch [287/782]: Loss 0.041386544704437256\n",
      "Training Batch [288/782]: Loss 0.016805587336421013\n",
      "Training Batch [289/782]: Loss 0.08083673566579819\n",
      "Training Batch [290/782]: Loss 0.009854190051555634\n",
      "Training Batch [291/782]: Loss 0.020993661135435104\n",
      "Training Batch [292/782]: Loss 0.13628295063972473\n",
      "Training Batch [293/782]: Loss 0.153981015086174\n",
      "Training Batch [294/782]: Loss 0.15980252623558044\n",
      "Training Batch [295/782]: Loss 0.018264560028910637\n",
      "Training Batch [296/782]: Loss 0.006719174794852734\n",
      "Training Batch [297/782]: Loss 0.03635110706090927\n",
      "Training Batch [298/782]: Loss 0.04969913884997368\n",
      "Training Batch [299/782]: Loss 0.014563231728971004\n",
      "Training Batch [300/782]: Loss 0.12140877544879913\n",
      "Training Batch [301/782]: Loss 0.043946657329797745\n",
      "Training Batch [302/782]: Loss 0.0629819855093956\n",
      "Training Batch [303/782]: Loss 0.03650222718715668\n",
      "Training Batch [304/782]: Loss 0.04498943313956261\n",
      "Training Batch [305/782]: Loss 0.0251158494502306\n",
      "Training Batch [306/782]: Loss 0.08608165383338928\n",
      "Training Batch [307/782]: Loss 0.0409248024225235\n",
      "Training Batch [308/782]: Loss 0.1958635449409485\n",
      "Training Batch [309/782]: Loss 0.05404602363705635\n",
      "Training Batch [310/782]: Loss 0.05050840601325035\n",
      "Training Batch [311/782]: Loss 0.12147234380245209\n",
      "Training Batch [312/782]: Loss 0.09389346092939377\n",
      "Training Batch [313/782]: Loss 0.021127603948116302\n",
      "Training Batch [314/782]: Loss 0.08475279808044434\n",
      "Training Batch [315/782]: Loss 0.023726211860775948\n",
      "Training Batch [316/782]: Loss 0.05291824787855148\n",
      "Training Batch [317/782]: Loss 0.07770863175392151\n",
      "Training Batch [318/782]: Loss 0.03370654955506325\n",
      "Training Batch [319/782]: Loss 0.07871761918067932\n",
      "Training Batch [320/782]: Loss 0.13959605991840363\n",
      "Training Batch [321/782]: Loss 0.014666383154690266\n",
      "Training Batch [322/782]: Loss 0.052944112569093704\n",
      "Training Batch [323/782]: Loss 0.05845776945352554\n",
      "Training Batch [324/782]: Loss 0.1368960738182068\n",
      "Training Batch [325/782]: Loss 0.04687856137752533\n",
      "Training Batch [326/782]: Loss 0.02167055942118168\n",
      "Training Batch [327/782]: Loss 0.11610348522663116\n",
      "Training Batch [328/782]: Loss 0.02000945247709751\n",
      "Training Batch [329/782]: Loss 0.1019962728023529\n",
      "Training Batch [330/782]: Loss 0.096397764980793\n",
      "Training Batch [331/782]: Loss 0.05204158276319504\n",
      "Training Batch [332/782]: Loss 0.05576898530125618\n",
      "Training Batch [333/782]: Loss 0.049618471413850784\n",
      "Training Batch [334/782]: Loss 0.05858296528458595\n",
      "Training Batch [335/782]: Loss 0.05066365748643875\n",
      "Training Batch [336/782]: Loss 0.19123411178588867\n",
      "Training Batch [337/782]: Loss 0.11389975994825363\n",
      "Training Batch [338/782]: Loss 0.029927613213658333\n",
      "Training Batch [339/782]: Loss 0.017599746584892273\n",
      "Training Batch [340/782]: Loss 0.06173845753073692\n",
      "Training Batch [341/782]: Loss 0.058896150439977646\n",
      "Training Batch [342/782]: Loss 0.030867312103509903\n",
      "Training Batch [343/782]: Loss 0.08256576955318451\n",
      "Training Batch [344/782]: Loss 0.07103809714317322\n",
      "Training Batch [345/782]: Loss 0.05443137139081955\n",
      "Training Batch [346/782]: Loss 0.03614643216133118\n",
      "Training Batch [347/782]: Loss 0.007596025243401527\n",
      "Training Batch [348/782]: Loss 0.10178552567958832\n",
      "Training Batch [349/782]: Loss 0.12181603908538818\n",
      "Training Batch [350/782]: Loss 0.024292683228850365\n",
      "Training Batch [351/782]: Loss 0.048492033034563065\n",
      "Training Batch [352/782]: Loss 0.013224312104284763\n",
      "Training Batch [353/782]: Loss 0.02581884153187275\n",
      "Training Batch [354/782]: Loss 0.07100921869277954\n",
      "Training Batch [355/782]: Loss 0.041309457272291183\n",
      "Training Batch [356/782]: Loss 0.06410880386829376\n",
      "Training Batch [357/782]: Loss 0.05057520419359207\n",
      "Training Batch [358/782]: Loss 0.052427370101213455\n",
      "Training Batch [359/782]: Loss 0.07859379053115845\n",
      "Training Batch [360/782]: Loss 0.05715048313140869\n",
      "Training Batch [361/782]: Loss 0.015846092253923416\n",
      "Training Batch [362/782]: Loss 0.07652103155851364\n",
      "Training Batch [363/782]: Loss 0.00740406708791852\n",
      "Training Batch [364/782]: Loss 0.031053882092237473\n",
      "Training Batch [365/782]: Loss 0.04901820793747902\n",
      "Training Batch [366/782]: Loss 0.10382363945245743\n",
      "Training Batch [367/782]: Loss 0.1093350499868393\n",
      "Training Batch [368/782]: Loss 0.10804256051778793\n",
      "Training Batch [369/782]: Loss 0.10594955086708069\n",
      "Training Batch [370/782]: Loss 0.030837925150990486\n",
      "Training Batch [371/782]: Loss 0.07953007519245148\n",
      "Training Batch [372/782]: Loss 0.05410522595047951\n",
      "Training Batch [373/782]: Loss 0.09045284241437912\n",
      "Training Batch [374/782]: Loss 0.08212965726852417\n",
      "Training Batch [375/782]: Loss 0.05959233269095421\n",
      "Training Batch [376/782]: Loss 0.09163321554660797\n",
      "Training Batch [377/782]: Loss 0.009218713268637657\n",
      "Training Batch [378/782]: Loss 0.1261763572692871\n",
      "Training Batch [379/782]: Loss 0.056064195930957794\n",
      "Training Batch [380/782]: Loss 0.05407290533185005\n",
      "Training Batch [381/782]: Loss 0.1598215401172638\n",
      "Training Batch [382/782]: Loss 0.1589059680700302\n",
      "Training Batch [383/782]: Loss 0.05843200162053108\n",
      "Training Batch [384/782]: Loss 0.10222100466489792\n",
      "Training Batch [385/782]: Loss 0.03424915298819542\n",
      "Training Batch [386/782]: Loss 0.05268285050988197\n",
      "Training Batch [387/782]: Loss 0.11256033182144165\n",
      "Training Batch [388/782]: Loss 0.11243095248937607\n",
      "Training Batch [389/782]: Loss 0.08807960152626038\n",
      "Training Batch [390/782]: Loss 0.08202004432678223\n",
      "Training Batch [391/782]: Loss 0.13202135264873505\n",
      "Training Batch [392/782]: Loss 0.1789371520280838\n",
      "Training Batch [393/782]: Loss 0.07838761806488037\n",
      "Training Batch [394/782]: Loss 0.025912687182426453\n",
      "Training Batch [395/782]: Loss 0.10375112295150757\n",
      "Training Batch [396/782]: Loss 0.0625646561384201\n",
      "Training Batch [397/782]: Loss 0.12143190950155258\n",
      "Training Batch [398/782]: Loss 0.07072701305150986\n",
      "Training Batch [399/782]: Loss 0.05767902731895447\n",
      "Training Batch [400/782]: Loss 0.054235298186540604\n",
      "Training Batch [401/782]: Loss 0.032579004764556885\n",
      "Training Batch [402/782]: Loss 0.04464622214436531\n",
      "Training Batch [403/782]: Loss 0.3148108720779419\n",
      "Training Batch [404/782]: Loss 0.12569989264011383\n",
      "Training Batch [405/782]: Loss 0.05042799562215805\n",
      "Training Batch [406/782]: Loss 0.049349185079336166\n",
      "Training Batch [407/782]: Loss 0.06093934178352356\n",
      "Training Batch [408/782]: Loss 0.006182730197906494\n",
      "Training Batch [409/782]: Loss 0.10225293040275574\n",
      "Training Batch [410/782]: Loss 0.01934460736811161\n",
      "Training Batch [411/782]: Loss 0.07795992493629456\n",
      "Training Batch [412/782]: Loss 0.011037848889827728\n",
      "Training Batch [413/782]: Loss 0.03898601606488228\n",
      "Training Batch [414/782]: Loss 0.04011254757642746\n",
      "Training Batch [415/782]: Loss 0.040782589465379715\n",
      "Training Batch [416/782]: Loss 0.10124531388282776\n",
      "Training Batch [417/782]: Loss 0.14892444014549255\n",
      "Training Batch [418/782]: Loss 0.04087179899215698\n",
      "Training Batch [419/782]: Loss 0.0743870958685875\n",
      "Training Batch [420/782]: Loss 0.042547617107629776\n",
      "Training Batch [421/782]: Loss 0.13089388608932495\n",
      "Training Batch [422/782]: Loss 0.13373631238937378\n",
      "Training Batch [423/782]: Loss 0.036705147475004196\n",
      "Training Batch [424/782]: Loss 0.041512783616781235\n",
      "Training Batch [425/782]: Loss 0.24716761708259583\n",
      "Training Batch [426/782]: Loss 0.062347833067178726\n",
      "Training Batch [427/782]: Loss 0.04323834553360939\n",
      "Training Batch [428/782]: Loss 0.10551820695400238\n",
      "Training Batch [429/782]: Loss 0.055277351289987564\n",
      "Training Batch [430/782]: Loss 0.10056651383638382\n",
      "Training Batch [431/782]: Loss 0.020294196903705597\n",
      "Training Batch [432/782]: Loss 0.09632282704114914\n",
      "Training Batch [433/782]: Loss 0.03031768649816513\n",
      "Training Batch [434/782]: Loss 0.19020184874534607\n",
      "Training Batch [435/782]: Loss 0.041006240993738174\n",
      "Training Batch [436/782]: Loss 0.08926687389612198\n",
      "Training Batch [437/782]: Loss 0.10574771463871002\n",
      "Training Batch [438/782]: Loss 0.045168593525886536\n",
      "Training Batch [439/782]: Loss 0.04637053981423378\n",
      "Training Batch [440/782]: Loss 0.09847557544708252\n",
      "Training Batch [441/782]: Loss 0.06423480063676834\n",
      "Training Batch [442/782]: Loss 0.04935944080352783\n",
      "Training Batch [443/782]: Loss 0.17224854230880737\n",
      "Training Batch [444/782]: Loss 0.024566547945141792\n",
      "Training Batch [445/782]: Loss 0.027186356484889984\n",
      "Training Batch [446/782]: Loss 0.00864298827946186\n",
      "Training Batch [447/782]: Loss 0.06678657233715057\n",
      "Training Batch [448/782]: Loss 0.09606881439685822\n",
      "Training Batch [449/782]: Loss 0.04879041761159897\n",
      "Training Batch [450/782]: Loss 0.09069138020277023\n",
      "Training Batch [451/782]: Loss 0.039080910384655\n",
      "Training Batch [452/782]: Loss 0.04833560064435005\n",
      "Training Batch [453/782]: Loss 0.0392426960170269\n",
      "Training Batch [454/782]: Loss 0.08698857575654984\n",
      "Training Batch [455/782]: Loss 0.12132567167282104\n",
      "Training Batch [456/782]: Loss 0.08646468818187714\n",
      "Training Batch [457/782]: Loss 0.13738717138767242\n",
      "Training Batch [458/782]: Loss 0.06424152851104736\n",
      "Training Batch [459/782]: Loss 0.1639232486486435\n",
      "Training Batch [460/782]: Loss 0.04267202317714691\n",
      "Training Batch [461/782]: Loss 0.04683772474527359\n",
      "Training Batch [462/782]: Loss 0.26516032218933105\n",
      "Training Batch [463/782]: Loss 0.04506664350628853\n",
      "Training Batch [464/782]: Loss 0.10781426727771759\n",
      "Training Batch [465/782]: Loss 0.11581647396087646\n",
      "Training Batch [466/782]: Loss 0.11835421621799469\n",
      "Training Batch [467/782]: Loss 0.09713296592235565\n",
      "Training Batch [468/782]: Loss 0.021881233900785446\n",
      "Training Batch [469/782]: Loss 0.08938948065042496\n",
      "Training Batch [470/782]: Loss 0.02008599415421486\n",
      "Training Batch [471/782]: Loss 0.09823334217071533\n",
      "Training Batch [472/782]: Loss 0.07124614715576172\n",
      "Training Batch [473/782]: Loss 0.04469963163137436\n",
      "Training Batch [474/782]: Loss 0.07411078363656998\n",
      "Training Batch [475/782]: Loss 0.058479517698287964\n",
      "Training Batch [476/782]: Loss 0.08473601937294006\n",
      "Training Batch [477/782]: Loss 0.213594451546669\n",
      "Training Batch [478/782]: Loss 0.10476541519165039\n",
      "Training Batch [479/782]: Loss 0.08075305819511414\n",
      "Training Batch [480/782]: Loss 0.0962013378739357\n",
      "Training Batch [481/782]: Loss 0.056521255522966385\n",
      "Training Batch [482/782]: Loss 0.049612436443567276\n",
      "Training Batch [483/782]: Loss 0.10164016485214233\n",
      "Training Batch [484/782]: Loss 0.059852246195077896\n",
      "Training Batch [485/782]: Loss 0.05769576504826546\n",
      "Training Batch [486/782]: Loss 0.022392291575670242\n",
      "Training Batch [487/782]: Loss 0.03431463614106178\n",
      "Training Batch [488/782]: Loss 0.04574844241142273\n",
      "Training Batch [489/782]: Loss 0.0389426089823246\n",
      "Training Batch [490/782]: Loss 0.01948031596839428\n",
      "Training Batch [491/782]: Loss 0.05031523481011391\n",
      "Training Batch [492/782]: Loss 0.10749971121549606\n",
      "Training Batch [493/782]: Loss 0.08678322285413742\n",
      "Training Batch [494/782]: Loss 0.25959980487823486\n",
      "Training Batch [495/782]: Loss 0.04708624258637428\n",
      "Training Batch [496/782]: Loss 0.14242993295192719\n",
      "Training Batch [497/782]: Loss 0.07953575998544693\n",
      "Training Batch [498/782]: Loss 0.027989324182271957\n",
      "Training Batch [499/782]: Loss 0.041405752301216125\n",
      "Training Batch [500/782]: Loss 0.057906631380319595\n",
      "Training Batch [501/782]: Loss 0.012335780076682568\n",
      "Training Batch [502/782]: Loss 0.078526571393013\n",
      "Training Batch [503/782]: Loss 0.045094914734363556\n",
      "Training Batch [504/782]: Loss 0.06870625913143158\n",
      "Training Batch [505/782]: Loss 0.12995247542858124\n",
      "Training Batch [506/782]: Loss 0.09580416977405548\n",
      "Training Batch [507/782]: Loss 0.05781947821378708\n",
      "Training Batch [508/782]: Loss 0.0765538439154625\n",
      "Training Batch [509/782]: Loss 0.23526929318904877\n",
      "Training Batch [510/782]: Loss 0.04001661762595177\n",
      "Training Batch [511/782]: Loss 0.034966424107551575\n",
      "Training Batch [512/782]: Loss 0.039623770862817764\n",
      "Training Batch [513/782]: Loss 0.16126427054405212\n",
      "Training Batch [514/782]: Loss 0.01102022361010313\n",
      "Training Batch [515/782]: Loss 0.0479494109749794\n",
      "Training Batch [516/782]: Loss 0.054215654730796814\n",
      "Training Batch [517/782]: Loss 0.03897504508495331\n",
      "Training Batch [518/782]: Loss 0.0214853473007679\n",
      "Training Batch [519/782]: Loss 0.053328972309827805\n",
      "Training Batch [520/782]: Loss 0.04703877121210098\n",
      "Training Batch [521/782]: Loss 0.11346860975027084\n",
      "Training Batch [522/782]: Loss 0.011070960201323032\n",
      "Training Batch [523/782]: Loss 0.07208981364965439\n",
      "Training Batch [524/782]: Loss 0.04306942969560623\n",
      "Training Batch [525/782]: Loss 0.014673484489321709\n",
      "Training Batch [526/782]: Loss 0.03404136747121811\n",
      "Training Batch [527/782]: Loss 0.08854923397302628\n",
      "Training Batch [528/782]: Loss 0.046157561242580414\n",
      "Training Batch [529/782]: Loss 0.04736212268471718\n",
      "Training Batch [530/782]: Loss 0.1358790546655655\n",
      "Training Batch [531/782]: Loss 0.04411199688911438\n",
      "Training Batch [532/782]: Loss 0.017493195831775665\n",
      "Training Batch [533/782]: Loss 0.08291304856538773\n",
      "Training Batch [534/782]: Loss 0.06132584437727928\n",
      "Training Batch [535/782]: Loss 0.1322161853313446\n",
      "Training Batch [536/782]: Loss 0.2005024403333664\n",
      "Training Batch [537/782]: Loss 0.12267878651618958\n",
      "Training Batch [538/782]: Loss 0.014852350577712059\n",
      "Training Batch [539/782]: Loss 0.03137863427400589\n",
      "Training Batch [540/782]: Loss 0.035274188965559006\n",
      "Training Batch [541/782]: Loss 0.006959740072488785\n",
      "Training Batch [542/782]: Loss 0.02917410060763359\n",
      "Training Batch [543/782]: Loss 0.0844467282295227\n",
      "Training Batch [544/782]: Loss 0.08154553174972534\n",
      "Training Batch [545/782]: Loss 0.059853870421648026\n",
      "Training Batch [546/782]: Loss 0.046270906925201416\n",
      "Training Batch [547/782]: Loss 0.12323799729347229\n",
      "Training Batch [548/782]: Loss 0.1327565312385559\n",
      "Training Batch [549/782]: Loss 0.03398890048265457\n",
      "Training Batch [550/782]: Loss 0.0460677333176136\n",
      "Training Batch [551/782]: Loss 0.011008081026375294\n",
      "Training Batch [552/782]: Loss 0.053846441209316254\n",
      "Training Batch [553/782]: Loss 0.015347923152148724\n",
      "Training Batch [554/782]: Loss 0.09359949082136154\n",
      "Training Batch [555/782]: Loss 0.158169686794281\n",
      "Training Batch [556/782]: Loss 0.2159723937511444\n",
      "Training Batch [557/782]: Loss 0.1704111099243164\n",
      "Training Batch [558/782]: Loss 0.05511574447154999\n",
      "Training Batch [559/782]: Loss 0.025555241852998734\n",
      "Training Batch [560/782]: Loss 0.13403134047985077\n",
      "Training Batch [561/782]: Loss 0.12157484889030457\n",
      "Training Batch [562/782]: Loss 0.08021223545074463\n",
      "Training Batch [563/782]: Loss 0.11565441638231277\n",
      "Training Batch [564/782]: Loss 0.09443619102239609\n",
      "Training Batch [565/782]: Loss 0.1813964545726776\n",
      "Training Batch [566/782]: Loss 0.03393644466996193\n",
      "Training Batch [567/782]: Loss 0.029694709926843643\n",
      "Training Batch [568/782]: Loss 0.12413837760686874\n",
      "Training Batch [569/782]: Loss 0.04917396232485771\n",
      "Training Batch [570/782]: Loss 0.09298823773860931\n",
      "Training Batch [571/782]: Loss 0.025857748463749886\n",
      "Training Batch [572/782]: Loss 0.16146820783615112\n",
      "Training Batch [573/782]: Loss 0.03801797702908516\n",
      "Training Batch [574/782]: Loss 0.05585908517241478\n",
      "Training Batch [575/782]: Loss 0.05729397013783455\n",
      "Training Batch [576/782]: Loss 0.18255643546581268\n",
      "Training Batch [577/782]: Loss 0.03943915292620659\n",
      "Training Batch [578/782]: Loss 0.12922760844230652\n",
      "Training Batch [579/782]: Loss 0.0731641873717308\n",
      "Training Batch [580/782]: Loss 0.03491044044494629\n",
      "Training Batch [581/782]: Loss 0.10665182769298553\n",
      "Training Batch [582/782]: Loss 0.0732235237956047\n",
      "Training Batch [583/782]: Loss 0.09967784583568573\n",
      "Training Batch [584/782]: Loss 0.17534437775611877\n",
      "Training Batch [585/782]: Loss 0.12527398765087128\n",
      "Training Batch [586/782]: Loss 0.05113443359732628\n",
      "Training Batch [587/782]: Loss 0.010474763810634613\n",
      "Training Batch [588/782]: Loss 0.053334854543209076\n",
      "Training Batch [589/782]: Loss 0.021396523341536522\n",
      "Training Batch [590/782]: Loss 0.01248239167034626\n",
      "Training Batch [591/782]: Loss 0.04782966524362564\n",
      "Training Batch [592/782]: Loss 0.05859839543700218\n",
      "Training Batch [593/782]: Loss 0.09579071402549744\n",
      "Training Batch [594/782]: Loss 0.05608275532722473\n",
      "Training Batch [595/782]: Loss 0.0769573301076889\n",
      "Training Batch [596/782]: Loss 0.06798230111598969\n",
      "Training Batch [597/782]: Loss 0.03208938241004944\n",
      "Training Batch [598/782]: Loss 0.06831522285938263\n",
      "Training Batch [599/782]: Loss 0.08455786854028702\n",
      "Training Batch [600/782]: Loss 0.1704411804676056\n",
      "Training Batch [601/782]: Loss 0.12961030006408691\n",
      "Training Batch [602/782]: Loss 0.1653166562318802\n",
      "Training Batch [603/782]: Loss 0.07354480773210526\n",
      "Training Batch [604/782]: Loss 0.0354837104678154\n",
      "Training Batch [605/782]: Loss 0.032122232019901276\n",
      "Training Batch [606/782]: Loss 0.19299434125423431\n",
      "Training Batch [607/782]: Loss 0.06932663917541504\n",
      "Training Batch [608/782]: Loss 0.2096630334854126\n",
      "Training Batch [609/782]: Loss 0.08319146931171417\n",
      "Training Batch [610/782]: Loss 0.09847790002822876\n",
      "Training Batch [611/782]: Loss 0.029999779537320137\n",
      "Training Batch [612/782]: Loss 0.23172204196453094\n",
      "Training Batch [613/782]: Loss 0.07502732425928116\n",
      "Training Batch [614/782]: Loss 0.043088529258966446\n",
      "Training Batch [615/782]: Loss 0.08236938714981079\n",
      "Training Batch [616/782]: Loss 0.03267413377761841\n",
      "Training Batch [617/782]: Loss 0.13429708778858185\n",
      "Training Batch [618/782]: Loss 0.1226256713271141\n",
      "Training Batch [619/782]: Loss 0.08748587965965271\n",
      "Training Batch [620/782]: Loss 0.07728096842765808\n",
      "Training Batch [621/782]: Loss 0.03426855802536011\n",
      "Training Batch [622/782]: Loss 0.09733635187149048\n",
      "Training Batch [623/782]: Loss 0.07991356402635574\n",
      "Training Batch [624/782]: Loss 0.15521201491355896\n",
      "Training Batch [625/782]: Loss 0.05668044090270996\n",
      "Training Batch [626/782]: Loss 0.1338784098625183\n",
      "Training Batch [627/782]: Loss 0.02033841237425804\n",
      "Training Batch [628/782]: Loss 0.022228622809052467\n",
      "Training Batch [629/782]: Loss 0.03110538050532341\n",
      "Training Batch [630/782]: Loss 0.05198971927165985\n",
      "Training Batch [631/782]: Loss 0.02841702662408352\n",
      "Training Batch [632/782]: Loss 0.23261888325214386\n",
      "Training Batch [633/782]: Loss 0.07672058790922165\n",
      "Training Batch [634/782]: Loss 0.09330806136131287\n",
      "Training Batch [635/782]: Loss 0.11418362706899643\n",
      "Training Batch [636/782]: Loss 0.04103880003094673\n",
      "Training Batch [637/782]: Loss 0.0556488111615181\n",
      "Training Batch [638/782]: Loss 0.04922231286764145\n",
      "Training Batch [639/782]: Loss 0.19197432696819305\n",
      "Training Batch [640/782]: Loss 0.10542180389165878\n",
      "Training Batch [641/782]: Loss 0.035790879279375076\n",
      "Training Batch [642/782]: Loss 0.03609412536025047\n",
      "Training Batch [643/782]: Loss 0.06280038505792618\n",
      "Training Batch [644/782]: Loss 0.05958664044737816\n",
      "Training Batch [645/782]: Loss 0.04186521843075752\n",
      "Training Batch [646/782]: Loss 0.07628064602613449\n",
      "Training Batch [647/782]: Loss 0.07272419333457947\n",
      "Training Batch [648/782]: Loss 0.028451494872570038\n",
      "Training Batch [649/782]: Loss 0.1783546358346939\n",
      "Training Batch [650/782]: Loss 0.08918215334415436\n",
      "Training Batch [651/782]: Loss 0.06774131208658218\n",
      "Training Batch [652/782]: Loss 0.08287003636360168\n",
      "Training Batch [653/782]: Loss 0.04884752631187439\n",
      "Training Batch [654/782]: Loss 0.06720732897520065\n",
      "Training Batch [655/782]: Loss 0.05676994472742081\n",
      "Training Batch [656/782]: Loss 0.02437593787908554\n",
      "Training Batch [657/782]: Loss 0.11678376793861389\n",
      "Training Batch [658/782]: Loss 0.07480720430612564\n",
      "Training Batch [659/782]: Loss 0.07744067162275314\n",
      "Training Batch [660/782]: Loss 0.05908295884728432\n",
      "Training Batch [661/782]: Loss 0.06749583035707474\n",
      "Training Batch [662/782]: Loss 0.17394520342350006\n",
      "Training Batch [663/782]: Loss 0.24449892342090607\n",
      "Training Batch [664/782]: Loss 0.019030069932341576\n",
      "Training Batch [665/782]: Loss 0.19141016900539398\n",
      "Training Batch [666/782]: Loss 0.06673829257488251\n",
      "Training Batch [667/782]: Loss 0.11285339295864105\n",
      "Training Batch [668/782]: Loss 0.025573721155524254\n",
      "Training Batch [669/782]: Loss 0.08220084011554718\n",
      "Training Batch [670/782]: Loss 0.048984263092279434\n",
      "Training Batch [671/782]: Loss 0.22855177521705627\n",
      "Training Batch [672/782]: Loss 0.018226075917482376\n",
      "Training Batch [673/782]: Loss 0.06146754324436188\n",
      "Training Batch [674/782]: Loss 0.09001780301332474\n",
      "Training Batch [675/782]: Loss 0.02134528011083603\n",
      "Training Batch [676/782]: Loss 0.11481435596942902\n",
      "Training Batch [677/782]: Loss 0.17080944776535034\n",
      "Training Batch [678/782]: Loss 0.029013512656092644\n",
      "Training Batch [679/782]: Loss 0.022005435079336166\n",
      "Training Batch [680/782]: Loss 0.13935348391532898\n",
      "Training Batch [681/782]: Loss 0.018555816262960434\n",
      "Training Batch [682/782]: Loss 0.07559213042259216\n",
      "Training Batch [683/782]: Loss 0.04936704784631729\n",
      "Training Batch [684/782]: Loss 0.08255860954523087\n",
      "Training Batch [685/782]: Loss 0.09540295600891113\n",
      "Training Batch [686/782]: Loss 0.19425050914287567\n",
      "Training Batch [687/782]: Loss 0.08554162085056305\n",
      "Training Batch [688/782]: Loss 0.07375749945640564\n",
      "Training Batch [689/782]: Loss 0.050168707966804504\n",
      "Training Batch [690/782]: Loss 0.03524506464600563\n",
      "Training Batch [691/782]: Loss 0.040716495364904404\n",
      "Training Batch [692/782]: Loss 0.03538312390446663\n",
      "Training Batch [693/782]: Loss 0.035700976848602295\n",
      "Training Batch [694/782]: Loss 0.11679553985595703\n",
      "Training Batch [695/782]: Loss 0.04234305024147034\n",
      "Training Batch [696/782]: Loss 0.05789894610643387\n",
      "Training Batch [697/782]: Loss 0.036874838173389435\n",
      "Training Batch [698/782]: Loss 0.0755976215004921\n",
      "Training Batch [699/782]: Loss 0.04559798538684845\n",
      "Training Batch [700/782]: Loss 0.08214998245239258\n",
      "Training Batch [701/782]: Loss 0.316053181886673\n",
      "Training Batch [702/782]: Loss 0.03543248400092125\n",
      "Training Batch [703/782]: Loss 0.07769706100225449\n",
      "Training Batch [704/782]: Loss 0.10960987955331802\n",
      "Training Batch [705/782]: Loss 0.04418987035751343\n",
      "Training Batch [706/782]: Loss 0.08879044651985168\n",
      "Training Batch [707/782]: Loss 0.0403805635869503\n",
      "Training Batch [708/782]: Loss 0.08969473838806152\n",
      "Training Batch [709/782]: Loss 0.2233394980430603\n",
      "Training Batch [710/782]: Loss 0.06574898958206177\n",
      "Training Batch [711/782]: Loss 0.03993282839655876\n",
      "Training Batch [712/782]: Loss 0.04402341693639755\n",
      "Training Batch [713/782]: Loss 0.10567495226860046\n",
      "Training Batch [714/782]: Loss 0.16868624091148376\n",
      "Training Batch [715/782]: Loss 0.05547252669930458\n",
      "Training Batch [716/782]: Loss 0.04927002266049385\n",
      "Training Batch [717/782]: Loss 0.1277109980583191\n",
      "Training Batch [718/782]: Loss 0.04484177753329277\n",
      "Training Batch [719/782]: Loss 0.09990531206130981\n",
      "Training Batch [720/782]: Loss 0.09609796851873398\n",
      "Training Batch [721/782]: Loss 0.040487319231033325\n",
      "Training Batch [722/782]: Loss 0.05995141714811325\n",
      "Training Batch [723/782]: Loss 0.05396989732980728\n",
      "Training Batch [724/782]: Loss 0.11175378412008286\n",
      "Training Batch [725/782]: Loss 0.032131683081388474\n",
      "Training Batch [726/782]: Loss 0.11569301038980484\n",
      "Training Batch [727/782]: Loss 0.04433779790997505\n",
      "Training Batch [728/782]: Loss 0.0301755852997303\n",
      "Training Batch [729/782]: Loss 0.06105656176805496\n",
      "Training Batch [730/782]: Loss 0.05189146101474762\n",
      "Training Batch [731/782]: Loss 0.07606741040945053\n",
      "Training Batch [732/782]: Loss 0.023173175752162933\n",
      "Training Batch [733/782]: Loss 0.07253818958997726\n",
      "Training Batch [734/782]: Loss 0.19488035142421722\n",
      "Training Batch [735/782]: Loss 0.15593743324279785\n",
      "Training Batch [736/782]: Loss 0.01790493354201317\n",
      "Training Batch [737/782]: Loss 0.05852935463190079\n",
      "Training Batch [738/782]: Loss 0.06457284837961197\n",
      "Training Batch [739/782]: Loss 0.030977392569184303\n",
      "Training Batch [740/782]: Loss 0.048131052404642105\n",
      "Training Batch [741/782]: Loss 0.09020506590604782\n",
      "Training Batch [742/782]: Loss 0.13368970155715942\n",
      "Training Batch [743/782]: Loss 0.1110071912407875\n",
      "Training Batch [744/782]: Loss 0.10808490216732025\n",
      "Training Batch [745/782]: Loss 0.1424645483493805\n",
      "Training Batch [746/782]: Loss 0.01832885667681694\n",
      "Training Batch [747/782]: Loss 0.2071380913257599\n",
      "Training Batch [748/782]: Loss 0.05872895196080208\n",
      "Training Batch [749/782]: Loss 0.017495105043053627\n",
      "Training Batch [750/782]: Loss 0.040585678070783615\n",
      "Training Batch [751/782]: Loss 0.26583245396614075\n",
      "Training Batch [752/782]: Loss 0.0408925898373127\n",
      "Training Batch [753/782]: Loss 0.09963997453451157\n",
      "Training Batch [754/782]: Loss 0.1903887242078781\n",
      "Training Batch [755/782]: Loss 0.2436799556016922\n",
      "Training Batch [756/782]: Loss 0.09553173184394836\n",
      "Training Batch [757/782]: Loss 0.052839495241642\n",
      "Training Batch [758/782]: Loss 0.05545170605182648\n",
      "Training Batch [759/782]: Loss 0.07340561598539352\n",
      "Training Batch [760/782]: Loss 0.041553474962711334\n",
      "Training Batch [761/782]: Loss 0.05986064672470093\n",
      "Training Batch [762/782]: Loss 0.03635453060269356\n",
      "Training Batch [763/782]: Loss 0.07595421373844147\n",
      "Training Batch [764/782]: Loss 0.02693173475563526\n",
      "Training Batch [765/782]: Loss 0.10549331456422806\n",
      "Training Batch [766/782]: Loss 0.19358091056346893\n",
      "Training Batch [767/782]: Loss 0.12157275527715683\n",
      "Training Batch [768/782]: Loss 0.040444936603307724\n",
      "Training Batch [769/782]: Loss 0.19052159786224365\n",
      "Training Batch [770/782]: Loss 0.2126041203737259\n",
      "Training Batch [771/782]: Loss 0.27839216589927673\n",
      "Training Batch [772/782]: Loss 0.05773694068193436\n",
      "Training Batch [773/782]: Loss 0.1120220199227333\n",
      "Training Batch [774/782]: Loss 0.06351327151060104\n",
      "Training Batch [775/782]: Loss 0.15470780432224274\n",
      "Training Batch [776/782]: Loss 0.10858934372663498\n",
      "Training Batch [777/782]: Loss 0.06836501508951187\n",
      "Training Batch [778/782]: Loss 0.0776730328798294\n",
      "Training Batch [779/782]: Loss 0.1410359889268875\n",
      "Training Batch [780/782]: Loss 0.08230169117450714\n",
      "Training Batch [781/782]: Loss 0.07336504012346268\n",
      "Training Batch [782/782]: Loss 0.010281507857143879\n",
      "Epoch 14 - Train Loss: 0.0823\n",
      "*********  Epoch 15/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.048462145030498505\n",
      "Training Batch [2/782]: Loss 0.026910103857517242\n",
      "Training Batch [3/782]: Loss 0.03526182472705841\n",
      "Training Batch [4/782]: Loss 0.13899844884872437\n",
      "Training Batch [5/782]: Loss 0.08609139174222946\n",
      "Training Batch [6/782]: Loss 0.10820578038692474\n",
      "Training Batch [7/782]: Loss 0.09539537876844406\n",
      "Training Batch [8/782]: Loss 0.0826115608215332\n",
      "Training Batch [9/782]: Loss 0.14141598343849182\n",
      "Training Batch [10/782]: Loss 0.09001895785331726\n",
      "Training Batch [11/782]: Loss 0.14099764823913574\n",
      "Training Batch [12/782]: Loss 0.006143782753497362\n",
      "Training Batch [13/782]: Loss 0.020087813958525658\n",
      "Training Batch [14/782]: Loss 0.1253228336572647\n",
      "Training Batch [15/782]: Loss 0.009045001119375229\n",
      "Training Batch [16/782]: Loss 0.015390899032354355\n",
      "Training Batch [17/782]: Loss 0.029146643355488777\n",
      "Training Batch [18/782]: Loss 0.051617223769426346\n",
      "Training Batch [19/782]: Loss 0.14802716672420502\n",
      "Training Batch [20/782]: Loss 0.18559516966342926\n",
      "Training Batch [21/782]: Loss 0.04877839982509613\n",
      "Training Batch [22/782]: Loss 0.1694411039352417\n",
      "Training Batch [23/782]: Loss 0.013834706507623196\n",
      "Training Batch [24/782]: Loss 0.014038159511983395\n",
      "Training Batch [25/782]: Loss 0.0598311647772789\n",
      "Training Batch [26/782]: Loss 0.02433088794350624\n",
      "Training Batch [27/782]: Loss 0.0573570653796196\n",
      "Training Batch [28/782]: Loss 0.030096624046564102\n",
      "Training Batch [29/782]: Loss 0.024387823417782784\n",
      "Training Batch [30/782]: Loss 0.12183354049921036\n",
      "Training Batch [31/782]: Loss 0.12104755640029907\n",
      "Training Batch [32/782]: Loss 0.09901835024356842\n",
      "Training Batch [33/782]: Loss 0.06845229119062424\n",
      "Training Batch [34/782]: Loss 0.04849763214588165\n",
      "Training Batch [35/782]: Loss 0.01285548321902752\n",
      "Training Batch [36/782]: Loss 0.04352974519133568\n",
      "Training Batch [37/782]: Loss 0.03218137100338936\n",
      "Training Batch [38/782]: Loss 0.028894048184156418\n",
      "Training Batch [39/782]: Loss 0.15339413285255432\n",
      "Training Batch [40/782]: Loss 0.08244159072637558\n",
      "Training Batch [41/782]: Loss 0.014124049805104733\n",
      "Training Batch [42/782]: Loss 0.02402499131858349\n",
      "Training Batch [43/782]: Loss 0.09835316985845566\n",
      "Training Batch [44/782]: Loss 0.0264065433293581\n",
      "Training Batch [45/782]: Loss 0.03861796110868454\n",
      "Training Batch [46/782]: Loss 0.10814669728279114\n",
      "Training Batch [47/782]: Loss 0.07648640871047974\n",
      "Training Batch [48/782]: Loss 0.09371665865182877\n",
      "Training Batch [49/782]: Loss 0.03476104885339737\n",
      "Training Batch [50/782]: Loss 0.03860985487699509\n",
      "Training Batch [51/782]: Loss 0.015253848396241665\n",
      "Training Batch [52/782]: Loss 0.02862398326396942\n",
      "Training Batch [53/782]: Loss 0.038855910301208496\n",
      "Training Batch [54/782]: Loss 0.02743922546505928\n",
      "Training Batch [55/782]: Loss 0.050149500370025635\n",
      "Training Batch [56/782]: Loss 0.051979243755340576\n",
      "Training Batch [57/782]: Loss 0.08689379692077637\n",
      "Training Batch [58/782]: Loss 0.022022010758519173\n",
      "Training Batch [59/782]: Loss 0.014985433779656887\n",
      "Training Batch [60/782]: Loss 0.11995956301689148\n",
      "Training Batch [61/782]: Loss 0.010548518039286137\n",
      "Training Batch [62/782]: Loss 0.07154597342014313\n",
      "Training Batch [63/782]: Loss 0.022374242544174194\n",
      "Training Batch [64/782]: Loss 0.12357183545827866\n",
      "Training Batch [65/782]: Loss 0.08812740445137024\n",
      "Training Batch [66/782]: Loss 0.03626649081707001\n",
      "Training Batch [67/782]: Loss 0.057543814182281494\n",
      "Training Batch [68/782]: Loss 0.11773689091205597\n",
      "Training Batch [69/782]: Loss 0.025378074496984482\n",
      "Training Batch [70/782]: Loss 0.03665415570139885\n",
      "Training Batch [71/782]: Loss 0.03784533590078354\n",
      "Training Batch [72/782]: Loss 0.014625223353505135\n",
      "Training Batch [73/782]: Loss 0.11719298362731934\n",
      "Training Batch [74/782]: Loss 0.01891685090959072\n",
      "Training Batch [75/782]: Loss 0.053578175604343414\n",
      "Training Batch [76/782]: Loss 0.02096978947520256\n",
      "Training Batch [77/782]: Loss 0.03575577586889267\n",
      "Training Batch [78/782]: Loss 0.026186209172010422\n",
      "Training Batch [79/782]: Loss 0.1136152595281601\n",
      "Training Batch [80/782]: Loss 0.1308996081352234\n",
      "Training Batch [81/782]: Loss 0.03298083692789078\n",
      "Training Batch [82/782]: Loss 0.02298956923186779\n",
      "Training Batch [83/782]: Loss 0.11590976268053055\n",
      "Training Batch [84/782]: Loss 0.05820822715759277\n",
      "Training Batch [85/782]: Loss 0.025984585285186768\n",
      "Training Batch [86/782]: Loss 0.04703904688358307\n",
      "Training Batch [87/782]: Loss 0.053346432745456696\n",
      "Training Batch [88/782]: Loss 0.056909456849098206\n",
      "Training Batch [89/782]: Loss 0.007980297319591045\n",
      "Training Batch [90/782]: Loss 0.15548421442508698\n",
      "Training Batch [91/782]: Loss 0.04192075878381729\n",
      "Training Batch [92/782]: Loss 0.06827841699123383\n",
      "Training Batch [93/782]: Loss 0.05941785126924515\n",
      "Training Batch [94/782]: Loss 0.077262282371521\n",
      "Training Batch [95/782]: Loss 0.07000794261693954\n",
      "Training Batch [96/782]: Loss 0.09114765375852585\n",
      "Training Batch [97/782]: Loss 0.02529044821858406\n",
      "Training Batch [98/782]: Loss 0.020072663202881813\n",
      "Training Batch [99/782]: Loss 0.10161305963993073\n",
      "Training Batch [100/782]: Loss 0.1844312697649002\n",
      "Training Batch [101/782]: Loss 0.0351233184337616\n",
      "Training Batch [102/782]: Loss 0.05411778390407562\n",
      "Training Batch [103/782]: Loss 0.03617366775870323\n",
      "Training Batch [104/782]: Loss 0.013248777948319912\n",
      "Training Batch [105/782]: Loss 0.07723789662122726\n",
      "Training Batch [106/782]: Loss 0.05527663603425026\n",
      "Training Batch [107/782]: Loss 0.011302459985017776\n",
      "Training Batch [108/782]: Loss 0.055454056710004807\n",
      "Training Batch [109/782]: Loss 0.012242426164448261\n",
      "Training Batch [110/782]: Loss 0.0029829167760908604\n",
      "Training Batch [111/782]: Loss 0.03424040973186493\n",
      "Training Batch [112/782]: Loss 0.06082295626401901\n",
      "Training Batch [113/782]: Loss 0.03409751504659653\n",
      "Training Batch [114/782]: Loss 0.03240072727203369\n",
      "Training Batch [115/782]: Loss 0.0311904214322567\n",
      "Training Batch [116/782]: Loss 0.07320360839366913\n",
      "Training Batch [117/782]: Loss 0.01736154779791832\n",
      "Training Batch [118/782]: Loss 0.028266098350286484\n",
      "Training Batch [119/782]: Loss 0.1052761897444725\n",
      "Training Batch [120/782]: Loss 0.07274721562862396\n",
      "Training Batch [121/782]: Loss 0.03663695976138115\n",
      "Training Batch [122/782]: Loss 0.012039856985211372\n",
      "Training Batch [123/782]: Loss 0.0502459779381752\n",
      "Training Batch [124/782]: Loss 0.05354706943035126\n",
      "Training Batch [125/782]: Loss 0.12633830308914185\n",
      "Training Batch [126/782]: Loss 0.016829201951622963\n",
      "Training Batch [127/782]: Loss 0.01894785650074482\n",
      "Training Batch [128/782]: Loss 0.030631180852651596\n",
      "Training Batch [129/782]: Loss 0.06300236284732819\n",
      "Training Batch [130/782]: Loss 0.027158454060554504\n",
      "Training Batch [131/782]: Loss 0.06344741582870483\n",
      "Training Batch [132/782]: Loss 0.03165353462100029\n",
      "Training Batch [133/782]: Loss 0.07652360945940018\n",
      "Training Batch [134/782]: Loss 0.03592119365930557\n",
      "Training Batch [135/782]: Loss 0.026255182921886444\n",
      "Training Batch [136/782]: Loss 0.048140592873096466\n",
      "Training Batch [137/782]: Loss 0.027467697858810425\n",
      "Training Batch [138/782]: Loss 0.05171393230557442\n",
      "Training Batch [139/782]: Loss 0.017157789319753647\n",
      "Training Batch [140/782]: Loss 0.03959554806351662\n",
      "Training Batch [141/782]: Loss 0.010040638037025928\n",
      "Training Batch [142/782]: Loss 0.04203973710536957\n",
      "Training Batch [143/782]: Loss 0.09572434425354004\n",
      "Training Batch [144/782]: Loss 0.07545417547225952\n",
      "Training Batch [145/782]: Loss 0.045567773282527924\n",
      "Training Batch [146/782]: Loss 0.04606686159968376\n",
      "Training Batch [147/782]: Loss 0.031141512095928192\n",
      "Training Batch [148/782]: Loss 0.03441057726740837\n",
      "Training Batch [149/782]: Loss 0.006607771851122379\n",
      "Training Batch [150/782]: Loss 0.007694863248616457\n",
      "Training Batch [151/782]: Loss 0.00836930051445961\n",
      "Training Batch [152/782]: Loss 0.02995111793279648\n",
      "Training Batch [153/782]: Loss 0.045224666595458984\n",
      "Training Batch [154/782]: Loss 0.09949210286140442\n",
      "Training Batch [155/782]: Loss 0.06581900268793106\n",
      "Training Batch [156/782]: Loss 0.06674440950155258\n",
      "Training Batch [157/782]: Loss 0.004032433498650789\n",
      "Training Batch [158/782]: Loss 0.014886140823364258\n",
      "Training Batch [159/782]: Loss 0.024735942482948303\n",
      "Training Batch [160/782]: Loss 0.025179969146847725\n",
      "Training Batch [161/782]: Loss 0.01883174292743206\n",
      "Training Batch [162/782]: Loss 0.14130298793315887\n",
      "Training Batch [163/782]: Loss 0.03265529125928879\n",
      "Training Batch [164/782]: Loss 0.026310525834560394\n",
      "Training Batch [165/782]: Loss 0.007390822749584913\n",
      "Training Batch [166/782]: Loss 0.12746450304985046\n",
      "Training Batch [167/782]: Loss 0.07829176634550095\n",
      "Training Batch [168/782]: Loss 0.014171389862895012\n",
      "Training Batch [169/782]: Loss 0.015528250485658646\n",
      "Training Batch [170/782]: Loss 0.02105255424976349\n",
      "Training Batch [171/782]: Loss 0.010688835754990578\n",
      "Training Batch [172/782]: Loss 0.055173348635435104\n",
      "Training Batch [173/782]: Loss 0.003331210697069764\n",
      "Training Batch [174/782]: Loss 0.06471015512943268\n",
      "Training Batch [175/782]: Loss 0.023764198645949364\n",
      "Training Batch [176/782]: Loss 0.09550276398658752\n",
      "Training Batch [177/782]: Loss 0.038671962916851044\n",
      "Training Batch [178/782]: Loss 0.04159211367368698\n",
      "Training Batch [179/782]: Loss 0.027020853012800217\n",
      "Training Batch [180/782]: Loss 0.013193855993449688\n",
      "Training Batch [181/782]: Loss 0.06005428358912468\n",
      "Training Batch [182/782]: Loss 0.13165028393268585\n",
      "Training Batch [183/782]: Loss 0.012659943662583828\n",
      "Training Batch [184/782]: Loss 0.0056679509580135345\n",
      "Training Batch [185/782]: Loss 0.0428723469376564\n",
      "Training Batch [186/782]: Loss 0.046301938593387604\n",
      "Training Batch [187/782]: Loss 0.08200978487730026\n",
      "Training Batch [188/782]: Loss 0.014564067125320435\n",
      "Training Batch [189/782]: Loss 0.1266736090183258\n",
      "Training Batch [190/782]: Loss 0.040486451238393784\n",
      "Training Batch [191/782]: Loss 0.0637957900762558\n",
      "Training Batch [192/782]: Loss 0.040237974375486374\n",
      "Training Batch [193/782]: Loss 0.05447438359260559\n",
      "Training Batch [194/782]: Loss 0.0278564915060997\n",
      "Training Batch [195/782]: Loss 0.13689231872558594\n",
      "Training Batch [196/782]: Loss 0.018132217228412628\n",
      "Training Batch [197/782]: Loss 0.0213030818849802\n",
      "Training Batch [198/782]: Loss 0.0350395068526268\n",
      "Training Batch [199/782]: Loss 0.12094801664352417\n",
      "Training Batch [200/782]: Loss 0.04884510114789009\n",
      "Training Batch [201/782]: Loss 0.03365679830312729\n",
      "Training Batch [202/782]: Loss 0.0179909598082304\n",
      "Training Batch [203/782]: Loss 0.14023572206497192\n",
      "Training Batch [204/782]: Loss 0.044071026146411896\n",
      "Training Batch [205/782]: Loss 0.041543636471033096\n",
      "Training Batch [206/782]: Loss 0.0694800615310669\n",
      "Training Batch [207/782]: Loss 0.04174736514687538\n",
      "Training Batch [208/782]: Loss 0.12046889215707779\n",
      "Training Batch [209/782]: Loss 0.024018337950110435\n",
      "Training Batch [210/782]: Loss 0.03200684115290642\n",
      "Training Batch [211/782]: Loss 0.041442714631557465\n",
      "Training Batch [212/782]: Loss 0.057854361832141876\n",
      "Training Batch [213/782]: Loss 0.02107960358262062\n",
      "Training Batch [214/782]: Loss 0.012307388707995415\n",
      "Training Batch [215/782]: Loss 0.1401848942041397\n",
      "Training Batch [216/782]: Loss 0.1077398881316185\n",
      "Training Batch [217/782]: Loss 0.05029875785112381\n",
      "Training Batch [218/782]: Loss 0.0771375298500061\n",
      "Training Batch [219/782]: Loss 0.03348471596837044\n",
      "Training Batch [220/782]: Loss 0.06401924788951874\n",
      "Training Batch [221/782]: Loss 0.02320576272904873\n",
      "Training Batch [222/782]: Loss 0.03302063047885895\n",
      "Training Batch [223/782]: Loss 0.02294998988509178\n",
      "Training Batch [224/782]: Loss 0.05939415097236633\n",
      "Training Batch [225/782]: Loss 0.03026619926095009\n",
      "Training Batch [226/782]: Loss 0.0948752760887146\n",
      "Training Batch [227/782]: Loss 0.13484449684619904\n",
      "Training Batch [228/782]: Loss 0.01142566092312336\n",
      "Training Batch [229/782]: Loss 0.09035508334636688\n",
      "Training Batch [230/782]: Loss 0.04860752820968628\n",
      "Training Batch [231/782]: Loss 0.027935434132814407\n",
      "Training Batch [232/782]: Loss 0.013423994183540344\n",
      "Training Batch [233/782]: Loss 0.12246578931808472\n",
      "Training Batch [234/782]: Loss 0.01971651241183281\n",
      "Training Batch [235/782]: Loss 0.012256165035068989\n",
      "Training Batch [236/782]: Loss 0.036123864352703094\n",
      "Training Batch [237/782]: Loss 0.07258747518062592\n",
      "Training Batch [238/782]: Loss 0.045933496206998825\n",
      "Training Batch [239/782]: Loss 0.013151091523468494\n",
      "Training Batch [240/782]: Loss 0.01759417913854122\n",
      "Training Batch [241/782]: Loss 0.08932512253522873\n",
      "Training Batch [242/782]: Loss 0.12906886637210846\n",
      "Training Batch [243/782]: Loss 0.020826928317546844\n",
      "Training Batch [244/782]: Loss 0.02553432621061802\n",
      "Training Batch [245/782]: Loss 0.033973343670368195\n",
      "Training Batch [246/782]: Loss 0.03363896161317825\n",
      "Training Batch [247/782]: Loss 0.01657313108444214\n",
      "Training Batch [248/782]: Loss 0.09052668511867523\n",
      "Training Batch [249/782]: Loss 0.01184055581688881\n",
      "Training Batch [250/782]: Loss 0.011877622455358505\n",
      "Training Batch [251/782]: Loss 0.012422747910022736\n",
      "Training Batch [252/782]: Loss 0.020683178678154945\n",
      "Training Batch [253/782]: Loss 0.07185008376836777\n",
      "Training Batch [254/782]: Loss 0.14135141670703888\n",
      "Training Batch [255/782]: Loss 0.06159921735525131\n",
      "Training Batch [256/782]: Loss 0.0992322787642479\n",
      "Training Batch [257/782]: Loss 0.00834998581558466\n",
      "Training Batch [258/782]: Loss 0.039724722504615784\n",
      "Training Batch [259/782]: Loss 0.0737481340765953\n",
      "Training Batch [260/782]: Loss 0.02456212043762207\n",
      "Training Batch [261/782]: Loss 0.012473845854401588\n",
      "Training Batch [262/782]: Loss 0.021200917661190033\n",
      "Training Batch [263/782]: Loss 0.13880246877670288\n",
      "Training Batch [264/782]: Loss 0.0236789770424366\n",
      "Training Batch [265/782]: Loss 0.022888462990522385\n",
      "Training Batch [266/782]: Loss 0.046686846762895584\n",
      "Training Batch [267/782]: Loss 0.01606619730591774\n",
      "Training Batch [268/782]: Loss 0.10275775194168091\n",
      "Training Batch [269/782]: Loss 0.03261421248316765\n",
      "Training Batch [270/782]: Loss 0.08475643396377563\n",
      "Training Batch [271/782]: Loss 0.028491919860243797\n",
      "Training Batch [272/782]: Loss 0.04562990739941597\n",
      "Training Batch [273/782]: Loss 0.014871629886329174\n",
      "Training Batch [274/782]: Loss 0.014193134382367134\n",
      "Training Batch [275/782]: Loss 0.041612863540649414\n",
      "Training Batch [276/782]: Loss 0.026589510962367058\n",
      "Training Batch [277/782]: Loss 0.0072389766573905945\n",
      "Training Batch [278/782]: Loss 0.06305795162916183\n",
      "Training Batch [279/782]: Loss 0.06257317960262299\n",
      "Training Batch [280/782]: Loss 0.10407380759716034\n",
      "Training Batch [281/782]: Loss 0.01369105838239193\n",
      "Training Batch [282/782]: Loss 0.016718484461307526\n",
      "Training Batch [283/782]: Loss 0.10668091475963593\n",
      "Training Batch [284/782]: Loss 0.01768329180777073\n",
      "Training Batch [285/782]: Loss 0.11052611470222473\n",
      "Training Batch [286/782]: Loss 0.01853749342262745\n",
      "Training Batch [287/782]: Loss 0.06151822209358215\n",
      "Training Batch [288/782]: Loss 0.061730049550533295\n",
      "Training Batch [289/782]: Loss 0.08759329468011856\n",
      "Training Batch [290/782]: Loss 0.06610748171806335\n",
      "Training Batch [291/782]: Loss 0.019548246636986732\n",
      "Training Batch [292/782]: Loss 0.02880372293293476\n",
      "Training Batch [293/782]: Loss 0.08272149413824081\n",
      "Training Batch [294/782]: Loss 0.043515682220458984\n",
      "Training Batch [295/782]: Loss 0.2006693184375763\n",
      "Training Batch [296/782]: Loss 0.09441955387592316\n",
      "Training Batch [297/782]: Loss 0.05797676369547844\n",
      "Training Batch [298/782]: Loss 0.043669573962688446\n",
      "Training Batch [299/782]: Loss 0.1854950189590454\n",
      "Training Batch [300/782]: Loss 0.09148766845464706\n",
      "Training Batch [301/782]: Loss 0.1115235686302185\n",
      "Training Batch [302/782]: Loss 0.006800531875342131\n",
      "Training Batch [303/782]: Loss 0.045415353029966354\n",
      "Training Batch [304/782]: Loss 0.05040064454078674\n",
      "Training Batch [305/782]: Loss 0.018248392269015312\n",
      "Training Batch [306/782]: Loss 0.02517564408481121\n",
      "Training Batch [307/782]: Loss 0.031170470640063286\n",
      "Training Batch [308/782]: Loss 0.015261665917932987\n",
      "Training Batch [309/782]: Loss 0.1264207810163498\n",
      "Training Batch [310/782]: Loss 0.22180862724781036\n",
      "Training Batch [311/782]: Loss 0.06404568254947662\n",
      "Training Batch [312/782]: Loss 0.11021920293569565\n",
      "Training Batch [313/782]: Loss 0.019989585503935814\n",
      "Training Batch [314/782]: Loss 0.023114314302802086\n",
      "Training Batch [315/782]: Loss 0.11971469223499298\n",
      "Training Batch [316/782]: Loss 0.025422917678952217\n",
      "Training Batch [317/782]: Loss 0.15003342926502228\n",
      "Training Batch [318/782]: Loss 0.05227828770875931\n",
      "Training Batch [319/782]: Loss 0.0289288479834795\n",
      "Training Batch [320/782]: Loss 0.05530150979757309\n",
      "Training Batch [321/782]: Loss 0.05719394609332085\n",
      "Training Batch [322/782]: Loss 0.02124929241836071\n",
      "Training Batch [323/782]: Loss 0.09845208376646042\n",
      "Training Batch [324/782]: Loss 0.12434013932943344\n",
      "Training Batch [325/782]: Loss 0.053158342838287354\n",
      "Training Batch [326/782]: Loss 0.0361509844660759\n",
      "Training Batch [327/782]: Loss 0.026583148166537285\n",
      "Training Batch [328/782]: Loss 0.033806972205638885\n",
      "Training Batch [329/782]: Loss 0.0327162928879261\n",
      "Training Batch [330/782]: Loss 0.07937495410442352\n",
      "Training Batch [331/782]: Loss 0.10343535989522934\n",
      "Training Batch [332/782]: Loss 0.06964051723480225\n",
      "Training Batch [333/782]: Loss 0.09105002135038376\n",
      "Training Batch [334/782]: Loss 0.13621650636196136\n",
      "Training Batch [335/782]: Loss 0.13138408958911896\n",
      "Training Batch [336/782]: Loss 0.05048389732837677\n",
      "Training Batch [337/782]: Loss 0.035223908722400665\n",
      "Training Batch [338/782]: Loss 0.10907894372940063\n",
      "Training Batch [339/782]: Loss 0.03675873205065727\n",
      "Training Batch [340/782]: Loss 0.04534577578306198\n",
      "Training Batch [341/782]: Loss 0.0979609340429306\n",
      "Training Batch [342/782]: Loss 0.0515710711479187\n",
      "Training Batch [343/782]: Loss 0.060888905078172684\n",
      "Training Batch [344/782]: Loss 0.07994940131902695\n",
      "Training Batch [345/782]: Loss 0.11726805567741394\n",
      "Training Batch [346/782]: Loss 0.031552188098430634\n",
      "Training Batch [347/782]: Loss 0.13216513395309448\n",
      "Training Batch [348/782]: Loss 0.011909899301826954\n",
      "Training Batch [349/782]: Loss 0.05324440076947212\n",
      "Training Batch [350/782]: Loss 0.0261277724057436\n",
      "Training Batch [351/782]: Loss 0.09128858894109726\n",
      "Training Batch [352/782]: Loss 0.11369296908378601\n",
      "Training Batch [353/782]: Loss 0.0910625159740448\n",
      "Training Batch [354/782]: Loss 0.08182039856910706\n",
      "Training Batch [355/782]: Loss 0.03991389274597168\n",
      "Training Batch [356/782]: Loss 0.14654797315597534\n",
      "Training Batch [357/782]: Loss 0.1452796757221222\n",
      "Training Batch [358/782]: Loss 0.0189334899187088\n",
      "Training Batch [359/782]: Loss 0.06747715175151825\n",
      "Training Batch [360/782]: Loss 0.0814441666007042\n",
      "Training Batch [361/782]: Loss 0.14126618206501007\n",
      "Training Batch [362/782]: Loss 0.19603456556797028\n",
      "Training Batch [363/782]: Loss 0.038898032158613205\n",
      "Training Batch [364/782]: Loss 0.10704504698514938\n",
      "Training Batch [365/782]: Loss 0.07912193983793259\n",
      "Training Batch [366/782]: Loss 0.05379974842071533\n",
      "Training Batch [367/782]: Loss 0.03587248548865318\n",
      "Training Batch [368/782]: Loss 0.04692354425787926\n",
      "Training Batch [369/782]: Loss 0.11076468974351883\n",
      "Training Batch [370/782]: Loss 0.07989366352558136\n",
      "Training Batch [371/782]: Loss 0.015482387505471706\n",
      "Training Batch [372/782]: Loss 0.08106749504804611\n",
      "Training Batch [373/782]: Loss 0.026485688984394073\n",
      "Training Batch [374/782]: Loss 0.1233648806810379\n",
      "Training Batch [375/782]: Loss 0.03420602157711983\n",
      "Training Batch [376/782]: Loss 0.007691455073654652\n",
      "Training Batch [377/782]: Loss 0.019034473225474358\n",
      "Training Batch [378/782]: Loss 0.05904803052544594\n",
      "Training Batch [379/782]: Loss 0.04908081889152527\n",
      "Training Batch [380/782]: Loss 0.02086007036268711\n",
      "Training Batch [381/782]: Loss 0.03913247957825661\n",
      "Training Batch [382/782]: Loss 0.05775066092610359\n",
      "Training Batch [383/782]: Loss 0.06801987439393997\n",
      "Training Batch [384/782]: Loss 0.026746947318315506\n",
      "Training Batch [385/782]: Loss 0.059088755398988724\n",
      "Training Batch [386/782]: Loss 0.07307799160480499\n",
      "Training Batch [387/782]: Loss 0.04451175034046173\n",
      "Training Batch [388/782]: Loss 0.05095895379781723\n",
      "Training Batch [389/782]: Loss 0.010688857175409794\n",
      "Training Batch [390/782]: Loss 0.08425881713628769\n",
      "Training Batch [391/782]: Loss 0.09747915714979172\n",
      "Training Batch [392/782]: Loss 0.07381337136030197\n",
      "Training Batch [393/782]: Loss 0.008267544209957123\n",
      "Training Batch [394/782]: Loss 0.09239362180233002\n",
      "Training Batch [395/782]: Loss 0.019745584577322006\n",
      "Training Batch [396/782]: Loss 0.01995791867375374\n",
      "Training Batch [397/782]: Loss 0.011148258112370968\n",
      "Training Batch [398/782]: Loss 0.051905132830142975\n",
      "Training Batch [399/782]: Loss 0.028150977566838264\n",
      "Training Batch [400/782]: Loss 0.019356731325387955\n",
      "Training Batch [401/782]: Loss 0.03133869916200638\n",
      "Training Batch [402/782]: Loss 0.1033719852566719\n",
      "Training Batch [403/782]: Loss 0.05420819669961929\n",
      "Training Batch [404/782]: Loss 0.12134005129337311\n",
      "Training Batch [405/782]: Loss 0.03272394463419914\n",
      "Training Batch [406/782]: Loss 0.03349733352661133\n",
      "Training Batch [407/782]: Loss 0.06388996541500092\n",
      "Training Batch [408/782]: Loss 0.05275574326515198\n",
      "Training Batch [409/782]: Loss 0.09172271937131882\n",
      "Training Batch [410/782]: Loss 0.04942477494478226\n",
      "Training Batch [411/782]: Loss 0.12144334614276886\n",
      "Training Batch [412/782]: Loss 0.022371888160705566\n",
      "Training Batch [413/782]: Loss 0.06443772464990616\n",
      "Training Batch [414/782]: Loss 0.0744146853685379\n",
      "Training Batch [415/782]: Loss 0.022336462512612343\n",
      "Training Batch [416/782]: Loss 0.07035429775714874\n",
      "Training Batch [417/782]: Loss 0.026764661073684692\n",
      "Training Batch [418/782]: Loss 0.007252563256770372\n",
      "Training Batch [419/782]: Loss 0.07539935410022736\n",
      "Training Batch [420/782]: Loss 0.01168251782655716\n",
      "Training Batch [421/782]: Loss 0.07921922951936722\n",
      "Training Batch [422/782]: Loss 0.08641502261161804\n",
      "Training Batch [423/782]: Loss 0.04073188453912735\n",
      "Training Batch [424/782]: Loss 0.11114130914211273\n",
      "Training Batch [425/782]: Loss 0.03534279763698578\n",
      "Training Batch [426/782]: Loss 0.0638907328248024\n",
      "Training Batch [427/782]: Loss 0.07786635309457779\n",
      "Training Batch [428/782]: Loss 0.06813139468431473\n",
      "Training Batch [429/782]: Loss 0.034161169081926346\n",
      "Training Batch [430/782]: Loss 0.048062220215797424\n",
      "Training Batch [431/782]: Loss 0.06416094303131104\n",
      "Training Batch [432/782]: Loss 0.003273759037256241\n",
      "Training Batch [433/782]: Loss 0.15717542171478271\n",
      "Training Batch [434/782]: Loss 0.04098343849182129\n",
      "Training Batch [435/782]: Loss 0.041612982749938965\n",
      "Training Batch [436/782]: Loss 0.015458425506949425\n",
      "Training Batch [437/782]: Loss 0.12090761959552765\n",
      "Training Batch [438/782]: Loss 0.1263452172279358\n",
      "Training Batch [439/782]: Loss 0.05698284134268761\n",
      "Training Batch [440/782]: Loss 0.04200723394751549\n",
      "Training Batch [441/782]: Loss 0.0864575207233429\n",
      "Training Batch [442/782]: Loss 0.0670003667473793\n",
      "Training Batch [443/782]: Loss 0.020834997296333313\n",
      "Training Batch [444/782]: Loss 0.03882788494229317\n",
      "Training Batch [445/782]: Loss 0.02307465299963951\n",
      "Training Batch [446/782]: Loss 0.1257266104221344\n",
      "Training Batch [447/782]: Loss 0.029708988964557648\n",
      "Training Batch [448/782]: Loss 0.0724140852689743\n",
      "Training Batch [449/782]: Loss 0.008212577551603317\n",
      "Training Batch [450/782]: Loss 0.055818822234869\n",
      "Training Batch [451/782]: Loss 0.1633000522851944\n",
      "Training Batch [452/782]: Loss 0.05771033465862274\n",
      "Training Batch [453/782]: Loss 0.03316592052578926\n",
      "Training Batch [454/782]: Loss 0.03283969685435295\n",
      "Training Batch [455/782]: Loss 0.14348644018173218\n",
      "Training Batch [456/782]: Loss 0.10480229556560516\n",
      "Training Batch [457/782]: Loss 0.021459098905324936\n",
      "Training Batch [458/782]: Loss 0.05708688125014305\n",
      "Training Batch [459/782]: Loss 0.030128981918096542\n",
      "Training Batch [460/782]: Loss 0.03017977625131607\n",
      "Training Batch [461/782]: Loss 0.0424550361931324\n",
      "Training Batch [462/782]: Loss 0.005805103573948145\n",
      "Training Batch [463/782]: Loss 0.05979287624359131\n",
      "Training Batch [464/782]: Loss 0.04185140132904053\n",
      "Training Batch [465/782]: Loss 0.01943514496088028\n",
      "Training Batch [466/782]: Loss 0.0868164673447609\n",
      "Training Batch [467/782]: Loss 0.1723802238702774\n",
      "Training Batch [468/782]: Loss 0.09785209596157074\n",
      "Training Batch [469/782]: Loss 0.09919983148574829\n",
      "Training Batch [470/782]: Loss 0.08476009964942932\n",
      "Training Batch [471/782]: Loss 0.10897529125213623\n",
      "Training Batch [472/782]: Loss 0.088186115026474\n",
      "Training Batch [473/782]: Loss 0.02108248695731163\n",
      "Training Batch [474/782]: Loss 0.05060374364256859\n",
      "Training Batch [475/782]: Loss 0.13014593720436096\n",
      "Training Batch [476/782]: Loss 0.0907125249505043\n",
      "Training Batch [477/782]: Loss 0.12262818217277527\n",
      "Training Batch [478/782]: Loss 0.04149462282657623\n",
      "Training Batch [479/782]: Loss 0.06493697315454483\n",
      "Training Batch [480/782]: Loss 0.058888472616672516\n",
      "Training Batch [481/782]: Loss 0.14292985200881958\n",
      "Training Batch [482/782]: Loss 0.08541230112314224\n",
      "Training Batch [483/782]: Loss 0.07436421513557434\n",
      "Training Batch [484/782]: Loss 0.1487109512090683\n",
      "Training Batch [485/782]: Loss 0.049248792231082916\n",
      "Training Batch [486/782]: Loss 0.062027473002672195\n",
      "Training Batch [487/782]: Loss 0.045710109174251556\n",
      "Training Batch [488/782]: Loss 0.041079550981521606\n",
      "Training Batch [489/782]: Loss 0.02498403750360012\n",
      "Training Batch [490/782]: Loss 0.027803339064121246\n",
      "Training Batch [491/782]: Loss 0.05178629606962204\n",
      "Training Batch [492/782]: Loss 0.0480620302259922\n",
      "Training Batch [493/782]: Loss 0.06961692869663239\n",
      "Training Batch [494/782]: Loss 0.07452956587076187\n",
      "Training Batch [495/782]: Loss 0.04144395515322685\n",
      "Training Batch [496/782]: Loss 0.057240080088377\n",
      "Training Batch [497/782]: Loss 0.044723864644765854\n",
      "Training Batch [498/782]: Loss 0.08885198086500168\n",
      "Training Batch [499/782]: Loss 0.05342429131269455\n",
      "Training Batch [500/782]: Loss 0.06581954658031464\n",
      "Training Batch [501/782]: Loss 0.011487750336527824\n",
      "Training Batch [502/782]: Loss 0.03901936486363411\n",
      "Training Batch [503/782]: Loss 0.0556408129632473\n",
      "Training Batch [504/782]: Loss 0.02449527382850647\n",
      "Training Batch [505/782]: Loss 0.031252168118953705\n",
      "Training Batch [506/782]: Loss 0.1021755114197731\n",
      "Training Batch [507/782]: Loss 0.021341009065508842\n",
      "Training Batch [508/782]: Loss 0.01693720370531082\n",
      "Training Batch [509/782]: Loss 0.09253433346748352\n",
      "Training Batch [510/782]: Loss 0.11058557033538818\n",
      "Training Batch [511/782]: Loss 0.11029088497161865\n",
      "Training Batch [512/782]: Loss 0.011327330954372883\n",
      "Training Batch [513/782]: Loss 0.062446270138025284\n",
      "Training Batch [514/782]: Loss 0.1219262182712555\n",
      "Training Batch [515/782]: Loss 0.05580257624387741\n",
      "Training Batch [516/782]: Loss 0.016061633825302124\n",
      "Training Batch [517/782]: Loss 0.03456329181790352\n",
      "Training Batch [518/782]: Loss 0.08383922278881073\n",
      "Training Batch [519/782]: Loss 0.03424834832549095\n",
      "Training Batch [520/782]: Loss 0.14147713780403137\n",
      "Training Batch [521/782]: Loss 0.08981440961360931\n",
      "Training Batch [522/782]: Loss 0.026593655347824097\n",
      "Training Batch [523/782]: Loss 0.05256032571196556\n",
      "Training Batch [524/782]: Loss 0.05477748066186905\n",
      "Training Batch [525/782]: Loss 0.06753844022750854\n",
      "Training Batch [526/782]: Loss 0.02727362886071205\n",
      "Training Batch [527/782]: Loss 0.03375755995512009\n",
      "Training Batch [528/782]: Loss 0.006347178481519222\n",
      "Training Batch [529/782]: Loss 0.1403728574514389\n",
      "Training Batch [530/782]: Loss 0.03235314041376114\n",
      "Training Batch [531/782]: Loss 0.0773700550198555\n",
      "Training Batch [532/782]: Loss 0.0806150808930397\n",
      "Training Batch [533/782]: Loss 0.08246950805187225\n",
      "Training Batch [534/782]: Loss 0.07278019189834595\n",
      "Training Batch [535/782]: Loss 0.08561258018016815\n",
      "Training Batch [536/782]: Loss 0.07398150861263275\n",
      "Training Batch [537/782]: Loss 0.07758809626102448\n",
      "Training Batch [538/782]: Loss 0.15852615237236023\n",
      "Training Batch [539/782]: Loss 0.01648922637104988\n",
      "Training Batch [540/782]: Loss 0.03644116595387459\n",
      "Training Batch [541/782]: Loss 0.07573062181472778\n",
      "Training Batch [542/782]: Loss 0.05087753385305405\n",
      "Training Batch [543/782]: Loss 0.12565138936042786\n",
      "Training Batch [544/782]: Loss 0.09530909359455109\n",
      "Training Batch [545/782]: Loss 0.04474898800253868\n",
      "Training Batch [546/782]: Loss 0.0823090448975563\n",
      "Training Batch [547/782]: Loss 0.07347483932971954\n",
      "Training Batch [548/782]: Loss 0.07221851497888565\n",
      "Training Batch [549/782]: Loss 0.04336647689342499\n",
      "Training Batch [550/782]: Loss 0.09019732475280762\n",
      "Training Batch [551/782]: Loss 0.032758284360170364\n",
      "Training Batch [552/782]: Loss 0.14744426310062408\n",
      "Training Batch [553/782]: Loss 0.053819507360458374\n",
      "Training Batch [554/782]: Loss 0.08903294801712036\n",
      "Training Batch [555/782]: Loss 0.07187865674495697\n",
      "Training Batch [556/782]: Loss 0.062391676008701324\n",
      "Training Batch [557/782]: Loss 0.18548502027988434\n",
      "Training Batch [558/782]: Loss 0.047656551003456116\n",
      "Training Batch [559/782]: Loss 0.04979638010263443\n",
      "Training Batch [560/782]: Loss 0.04206009581685066\n",
      "Training Batch [561/782]: Loss 0.037082184106111526\n",
      "Training Batch [562/782]: Loss 0.05830070376396179\n",
      "Training Batch [563/782]: Loss 0.11522512882947922\n",
      "Training Batch [564/782]: Loss 0.07912475615739822\n",
      "Training Batch [565/782]: Loss 0.03291742131114006\n",
      "Training Batch [566/782]: Loss 0.0635518878698349\n",
      "Training Batch [567/782]: Loss 0.16009391844272614\n",
      "Training Batch [568/782]: Loss 0.09974747151136398\n",
      "Training Batch [569/782]: Loss 0.08341651409864426\n",
      "Training Batch [570/782]: Loss 0.0937298834323883\n",
      "Training Batch [571/782]: Loss 0.10338464379310608\n",
      "Training Batch [572/782]: Loss 0.10515064746141434\n",
      "Training Batch [573/782]: Loss 0.08196720480918884\n",
      "Training Batch [574/782]: Loss 0.06152210012078285\n",
      "Training Batch [575/782]: Loss 0.0632288008928299\n",
      "Training Batch [576/782]: Loss 0.11868542432785034\n",
      "Training Batch [577/782]: Loss 0.028394656255841255\n",
      "Training Batch [578/782]: Loss 0.16986528038978577\n",
      "Training Batch [579/782]: Loss 0.15184766054153442\n",
      "Training Batch [580/782]: Loss 0.09152660518884659\n",
      "Training Batch [581/782]: Loss 0.10762534290552139\n",
      "Training Batch [582/782]: Loss 0.02558366395533085\n",
      "Training Batch [583/782]: Loss 0.07285208255052567\n",
      "Training Batch [584/782]: Loss 0.017938226461410522\n",
      "Training Batch [585/782]: Loss 0.026068370789289474\n",
      "Training Batch [586/782]: Loss 0.06409692019224167\n",
      "Training Batch [587/782]: Loss 0.02839989773929119\n",
      "Training Batch [588/782]: Loss 0.06885121762752533\n",
      "Training Batch [589/782]: Loss 0.061431899666786194\n",
      "Training Batch [590/782]: Loss 0.047129008919000626\n",
      "Training Batch [591/782]: Loss 0.05296815186738968\n",
      "Training Batch [592/782]: Loss 0.01336653158068657\n",
      "Training Batch [593/782]: Loss 0.0570683553814888\n",
      "Training Batch [594/782]: Loss 0.030989157035946846\n",
      "Training Batch [595/782]: Loss 0.04679904878139496\n",
      "Training Batch [596/782]: Loss 0.04564742371439934\n",
      "Training Batch [597/782]: Loss 0.13231556117534637\n",
      "Training Batch [598/782]: Loss 0.041642721742391586\n",
      "Training Batch [599/782]: Loss 0.02878618985414505\n",
      "Training Batch [600/782]: Loss 0.07160282880067825\n",
      "Training Batch [601/782]: Loss 0.03142906352877617\n",
      "Training Batch [602/782]: Loss 0.062237370759248734\n",
      "Training Batch [603/782]: Loss 0.19568611681461334\n",
      "Training Batch [604/782]: Loss 0.04306889697909355\n",
      "Training Batch [605/782]: Loss 0.007369261234998703\n",
      "Training Batch [606/782]: Loss 0.13207384943962097\n",
      "Training Batch [607/782]: Loss 0.030165739357471466\n",
      "Training Batch [608/782]: Loss 0.09001199156045914\n",
      "Training Batch [609/782]: Loss 0.0681876540184021\n",
      "Training Batch [610/782]: Loss 0.01772644743323326\n",
      "Training Batch [611/782]: Loss 0.07004492729902267\n",
      "Training Batch [612/782]: Loss 0.1463163048028946\n",
      "Training Batch [613/782]: Loss 0.06320422142744064\n",
      "Training Batch [614/782]: Loss 0.05650660768151283\n",
      "Training Batch [615/782]: Loss 0.022031206637620926\n",
      "Training Batch [616/782]: Loss 0.06671030819416046\n",
      "Training Batch [617/782]: Loss 0.05965574458241463\n",
      "Training Batch [618/782]: Loss 0.09357944875955582\n",
      "Training Batch [619/782]: Loss 0.037528764456510544\n",
      "Training Batch [620/782]: Loss 0.03871091455221176\n",
      "Training Batch [621/782]: Loss 0.28300681710243225\n",
      "Training Batch [622/782]: Loss 0.08517128229141235\n",
      "Training Batch [623/782]: Loss 0.06393282115459442\n",
      "Training Batch [624/782]: Loss 0.03786299005150795\n",
      "Training Batch [625/782]: Loss 0.1713254600763321\n",
      "Training Batch [626/782]: Loss 0.08879951387643814\n",
      "Training Batch [627/782]: Loss 0.08081742376089096\n",
      "Training Batch [628/782]: Loss 0.01650249771773815\n",
      "Training Batch [629/782]: Loss 0.16932374238967896\n",
      "Training Batch [630/782]: Loss 0.01932521164417267\n",
      "Training Batch [631/782]: Loss 0.06353022903203964\n",
      "Training Batch [632/782]: Loss 0.10737606137990952\n",
      "Training Batch [633/782]: Loss 0.10564687103033066\n",
      "Training Batch [634/782]: Loss 0.11001569032669067\n",
      "Training Batch [635/782]: Loss 0.15592683851718903\n",
      "Training Batch [636/782]: Loss 0.049404922872781754\n",
      "Training Batch [637/782]: Loss 0.05371442064642906\n",
      "Training Batch [638/782]: Loss 0.03364504873752594\n",
      "Training Batch [639/782]: Loss 0.06670921295881271\n",
      "Training Batch [640/782]: Loss 0.19930709898471832\n",
      "Training Batch [641/782]: Loss 0.06045256927609444\n",
      "Training Batch [642/782]: Loss 0.07044953852891922\n",
      "Training Batch [643/782]: Loss 0.03799620270729065\n",
      "Training Batch [644/782]: Loss 0.06147108972072601\n",
      "Training Batch [645/782]: Loss 0.0949086919426918\n",
      "Training Batch [646/782]: Loss 0.030893942341208458\n",
      "Training Batch [647/782]: Loss 0.04145694896578789\n",
      "Training Batch [648/782]: Loss 0.009550007991492748\n",
      "Training Batch [649/782]: Loss 0.08661331981420517\n",
      "Training Batch [650/782]: Loss 0.2014903873205185\n",
      "Training Batch [651/782]: Loss 0.03750942274928093\n",
      "Training Batch [652/782]: Loss 0.03725109249353409\n",
      "Training Batch [653/782]: Loss 0.16467362642288208\n",
      "Training Batch [654/782]: Loss 0.1306641846895218\n",
      "Training Batch [655/782]: Loss 0.028988473117351532\n",
      "Training Batch [656/782]: Loss 0.045612916350364685\n",
      "Training Batch [657/782]: Loss 0.10373179614543915\n",
      "Training Batch [658/782]: Loss 0.012101353146135807\n",
      "Training Batch [659/782]: Loss 0.12170810997486115\n",
      "Training Batch [660/782]: Loss 0.07730896770954132\n",
      "Training Batch [661/782]: Loss 0.02846732921898365\n",
      "Training Batch [662/782]: Loss 0.02380361594259739\n",
      "Training Batch [663/782]: Loss 0.06291027367115021\n",
      "Training Batch [664/782]: Loss 0.04245923087000847\n",
      "Training Batch [665/782]: Loss 0.03880075365304947\n",
      "Training Batch [666/782]: Loss 0.04681460186839104\n",
      "Training Batch [667/782]: Loss 0.040919989347457886\n",
      "Training Batch [668/782]: Loss 0.027020283043384552\n",
      "Training Batch [669/782]: Loss 0.04534072428941727\n",
      "Training Batch [670/782]: Loss 0.10481569170951843\n",
      "Training Batch [671/782]: Loss 0.03765403851866722\n",
      "Training Batch [672/782]: Loss 0.050471290946006775\n",
      "Training Batch [673/782]: Loss 0.07500769197940826\n",
      "Training Batch [674/782]: Loss 0.11056621372699738\n",
      "Training Batch [675/782]: Loss 0.06063343584537506\n",
      "Training Batch [676/782]: Loss 0.05466749519109726\n",
      "Training Batch [677/782]: Loss 0.2199232280254364\n",
      "Training Batch [678/782]: Loss 0.06386646628379822\n",
      "Training Batch [679/782]: Loss 0.09145267307758331\n",
      "Training Batch [680/782]: Loss 0.02008887380361557\n",
      "Training Batch [681/782]: Loss 0.03395668417215347\n",
      "Training Batch [682/782]: Loss 0.03994195908308029\n",
      "Training Batch [683/782]: Loss 0.06592211127281189\n",
      "Training Batch [684/782]: Loss 0.04629907384514809\n",
      "Training Batch [685/782]: Loss 0.037400998175144196\n",
      "Training Batch [686/782]: Loss 0.10229625552892685\n",
      "Training Batch [687/782]: Loss 0.01764587312936783\n",
      "Training Batch [688/782]: Loss 0.011746184900403023\n",
      "Training Batch [689/782]: Loss 0.03980686515569687\n",
      "Training Batch [690/782]: Loss 0.17839574813842773\n",
      "Training Batch [691/782]: Loss 0.052171483635902405\n",
      "Training Batch [692/782]: Loss 0.07884421944618225\n",
      "Training Batch [693/782]: Loss 0.06023608520627022\n",
      "Training Batch [694/782]: Loss 0.12325210124254227\n",
      "Training Batch [695/782]: Loss 0.1037345826625824\n",
      "Training Batch [696/782]: Loss 0.06057698652148247\n",
      "Training Batch [697/782]: Loss 0.1285809427499771\n",
      "Training Batch [698/782]: Loss 0.043017931282520294\n",
      "Training Batch [699/782]: Loss 0.04499053955078125\n",
      "Training Batch [700/782]: Loss 0.10358481854200363\n",
      "Training Batch [701/782]: Loss 0.07541448622941971\n",
      "Training Batch [702/782]: Loss 0.025518687441945076\n",
      "Training Batch [703/782]: Loss 0.1200466975569725\n",
      "Training Batch [704/782]: Loss 0.13400493562221527\n",
      "Training Batch [705/782]: Loss 0.02896314673125744\n",
      "Training Batch [706/782]: Loss 0.014301006682217121\n",
      "Training Batch [707/782]: Loss 0.02527637593448162\n",
      "Training Batch [708/782]: Loss 0.10892140120267868\n",
      "Training Batch [709/782]: Loss 0.035537414252758026\n",
      "Training Batch [710/782]: Loss 0.031936876475811005\n",
      "Training Batch [711/782]: Loss 0.20428410172462463\n",
      "Training Batch [712/782]: Loss 0.12334025651216507\n",
      "Training Batch [713/782]: Loss 0.1338808536529541\n",
      "Training Batch [714/782]: Loss 0.09899444133043289\n",
      "Training Batch [715/782]: Loss 0.07931265980005264\n",
      "Training Batch [716/782]: Loss 0.05359485745429993\n",
      "Training Batch [717/782]: Loss 0.005762305576354265\n",
      "Training Batch [718/782]: Loss 0.08817242830991745\n",
      "Training Batch [719/782]: Loss 0.05334675312042236\n",
      "Training Batch [720/782]: Loss 0.07302317023277283\n",
      "Training Batch [721/782]: Loss 0.05257231742143631\n",
      "Training Batch [722/782]: Loss 0.03603171929717064\n",
      "Training Batch [723/782]: Loss 0.0679120197892189\n",
      "Training Batch [724/782]: Loss 0.04669604450464249\n",
      "Training Batch [725/782]: Loss 0.028741540387272835\n",
      "Training Batch [726/782]: Loss 0.05467056483030319\n",
      "Training Batch [727/782]: Loss 0.0824447050690651\n",
      "Training Batch [728/782]: Loss 0.02254592999815941\n",
      "Training Batch [729/782]: Loss 0.18326552212238312\n",
      "Training Batch [730/782]: Loss 0.060973379760980606\n",
      "Training Batch [731/782]: Loss 0.10321567207574844\n",
      "Training Batch [732/782]: Loss 0.034943774342536926\n",
      "Training Batch [733/782]: Loss 0.014137854799628258\n",
      "Training Batch [734/782]: Loss 0.061645474284887314\n",
      "Training Batch [735/782]: Loss 0.06683791428804398\n",
      "Training Batch [736/782]: Loss 0.0405961349606514\n",
      "Training Batch [737/782]: Loss 0.12225570529699326\n",
      "Training Batch [738/782]: Loss 0.08201723545789719\n",
      "Training Batch [739/782]: Loss 0.07866417616605759\n",
      "Training Batch [740/782]: Loss 0.06540844589471817\n",
      "Training Batch [741/782]: Loss 0.16024070978164673\n",
      "Training Batch [742/782]: Loss 0.11211580783128738\n",
      "Training Batch [743/782]: Loss 0.04719213768839836\n",
      "Training Batch [744/782]: Loss 0.0743052065372467\n",
      "Training Batch [745/782]: Loss 0.06845442950725555\n",
      "Training Batch [746/782]: Loss 0.03925388678908348\n",
      "Training Batch [747/782]: Loss 0.06185609847307205\n",
      "Training Batch [748/782]: Loss 0.07228746265172958\n",
      "Training Batch [749/782]: Loss 0.051137376576662064\n",
      "Training Batch [750/782]: Loss 0.1828451305627823\n",
      "Training Batch [751/782]: Loss 0.1081923246383667\n",
      "Training Batch [752/782]: Loss 0.0988040566444397\n",
      "Training Batch [753/782]: Loss 0.03930901736021042\n",
      "Training Batch [754/782]: Loss 0.035386402159929276\n",
      "Training Batch [755/782]: Loss 0.016437022015452385\n",
      "Training Batch [756/782]: Loss 0.171476349234581\n",
      "Training Batch [757/782]: Loss 0.15535442531108856\n",
      "Training Batch [758/782]: Loss 0.01973709836602211\n",
      "Training Batch [759/782]: Loss 0.254414826631546\n",
      "Training Batch [760/782]: Loss 0.12783946096897125\n",
      "Training Batch [761/782]: Loss 0.04126815125346184\n",
      "Training Batch [762/782]: Loss 0.09958945214748383\n",
      "Training Batch [763/782]: Loss 0.044873710721731186\n",
      "Training Batch [764/782]: Loss 0.05161021649837494\n",
      "Training Batch [765/782]: Loss 0.02030441164970398\n",
      "Training Batch [766/782]: Loss 0.07156436890363693\n",
      "Training Batch [767/782]: Loss 0.062055639922618866\n",
      "Training Batch [768/782]: Loss 0.1348094642162323\n",
      "Training Batch [769/782]: Loss 0.13042663037776947\n",
      "Training Batch [770/782]: Loss 0.01023105438798666\n",
      "Training Batch [771/782]: Loss 0.11495615541934967\n",
      "Training Batch [772/782]: Loss 0.04462164267897606\n",
      "Training Batch [773/782]: Loss 0.060512006282806396\n",
      "Training Batch [774/782]: Loss 0.011821006424725056\n",
      "Training Batch [775/782]: Loss 0.04677470773458481\n",
      "Training Batch [776/782]: Loss 0.16019955277442932\n",
      "Training Batch [777/782]: Loss 0.1942884474992752\n",
      "Training Batch [778/782]: Loss 0.05937492847442627\n",
      "Training Batch [779/782]: Loss 0.019366752356290817\n",
      "Training Batch [780/782]: Loss 0.05444642901420593\n",
      "Training Batch [781/782]: Loss 0.08505722135305405\n",
      "Training Batch [782/782]: Loss 0.33296528458595276\n",
      "Epoch 15 - Train Loss: 0.0638\n",
      "*********  Epoch 16/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.026016496121883392\n",
      "Training Batch [2/782]: Loss 0.025843417271971703\n",
      "Training Batch [3/782]: Loss 0.022559313103556633\n",
      "Training Batch [4/782]: Loss 0.08683820813894272\n",
      "Training Batch [5/782]: Loss 0.04084640368819237\n",
      "Training Batch [6/782]: Loss 0.04606059566140175\n",
      "Training Batch [7/782]: Loss 0.03218395262956619\n",
      "Training Batch [8/782]: Loss 0.04021962359547615\n",
      "Training Batch [9/782]: Loss 0.029377799481153488\n",
      "Training Batch [10/782]: Loss 0.16171202063560486\n",
      "Training Batch [11/782]: Loss 0.0726771429181099\n",
      "Training Batch [12/782]: Loss 0.10993331670761108\n",
      "Training Batch [13/782]: Loss 0.03533262386918068\n",
      "Training Batch [14/782]: Loss 0.039185065776109695\n",
      "Training Batch [15/782]: Loss 0.05005589872598648\n",
      "Training Batch [16/782]: Loss 0.04140431433916092\n",
      "Training Batch [17/782]: Loss 0.05061326175928116\n",
      "Training Batch [18/782]: Loss 0.02284315973520279\n",
      "Training Batch [19/782]: Loss 0.02980084717273712\n",
      "Training Batch [20/782]: Loss 0.045019473880529404\n",
      "Training Batch [21/782]: Loss 0.02686421386897564\n",
      "Training Batch [22/782]: Loss 0.09739220142364502\n",
      "Training Batch [23/782]: Loss 0.061258722096681595\n",
      "Training Batch [24/782]: Loss 0.053676098585128784\n",
      "Training Batch [25/782]: Loss 0.12229401618242264\n",
      "Training Batch [26/782]: Loss 0.016876328736543655\n",
      "Training Batch [27/782]: Loss 0.05495423078536987\n",
      "Training Batch [28/782]: Loss 0.02898668684065342\n",
      "Training Batch [29/782]: Loss 0.041212208569049835\n",
      "Training Batch [30/782]: Loss 0.06941020488739014\n",
      "Training Batch [31/782]: Loss 0.08908966183662415\n",
      "Training Batch [32/782]: Loss 0.1353876143693924\n",
      "Training Batch [33/782]: Loss 0.036266863346099854\n",
      "Training Batch [34/782]: Loss 0.12711162865161896\n",
      "Training Batch [35/782]: Loss 0.0052700224332511425\n",
      "Training Batch [36/782]: Loss 0.030154623091220856\n",
      "Training Batch [37/782]: Loss 0.021956805139780045\n",
      "Training Batch [38/782]: Loss 0.07985706627368927\n",
      "Training Batch [39/782]: Loss 0.04495878145098686\n",
      "Training Batch [40/782]: Loss 0.07789265364408493\n",
      "Training Batch [41/782]: Loss 0.05984000489115715\n",
      "Training Batch [42/782]: Loss 0.05158309265971184\n",
      "Training Batch [43/782]: Loss 0.0364033468067646\n",
      "Training Batch [44/782]: Loss 0.05165679752826691\n",
      "Training Batch [45/782]: Loss 0.04590187221765518\n",
      "Training Batch [46/782]: Loss 0.13392096757888794\n",
      "Training Batch [47/782]: Loss 0.04422227293252945\n",
      "Training Batch [48/782]: Loss 0.042487699538469315\n",
      "Training Batch [49/782]: Loss 0.026365146040916443\n",
      "Training Batch [50/782]: Loss 0.013857644982635975\n",
      "Training Batch [51/782]: Loss 0.03825247287750244\n",
      "Training Batch [52/782]: Loss 0.048812445253133774\n",
      "Training Batch [53/782]: Loss 0.018722280859947205\n",
      "Training Batch [54/782]: Loss 0.022241326048970222\n",
      "Training Batch [55/782]: Loss 0.005387688986957073\n",
      "Training Batch [56/782]: Loss 0.04378233104944229\n",
      "Training Batch [57/782]: Loss 0.0626467689871788\n",
      "Training Batch [58/782]: Loss 0.06252594292163849\n",
      "Training Batch [59/782]: Loss 0.010109834372997284\n",
      "Training Batch [60/782]: Loss 0.020465364679694176\n",
      "Training Batch [61/782]: Loss 0.007635340094566345\n",
      "Training Batch [62/782]: Loss 0.05892663076519966\n",
      "Training Batch [63/782]: Loss 0.06994344294071198\n",
      "Training Batch [64/782]: Loss 0.15280002355575562\n",
      "Training Batch [65/782]: Loss 0.15119732916355133\n",
      "Training Batch [66/782]: Loss 0.03788861632347107\n",
      "Training Batch [67/782]: Loss 0.1014038696885109\n",
      "Training Batch [68/782]: Loss 0.019937613978981972\n",
      "Training Batch [69/782]: Loss 0.08232409507036209\n",
      "Training Batch [70/782]: Loss 0.09843029081821442\n",
      "Training Batch [71/782]: Loss 0.017661331221461296\n",
      "Training Batch [72/782]: Loss 0.05138729512691498\n",
      "Training Batch [73/782]: Loss 0.00772447045892477\n",
      "Training Batch [74/782]: Loss 0.10998870432376862\n",
      "Training Batch [75/782]: Loss 0.10230916738510132\n",
      "Training Batch [76/782]: Loss 0.0085708387196064\n",
      "Training Batch [77/782]: Loss 0.019825467839837074\n",
      "Training Batch [78/782]: Loss 0.011832622811198235\n",
      "Training Batch [79/782]: Loss 0.015878845006227493\n",
      "Training Batch [80/782]: Loss 0.036373596638441086\n",
      "Training Batch [81/782]: Loss 0.12135104089975357\n",
      "Training Batch [82/782]: Loss 0.10121414810419083\n",
      "Training Batch [83/782]: Loss 0.025202572345733643\n",
      "Training Batch [84/782]: Loss 0.07088346034288406\n",
      "Training Batch [85/782]: Loss 0.03312192112207413\n",
      "Training Batch [86/782]: Loss 0.029857823625206947\n",
      "Training Batch [87/782]: Loss 0.006319640204310417\n",
      "Training Batch [88/782]: Loss 0.018753957003355026\n",
      "Training Batch [89/782]: Loss 0.039126090705394745\n",
      "Training Batch [90/782]: Loss 0.006420699413865805\n",
      "Training Batch [91/782]: Loss 0.04217887669801712\n",
      "Training Batch [92/782]: Loss 0.07595997303724289\n",
      "Training Batch [93/782]: Loss 0.050030726939439774\n",
      "Training Batch [94/782]: Loss 0.023514820262789726\n",
      "Training Batch [95/782]: Loss 0.08607568591833115\n",
      "Training Batch [96/782]: Loss 0.03143779933452606\n",
      "Training Batch [97/782]: Loss 0.01928144320845604\n",
      "Training Batch [98/782]: Loss 0.06313712149858475\n",
      "Training Batch [99/782]: Loss 0.05877060443162918\n",
      "Training Batch [100/782]: Loss 0.049386557191610336\n",
      "Training Batch [101/782]: Loss 0.028100328519940376\n",
      "Training Batch [102/782]: Loss 0.036326393485069275\n",
      "Training Batch [103/782]: Loss 0.08851034939289093\n",
      "Training Batch [104/782]: Loss 0.007780597545206547\n",
      "Training Batch [105/782]: Loss 0.021843893453478813\n",
      "Training Batch [106/782]: Loss 0.04730220139026642\n",
      "Training Batch [107/782]: Loss 0.03545072674751282\n",
      "Training Batch [108/782]: Loss 0.016704196110367775\n",
      "Training Batch [109/782]: Loss 0.013215837068855762\n",
      "Training Batch [110/782]: Loss 0.03957220911979675\n",
      "Training Batch [111/782]: Loss 0.10126913338899612\n",
      "Training Batch [112/782]: Loss 0.03205370530486107\n",
      "Training Batch [113/782]: Loss 0.0550595223903656\n",
      "Training Batch [114/782]: Loss 0.046929776668548584\n",
      "Training Batch [115/782]: Loss 0.011482488363981247\n",
      "Training Batch [116/782]: Loss 0.025557758286595345\n",
      "Training Batch [117/782]: Loss 0.09919862449169159\n",
      "Training Batch [118/782]: Loss 0.02848636358976364\n",
      "Training Batch [119/782]: Loss 0.0327281579375267\n",
      "Training Batch [120/782]: Loss 0.009959204122424126\n",
      "Training Batch [121/782]: Loss 0.07106269896030426\n",
      "Training Batch [122/782]: Loss 0.018474606797099113\n",
      "Training Batch [123/782]: Loss 0.013570872135460377\n",
      "Training Batch [124/782]: Loss 0.01636410877108574\n",
      "Training Batch [125/782]: Loss 0.035152968019247055\n",
      "Training Batch [126/782]: Loss 0.022258687764406204\n",
      "Training Batch [127/782]: Loss 0.018395498394966125\n",
      "Training Batch [128/782]: Loss 0.020006462931632996\n",
      "Training Batch [129/782]: Loss 0.13585390150547028\n",
      "Training Batch [130/782]: Loss 0.10774490237236023\n",
      "Training Batch [131/782]: Loss 0.016691571101546288\n",
      "Training Batch [132/782]: Loss 0.0602080300450325\n",
      "Training Batch [133/782]: Loss 0.017480162903666496\n",
      "Training Batch [134/782]: Loss 0.007488277740776539\n",
      "Training Batch [135/782]: Loss 0.04694797471165657\n",
      "Training Batch [136/782]: Loss 0.08352947235107422\n",
      "Training Batch [137/782]: Loss 0.016627388074994087\n",
      "Training Batch [138/782]: Loss 0.01984015852212906\n",
      "Training Batch [139/782]: Loss 0.03735155612230301\n",
      "Training Batch [140/782]: Loss 0.039198458194732666\n",
      "Training Batch [141/782]: Loss 0.011804037727415562\n",
      "Training Batch [142/782]: Loss 0.01077045127749443\n",
      "Training Batch [143/782]: Loss 0.03114187717437744\n",
      "Training Batch [144/782]: Loss 0.06550848484039307\n",
      "Training Batch [145/782]: Loss 0.05485348403453827\n",
      "Training Batch [146/782]: Loss 0.09628520160913467\n",
      "Training Batch [147/782]: Loss 0.05003601312637329\n",
      "Training Batch [148/782]: Loss 0.022945905104279518\n",
      "Training Batch [149/782]: Loss 0.06175442785024643\n",
      "Training Batch [150/782]: Loss 0.040067221969366074\n",
      "Training Batch [151/782]: Loss 0.02985738217830658\n",
      "Training Batch [152/782]: Loss 0.023800987750291824\n",
      "Training Batch [153/782]: Loss 0.010649032890796661\n",
      "Training Batch [154/782]: Loss 0.03912021219730377\n",
      "Training Batch [155/782]: Loss 0.024585768580436707\n",
      "Training Batch [156/782]: Loss 0.04343375563621521\n",
      "Training Batch [157/782]: Loss 0.010105914436280727\n",
      "Training Batch [158/782]: Loss 0.036918360739946365\n",
      "Training Batch [159/782]: Loss 0.06476534903049469\n",
      "Training Batch [160/782]: Loss 0.02286490611732006\n",
      "Training Batch [161/782]: Loss 0.008564124815165997\n",
      "Training Batch [162/782]: Loss 0.029182737693190575\n",
      "Training Batch [163/782]: Loss 0.08565346151590347\n",
      "Training Batch [164/782]: Loss 0.012150242924690247\n",
      "Training Batch [165/782]: Loss 0.07509170472621918\n",
      "Training Batch [166/782]: Loss 0.022374454885721207\n",
      "Training Batch [167/782]: Loss 0.005693056620657444\n",
      "Training Batch [168/782]: Loss 0.08187265694141388\n",
      "Training Batch [169/782]: Loss 0.03643729165196419\n",
      "Training Batch [170/782]: Loss 0.08745467662811279\n",
      "Training Batch [171/782]: Loss 0.02667173370718956\n",
      "Training Batch [172/782]: Loss 0.018345534801483154\n",
      "Training Batch [173/782]: Loss 0.008492197841405869\n",
      "Training Batch [174/782]: Loss 0.15517161786556244\n",
      "Training Batch [175/782]: Loss 0.04686877131462097\n",
      "Training Batch [176/782]: Loss 0.009624728932976723\n",
      "Training Batch [177/782]: Loss 0.028050696477293968\n",
      "Training Batch [178/782]: Loss 0.07526763528585434\n",
      "Training Batch [179/782]: Loss 0.030681200325489044\n",
      "Training Batch [180/782]: Loss 0.03608521446585655\n",
      "Training Batch [181/782]: Loss 0.005200817249715328\n",
      "Training Batch [182/782]: Loss 0.013337007723748684\n",
      "Training Batch [183/782]: Loss 0.07550632953643799\n",
      "Training Batch [184/782]: Loss 0.03159011900424957\n",
      "Training Batch [185/782]: Loss 0.04326280578970909\n",
      "Training Batch [186/782]: Loss 0.025791823863983154\n",
      "Training Batch [187/782]: Loss 0.03571191802620888\n",
      "Training Batch [188/782]: Loss 0.09102918952703476\n",
      "Training Batch [189/782]: Loss 0.058257170021533966\n",
      "Training Batch [190/782]: Loss 0.013027596287429333\n",
      "Training Batch [191/782]: Loss 0.07239119708538055\n",
      "Training Batch [192/782]: Loss 0.043670404702425\n",
      "Training Batch [193/782]: Loss 0.018427155911922455\n",
      "Training Batch [194/782]: Loss 0.025463536381721497\n",
      "Training Batch [195/782]: Loss 0.027480853721499443\n",
      "Training Batch [196/782]: Loss 0.02335309237241745\n",
      "Training Batch [197/782]: Loss 0.12172821909189224\n",
      "Training Batch [198/782]: Loss 0.12121107429265976\n",
      "Training Batch [199/782]: Loss 0.059709254652261734\n",
      "Training Batch [200/782]: Loss 0.07283585518598557\n",
      "Training Batch [201/782]: Loss 0.08579999208450317\n",
      "Training Batch [202/782]: Loss 0.03836513310670853\n",
      "Training Batch [203/782]: Loss 0.026370016857981682\n",
      "Training Batch [204/782]: Loss 0.09592654556035995\n",
      "Training Batch [205/782]: Loss 0.0076915123499929905\n",
      "Training Batch [206/782]: Loss 0.11384319514036179\n",
      "Training Batch [207/782]: Loss 0.06666098535060883\n",
      "Training Batch [208/782]: Loss 0.010754533112049103\n",
      "Training Batch [209/782]: Loss 0.0286721158772707\n",
      "Training Batch [210/782]: Loss 0.03023514151573181\n",
      "Training Batch [211/782]: Loss 0.033036354929208755\n",
      "Training Batch [212/782]: Loss 0.03144097700715065\n",
      "Training Batch [213/782]: Loss 0.06492260843515396\n",
      "Training Batch [214/782]: Loss 0.007121049799025059\n",
      "Training Batch [215/782]: Loss 0.08512776345014572\n",
      "Training Batch [216/782]: Loss 0.06526891887187958\n",
      "Training Batch [217/782]: Loss 0.027745867148041725\n",
      "Training Batch [218/782]: Loss 0.030059542506933212\n",
      "Training Batch [219/782]: Loss 0.008567781187593937\n",
      "Training Batch [220/782]: Loss 0.011479700915515423\n",
      "Training Batch [221/782]: Loss 0.011796768754720688\n",
      "Training Batch [222/782]: Loss 0.07343114167451859\n",
      "Training Batch [223/782]: Loss 0.02034693956375122\n",
      "Training Batch [224/782]: Loss 0.05734875425696373\n",
      "Training Batch [225/782]: Loss 0.025146624073386192\n",
      "Training Batch [226/782]: Loss 0.017466947436332703\n",
      "Training Batch [227/782]: Loss 0.024750499054789543\n",
      "Training Batch [228/782]: Loss 0.049481675028800964\n",
      "Training Batch [229/782]: Loss 0.01584855653345585\n",
      "Training Batch [230/782]: Loss 0.06537064164876938\n",
      "Training Batch [231/782]: Loss 0.012176308780908585\n",
      "Training Batch [232/782]: Loss 0.050814248621463776\n",
      "Training Batch [233/782]: Loss 0.03166859596967697\n",
      "Training Batch [234/782]: Loss 0.0036383650731295347\n",
      "Training Batch [235/782]: Loss 0.04221264645457268\n",
      "Training Batch [236/782]: Loss 0.10563277453184128\n",
      "Training Batch [237/782]: Loss 0.06023488566279411\n",
      "Training Batch [238/782]: Loss 0.018011383712291718\n",
      "Training Batch [239/782]: Loss 0.15509042143821716\n",
      "Training Batch [240/782]: Loss 0.02642436884343624\n",
      "Training Batch [241/782]: Loss 0.060770608484745026\n",
      "Training Batch [242/782]: Loss 0.010471012443304062\n",
      "Training Batch [243/782]: Loss 0.16987858712673187\n",
      "Training Batch [244/782]: Loss 0.02355949394404888\n",
      "Training Batch [245/782]: Loss 0.04599401354789734\n",
      "Training Batch [246/782]: Loss 0.02959842048585415\n",
      "Training Batch [247/782]: Loss 0.023237936198711395\n",
      "Training Batch [248/782]: Loss 0.0997665524482727\n",
      "Training Batch [249/782]: Loss 0.02079814113676548\n",
      "Training Batch [250/782]: Loss 0.03482241928577423\n",
      "Training Batch [251/782]: Loss 0.035715602338314056\n",
      "Training Batch [252/782]: Loss 0.0760268121957779\n",
      "Training Batch [253/782]: Loss 0.026503682136535645\n",
      "Training Batch [254/782]: Loss 0.005965821910649538\n",
      "Training Batch [255/782]: Loss 0.07936752587556839\n",
      "Training Batch [256/782]: Loss 0.02444482408463955\n",
      "Training Batch [257/782]: Loss 0.07950415462255478\n",
      "Training Batch [258/782]: Loss 0.037004582583904266\n",
      "Training Batch [259/782]: Loss 0.12905670702457428\n",
      "Training Batch [260/782]: Loss 0.10002779960632324\n",
      "Training Batch [261/782]: Loss 0.019869735464453697\n",
      "Training Batch [262/782]: Loss 0.034836407750844955\n",
      "Training Batch [263/782]: Loss 0.12893632054328918\n",
      "Training Batch [264/782]: Loss 0.04694787785410881\n",
      "Training Batch [265/782]: Loss 0.049381520599126816\n",
      "Training Batch [266/782]: Loss 0.13409821689128876\n",
      "Training Batch [267/782]: Loss 0.019868994131684303\n",
      "Training Batch [268/782]: Loss 0.0260084867477417\n",
      "Training Batch [269/782]: Loss 0.013396555557847023\n",
      "Training Batch [270/782]: Loss 0.01302711945027113\n",
      "Training Batch [271/782]: Loss 0.008400732651352882\n",
      "Training Batch [272/782]: Loss 0.1261977255344391\n",
      "Training Batch [273/782]: Loss 0.039722904562950134\n",
      "Training Batch [274/782]: Loss 0.03487545624375343\n",
      "Training Batch [275/782]: Loss 0.05115199089050293\n",
      "Training Batch [276/782]: Loss 0.17508243024349213\n",
      "Training Batch [277/782]: Loss 0.05344133824110031\n",
      "Training Batch [278/782]: Loss 0.19512471556663513\n",
      "Training Batch [279/782]: Loss 0.017580270767211914\n",
      "Training Batch [280/782]: Loss 0.02884242869913578\n",
      "Training Batch [281/782]: Loss 0.049508675932884216\n",
      "Training Batch [282/782]: Loss 0.06880400329828262\n",
      "Training Batch [283/782]: Loss 0.01529757771641016\n",
      "Training Batch [284/782]: Loss 0.05600300058722496\n",
      "Training Batch [285/782]: Loss 0.028899500146508217\n",
      "Training Batch [286/782]: Loss 0.01163529884070158\n",
      "Training Batch [287/782]: Loss 0.05695120990276337\n",
      "Training Batch [288/782]: Loss 0.0815209373831749\n",
      "Training Batch [289/782]: Loss 0.06182362511754036\n",
      "Training Batch [290/782]: Loss 0.06487856060266495\n",
      "Training Batch [291/782]: Loss 0.1535269170999527\n",
      "Training Batch [292/782]: Loss 0.020103728398680687\n",
      "Training Batch [293/782]: Loss 0.03846900537610054\n",
      "Training Batch [294/782]: Loss 0.033828284591436386\n",
      "Training Batch [295/782]: Loss 0.04003453999757767\n",
      "Training Batch [296/782]: Loss 0.06143404543399811\n",
      "Training Batch [297/782]: Loss 0.06657150387763977\n",
      "Training Batch [298/782]: Loss 0.01364026591181755\n",
      "Training Batch [299/782]: Loss 0.05167363956570625\n",
      "Training Batch [300/782]: Loss 0.04311496764421463\n",
      "Training Batch [301/782]: Loss 0.050381194800138474\n",
      "Training Batch [302/782]: Loss 0.19848908483982086\n",
      "Training Batch [303/782]: Loss 0.07374788075685501\n",
      "Training Batch [304/782]: Loss 0.010837200097739697\n",
      "Training Batch [305/782]: Loss 0.10271923989057541\n",
      "Training Batch [306/782]: Loss 0.04654795676469803\n",
      "Training Batch [307/782]: Loss 0.02484297566115856\n",
      "Training Batch [308/782]: Loss 0.009399055503308773\n",
      "Training Batch [309/782]: Loss 0.036047324538230896\n",
      "Training Batch [310/782]: Loss 0.05234048515558243\n",
      "Training Batch [311/782]: Loss 0.026770565658807755\n",
      "Training Batch [312/782]: Loss 0.05206367000937462\n",
      "Training Batch [313/782]: Loss 0.11829196661710739\n",
      "Training Batch [314/782]: Loss 0.029658880084753036\n",
      "Training Batch [315/782]: Loss 0.037072718143463135\n",
      "Training Batch [316/782]: Loss 0.024380141869187355\n",
      "Training Batch [317/782]: Loss 0.025707239285111427\n",
      "Training Batch [318/782]: Loss 0.2684495151042938\n",
      "Training Batch [319/782]: Loss 0.04237969219684601\n",
      "Training Batch [320/782]: Loss 0.027404217049479485\n",
      "Training Batch [321/782]: Loss 0.015074625611305237\n",
      "Training Batch [322/782]: Loss 0.0866139829158783\n",
      "Training Batch [323/782]: Loss 0.049439385533332825\n",
      "Training Batch [324/782]: Loss 0.03448263928294182\n",
      "Training Batch [325/782]: Loss 0.16631962358951569\n",
      "Training Batch [326/782]: Loss 0.03618977591395378\n",
      "Training Batch [327/782]: Loss 0.034641191363334656\n",
      "Training Batch [328/782]: Loss 0.0178363136947155\n",
      "Training Batch [329/782]: Loss 0.06405111402273178\n",
      "Training Batch [330/782]: Loss 0.196798637509346\n",
      "Training Batch [331/782]: Loss 0.09955790638923645\n",
      "Training Batch [332/782]: Loss 0.15696367621421814\n",
      "Training Batch [333/782]: Loss 0.011437003500759602\n",
      "Training Batch [334/782]: Loss 0.0319877453148365\n",
      "Training Batch [335/782]: Loss 0.06043868511915207\n",
      "Training Batch [336/782]: Loss 0.024861417710781097\n",
      "Training Batch [337/782]: Loss 0.042929284274578094\n",
      "Training Batch [338/782]: Loss 0.11035661399364471\n",
      "Training Batch [339/782]: Loss 0.007800360210239887\n",
      "Training Batch [340/782]: Loss 0.040516797453165054\n",
      "Training Batch [341/782]: Loss 0.024541787803173065\n",
      "Training Batch [342/782]: Loss 0.05025405064225197\n",
      "Training Batch [343/782]: Loss 0.012876864522695541\n",
      "Training Batch [344/782]: Loss 0.04390184208750725\n",
      "Training Batch [345/782]: Loss 0.04157521203160286\n",
      "Training Batch [346/782]: Loss 0.0629967451095581\n",
      "Training Batch [347/782]: Loss 0.09027288109064102\n",
      "Training Batch [348/782]: Loss 0.11396164447069168\n",
      "Training Batch [349/782]: Loss 0.061609815806150436\n",
      "Training Batch [350/782]: Loss 0.06795550882816315\n",
      "Training Batch [351/782]: Loss 0.044901732355356216\n",
      "Training Batch [352/782]: Loss 0.026050996035337448\n",
      "Training Batch [353/782]: Loss 0.009295606054365635\n",
      "Training Batch [354/782]: Loss 0.009650759398937225\n",
      "Training Batch [355/782]: Loss 0.08399426192045212\n",
      "Training Batch [356/782]: Loss 0.027258584275841713\n",
      "Training Batch [357/782]: Loss 0.1482192575931549\n",
      "Training Batch [358/782]: Loss 0.04361553117632866\n",
      "Training Batch [359/782]: Loss 0.08943305164575577\n",
      "Training Batch [360/782]: Loss 0.046258892863988876\n",
      "Training Batch [361/782]: Loss 0.02640259638428688\n",
      "Training Batch [362/782]: Loss 0.07137446105480194\n",
      "Training Batch [363/782]: Loss 0.15901066362857819\n",
      "Training Batch [364/782]: Loss 0.009145925752818584\n",
      "Training Batch [365/782]: Loss 0.08754246681928635\n",
      "Training Batch [366/782]: Loss 0.03640970215201378\n",
      "Training Batch [367/782]: Loss 0.06724203377962112\n",
      "Training Batch [368/782]: Loss 0.05203389376401901\n",
      "Training Batch [369/782]: Loss 0.07191205769777298\n",
      "Training Batch [370/782]: Loss 0.10778848826885223\n",
      "Training Batch [371/782]: Loss 0.03242997080087662\n",
      "Training Batch [372/782]: Loss 0.0090427715331316\n",
      "Training Batch [373/782]: Loss 0.06723739206790924\n",
      "Training Batch [374/782]: Loss 0.1205875501036644\n",
      "Training Batch [375/782]: Loss 0.13799090683460236\n",
      "Training Batch [376/782]: Loss 0.05041109025478363\n",
      "Training Batch [377/782]: Loss 0.09910071641206741\n",
      "Training Batch [378/782]: Loss 0.017048584297299385\n",
      "Training Batch [379/782]: Loss 0.05028022080659866\n",
      "Training Batch [380/782]: Loss 0.017068248242139816\n",
      "Training Batch [381/782]: Loss 0.08012732863426208\n",
      "Training Batch [382/782]: Loss 0.014992273412644863\n",
      "Training Batch [383/782]: Loss 0.017998434603214264\n",
      "Training Batch [384/782]: Loss 0.11693260818719864\n",
      "Training Batch [385/782]: Loss 0.13555750250816345\n",
      "Training Batch [386/782]: Loss 0.1112247034907341\n",
      "Training Batch [387/782]: Loss 0.03988641873002052\n",
      "Training Batch [388/782]: Loss 0.0644695833325386\n",
      "Training Batch [389/782]: Loss 0.14220081269741058\n",
      "Training Batch [390/782]: Loss 0.07508226484060287\n",
      "Training Batch [391/782]: Loss 0.018954798579216003\n",
      "Training Batch [392/782]: Loss 0.005044261459261179\n",
      "Training Batch [393/782]: Loss 0.03127194195985794\n",
      "Training Batch [394/782]: Loss 0.04034345597028732\n",
      "Training Batch [395/782]: Loss 0.08208798617124557\n",
      "Training Batch [396/782]: Loss 0.015111175365746021\n",
      "Training Batch [397/782]: Loss 0.046105172485113144\n",
      "Training Batch [398/782]: Loss 0.02572738379240036\n",
      "Training Batch [399/782]: Loss 0.08986856043338776\n",
      "Training Batch [400/782]: Loss 0.06635093688964844\n",
      "Training Batch [401/782]: Loss 0.06306521594524384\n",
      "Training Batch [402/782]: Loss 0.07841464132070541\n",
      "Training Batch [403/782]: Loss 0.048028528690338135\n",
      "Training Batch [404/782]: Loss 0.03457011282444\n",
      "Training Batch [405/782]: Loss 0.07963522523641586\n",
      "Training Batch [406/782]: Loss 0.08084964007139206\n",
      "Training Batch [407/782]: Loss 0.06388457864522934\n",
      "Training Batch [408/782]: Loss 0.03063248097896576\n",
      "Training Batch [409/782]: Loss 0.040637873113155365\n",
      "Training Batch [410/782]: Loss 0.0861213207244873\n",
      "Training Batch [411/782]: Loss 0.013145956210792065\n",
      "Training Batch [412/782]: Loss 0.014366217888891697\n",
      "Training Batch [413/782]: Loss 0.06173539534211159\n",
      "Training Batch [414/782]: Loss 0.05178561806678772\n",
      "Training Batch [415/782]: Loss 0.007118661887943745\n",
      "Training Batch [416/782]: Loss 0.011121109127998352\n",
      "Training Batch [417/782]: Loss 0.08485046774148941\n",
      "Training Batch [418/782]: Loss 0.06787440180778503\n",
      "Training Batch [419/782]: Loss 0.10852662473917007\n",
      "Training Batch [420/782]: Loss 0.06952843815088272\n",
      "Training Batch [421/782]: Loss 0.09646564722061157\n",
      "Training Batch [422/782]: Loss 0.03153684735298157\n",
      "Training Batch [423/782]: Loss 0.02418016642332077\n",
      "Training Batch [424/782]: Loss 0.026285700500011444\n",
      "Training Batch [425/782]: Loss 0.051425110548734665\n",
      "Training Batch [426/782]: Loss 0.06240405514836311\n",
      "Training Batch [427/782]: Loss 0.04059133306145668\n",
      "Training Batch [428/782]: Loss 0.025068605318665504\n",
      "Training Batch [429/782]: Loss 0.05947207286953926\n",
      "Training Batch [430/782]: Loss 0.15324105322360992\n",
      "Training Batch [431/782]: Loss 0.02092052437365055\n",
      "Training Batch [432/782]: Loss 0.08880135416984558\n",
      "Training Batch [433/782]: Loss 0.012681785970926285\n",
      "Training Batch [434/782]: Loss 0.03540774807333946\n",
      "Training Batch [435/782]: Loss 0.028694389387965202\n",
      "Training Batch [436/782]: Loss 0.04417179524898529\n",
      "Training Batch [437/782]: Loss 0.022955551743507385\n",
      "Training Batch [438/782]: Loss 0.05812010541558266\n",
      "Training Batch [439/782]: Loss 0.029121622443199158\n",
      "Training Batch [440/782]: Loss 0.03810151666402817\n",
      "Training Batch [441/782]: Loss 0.02980315312743187\n",
      "Training Batch [442/782]: Loss 0.025308361276984215\n",
      "Training Batch [443/782]: Loss 0.11810895055532455\n",
      "Training Batch [444/782]: Loss 0.06832253932952881\n",
      "Training Batch [445/782]: Loss 0.11835946887731552\n",
      "Training Batch [446/782]: Loss 0.10614611953496933\n",
      "Training Batch [447/782]: Loss 0.029621871188282967\n",
      "Training Batch [448/782]: Loss 0.06130868196487427\n",
      "Training Batch [449/782]: Loss 0.04196988418698311\n",
      "Training Batch [450/782]: Loss 0.03702077642083168\n",
      "Training Batch [451/782]: Loss 0.060206592082977295\n",
      "Training Batch [452/782]: Loss 0.016687601804733276\n",
      "Training Batch [453/782]: Loss 0.029048316180706024\n",
      "Training Batch [454/782]: Loss 0.017979908734560013\n",
      "Training Batch [455/782]: Loss 0.021236620843410492\n",
      "Training Batch [456/782]: Loss 0.10665897279977798\n",
      "Training Batch [457/782]: Loss 0.029577311128377914\n",
      "Training Batch [458/782]: Loss 0.052728570997714996\n",
      "Training Batch [459/782]: Loss 0.014644955284893513\n",
      "Training Batch [460/782]: Loss 0.024645868688821793\n",
      "Training Batch [461/782]: Loss 0.052837420254945755\n",
      "Training Batch [462/782]: Loss 0.00791865773499012\n",
      "Training Batch [463/782]: Loss 0.03382178023457527\n",
      "Training Batch [464/782]: Loss 0.06133147329092026\n",
      "Training Batch [465/782]: Loss 0.03981620818376541\n",
      "Training Batch [466/782]: Loss 0.0020429491996765137\n",
      "Training Batch [467/782]: Loss 0.044271305203437805\n",
      "Training Batch [468/782]: Loss 0.010723202489316463\n",
      "Training Batch [469/782]: Loss 0.029818816110491753\n",
      "Training Batch [470/782]: Loss 0.03416883572936058\n",
      "Training Batch [471/782]: Loss 0.08057545870542526\n",
      "Training Batch [472/782]: Loss 0.04622320085763931\n",
      "Training Batch [473/782]: Loss 0.044125717133283615\n",
      "Training Batch [474/782]: Loss 0.007063290569931269\n",
      "Training Batch [475/782]: Loss 0.12508000433444977\n",
      "Training Batch [476/782]: Loss 0.017011603340506554\n",
      "Training Batch [477/782]: Loss 0.02697628363966942\n",
      "Training Batch [478/782]: Loss 0.0461854562163353\n",
      "Training Batch [479/782]: Loss 0.00988833885639906\n",
      "Training Batch [480/782]: Loss 0.12220622599124908\n",
      "Training Batch [481/782]: Loss 0.023293511942029\n",
      "Training Batch [482/782]: Loss 0.010593824088573456\n",
      "Training Batch [483/782]: Loss 0.08259477466344833\n",
      "Training Batch [484/782]: Loss 0.03373338654637337\n",
      "Training Batch [485/782]: Loss 0.0393684059381485\n",
      "Training Batch [486/782]: Loss 0.03423565626144409\n",
      "Training Batch [487/782]: Loss 0.013160926289856434\n",
      "Training Batch [488/782]: Loss 0.04328113794326782\n",
      "Training Batch [489/782]: Loss 0.08137050271034241\n",
      "Training Batch [490/782]: Loss 0.029899675399065018\n",
      "Training Batch [491/782]: Loss 0.02017027512192726\n",
      "Training Batch [492/782]: Loss 0.09470946341753006\n",
      "Training Batch [493/782]: Loss 0.05224420130252838\n",
      "Training Batch [494/782]: Loss 0.008261405862867832\n",
      "Training Batch [495/782]: Loss 0.019785143435001373\n",
      "Training Batch [496/782]: Loss 0.10867366194725037\n",
      "Training Batch [497/782]: Loss 0.022409986704587936\n",
      "Training Batch [498/782]: Loss 0.034019604325294495\n",
      "Training Batch [499/782]: Loss 0.0063127558678388596\n",
      "Training Batch [500/782]: Loss 0.1129184365272522\n",
      "Training Batch [501/782]: Loss 0.023861855268478394\n",
      "Training Batch [502/782]: Loss 0.08388146758079529\n",
      "Training Batch [503/782]: Loss 0.09967639297246933\n",
      "Training Batch [504/782]: Loss 0.048144176602363586\n",
      "Training Batch [505/782]: Loss 0.017943216487765312\n",
      "Training Batch [506/782]: Loss 0.05483771115541458\n",
      "Training Batch [507/782]: Loss 0.06220372021198273\n",
      "Training Batch [508/782]: Loss 0.12745879590511322\n",
      "Training Batch [509/782]: Loss 0.03834228590130806\n",
      "Training Batch [510/782]: Loss 0.04941153526306152\n",
      "Training Batch [511/782]: Loss 0.07773236185312271\n",
      "Training Batch [512/782]: Loss 0.1215275228023529\n",
      "Training Batch [513/782]: Loss 0.017205078154802322\n",
      "Training Batch [514/782]: Loss 0.10444691777229309\n",
      "Training Batch [515/782]: Loss 0.07924845069646835\n",
      "Training Batch [516/782]: Loss 0.10697255283594131\n",
      "Training Batch [517/782]: Loss 0.038777947425842285\n",
      "Training Batch [518/782]: Loss 0.06530642509460449\n",
      "Training Batch [519/782]: Loss 0.13362230360507965\n",
      "Training Batch [520/782]: Loss 0.02640996314585209\n",
      "Training Batch [521/782]: Loss 0.09805571287870407\n",
      "Training Batch [522/782]: Loss 0.08919844776391983\n",
      "Training Batch [523/782]: Loss 0.05810437723994255\n",
      "Training Batch [524/782]: Loss 0.004852110054343939\n",
      "Training Batch [525/782]: Loss 0.059106819331645966\n",
      "Training Batch [526/782]: Loss 0.11411967873573303\n",
      "Training Batch [527/782]: Loss 0.02072771079838276\n",
      "Training Batch [528/782]: Loss 0.026722202077507973\n",
      "Training Batch [529/782]: Loss 0.09127966314554214\n",
      "Training Batch [530/782]: Loss 0.0693255364894867\n",
      "Training Batch [531/782]: Loss 0.10396856814622879\n",
      "Training Batch [532/782]: Loss 0.03650332987308502\n",
      "Training Batch [533/782]: Loss 0.07218899577856064\n",
      "Training Batch [534/782]: Loss 0.06696581095457077\n",
      "Training Batch [535/782]: Loss 0.06645328551530838\n",
      "Training Batch [536/782]: Loss 0.08049842715263367\n",
      "Training Batch [537/782]: Loss 0.11476040631532669\n",
      "Training Batch [538/782]: Loss 0.011576841585338116\n",
      "Training Batch [539/782]: Loss 0.07302451878786087\n",
      "Training Batch [540/782]: Loss 0.05549963191151619\n",
      "Training Batch [541/782]: Loss 0.11132735013961792\n",
      "Training Batch [542/782]: Loss 0.031859140843153\n",
      "Training Batch [543/782]: Loss 0.04005124792456627\n",
      "Training Batch [544/782]: Loss 0.09829081594944\n",
      "Training Batch [545/782]: Loss 0.15299659967422485\n",
      "Training Batch [546/782]: Loss 0.027636488899588585\n",
      "Training Batch [547/782]: Loss 0.1902836114168167\n",
      "Training Batch [548/782]: Loss 0.06252258270978928\n",
      "Training Batch [549/782]: Loss 0.09649763256311417\n",
      "Training Batch [550/782]: Loss 0.021946949884295464\n",
      "Training Batch [551/782]: Loss 0.03509533777832985\n",
      "Training Batch [552/782]: Loss 0.029044291004538536\n",
      "Training Batch [553/782]: Loss 0.04494035616517067\n",
      "Training Batch [554/782]: Loss 0.10778873413801193\n",
      "Training Batch [555/782]: Loss 0.028600316494703293\n",
      "Training Batch [556/782]: Loss 0.0850982740521431\n",
      "Training Batch [557/782]: Loss 0.09018366038799286\n",
      "Training Batch [558/782]: Loss 0.17565788328647614\n",
      "Training Batch [559/782]: Loss 0.041999246925115585\n",
      "Training Batch [560/782]: Loss 0.024275880306959152\n",
      "Training Batch [561/782]: Loss 0.018029196187853813\n",
      "Training Batch [562/782]: Loss 0.04977470636367798\n",
      "Training Batch [563/782]: Loss 0.008882090449333191\n",
      "Training Batch [564/782]: Loss 0.1581030935049057\n",
      "Training Batch [565/782]: Loss 0.05877646803855896\n",
      "Training Batch [566/782]: Loss 0.10926631093025208\n",
      "Training Batch [567/782]: Loss 0.048357632011175156\n",
      "Training Batch [568/782]: Loss 0.026918958872556686\n",
      "Training Batch [569/782]: Loss 0.01879984512925148\n",
      "Training Batch [570/782]: Loss 0.10821110755205154\n",
      "Training Batch [571/782]: Loss 0.021014254540205002\n",
      "Training Batch [572/782]: Loss 0.025049546733498573\n",
      "Training Batch [573/782]: Loss 0.07408109307289124\n",
      "Training Batch [574/782]: Loss 0.06503259390592575\n",
      "Training Batch [575/782]: Loss 0.00759465154260397\n",
      "Training Batch [576/782]: Loss 0.09213287383317947\n",
      "Training Batch [577/782]: Loss 0.020353946834802628\n",
      "Training Batch [578/782]: Loss 0.06215132400393486\n",
      "Training Batch [579/782]: Loss 0.05179324373602867\n",
      "Training Batch [580/782]: Loss 0.02334483712911606\n",
      "Training Batch [581/782]: Loss 0.17388492822647095\n",
      "Training Batch [582/782]: Loss 0.023796668276190758\n",
      "Training Batch [583/782]: Loss 0.06802687793970108\n",
      "Training Batch [584/782]: Loss 0.06650791317224503\n",
      "Training Batch [585/782]: Loss 0.1415126621723175\n",
      "Training Batch [586/782]: Loss 0.0574890561401844\n",
      "Training Batch [587/782]: Loss 0.01676173321902752\n",
      "Training Batch [588/782]: Loss 0.0163826122879982\n",
      "Training Batch [589/782]: Loss 0.09604355692863464\n",
      "Training Batch [590/782]: Loss 0.061977386474609375\n",
      "Training Batch [591/782]: Loss 0.17428365349769592\n",
      "Training Batch [592/782]: Loss 0.13439474999904633\n",
      "Training Batch [593/782]: Loss 0.05806981027126312\n",
      "Training Batch [594/782]: Loss 0.1052544042468071\n",
      "Training Batch [595/782]: Loss 0.019093554466962814\n",
      "Training Batch [596/782]: Loss 0.027416003867983818\n",
      "Training Batch [597/782]: Loss 0.04732297733426094\n",
      "Training Batch [598/782]: Loss 0.04979824647307396\n",
      "Training Batch [599/782]: Loss 0.05273664742708206\n",
      "Training Batch [600/782]: Loss 0.08369982242584229\n",
      "Training Batch [601/782]: Loss 0.07407940924167633\n",
      "Training Batch [602/782]: Loss 0.035671886056661606\n",
      "Training Batch [603/782]: Loss 0.03652495890855789\n",
      "Training Batch [604/782]: Loss 0.13063235580921173\n",
      "Training Batch [605/782]: Loss 0.044954970479011536\n",
      "Training Batch [606/782]: Loss 0.07722846418619156\n",
      "Training Batch [607/782]: Loss 0.03801766782999039\n",
      "Training Batch [608/782]: Loss 0.11136336624622345\n",
      "Training Batch [609/782]: Loss 0.032109662890434265\n",
      "Training Batch [610/782]: Loss 0.05534787476062775\n",
      "Training Batch [611/782]: Loss 0.03829669952392578\n",
      "Training Batch [612/782]: Loss 0.018686365336179733\n",
      "Training Batch [613/782]: Loss 0.08839783817529678\n",
      "Training Batch [614/782]: Loss 0.11890567094087601\n",
      "Training Batch [615/782]: Loss 0.1330246925354004\n",
      "Training Batch [616/782]: Loss 0.07812567800283432\n",
      "Training Batch [617/782]: Loss 0.16806718707084656\n",
      "Training Batch [618/782]: Loss 0.10083891451358795\n",
      "Training Batch [619/782]: Loss 0.04911230877041817\n",
      "Training Batch [620/782]: Loss 0.07439914345741272\n",
      "Training Batch [621/782]: Loss 0.05887867137789726\n",
      "Training Batch [622/782]: Loss 0.12485180795192719\n",
      "Training Batch [623/782]: Loss 0.09952744841575623\n",
      "Training Batch [624/782]: Loss 0.025191307067871094\n",
      "Training Batch [625/782]: Loss 0.04634047672152519\n",
      "Training Batch [626/782]: Loss 0.03391863778233528\n",
      "Training Batch [627/782]: Loss 0.2291361689567566\n",
      "Training Batch [628/782]: Loss 0.036309897899627686\n",
      "Training Batch [629/782]: Loss 0.11817146092653275\n",
      "Training Batch [630/782]: Loss 0.10088972002267838\n",
      "Training Batch [631/782]: Loss 0.015126541256904602\n",
      "Training Batch [632/782]: Loss 0.07186111807823181\n",
      "Training Batch [633/782]: Loss 0.03900930657982826\n",
      "Training Batch [634/782]: Loss 0.11516348272562027\n",
      "Training Batch [635/782]: Loss 0.18200421333312988\n",
      "Training Batch [636/782]: Loss 0.02923157438635826\n",
      "Training Batch [637/782]: Loss 0.005879662465304136\n",
      "Training Batch [638/782]: Loss 0.029080264270305634\n",
      "Training Batch [639/782]: Loss 0.24091778695583344\n",
      "Training Batch [640/782]: Loss 0.09634564816951752\n",
      "Training Batch [641/782]: Loss 0.06119030714035034\n",
      "Training Batch [642/782]: Loss 0.031622808426618576\n",
      "Training Batch [643/782]: Loss 0.21439576148986816\n",
      "Training Batch [644/782]: Loss 0.13700301945209503\n",
      "Training Batch [645/782]: Loss 0.071823351085186\n",
      "Training Batch [646/782]: Loss 0.08956258744001389\n",
      "Training Batch [647/782]: Loss 0.006955739576369524\n",
      "Training Batch [648/782]: Loss 0.09604167193174362\n",
      "Training Batch [649/782]: Loss 0.035362083464860916\n",
      "Training Batch [650/782]: Loss 0.054541632533073425\n",
      "Training Batch [651/782]: Loss 0.12083881348371506\n",
      "Training Batch [652/782]: Loss 0.12066717445850372\n",
      "Training Batch [653/782]: Loss 0.08074847608804703\n",
      "Training Batch [654/782]: Loss 0.12603451311588287\n",
      "Training Batch [655/782]: Loss 0.15479916334152222\n",
      "Training Batch [656/782]: Loss 0.14728346467018127\n",
      "Training Batch [657/782]: Loss 0.08296999335289001\n",
      "Training Batch [658/782]: Loss 0.15444199740886688\n",
      "Training Batch [659/782]: Loss 0.2189749777317047\n",
      "Training Batch [660/782]: Loss 0.08314979076385498\n",
      "Training Batch [661/782]: Loss 0.16022782027721405\n",
      "Training Batch [662/782]: Loss 0.08397336304187775\n",
      "Training Batch [663/782]: Loss 0.0934922993183136\n",
      "Training Batch [664/782]: Loss 0.19694821536540985\n",
      "Training Batch [665/782]: Loss 0.08850352466106415\n",
      "Training Batch [666/782]: Loss 0.1340509057044983\n",
      "Training Batch [667/782]: Loss 0.08004835993051529\n",
      "Training Batch [668/782]: Loss 0.054059602320194244\n",
      "Training Batch [669/782]: Loss 0.026998845860362053\n",
      "Training Batch [670/782]: Loss 0.10938596725463867\n",
      "Training Batch [671/782]: Loss 0.07625267654657364\n",
      "Training Batch [672/782]: Loss 0.02633296698331833\n",
      "Training Batch [673/782]: Loss 0.061704352498054504\n",
      "Training Batch [674/782]: Loss 0.06756728887557983\n",
      "Training Batch [675/782]: Loss 0.12733088433742523\n",
      "Training Batch [676/782]: Loss 0.09403520077466965\n",
      "Training Batch [677/782]: Loss 0.14141574501991272\n",
      "Training Batch [678/782]: Loss 0.04346412047743797\n",
      "Training Batch [679/782]: Loss 0.07626696676015854\n",
      "Training Batch [680/782]: Loss 0.034656815230846405\n",
      "Training Batch [681/782]: Loss 0.06907361000776291\n",
      "Training Batch [682/782]: Loss 0.11787443608045578\n",
      "Training Batch [683/782]: Loss 0.16175635159015656\n",
      "Training Batch [684/782]: Loss 0.053721558302640915\n",
      "Training Batch [685/782]: Loss 0.12457147985696793\n",
      "Training Batch [686/782]: Loss 0.10514060407876968\n",
      "Training Batch [687/782]: Loss 0.07180912792682648\n",
      "Training Batch [688/782]: Loss 0.12019946426153183\n",
      "Training Batch [689/782]: Loss 0.06559623777866364\n",
      "Training Batch [690/782]: Loss 0.05228928476572037\n",
      "Training Batch [691/782]: Loss 0.11435282975435257\n",
      "Training Batch [692/782]: Loss 0.024287549778819084\n",
      "Training Batch [693/782]: Loss 0.23148174583911896\n",
      "Training Batch [694/782]: Loss 0.02861621044576168\n",
      "Training Batch [695/782]: Loss 0.072186179459095\n",
      "Training Batch [696/782]: Loss 0.10830894112586975\n",
      "Training Batch [697/782]: Loss 0.14271295070648193\n",
      "Training Batch [698/782]: Loss 0.18687036633491516\n",
      "Training Batch [699/782]: Loss 0.11901181191205978\n",
      "Training Batch [700/782]: Loss 0.05822416767477989\n",
      "Training Batch [701/782]: Loss 0.043280281126499176\n",
      "Training Batch [702/782]: Loss 0.08213821798563004\n",
      "Training Batch [703/782]: Loss 0.16549287736415863\n",
      "Training Batch [704/782]: Loss 0.060169294476509094\n",
      "Training Batch [705/782]: Loss 0.030302124097943306\n",
      "Training Batch [706/782]: Loss 0.12024862319231033\n",
      "Training Batch [707/782]: Loss 0.13490349054336548\n",
      "Training Batch [708/782]: Loss 0.1440109759569168\n",
      "Training Batch [709/782]: Loss 0.09199333935976028\n",
      "Training Batch [710/782]: Loss 0.10892347991466522\n",
      "Training Batch [711/782]: Loss 0.033273834735155106\n",
      "Training Batch [712/782]: Loss 0.13492825627326965\n",
      "Training Batch [713/782]: Loss 0.007007027510553598\n",
      "Training Batch [714/782]: Loss 0.022098826244473457\n",
      "Training Batch [715/782]: Loss 0.04709374159574509\n",
      "Training Batch [716/782]: Loss 0.08409947901964188\n",
      "Training Batch [717/782]: Loss 0.021898146718740463\n",
      "Training Batch [718/782]: Loss 0.024476155638694763\n",
      "Training Batch [719/782]: Loss 0.13857851922512054\n",
      "Training Batch [720/782]: Loss 0.12279871851205826\n",
      "Training Batch [721/782]: Loss 0.1835554540157318\n",
      "Training Batch [722/782]: Loss 0.2857438921928406\n",
      "Training Batch [723/782]: Loss 0.04932738095521927\n",
      "Training Batch [724/782]: Loss 0.019375888630747795\n",
      "Training Batch [725/782]: Loss 0.03573185205459595\n",
      "Training Batch [726/782]: Loss 0.010484813712537289\n",
      "Training Batch [727/782]: Loss 0.05735919997096062\n",
      "Training Batch [728/782]: Loss 0.07222311943769455\n",
      "Training Batch [729/782]: Loss 0.05552675202488899\n",
      "Training Batch [730/782]: Loss 0.045060914009809494\n",
      "Training Batch [731/782]: Loss 0.027243413031101227\n",
      "Training Batch [732/782]: Loss 0.08507302403450012\n",
      "Training Batch [733/782]: Loss 0.08624856173992157\n",
      "Training Batch [734/782]: Loss 0.10620780289173126\n",
      "Training Batch [735/782]: Loss 0.11059930175542831\n",
      "Training Batch [736/782]: Loss 0.14441686868667603\n",
      "Training Batch [737/782]: Loss 0.11179283261299133\n",
      "Training Batch [738/782]: Loss 0.12303115427494049\n",
      "Training Batch [739/782]: Loss 0.03756492957472801\n",
      "Training Batch [740/782]: Loss 0.10403590649366379\n",
      "Training Batch [741/782]: Loss 0.16645632684230804\n",
      "Training Batch [742/782]: Loss 0.12124522775411606\n",
      "Training Batch [743/782]: Loss 0.11587033420801163\n",
      "Training Batch [744/782]: Loss 0.06542506814002991\n",
      "Training Batch [745/782]: Loss 0.08530961722135544\n",
      "Training Batch [746/782]: Loss 0.04772336408495903\n",
      "Training Batch [747/782]: Loss 0.07060041278600693\n",
      "Training Batch [748/782]: Loss 0.0930016040802002\n",
      "Training Batch [749/782]: Loss 0.07275007665157318\n",
      "Training Batch [750/782]: Loss 0.09166417270898819\n",
      "Training Batch [751/782]: Loss 0.031790655106306076\n",
      "Training Batch [752/782]: Loss 0.06589335948228836\n",
      "Training Batch [753/782]: Loss 0.020519020035862923\n",
      "Training Batch [754/782]: Loss 0.02427827939391136\n",
      "Training Batch [755/782]: Loss 0.3349350690841675\n",
      "Training Batch [756/782]: Loss 0.07365525513887405\n",
      "Training Batch [757/782]: Loss 0.0783783420920372\n",
      "Training Batch [758/782]: Loss 0.13904160261154175\n",
      "Training Batch [759/782]: Loss 0.10104677826166153\n",
      "Training Batch [760/782]: Loss 0.03983045369386673\n",
      "Training Batch [761/782]: Loss 0.07655984908342361\n",
      "Training Batch [762/782]: Loss 0.09888613969087601\n",
      "Training Batch [763/782]: Loss 0.20682506263256073\n",
      "Training Batch [764/782]: Loss 0.12192291021347046\n",
      "Training Batch [765/782]: Loss 0.09121166914701462\n",
      "Training Batch [766/782]: Loss 0.01139926165342331\n",
      "Training Batch [767/782]: Loss 0.011627765372395515\n",
      "Training Batch [768/782]: Loss 0.1957378089427948\n",
      "Training Batch [769/782]: Loss 0.03158985823392868\n",
      "Training Batch [770/782]: Loss 0.0662216991186142\n",
      "Training Batch [771/782]: Loss 0.05754545331001282\n",
      "Training Batch [772/782]: Loss 0.0466914027929306\n",
      "Training Batch [773/782]: Loss 0.12747852504253387\n",
      "Training Batch [774/782]: Loss 0.07596009224653244\n",
      "Training Batch [775/782]: Loss 0.167377769947052\n",
      "Training Batch [776/782]: Loss 0.07086583971977234\n",
      "Training Batch [777/782]: Loss 0.21225015819072723\n",
      "Training Batch [778/782]: Loss 0.058011360466480255\n",
      "Training Batch [779/782]: Loss 0.07911917567253113\n",
      "Training Batch [780/782]: Loss 0.04549748823046684\n",
      "Training Batch [781/782]: Loss 0.10294964909553528\n",
      "Training Batch [782/782]: Loss 0.28681811690330505\n",
      "Epoch 16 - Train Loss: 0.0622\n",
      "*********  Epoch 17/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.04507816582918167\n",
      "Training Batch [2/782]: Loss 0.1355670541524887\n",
      "Training Batch [3/782]: Loss 0.028395622968673706\n",
      "Training Batch [4/782]: Loss 0.05525147542357445\n",
      "Training Batch [5/782]: Loss 0.03458457812666893\n",
      "Training Batch [6/782]: Loss 0.04737276956439018\n",
      "Training Batch [7/782]: Loss 0.09915130585432053\n",
      "Training Batch [8/782]: Loss 0.043087590485811234\n",
      "Training Batch [9/782]: Loss 0.17660145461559296\n",
      "Training Batch [10/782]: Loss 0.1031300276517868\n",
      "Training Batch [11/782]: Loss 0.0034771952778100967\n",
      "Training Batch [12/782]: Loss 0.0388760082423687\n",
      "Training Batch [13/782]: Loss 0.17106685042381287\n",
      "Training Batch [14/782]: Loss 0.09711470454931259\n",
      "Training Batch [15/782]: Loss 0.03943207487463951\n",
      "Training Batch [16/782]: Loss 0.06272899359464645\n",
      "Training Batch [17/782]: Loss 0.008329429663717747\n",
      "Training Batch [18/782]: Loss 0.08214032649993896\n",
      "Training Batch [19/782]: Loss 0.03794296458363533\n",
      "Training Batch [20/782]: Loss 0.07594463229179382\n",
      "Training Batch [21/782]: Loss 0.09365107119083405\n",
      "Training Batch [22/782]: Loss 0.06961944699287415\n",
      "Training Batch [23/782]: Loss 0.08602248132228851\n",
      "Training Batch [24/782]: Loss 0.004947626031935215\n",
      "Training Batch [25/782]: Loss 0.045527320355176926\n",
      "Training Batch [26/782]: Loss 0.008530768565833569\n",
      "Training Batch [27/782]: Loss 0.012435780838131905\n",
      "Training Batch [28/782]: Loss 0.014966673217713833\n",
      "Training Batch [29/782]: Loss 0.08305879682302475\n",
      "Training Batch [30/782]: Loss 0.033419981598854065\n",
      "Training Batch [31/782]: Loss 0.05953710898756981\n",
      "Training Batch [32/782]: Loss 0.13983047008514404\n",
      "Training Batch [33/782]: Loss 0.07060854882001877\n",
      "Training Batch [34/782]: Loss 0.08519323915243149\n",
      "Training Batch [35/782]: Loss 0.09481853246688843\n",
      "Training Batch [36/782]: Loss 0.027559911832213402\n",
      "Training Batch [37/782]: Loss 0.021476032212376595\n",
      "Training Batch [38/782]: Loss 0.028937166556715965\n",
      "Training Batch [39/782]: Loss 0.0466400682926178\n",
      "Training Batch [40/782]: Loss 0.14725472033023834\n",
      "Training Batch [41/782]: Loss 0.028374936431646347\n",
      "Training Batch [42/782]: Loss 0.027468403801321983\n",
      "Training Batch [43/782]: Loss 0.06447728723287582\n",
      "Training Batch [44/782]: Loss 0.041901860386133194\n",
      "Training Batch [45/782]: Loss 0.0029846096877008677\n",
      "Training Batch [46/782]: Loss 0.122500479221344\n",
      "Training Batch [47/782]: Loss 0.20030319690704346\n",
      "Training Batch [48/782]: Loss 0.054714128375053406\n",
      "Training Batch [49/782]: Loss 0.024113774299621582\n",
      "Training Batch [50/782]: Loss 0.0192596185952425\n",
      "Training Batch [51/782]: Loss 0.05609387159347534\n",
      "Training Batch [52/782]: Loss 0.08730219304561615\n",
      "Training Batch [53/782]: Loss 0.03132609650492668\n",
      "Training Batch [54/782]: Loss 0.017586849629878998\n",
      "Training Batch [55/782]: Loss 0.019436683505773544\n",
      "Training Batch [56/782]: Loss 0.032264403998851776\n",
      "Training Batch [57/782]: Loss 0.04459460452198982\n",
      "Training Batch [58/782]: Loss 0.07060190290212631\n",
      "Training Batch [59/782]: Loss 0.03821457177400589\n",
      "Training Batch [60/782]: Loss 0.07269944995641708\n",
      "Training Batch [61/782]: Loss 0.04021190479397774\n",
      "Training Batch [62/782]: Loss 0.024331223219633102\n",
      "Training Batch [63/782]: Loss 0.031247207894921303\n",
      "Training Batch [64/782]: Loss 0.018178896978497505\n",
      "Training Batch [65/782]: Loss 0.07418909668922424\n",
      "Training Batch [66/782]: Loss 0.10420583933591843\n",
      "Training Batch [67/782]: Loss 0.03170907869935036\n",
      "Training Batch [68/782]: Loss 0.010298259556293488\n",
      "Training Batch [69/782]: Loss 0.05023786053061485\n",
      "Training Batch [70/782]: Loss 0.13313491642475128\n",
      "Training Batch [71/782]: Loss 0.04026594012975693\n",
      "Training Batch [72/782]: Loss 0.06986301392316818\n",
      "Training Batch [73/782]: Loss 0.09714364260435104\n",
      "Training Batch [74/782]: Loss 0.033288467675447464\n",
      "Training Batch [75/782]: Loss 0.03880487382411957\n",
      "Training Batch [76/782]: Loss 0.03593231737613678\n",
      "Training Batch [77/782]: Loss 0.08295606076717377\n",
      "Training Batch [78/782]: Loss 0.01985773630440235\n",
      "Training Batch [79/782]: Loss 0.10074968636035919\n",
      "Training Batch [80/782]: Loss 0.004345489665865898\n",
      "Training Batch [81/782]: Loss 0.03328699618577957\n",
      "Training Batch [82/782]: Loss 0.0348169207572937\n",
      "Training Batch [83/782]: Loss 0.01881401799619198\n",
      "Training Batch [84/782]: Loss 0.01237341109663248\n",
      "Training Batch [85/782]: Loss 0.035678137093782425\n",
      "Training Batch [86/782]: Loss 0.1556800901889801\n",
      "Training Batch [87/782]: Loss 0.14377877116203308\n",
      "Training Batch [88/782]: Loss 0.14145074784755707\n",
      "Training Batch [89/782]: Loss 0.016467658802866936\n",
      "Training Batch [90/782]: Loss 0.027604524046182632\n",
      "Training Batch [91/782]: Loss 0.0655674934387207\n",
      "Training Batch [92/782]: Loss 0.03391503915190697\n",
      "Training Batch [93/782]: Loss 0.03785751014947891\n",
      "Training Batch [94/782]: Loss 0.060796771198511124\n",
      "Training Batch [95/782]: Loss 0.02490292116999626\n",
      "Training Batch [96/782]: Loss 0.09343200922012329\n",
      "Training Batch [97/782]: Loss 0.044100016355514526\n",
      "Training Batch [98/782]: Loss 0.012834031134843826\n",
      "Training Batch [99/782]: Loss 0.01372714713215828\n",
      "Training Batch [100/782]: Loss 0.07774483412504196\n",
      "Training Batch [101/782]: Loss 0.01034160889685154\n",
      "Training Batch [102/782]: Loss 0.01800784468650818\n",
      "Training Batch [103/782]: Loss 0.034836143255233765\n",
      "Training Batch [104/782]: Loss 0.015514888800680637\n",
      "Training Batch [105/782]: Loss 0.077059805393219\n",
      "Training Batch [106/782]: Loss 0.03447650745511055\n",
      "Training Batch [107/782]: Loss 0.025566302239894867\n",
      "Training Batch [108/782]: Loss 0.040784746408462524\n",
      "Training Batch [109/782]: Loss 0.0708429366350174\n",
      "Training Batch [110/782]: Loss 0.05287941172719002\n",
      "Training Batch [111/782]: Loss 0.020757246762514114\n",
      "Training Batch [112/782]: Loss 0.00837040226906538\n",
      "Training Batch [113/782]: Loss 0.07926158607006073\n",
      "Training Batch [114/782]: Loss 0.016591669991612434\n",
      "Training Batch [115/782]: Loss 0.016437234356999397\n",
      "Training Batch [116/782]: Loss 0.08041644096374512\n",
      "Training Batch [117/782]: Loss 0.03768150508403778\n",
      "Training Batch [118/782]: Loss 0.050215594470500946\n",
      "Training Batch [119/782]: Loss 0.023496339097619057\n",
      "Training Batch [120/782]: Loss 0.009011090733110905\n",
      "Training Batch [121/782]: Loss 0.04196135699748993\n",
      "Training Batch [122/782]: Loss 0.01285704318434\n",
      "Training Batch [123/782]: Loss 0.12139342725276947\n",
      "Training Batch [124/782]: Loss 0.07641363888978958\n",
      "Training Batch [125/782]: Loss 0.017445433884859085\n",
      "Training Batch [126/782]: Loss 0.01560614351183176\n",
      "Training Batch [127/782]: Loss 0.05055663362145424\n",
      "Training Batch [128/782]: Loss 0.007668830454349518\n",
      "Training Batch [129/782]: Loss 0.05219597741961479\n",
      "Training Batch [130/782]: Loss 0.10091807693243027\n",
      "Training Batch [131/782]: Loss 0.026664629578590393\n",
      "Training Batch [132/782]: Loss 0.04244991019368172\n",
      "Training Batch [133/782]: Loss 0.008195456117391586\n",
      "Training Batch [134/782]: Loss 0.0357184000313282\n",
      "Training Batch [135/782]: Loss 0.06071498617529869\n",
      "Training Batch [136/782]: Loss 0.0211392343044281\n",
      "Training Batch [137/782]: Loss 0.0036454012151807547\n",
      "Training Batch [138/782]: Loss 0.03405262157320976\n",
      "Training Batch [139/782]: Loss 0.019872013479471207\n",
      "Training Batch [140/782]: Loss 0.005975278560072184\n",
      "Training Batch [141/782]: Loss 0.01742110401391983\n",
      "Training Batch [142/782]: Loss 0.17535248398780823\n",
      "Training Batch [143/782]: Loss 0.027834398671984673\n",
      "Training Batch [144/782]: Loss 0.00785360112786293\n",
      "Training Batch [145/782]: Loss 0.027166754007339478\n",
      "Training Batch [146/782]: Loss 0.05252532660961151\n",
      "Training Batch [147/782]: Loss 0.015805397182703018\n",
      "Training Batch [148/782]: Loss 0.030018704012036324\n",
      "Training Batch [149/782]: Loss 0.15152764320373535\n",
      "Training Batch [150/782]: Loss 0.04077685996890068\n",
      "Training Batch [151/782]: Loss 0.016313636675477028\n",
      "Training Batch [152/782]: Loss 0.0392204187810421\n",
      "Training Batch [153/782]: Loss 0.12732063233852386\n",
      "Training Batch [154/782]: Loss 0.027653194963932037\n",
      "Training Batch [155/782]: Loss 0.07625048607587814\n",
      "Training Batch [156/782]: Loss 0.005425846669822931\n",
      "Training Batch [157/782]: Loss 0.05020317807793617\n",
      "Training Batch [158/782]: Loss 0.06743127852678299\n",
      "Training Batch [159/782]: Loss 0.006978706922382116\n",
      "Training Batch [160/782]: Loss 0.004176303278654814\n",
      "Training Batch [161/782]: Loss 0.013363601639866829\n",
      "Training Batch [162/782]: Loss 0.15746952593326569\n",
      "Training Batch [163/782]: Loss 0.035433534532785416\n",
      "Training Batch [164/782]: Loss 0.027049068361520767\n",
      "Training Batch [165/782]: Loss 0.007267303299158812\n",
      "Training Batch [166/782]: Loss 0.008231104351580143\n",
      "Training Batch [167/782]: Loss 0.05712230131030083\n",
      "Training Batch [168/782]: Loss 0.05781933292746544\n",
      "Training Batch [169/782]: Loss 0.05177919194102287\n",
      "Training Batch [170/782]: Loss 0.0033330172300338745\n",
      "Training Batch [171/782]: Loss 0.023456480354070663\n",
      "Training Batch [172/782]: Loss 0.02719261310994625\n",
      "Training Batch [173/782]: Loss 0.05132554844021797\n",
      "Training Batch [174/782]: Loss 0.13331083953380585\n",
      "Training Batch [175/782]: Loss 0.0183795765042305\n",
      "Training Batch [176/782]: Loss 0.14552488923072815\n",
      "Training Batch [177/782]: Loss 0.029190344735980034\n",
      "Training Batch [178/782]: Loss 0.01600627228617668\n",
      "Training Batch [179/782]: Loss 0.014358209446072578\n",
      "Training Batch [180/782]: Loss 0.049266502261161804\n",
      "Training Batch [181/782]: Loss 0.00888112559914589\n",
      "Training Batch [182/782]: Loss 0.05471065267920494\n",
      "Training Batch [183/782]: Loss 0.04527046158909798\n",
      "Training Batch [184/782]: Loss 0.006326788570731878\n",
      "Training Batch [185/782]: Loss 0.10180611908435822\n",
      "Training Batch [186/782]: Loss 0.094953253865242\n",
      "Training Batch [187/782]: Loss 0.0037222180981189013\n",
      "Training Batch [188/782]: Loss 0.013871125876903534\n",
      "Training Batch [189/782]: Loss 0.04811938479542732\n",
      "Training Batch [190/782]: Loss 0.06350231915712357\n",
      "Training Batch [191/782]: Loss 0.014650657773017883\n",
      "Training Batch [192/782]: Loss 0.046986524015665054\n",
      "Training Batch [193/782]: Loss 0.04175290837883949\n",
      "Training Batch [194/782]: Loss 0.025108763948082924\n",
      "Training Batch [195/782]: Loss 0.033533819019794464\n",
      "Training Batch [196/782]: Loss 0.030746955424547195\n",
      "Training Batch [197/782]: Loss 0.03323367238044739\n",
      "Training Batch [198/782]: Loss 0.02976400963962078\n",
      "Training Batch [199/782]: Loss 0.04599960893392563\n",
      "Training Batch [200/782]: Loss 0.07761210203170776\n",
      "Training Batch [201/782]: Loss 0.028204552829265594\n",
      "Training Batch [202/782]: Loss 0.09277579188346863\n",
      "Training Batch [203/782]: Loss 0.05325016751885414\n",
      "Training Batch [204/782]: Loss 0.02801963873207569\n",
      "Training Batch [205/782]: Loss 0.03362322226166725\n",
      "Training Batch [206/782]: Loss 0.04063250124454498\n",
      "Training Batch [207/782]: Loss 0.016104863956570625\n",
      "Training Batch [208/782]: Loss 0.03954028710722923\n",
      "Training Batch [209/782]: Loss 0.01952415518462658\n",
      "Training Batch [210/782]: Loss 0.01998799480497837\n",
      "Training Batch [211/782]: Loss 0.03780550882220268\n",
      "Training Batch [212/782]: Loss 0.06227162480354309\n",
      "Training Batch [213/782]: Loss 0.13041988015174866\n",
      "Training Batch [214/782]: Loss 0.11121251434087753\n",
      "Training Batch [215/782]: Loss 0.06662320345640182\n",
      "Training Batch [216/782]: Loss 0.06497495621442795\n",
      "Training Batch [217/782]: Loss 0.021920766681432724\n",
      "Training Batch [218/782]: Loss 0.008807925507426262\n",
      "Training Batch [219/782]: Loss 0.009016401134431362\n",
      "Training Batch [220/782]: Loss 0.014835807494819164\n",
      "Training Batch [221/782]: Loss 0.03442457318305969\n",
      "Training Batch [222/782]: Loss 0.008040675893425941\n",
      "Training Batch [223/782]: Loss 0.016252774745225906\n",
      "Training Batch [224/782]: Loss 0.015916647389531136\n",
      "Training Batch [225/782]: Loss 0.05219491943717003\n",
      "Training Batch [226/782]: Loss 0.09562864899635315\n",
      "Training Batch [227/782]: Loss 0.015639852732419968\n",
      "Training Batch [228/782]: Loss 0.06715008616447449\n",
      "Training Batch [229/782]: Loss 0.056232769042253494\n",
      "Training Batch [230/782]: Loss 0.06103967875242233\n",
      "Training Batch [231/782]: Loss 0.00925783533602953\n",
      "Training Batch [232/782]: Loss 0.06755610555410385\n",
      "Training Batch [233/782]: Loss 0.04673806577920914\n",
      "Training Batch [234/782]: Loss 0.07253137975931168\n",
      "Training Batch [235/782]: Loss 0.003817466553300619\n",
      "Training Batch [236/782]: Loss 0.0745311975479126\n",
      "Training Batch [237/782]: Loss 0.044638242572546005\n",
      "Training Batch [238/782]: Loss 0.10040273517370224\n",
      "Training Batch [239/782]: Loss 0.03195933625102043\n",
      "Training Batch [240/782]: Loss 0.11077041923999786\n",
      "Training Batch [241/782]: Loss 0.077543705701828\n",
      "Training Batch [242/782]: Loss 0.11857783794403076\n",
      "Training Batch [243/782]: Loss 0.03785494342446327\n",
      "Training Batch [244/782]: Loss 0.05517706647515297\n",
      "Training Batch [245/782]: Loss 0.11903556436300278\n",
      "Training Batch [246/782]: Loss 0.060859497636556625\n",
      "Training Batch [247/782]: Loss 0.048308975994586945\n",
      "Training Batch [248/782]: Loss 0.04752699285745621\n",
      "Training Batch [249/782]: Loss 0.15240968763828278\n",
      "Training Batch [250/782]: Loss 0.019395969808101654\n",
      "Training Batch [251/782]: Loss 0.1288905292749405\n",
      "Training Batch [252/782]: Loss 0.04884544387459755\n",
      "Training Batch [253/782]: Loss 0.08336851000785828\n",
      "Training Batch [254/782]: Loss 0.014153065159916878\n",
      "Training Batch [255/782]: Loss 0.04596056044101715\n",
      "Training Batch [256/782]: Loss 0.005078664515167475\n",
      "Training Batch [257/782]: Loss 0.07621017843484879\n",
      "Training Batch [258/782]: Loss 0.016045572236180305\n",
      "Training Batch [259/782]: Loss 0.09481117874383926\n",
      "Training Batch [260/782]: Loss 0.06127535551786423\n",
      "Training Batch [261/782]: Loss 0.14319384098052979\n",
      "Training Batch [262/782]: Loss 0.017461808398365974\n",
      "Training Batch [263/782]: Loss 0.14540058374404907\n",
      "Training Batch [264/782]: Loss 0.045977141708135605\n",
      "Training Batch [265/782]: Loss 0.04038340225815773\n",
      "Training Batch [266/782]: Loss 0.025738947093486786\n",
      "Training Batch [267/782]: Loss 0.06313890963792801\n",
      "Training Batch [268/782]: Loss 0.12195238471031189\n",
      "Training Batch [269/782]: Loss 0.1887349933385849\n",
      "Training Batch [270/782]: Loss 0.021692894399166107\n",
      "Training Batch [271/782]: Loss 0.07809779047966003\n",
      "Training Batch [272/782]: Loss 0.1652439385652542\n",
      "Training Batch [273/782]: Loss 0.0327058844268322\n",
      "Training Batch [274/782]: Loss 0.09756342321634293\n",
      "Training Batch [275/782]: Loss 0.08978873491287231\n",
      "Training Batch [276/782]: Loss 0.05961738899350166\n",
      "Training Batch [277/782]: Loss 0.04800117760896683\n",
      "Training Batch [278/782]: Loss 0.009130334481596947\n",
      "Training Batch [279/782]: Loss 0.06467421352863312\n",
      "Training Batch [280/782]: Loss 0.10350433737039566\n",
      "Training Batch [281/782]: Loss 0.022616982460021973\n",
      "Training Batch [282/782]: Loss 0.06928651034832001\n",
      "Training Batch [283/782]: Loss 0.09685343503952026\n",
      "Training Batch [284/782]: Loss 0.06950350105762482\n",
      "Training Batch [285/782]: Loss 0.006206910591572523\n",
      "Training Batch [286/782]: Loss 0.11906211078166962\n",
      "Training Batch [287/782]: Loss 0.10136668384075165\n",
      "Training Batch [288/782]: Loss 0.22733482718467712\n",
      "Training Batch [289/782]: Loss 0.05209482088685036\n",
      "Training Batch [290/782]: Loss 0.054758235812187195\n",
      "Training Batch [291/782]: Loss 0.15790235996246338\n",
      "Training Batch [292/782]: Loss 0.020785856992006302\n",
      "Training Batch [293/782]: Loss 0.060343433171510696\n",
      "Training Batch [294/782]: Loss 0.021311955526471138\n",
      "Training Batch [295/782]: Loss 0.05566844344139099\n",
      "Training Batch [296/782]: Loss 0.06513459980487823\n",
      "Training Batch [297/782]: Loss 0.0372045673429966\n",
      "Training Batch [298/782]: Loss 0.01431210059672594\n",
      "Training Batch [299/782]: Loss 0.023921562358736992\n",
      "Training Batch [300/782]: Loss 0.12079823762178421\n",
      "Training Batch [301/782]: Loss 0.08169388025999069\n",
      "Training Batch [302/782]: Loss 0.03667442500591278\n",
      "Training Batch [303/782]: Loss 0.009573573246598244\n",
      "Training Batch [304/782]: Loss 0.04351601377129555\n",
      "Training Batch [305/782]: Loss 0.1269131451845169\n",
      "Training Batch [306/782]: Loss 0.06376515328884125\n",
      "Training Batch [307/782]: Loss 0.011433209292590618\n",
      "Training Batch [308/782]: Loss 0.1050441786646843\n",
      "Training Batch [309/782]: Loss 0.1544732302427292\n",
      "Training Batch [310/782]: Loss 0.020236970856785774\n",
      "Training Batch [311/782]: Loss 0.1647217720746994\n",
      "Training Batch [312/782]: Loss 0.14411790668964386\n",
      "Training Batch [313/782]: Loss 0.01164451614022255\n",
      "Training Batch [314/782]: Loss 0.003570744302123785\n",
      "Training Batch [315/782]: Loss 0.10483825951814651\n",
      "Training Batch [316/782]: Loss 0.09419088065624237\n",
      "Training Batch [317/782]: Loss 0.059000976383686066\n",
      "Training Batch [318/782]: Loss 0.18390275537967682\n",
      "Training Batch [319/782]: Loss 0.07296714931726456\n",
      "Training Batch [320/782]: Loss 0.04194977134466171\n",
      "Training Batch [321/782]: Loss 0.026400959119200706\n",
      "Training Batch [322/782]: Loss 0.02597993053495884\n",
      "Training Batch [323/782]: Loss 0.09800785034894943\n",
      "Training Batch [324/782]: Loss 0.06234663352370262\n",
      "Training Batch [325/782]: Loss 0.08008913695812225\n",
      "Training Batch [326/782]: Loss 0.15385664999485016\n",
      "Training Batch [327/782]: Loss 0.15339556336402893\n",
      "Training Batch [328/782]: Loss 0.059419505298137665\n",
      "Training Batch [329/782]: Loss 0.07842426002025604\n",
      "Training Batch [330/782]: Loss 0.01890699937939644\n",
      "Training Batch [331/782]: Loss 0.015776755288243294\n",
      "Training Batch [332/782]: Loss 0.026304366067051888\n",
      "Training Batch [333/782]: Loss 0.018742894753813744\n",
      "Training Batch [334/782]: Loss 0.1120319664478302\n",
      "Training Batch [335/782]: Loss 0.03210959956049919\n",
      "Training Batch [336/782]: Loss 0.0849815309047699\n",
      "Training Batch [337/782]: Loss 0.06271107494831085\n",
      "Training Batch [338/782]: Loss 0.11197121441364288\n",
      "Training Batch [339/782]: Loss 0.012131704948842525\n",
      "Training Batch [340/782]: Loss 0.08780164271593094\n",
      "Training Batch [341/782]: Loss 0.11024126410484314\n",
      "Training Batch [342/782]: Loss 0.0488334521651268\n",
      "Training Batch [343/782]: Loss 0.022023417055606842\n",
      "Training Batch [344/782]: Loss 0.021970881149172783\n",
      "Training Batch [345/782]: Loss 0.12030071765184402\n",
      "Training Batch [346/782]: Loss 0.021969156339764595\n",
      "Training Batch [347/782]: Loss 0.09505162388086319\n",
      "Training Batch [348/782]: Loss 0.19596408307552338\n",
      "Training Batch [349/782]: Loss 0.07681538909673691\n",
      "Training Batch [350/782]: Loss 0.08533382415771484\n",
      "Training Batch [351/782]: Loss 0.06054119020700455\n",
      "Training Batch [352/782]: Loss 0.09993097186088562\n",
      "Training Batch [353/782]: Loss 0.20296365022659302\n",
      "Training Batch [354/782]: Loss 0.032251108437776566\n",
      "Training Batch [355/782]: Loss 0.014763768762350082\n",
      "Training Batch [356/782]: Loss 0.13216613233089447\n",
      "Training Batch [357/782]: Loss 0.04374589025974274\n",
      "Training Batch [358/782]: Loss 0.06183844432234764\n",
      "Training Batch [359/782]: Loss 0.013066064566373825\n",
      "Training Batch [360/782]: Loss 0.09918658435344696\n",
      "Training Batch [361/782]: Loss 0.0688614547252655\n",
      "Training Batch [362/782]: Loss 0.05372168868780136\n",
      "Training Batch [363/782]: Loss 0.04430185258388519\n",
      "Training Batch [364/782]: Loss 0.04199199751019478\n",
      "Training Batch [365/782]: Loss 0.037725694477558136\n",
      "Training Batch [366/782]: Loss 0.09699129313230515\n",
      "Training Batch [367/782]: Loss 0.021330690011382103\n",
      "Training Batch [368/782]: Loss 0.029550284147262573\n",
      "Training Batch [369/782]: Loss 0.1005198135972023\n",
      "Training Batch [370/782]: Loss 0.11605460196733475\n",
      "Training Batch [371/782]: Loss 0.006217882502824068\n",
      "Training Batch [372/782]: Loss 0.01880541443824768\n",
      "Training Batch [373/782]: Loss 0.016344081610441208\n",
      "Training Batch [374/782]: Loss 0.01233809906989336\n",
      "Training Batch [375/782]: Loss 0.02062567137181759\n",
      "Training Batch [376/782]: Loss 0.014380769804120064\n",
      "Training Batch [377/782]: Loss 0.12829485535621643\n",
      "Training Batch [378/782]: Loss 0.0054440489038825035\n",
      "Training Batch [379/782]: Loss 0.00795815885066986\n",
      "Training Batch [380/782]: Loss 0.04758541285991669\n",
      "Training Batch [381/782]: Loss 0.12148447334766388\n",
      "Training Batch [382/782]: Loss 0.031056975945830345\n",
      "Training Batch [383/782]: Loss 0.06956084817647934\n",
      "Training Batch [384/782]: Loss 0.021834267303347588\n",
      "Training Batch [385/782]: Loss 0.012254058383405209\n",
      "Training Batch [386/782]: Loss 0.06757991015911102\n",
      "Training Batch [387/782]: Loss 0.09496909379959106\n",
      "Training Batch [388/782]: Loss 0.01839960739016533\n",
      "Training Batch [389/782]: Loss 0.04534761235117912\n",
      "Training Batch [390/782]: Loss 0.03975282609462738\n",
      "Training Batch [391/782]: Loss 0.05590762570500374\n",
      "Training Batch [392/782]: Loss 0.029523147270083427\n",
      "Training Batch [393/782]: Loss 0.0126795107498765\n",
      "Training Batch [394/782]: Loss 0.08275911211967468\n",
      "Training Batch [395/782]: Loss 0.052897147834300995\n",
      "Training Batch [396/782]: Loss 0.21522505581378937\n",
      "Training Batch [397/782]: Loss 0.033087436109781265\n",
      "Training Batch [398/782]: Loss 0.041022155433893204\n",
      "Training Batch [399/782]: Loss 0.05848448723554611\n",
      "Training Batch [400/782]: Loss 0.0015885757748037577\n",
      "Training Batch [401/782]: Loss 0.020123040303587914\n",
      "Training Batch [402/782]: Loss 0.07916859537363052\n",
      "Training Batch [403/782]: Loss 0.15660914778709412\n",
      "Training Batch [404/782]: Loss 0.06371068209409714\n",
      "Training Batch [405/782]: Loss 0.1070963442325592\n",
      "Training Batch [406/782]: Loss 0.015452124178409576\n",
      "Training Batch [407/782]: Loss 0.009558427147567272\n",
      "Training Batch [408/782]: Loss 0.0658465251326561\n",
      "Training Batch [409/782]: Loss 0.06562136858701706\n",
      "Training Batch [410/782]: Loss 0.06706713140010834\n",
      "Training Batch [411/782]: Loss 0.07943473011255264\n",
      "Training Batch [412/782]: Loss 0.12057939171791077\n",
      "Training Batch [413/782]: Loss 0.17320194840431213\n",
      "Training Batch [414/782]: Loss 0.00846697948873043\n",
      "Training Batch [415/782]: Loss 0.015017716214060783\n",
      "Training Batch [416/782]: Loss 0.03299279510974884\n",
      "Training Batch [417/782]: Loss 0.05906030535697937\n",
      "Training Batch [418/782]: Loss 0.02878611348569393\n",
      "Training Batch [419/782]: Loss 0.06350260972976685\n",
      "Training Batch [420/782]: Loss 0.06052611768245697\n",
      "Training Batch [421/782]: Loss 0.05828733369708061\n",
      "Training Batch [422/782]: Loss 0.025291429832577705\n",
      "Training Batch [423/782]: Loss 0.05923004075884819\n",
      "Training Batch [424/782]: Loss 0.08206015825271606\n",
      "Training Batch [425/782]: Loss 0.04001549631357193\n",
      "Training Batch [426/782]: Loss 0.07922141253948212\n",
      "Training Batch [427/782]: Loss 0.08711609244346619\n",
      "Training Batch [428/782]: Loss 0.09887877106666565\n",
      "Training Batch [429/782]: Loss 0.025718102231621742\n",
      "Training Batch [430/782]: Loss 0.04371747747063637\n",
      "Training Batch [431/782]: Loss 0.1126088947057724\n",
      "Training Batch [432/782]: Loss 0.01738300733268261\n",
      "Training Batch [433/782]: Loss 0.06417416781187057\n",
      "Training Batch [434/782]: Loss 0.008702072314918041\n",
      "Training Batch [435/782]: Loss 0.06708754599094391\n",
      "Training Batch [436/782]: Loss 0.030912768095731735\n",
      "Training Batch [437/782]: Loss 0.10283885896205902\n",
      "Training Batch [438/782]: Loss 0.006277554202824831\n",
      "Training Batch [439/782]: Loss 0.028003020212054253\n",
      "Training Batch [440/782]: Loss 0.020028267055749893\n",
      "Training Batch [441/782]: Loss 0.033520035445690155\n",
      "Training Batch [442/782]: Loss 0.027007989585399628\n",
      "Training Batch [443/782]: Loss 0.04694676399230957\n",
      "Training Batch [444/782]: Loss 0.03779273480176926\n",
      "Training Batch [445/782]: Loss 0.006488726008683443\n",
      "Training Batch [446/782]: Loss 0.1251307725906372\n",
      "Training Batch [447/782]: Loss 0.059072624891996384\n",
      "Training Batch [448/782]: Loss 0.008210184052586555\n",
      "Training Batch [449/782]: Loss 0.040552400052547455\n",
      "Training Batch [450/782]: Loss 0.011730130761861801\n",
      "Training Batch [451/782]: Loss 0.06767761707305908\n",
      "Training Batch [452/782]: Loss 0.007748221047222614\n",
      "Training Batch [453/782]: Loss 0.11196347326040268\n",
      "Training Batch [454/782]: Loss 0.019188247621059418\n",
      "Training Batch [455/782]: Loss 0.040589530020952225\n",
      "Training Batch [456/782]: Loss 0.09157878905534744\n",
      "Training Batch [457/782]: Loss 0.007307348307222128\n",
      "Training Batch [458/782]: Loss 0.034016307443380356\n",
      "Training Batch [459/782]: Loss 0.08367874473333359\n",
      "Training Batch [460/782]: Loss 0.017823221161961555\n",
      "Training Batch [461/782]: Loss 0.12266182899475098\n",
      "Training Batch [462/782]: Loss 0.12116673588752747\n",
      "Training Batch [463/782]: Loss 0.010997224599123001\n",
      "Training Batch [464/782]: Loss 0.017353367060422897\n",
      "Training Batch [465/782]: Loss 0.034185755997896194\n",
      "Training Batch [466/782]: Loss 0.039997026324272156\n",
      "Training Batch [467/782]: Loss 0.11319387704133987\n",
      "Training Batch [468/782]: Loss 0.012293655425310135\n",
      "Training Batch [469/782]: Loss 0.07746034860610962\n",
      "Training Batch [470/782]: Loss 0.03797535598278046\n",
      "Training Batch [471/782]: Loss 0.0787554681301117\n",
      "Training Batch [472/782]: Loss 0.03921931982040405\n",
      "Training Batch [473/782]: Loss 0.06614372134208679\n",
      "Training Batch [474/782]: Loss 0.014505426399409771\n",
      "Training Batch [475/782]: Loss 0.0784781351685524\n",
      "Training Batch [476/782]: Loss 0.09442399442195892\n",
      "Training Batch [477/782]: Loss 0.09545493870973587\n",
      "Training Batch [478/782]: Loss 0.008382436819374561\n",
      "Training Batch [479/782]: Loss 0.041267503052949905\n",
      "Training Batch [480/782]: Loss 0.02483203262090683\n",
      "Training Batch [481/782]: Loss 0.09132605791091919\n",
      "Training Batch [482/782]: Loss 0.04258246719837189\n",
      "Training Batch [483/782]: Loss 0.08183670789003372\n",
      "Training Batch [484/782]: Loss 0.07280352711677551\n",
      "Training Batch [485/782]: Loss 0.011948217637836933\n",
      "Training Batch [486/782]: Loss 0.041782595217227936\n",
      "Training Batch [487/782]: Loss 0.04613534361124039\n",
      "Training Batch [488/782]: Loss 0.03559499979019165\n",
      "Training Batch [489/782]: Loss 0.040358081459999084\n",
      "Training Batch [490/782]: Loss 0.04336375743150711\n",
      "Training Batch [491/782]: Loss 0.0303622055798769\n",
      "Training Batch [492/782]: Loss 0.026147745549678802\n",
      "Training Batch [493/782]: Loss 0.05319654569029808\n",
      "Training Batch [494/782]: Loss 0.05670655518770218\n",
      "Training Batch [495/782]: Loss 0.010255885310471058\n",
      "Training Batch [496/782]: Loss 0.05105692893266678\n",
      "Training Batch [497/782]: Loss 0.05711417272686958\n",
      "Training Batch [498/782]: Loss 0.015738211572170258\n",
      "Training Batch [499/782]: Loss 0.07880232483148575\n",
      "Training Batch [500/782]: Loss 0.051277317106723785\n",
      "Training Batch [501/782]: Loss 0.04359379783272743\n",
      "Training Batch [502/782]: Loss 0.05575590580701828\n",
      "Training Batch [503/782]: Loss 0.024229027330875397\n",
      "Training Batch [504/782]: Loss 0.08674883842468262\n",
      "Training Batch [505/782]: Loss 0.052496738731861115\n",
      "Training Batch [506/782]: Loss 0.0033268367405980825\n",
      "Training Batch [507/782]: Loss 0.07461840659379959\n",
      "Training Batch [508/782]: Loss 0.015018569305539131\n",
      "Training Batch [509/782]: Loss 0.013416196219623089\n",
      "Training Batch [510/782]: Loss 0.04580163210630417\n",
      "Training Batch [511/782]: Loss 0.02879304438829422\n",
      "Training Batch [512/782]: Loss 0.1481344997882843\n",
      "Training Batch [513/782]: Loss 0.06292764097452164\n",
      "Training Batch [514/782]: Loss 0.06356864422559738\n",
      "Training Batch [515/782]: Loss 0.01981775462627411\n",
      "Training Batch [516/782]: Loss 0.03326971456408501\n",
      "Training Batch [517/782]: Loss 0.018851112574338913\n",
      "Training Batch [518/782]: Loss 0.026453321799635887\n",
      "Training Batch [519/782]: Loss 0.10214753448963165\n",
      "Training Batch [520/782]: Loss 0.10227224230766296\n",
      "Training Batch [521/782]: Loss 0.015421733260154724\n",
      "Training Batch [522/782]: Loss 0.017775125801563263\n",
      "Training Batch [523/782]: Loss 0.017994970083236694\n",
      "Training Batch [524/782]: Loss 0.005240840837359428\n",
      "Training Batch [525/782]: Loss 0.09161695092916489\n",
      "Training Batch [526/782]: Loss 0.023771625012159348\n",
      "Training Batch [527/782]: Loss 0.024504421278834343\n",
      "Training Batch [528/782]: Loss 0.11946354806423187\n",
      "Training Batch [529/782]: Loss 0.04798657074570656\n",
      "Training Batch [530/782]: Loss 0.03340861573815346\n",
      "Training Batch [531/782]: Loss 0.02518984116613865\n",
      "Training Batch [532/782]: Loss 0.07486699521541595\n",
      "Training Batch [533/782]: Loss 0.007893669418990612\n",
      "Training Batch [534/782]: Loss 0.04467219114303589\n",
      "Training Batch [535/782]: Loss 0.05632908269762993\n",
      "Training Batch [536/782]: Loss 0.06334883719682693\n",
      "Training Batch [537/782]: Loss 0.04398715868592262\n",
      "Training Batch [538/782]: Loss 0.0067418902181088924\n",
      "Training Batch [539/782]: Loss 0.018985019996762276\n",
      "Training Batch [540/782]: Loss 0.06464394927024841\n",
      "Training Batch [541/782]: Loss 0.023192742839455605\n",
      "Training Batch [542/782]: Loss 0.0584234818816185\n",
      "Training Batch [543/782]: Loss 0.017199799418449402\n",
      "Training Batch [544/782]: Loss 0.014885963872075081\n",
      "Training Batch [545/782]: Loss 0.039726585149765015\n",
      "Training Batch [546/782]: Loss 0.07668997347354889\n",
      "Training Batch [547/782]: Loss 0.01853088103234768\n",
      "Training Batch [548/782]: Loss 0.05344770476222038\n",
      "Training Batch [549/782]: Loss 0.03222673758864403\n",
      "Training Batch [550/782]: Loss 0.06575004756450653\n",
      "Training Batch [551/782]: Loss 0.010875982232391834\n",
      "Training Batch [552/782]: Loss 0.06033972278237343\n",
      "Training Batch [553/782]: Loss 0.02252805605530739\n",
      "Training Batch [554/782]: Loss 0.051232773810625076\n",
      "Training Batch [555/782]: Loss 0.003728674491867423\n",
      "Training Batch [556/782]: Loss 0.0167183056473732\n",
      "Training Batch [557/782]: Loss 0.0068301185965538025\n",
      "Training Batch [558/782]: Loss 0.04427481070160866\n",
      "Training Batch [559/782]: Loss 0.0038573911879211664\n",
      "Training Batch [560/782]: Loss 0.013947258703410625\n",
      "Training Batch [561/782]: Loss 0.03427562490105629\n",
      "Training Batch [562/782]: Loss 0.14347980916500092\n",
      "Training Batch [563/782]: Loss 0.040890760719776154\n",
      "Training Batch [564/782]: Loss 0.017946971580386162\n",
      "Training Batch [565/782]: Loss 0.02278021350502968\n",
      "Training Batch [566/782]: Loss 0.026197727769613266\n",
      "Training Batch [567/782]: Loss 0.0625053346157074\n",
      "Training Batch [568/782]: Loss 0.016177991405129433\n",
      "Training Batch [569/782]: Loss 0.05118001997470856\n",
      "Training Batch [570/782]: Loss 0.019385235384106636\n",
      "Training Batch [571/782]: Loss 0.11485051363706589\n",
      "Training Batch [572/782]: Loss 0.06552545726299286\n",
      "Training Batch [573/782]: Loss 0.062278859317302704\n",
      "Training Batch [574/782]: Loss 0.012168346904218197\n",
      "Training Batch [575/782]: Loss 0.039905671030282974\n",
      "Training Batch [576/782]: Loss 0.05118389427661896\n",
      "Training Batch [577/782]: Loss 0.00805226806551218\n",
      "Training Batch [578/782]: Loss 0.012589900754392147\n",
      "Training Batch [579/782]: Loss 0.03722609579563141\n",
      "Training Batch [580/782]: Loss 0.011509861797094345\n",
      "Training Batch [581/782]: Loss 0.11502603441476822\n",
      "Training Batch [582/782]: Loss 0.07048486173152924\n",
      "Training Batch [583/782]: Loss 0.012784004211425781\n",
      "Training Batch [584/782]: Loss 0.06183365359902382\n",
      "Training Batch [585/782]: Loss 0.06499480456113815\n",
      "Training Batch [586/782]: Loss 0.006774043198674917\n",
      "Training Batch [587/782]: Loss 0.03902263566851616\n",
      "Training Batch [588/782]: Loss 0.11443516612052917\n",
      "Training Batch [589/782]: Loss 0.03624030202627182\n",
      "Training Batch [590/782]: Loss 0.007557468954473734\n",
      "Training Batch [591/782]: Loss 0.02943042293190956\n",
      "Training Batch [592/782]: Loss 0.020419377833604813\n",
      "Training Batch [593/782]: Loss 0.026891373097896576\n",
      "Training Batch [594/782]: Loss 0.11859146505594254\n",
      "Training Batch [595/782]: Loss 0.013516769744455814\n",
      "Training Batch [596/782]: Loss 0.015255775302648544\n",
      "Training Batch [597/782]: Loss 0.06755904108285904\n",
      "Training Batch [598/782]: Loss 0.04992126300930977\n",
      "Training Batch [599/782]: Loss 0.11133300513029099\n",
      "Training Batch [600/782]: Loss 0.07657644897699356\n",
      "Training Batch [601/782]: Loss 0.022249285131692886\n",
      "Training Batch [602/782]: Loss 0.16415658593177795\n",
      "Training Batch [603/782]: Loss 0.013924028724431992\n",
      "Training Batch [604/782]: Loss 0.023316701874136925\n",
      "Training Batch [605/782]: Loss 0.04126802831888199\n",
      "Training Batch [606/782]: Loss 0.03496188670396805\n",
      "Training Batch [607/782]: Loss 0.00859047845005989\n",
      "Training Batch [608/782]: Loss 0.10668309777975082\n",
      "Training Batch [609/782]: Loss 0.01632695086300373\n",
      "Training Batch [610/782]: Loss 0.023050183430314064\n",
      "Training Batch [611/782]: Loss 0.16207647323608398\n",
      "Training Batch [612/782]: Loss 0.0573728084564209\n",
      "Training Batch [613/782]: Loss 0.025418970733880997\n",
      "Training Batch [614/782]: Loss 0.013889052905142307\n",
      "Training Batch [615/782]: Loss 0.13419561088085175\n",
      "Training Batch [616/782]: Loss 0.11260869354009628\n",
      "Training Batch [617/782]: Loss 0.02393878810107708\n",
      "Training Batch [618/782]: Loss 0.03579475358128548\n",
      "Training Batch [619/782]: Loss 0.08796990662813187\n",
      "Training Batch [620/782]: Loss 0.025999056175351143\n",
      "Training Batch [621/782]: Loss 0.1091153547167778\n",
      "Training Batch [622/782]: Loss 0.06036248430609703\n",
      "Training Batch [623/782]: Loss 0.04693404585123062\n",
      "Training Batch [624/782]: Loss 0.02478547766804695\n",
      "Training Batch [625/782]: Loss 0.008897611871361732\n",
      "Training Batch [626/782]: Loss 0.027952799573540688\n",
      "Training Batch [627/782]: Loss 0.06163480505347252\n",
      "Training Batch [628/782]: Loss 0.03396271541714668\n",
      "Training Batch [629/782]: Loss 0.03544880077242851\n",
      "Training Batch [630/782]: Loss 0.15074993669986725\n",
      "Training Batch [631/782]: Loss 0.07359597086906433\n",
      "Training Batch [632/782]: Loss 0.026730919256806374\n",
      "Training Batch [633/782]: Loss 0.017177393659949303\n",
      "Training Batch [634/782]: Loss 0.048583682626485825\n",
      "Training Batch [635/782]: Loss 0.09223447740077972\n",
      "Training Batch [636/782]: Loss 0.0852210521697998\n",
      "Training Batch [637/782]: Loss 0.08447347581386566\n",
      "Training Batch [638/782]: Loss 0.09209086000919342\n",
      "Training Batch [639/782]: Loss 0.043475814163684845\n",
      "Training Batch [640/782]: Loss 0.04983925819396973\n",
      "Training Batch [641/782]: Loss 0.07176920026540756\n",
      "Training Batch [642/782]: Loss 0.08877069503068924\n",
      "Training Batch [643/782]: Loss 0.03740018233656883\n",
      "Training Batch [644/782]: Loss 0.18416796624660492\n",
      "Training Batch [645/782]: Loss 0.025061674416065216\n",
      "Training Batch [646/782]: Loss 0.12742571532726288\n",
      "Training Batch [647/782]: Loss 0.03569621965289116\n",
      "Training Batch [648/782]: Loss 0.04619552567601204\n",
      "Training Batch [649/782]: Loss 0.07164554297924042\n",
      "Training Batch [650/782]: Loss 0.08988729119300842\n",
      "Training Batch [651/782]: Loss 0.1137559711933136\n",
      "Training Batch [652/782]: Loss 0.03611911088228226\n",
      "Training Batch [653/782]: Loss 0.029733287170529366\n",
      "Training Batch [654/782]: Loss 0.06131711229681969\n",
      "Training Batch [655/782]: Loss 0.22422310709953308\n",
      "Training Batch [656/782]: Loss 0.053295496851205826\n",
      "Training Batch [657/782]: Loss 0.014409002847969532\n",
      "Training Batch [658/782]: Loss 0.045142222195863724\n",
      "Training Batch [659/782]: Loss 0.03595813736319542\n",
      "Training Batch [660/782]: Loss 0.023433387279510498\n",
      "Training Batch [661/782]: Loss 0.04527602344751358\n",
      "Training Batch [662/782]: Loss 0.016360588371753693\n",
      "Training Batch [663/782]: Loss 0.031249497085809708\n",
      "Training Batch [664/782]: Loss 0.04068056866526604\n",
      "Training Batch [665/782]: Loss 0.03164539486169815\n",
      "Training Batch [666/782]: Loss 0.019083522260189056\n",
      "Training Batch [667/782]: Loss 0.022407835349440575\n",
      "Training Batch [668/782]: Loss 0.1287572830915451\n",
      "Training Batch [669/782]: Loss 0.04515939950942993\n",
      "Training Batch [670/782]: Loss 0.12715113162994385\n",
      "Training Batch [671/782]: Loss 0.09735950082540512\n",
      "Training Batch [672/782]: Loss 0.03801889717578888\n",
      "Training Batch [673/782]: Loss 0.014260198920965195\n",
      "Training Batch [674/782]: Loss 0.060676317662000656\n",
      "Training Batch [675/782]: Loss 0.03340380638837814\n",
      "Training Batch [676/782]: Loss 0.011494306847453117\n",
      "Training Batch [677/782]: Loss 0.030437316745519638\n",
      "Training Batch [678/782]: Loss 0.021140486001968384\n",
      "Training Batch [679/782]: Loss 0.07937463372945786\n",
      "Training Batch [680/782]: Loss 0.013027974404394627\n",
      "Training Batch [681/782]: Loss 0.0053587122820317745\n",
      "Training Batch [682/782]: Loss 0.012183625251054764\n",
      "Training Batch [683/782]: Loss 0.043044645339250565\n",
      "Training Batch [684/782]: Loss 0.012821627780795097\n",
      "Training Batch [685/782]: Loss 0.09969512373209\n",
      "Training Batch [686/782]: Loss 0.06962453573942184\n",
      "Training Batch [687/782]: Loss 0.05625985935330391\n",
      "Training Batch [688/782]: Loss 0.022560376673936844\n",
      "Training Batch [689/782]: Loss 0.01947961188852787\n",
      "Training Batch [690/782]: Loss 0.026528095826506615\n",
      "Training Batch [691/782]: Loss 0.04716935381293297\n",
      "Training Batch [692/782]: Loss 0.05762706696987152\n",
      "Training Batch [693/782]: Loss 0.056301772594451904\n",
      "Training Batch [694/782]: Loss 0.06699942797422409\n",
      "Training Batch [695/782]: Loss 0.05598196014761925\n",
      "Training Batch [696/782]: Loss 0.08286866545677185\n",
      "Training Batch [697/782]: Loss 0.01819620467722416\n",
      "Training Batch [698/782]: Loss 0.018485572189092636\n",
      "Training Batch [699/782]: Loss 0.045867934823036194\n",
      "Training Batch [700/782]: Loss 0.007312909699976444\n",
      "Training Batch [701/782]: Loss 0.01716722920536995\n",
      "Training Batch [702/782]: Loss 0.021289365366101265\n",
      "Training Batch [703/782]: Loss 0.056084319949150085\n",
      "Training Batch [704/782]: Loss 0.11057247966527939\n",
      "Training Batch [705/782]: Loss 0.020051317289471626\n",
      "Training Batch [706/782]: Loss 0.04499862715601921\n",
      "Training Batch [707/782]: Loss 0.00951655488461256\n",
      "Training Batch [708/782]: Loss 0.019673412665724754\n",
      "Training Batch [709/782]: Loss 0.042785994708538055\n",
      "Training Batch [710/782]: Loss 0.08003111928701401\n",
      "Training Batch [711/782]: Loss 0.052577905356884\n",
      "Training Batch [712/782]: Loss 0.060083433985710144\n",
      "Training Batch [713/782]: Loss 0.07275165617465973\n",
      "Training Batch [714/782]: Loss 0.014828795567154884\n",
      "Training Batch [715/782]: Loss 0.038588665425777435\n",
      "Training Batch [716/782]: Loss 0.03211034834384918\n",
      "Training Batch [717/782]: Loss 0.11211863905191422\n",
      "Training Batch [718/782]: Loss 0.02190098725259304\n",
      "Training Batch [719/782]: Loss 0.02731039747595787\n",
      "Training Batch [720/782]: Loss 0.11624657362699509\n",
      "Training Batch [721/782]: Loss 0.03257063403725624\n",
      "Training Batch [722/782]: Loss 0.012672442942857742\n",
      "Training Batch [723/782]: Loss 0.025760149583220482\n",
      "Training Batch [724/782]: Loss 0.014087601564824581\n",
      "Training Batch [725/782]: Loss 0.018734218552708626\n",
      "Training Batch [726/782]: Loss 0.007718137465417385\n",
      "Training Batch [727/782]: Loss 0.027295099571347237\n",
      "Training Batch [728/782]: Loss 0.110205739736557\n",
      "Training Batch [729/782]: Loss 0.016586557030677795\n",
      "Training Batch [730/782]: Loss 0.04723084345459938\n",
      "Training Batch [731/782]: Loss 0.049463823437690735\n",
      "Training Batch [732/782]: Loss 0.035866629332304\n",
      "Training Batch [733/782]: Loss 0.13051822781562805\n",
      "Training Batch [734/782]: Loss 0.037975817918777466\n",
      "Training Batch [735/782]: Loss 0.02790447697043419\n",
      "Training Batch [736/782]: Loss 0.013631340116262436\n",
      "Training Batch [737/782]: Loss 0.08333923667669296\n",
      "Training Batch [738/782]: Loss 0.03190324828028679\n",
      "Training Batch [739/782]: Loss 0.06499700248241425\n",
      "Training Batch [740/782]: Loss 0.008382299914956093\n",
      "Training Batch [741/782]: Loss 0.007546757347881794\n",
      "Training Batch [742/782]: Loss 0.06880493462085724\n",
      "Training Batch [743/782]: Loss 0.002963320817798376\n",
      "Training Batch [744/782]: Loss 0.038064949214458466\n",
      "Training Batch [745/782]: Loss 0.0462828166782856\n",
      "Training Batch [746/782]: Loss 0.014173918403685093\n",
      "Training Batch [747/782]: Loss 0.047024887055158615\n",
      "Training Batch [748/782]: Loss 0.06311628967523575\n",
      "Training Batch [749/782]: Loss 0.09527745842933655\n",
      "Training Batch [750/782]: Loss 0.07753758132457733\n",
      "Training Batch [751/782]: Loss 0.029642147943377495\n",
      "Training Batch [752/782]: Loss 0.023917246609926224\n",
      "Training Batch [753/782]: Loss 0.01031270157545805\n",
      "Training Batch [754/782]: Loss 0.01350345741957426\n",
      "Training Batch [755/782]: Loss 0.035056617110967636\n",
      "Training Batch [756/782]: Loss 0.08575543016195297\n",
      "Training Batch [757/782]: Loss 0.01508744154125452\n",
      "Training Batch [758/782]: Loss 0.049895137548446655\n",
      "Training Batch [759/782]: Loss 0.01244742888957262\n",
      "Training Batch [760/782]: Loss 0.024104783311486244\n",
      "Training Batch [761/782]: Loss 0.035373665392398834\n",
      "Training Batch [762/782]: Loss 0.10268642008304596\n",
      "Training Batch [763/782]: Loss 0.03625790402293205\n",
      "Training Batch [764/782]: Loss 0.03197937831282616\n",
      "Training Batch [765/782]: Loss 0.025562573224306107\n",
      "Training Batch [766/782]: Loss 0.029281601309776306\n",
      "Training Batch [767/782]: Loss 0.008151539601385593\n",
      "Training Batch [768/782]: Loss 0.026441216468811035\n",
      "Training Batch [769/782]: Loss 0.04493662342429161\n",
      "Training Batch [770/782]: Loss 0.016437331214547157\n",
      "Training Batch [771/782]: Loss 0.028438257053494453\n",
      "Training Batch [772/782]: Loss 0.1692882478237152\n",
      "Training Batch [773/782]: Loss 0.03166225552558899\n",
      "Training Batch [774/782]: Loss 0.03915302827954292\n",
      "Training Batch [775/782]: Loss 0.04187469184398651\n",
      "Training Batch [776/782]: Loss 0.009544631466269493\n",
      "Training Batch [777/782]: Loss 0.035694804042577744\n",
      "Training Batch [778/782]: Loss 0.07337073236703873\n",
      "Training Batch [779/782]: Loss 0.004023918882012367\n",
      "Training Batch [780/782]: Loss 0.007296146359294653\n",
      "Training Batch [781/782]: Loss 0.012645700946450233\n",
      "Training Batch [782/782]: Loss 0.050376374274492264\n",
      "Epoch 17 - Train Loss: 0.0524\n",
      "*********  Epoch 18/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.017394397407770157\n",
      "Training Batch [2/782]: Loss 0.03942537307739258\n",
      "Training Batch [3/782]: Loss 0.04941323772072792\n",
      "Training Batch [4/782]: Loss 0.009529132395982742\n",
      "Training Batch [5/782]: Loss 0.022444628179073334\n",
      "Training Batch [6/782]: Loss 0.05379017814993858\n",
      "Training Batch [7/782]: Loss 0.03171065077185631\n",
      "Training Batch [8/782]: Loss 0.08033395558595657\n",
      "Training Batch [9/782]: Loss 0.008929365314543247\n",
      "Training Batch [10/782]: Loss 0.04145905375480652\n",
      "Training Batch [11/782]: Loss 0.015186850912868977\n",
      "Training Batch [12/782]: Loss 0.02481311559677124\n",
      "Training Batch [13/782]: Loss 0.059732623398303986\n",
      "Training Batch [14/782]: Loss 0.018348241224884987\n",
      "Training Batch [15/782]: Loss 0.06434718519449234\n",
      "Training Batch [16/782]: Loss 0.004720725584775209\n",
      "Training Batch [17/782]: Loss 0.0021509367506951094\n",
      "Training Batch [18/782]: Loss 0.009903422556817532\n",
      "Training Batch [19/782]: Loss 0.0626746192574501\n",
      "Training Batch [20/782]: Loss 0.015532223507761955\n",
      "Training Batch [21/782]: Loss 0.08585502207279205\n",
      "Training Batch [22/782]: Loss 0.016406487673521042\n",
      "Training Batch [23/782]: Loss 0.04851870611310005\n",
      "Training Batch [24/782]: Loss 0.07216919213533401\n",
      "Training Batch [25/782]: Loss 0.0494505949318409\n",
      "Training Batch [26/782]: Loss 0.02625676430761814\n",
      "Training Batch [27/782]: Loss 0.006487695500254631\n",
      "Training Batch [28/782]: Loss 0.05497464910149574\n",
      "Training Batch [29/782]: Loss 0.01847689040005207\n",
      "Training Batch [30/782]: Loss 0.02379821240901947\n",
      "Training Batch [31/782]: Loss 0.008565070107579231\n",
      "Training Batch [32/782]: Loss 0.03343205526471138\n",
      "Training Batch [33/782]: Loss 0.00436049560084939\n",
      "Training Batch [34/782]: Loss 0.023912876844406128\n",
      "Training Batch [35/782]: Loss 0.05387473106384277\n",
      "Training Batch [36/782]: Loss 0.0110459690913558\n",
      "Training Batch [37/782]: Loss 0.0055410973727703094\n",
      "Training Batch [38/782]: Loss 0.0254001934081316\n",
      "Training Batch [39/782]: Loss 0.037833716720342636\n",
      "Training Batch [40/782]: Loss 0.1279931366443634\n",
      "Training Batch [41/782]: Loss 0.09187053143978119\n",
      "Training Batch [42/782]: Loss 0.031314119696617126\n",
      "Training Batch [43/782]: Loss 0.01391565054655075\n",
      "Training Batch [44/782]: Loss 0.02706158719956875\n",
      "Training Batch [45/782]: Loss 0.0024452584329992533\n",
      "Training Batch [46/782]: Loss 0.015453052707016468\n",
      "Training Batch [47/782]: Loss 0.07296019047498703\n",
      "Training Batch [48/782]: Loss 0.02358461171388626\n",
      "Training Batch [49/782]: Loss 0.011981725692749023\n",
      "Training Batch [50/782]: Loss 0.030556846410036087\n",
      "Training Batch [51/782]: Loss 0.0513216108083725\n",
      "Training Batch [52/782]: Loss 0.05376362055540085\n",
      "Training Batch [53/782]: Loss 0.007545860018581152\n",
      "Training Batch [54/782]: Loss 0.00212273420765996\n",
      "Training Batch [55/782]: Loss 0.011369825340807438\n",
      "Training Batch [56/782]: Loss 0.02201276458799839\n",
      "Training Batch [57/782]: Loss 0.04847555235028267\n",
      "Training Batch [58/782]: Loss 0.011426161974668503\n",
      "Training Batch [59/782]: Loss 0.017238842323422432\n",
      "Training Batch [60/782]: Loss 0.0034930321853607893\n",
      "Training Batch [61/782]: Loss 0.054521095007658005\n",
      "Training Batch [62/782]: Loss 0.009640147909522057\n",
      "Training Batch [63/782]: Loss 0.02200401946902275\n",
      "Training Batch [64/782]: Loss 0.06892604380846024\n",
      "Training Batch [65/782]: Loss 0.029564106836915016\n",
      "Training Batch [66/782]: Loss 0.010720383375883102\n",
      "Training Batch [67/782]: Loss 0.027675487101078033\n",
      "Training Batch [68/782]: Loss 0.008944384753704071\n",
      "Training Batch [69/782]: Loss 0.002389594679698348\n",
      "Training Batch [70/782]: Loss 0.05136372148990631\n",
      "Training Batch [71/782]: Loss 0.054744649678468704\n",
      "Training Batch [72/782]: Loss 0.015495200641453266\n",
      "Training Batch [73/782]: Loss 0.045750901103019714\n",
      "Training Batch [74/782]: Loss 0.004777830559760332\n",
      "Training Batch [75/782]: Loss 0.03338997811079025\n",
      "Training Batch [76/782]: Loss 0.012294700369238853\n",
      "Training Batch [77/782]: Loss 0.007464899215847254\n",
      "Training Batch [78/782]: Loss 0.06046082079410553\n",
      "Training Batch [79/782]: Loss 0.006030010059475899\n",
      "Training Batch [80/782]: Loss 0.015099274925887585\n",
      "Training Batch [81/782]: Loss 0.027057578787207603\n",
      "Training Batch [82/782]: Loss 0.027478720992803574\n",
      "Training Batch [83/782]: Loss 0.009145557880401611\n",
      "Training Batch [84/782]: Loss 0.024226482957601547\n",
      "Training Batch [85/782]: Loss 0.008054008707404137\n",
      "Training Batch [86/782]: Loss 0.01748698763549328\n",
      "Training Batch [87/782]: Loss 0.005603187717497349\n",
      "Training Batch [88/782]: Loss 0.05323363468050957\n",
      "Training Batch [89/782]: Loss 0.009144452400505543\n",
      "Training Batch [90/782]: Loss 0.004613438155502081\n",
      "Training Batch [91/782]: Loss 0.040020767599344254\n",
      "Training Batch [92/782]: Loss 0.015422780066728592\n",
      "Training Batch [93/782]: Loss 0.02080455981194973\n",
      "Training Batch [94/782]: Loss 0.019503816962242126\n",
      "Training Batch [95/782]: Loss 0.1382078230381012\n",
      "Training Batch [96/782]: Loss 0.03527279570698738\n",
      "Training Batch [97/782]: Loss 0.13240951299667358\n",
      "Training Batch [98/782]: Loss 0.020322266966104507\n",
      "Training Batch [99/782]: Loss 0.004270740319043398\n",
      "Training Batch [100/782]: Loss 0.1452147513628006\n",
      "Training Batch [101/782]: Loss 0.043531060218811035\n",
      "Training Batch [102/782]: Loss 0.004201716277748346\n",
      "Training Batch [103/782]: Loss 0.012125641107559204\n",
      "Training Batch [104/782]: Loss 0.027848754078149796\n",
      "Training Batch [105/782]: Loss 0.048465948551893234\n",
      "Training Batch [106/782]: Loss 0.008746548555791378\n",
      "Training Batch [107/782]: Loss 0.059614066034555435\n",
      "Training Batch [108/782]: Loss 0.07705532759428024\n",
      "Training Batch [109/782]: Loss 0.07855604588985443\n",
      "Training Batch [110/782]: Loss 0.03154250606894493\n",
      "Training Batch [111/782]: Loss 0.025202052667737007\n",
      "Training Batch [112/782]: Loss 0.033248286694288254\n",
      "Training Batch [113/782]: Loss 0.02312309853732586\n",
      "Training Batch [114/782]: Loss 0.034511469304561615\n",
      "Training Batch [115/782]: Loss 0.023465443402528763\n",
      "Training Batch [116/782]: Loss 0.020128436386585236\n",
      "Training Batch [117/782]: Loss 0.023072658106684685\n",
      "Training Batch [118/782]: Loss 0.04163472354412079\n",
      "Training Batch [119/782]: Loss 0.022337527945637703\n",
      "Training Batch [120/782]: Loss 0.07572288811206818\n",
      "Training Batch [121/782]: Loss 0.011202260851860046\n",
      "Training Batch [122/782]: Loss 0.032756611704826355\n",
      "Training Batch [123/782]: Loss 0.13336819410324097\n",
      "Training Batch [124/782]: Loss 0.04886127635836601\n",
      "Training Batch [125/782]: Loss 0.0592489019036293\n",
      "Training Batch [126/782]: Loss 0.06063215807080269\n",
      "Training Batch [127/782]: Loss 0.0032540378160774708\n",
      "Training Batch [128/782]: Loss 0.01778530701994896\n",
      "Training Batch [129/782]: Loss 0.01865655556321144\n",
      "Training Batch [130/782]: Loss 0.02474832534790039\n",
      "Training Batch [131/782]: Loss 0.030186237767338753\n",
      "Training Batch [132/782]: Loss 0.04073130711913109\n",
      "Training Batch [133/782]: Loss 0.014332379214465618\n",
      "Training Batch [134/782]: Loss 0.007836293429136276\n",
      "Training Batch [135/782]: Loss 0.05795849859714508\n",
      "Training Batch [136/782]: Loss 0.0636342391371727\n",
      "Training Batch [137/782]: Loss 0.006547125522047281\n",
      "Training Batch [138/782]: Loss 0.009279410354793072\n",
      "Training Batch [139/782]: Loss 0.096011221408844\n",
      "Training Batch [140/782]: Loss 0.012600812129676342\n",
      "Training Batch [141/782]: Loss 0.019872305914759636\n",
      "Training Batch [142/782]: Loss 0.05275242030620575\n",
      "Training Batch [143/782]: Loss 0.0074234576895833015\n",
      "Training Batch [144/782]: Loss 0.09672275930643082\n",
      "Training Batch [145/782]: Loss 0.0409933440387249\n",
      "Training Batch [146/782]: Loss 0.016060922294855118\n",
      "Training Batch [147/782]: Loss 0.039902761578559875\n",
      "Training Batch [148/782]: Loss 0.02014467678964138\n",
      "Training Batch [149/782]: Loss 0.00915373396128416\n",
      "Training Batch [150/782]: Loss 0.01682642474770546\n",
      "Training Batch [151/782]: Loss 0.0071205636486411095\n",
      "Training Batch [152/782]: Loss 0.06473638117313385\n",
      "Training Batch [153/782]: Loss 0.044374607503414154\n",
      "Training Batch [154/782]: Loss 0.07001082599163055\n",
      "Training Batch [155/782]: Loss 0.05030569061636925\n",
      "Training Batch [156/782]: Loss 0.05213918909430504\n",
      "Training Batch [157/782]: Loss 0.01405417826026678\n",
      "Training Batch [158/782]: Loss 0.0026096210349351168\n",
      "Training Batch [159/782]: Loss 0.016841474920511246\n",
      "Training Batch [160/782]: Loss 0.003785718698054552\n",
      "Training Batch [161/782]: Loss 0.0031083214562386274\n",
      "Training Batch [162/782]: Loss 0.10600973665714264\n",
      "Training Batch [163/782]: Loss 0.05128207430243492\n",
      "Training Batch [164/782]: Loss 0.04374992474913597\n",
      "Training Batch [165/782]: Loss 0.04889941215515137\n",
      "Training Batch [166/782]: Loss 0.017285609617829323\n",
      "Training Batch [167/782]: Loss 0.045207805931568146\n",
      "Training Batch [168/782]: Loss 0.019359581172466278\n",
      "Training Batch [169/782]: Loss 0.012016721069812775\n",
      "Training Batch [170/782]: Loss 0.018987104296684265\n",
      "Training Batch [171/782]: Loss 0.021509310230612755\n",
      "Training Batch [172/782]: Loss 0.028271839022636414\n",
      "Training Batch [173/782]: Loss 0.024583477526903152\n",
      "Training Batch [174/782]: Loss 0.04489238187670708\n",
      "Training Batch [175/782]: Loss 0.07139839977025986\n",
      "Training Batch [176/782]: Loss 0.00324554112739861\n",
      "Training Batch [177/782]: Loss 0.00921725481748581\n",
      "Training Batch [178/782]: Loss 0.042897097766399384\n",
      "Training Batch [179/782]: Loss 0.011857698671519756\n",
      "Training Batch [180/782]: Loss 0.12157676368951797\n",
      "Training Batch [181/782]: Loss 0.0024601719342172146\n",
      "Training Batch [182/782]: Loss 0.01403309591114521\n",
      "Training Batch [183/782]: Loss 0.04872588813304901\n",
      "Training Batch [184/782]: Loss 0.009262842126190662\n",
      "Training Batch [185/782]: Loss 0.04295109584927559\n",
      "Training Batch [186/782]: Loss 0.06837273389101028\n",
      "Training Batch [187/782]: Loss 0.00439077615737915\n",
      "Training Batch [188/782]: Loss 0.015115495771169662\n",
      "Training Batch [189/782]: Loss 0.014534056186676025\n",
      "Training Batch [190/782]: Loss 0.008246593177318573\n",
      "Training Batch [191/782]: Loss 0.0670415610074997\n",
      "Training Batch [192/782]: Loss 0.002557201310992241\n",
      "Training Batch [193/782]: Loss 0.02920473739504814\n",
      "Training Batch [194/782]: Loss 0.014441094361245632\n",
      "Training Batch [195/782]: Loss 0.02927369996905327\n",
      "Training Batch [196/782]: Loss 0.010996829718351364\n",
      "Training Batch [197/782]: Loss 0.018428364768624306\n",
      "Training Batch [198/782]: Loss 0.009093659929931164\n",
      "Training Batch [199/782]: Loss 0.07237742841243744\n",
      "Training Batch [200/782]: Loss 0.014959053136408329\n",
      "Training Batch [201/782]: Loss 0.017482353374361992\n",
      "Training Batch [202/782]: Loss 0.00444957846775651\n",
      "Training Batch [203/782]: Loss 0.0067446427419781685\n",
      "Training Batch [204/782]: Loss 0.02386060543358326\n",
      "Training Batch [205/782]: Loss 0.08979934453964233\n",
      "Training Batch [206/782]: Loss 0.022362982854247093\n",
      "Training Batch [207/782]: Loss 0.05187956988811493\n",
      "Training Batch [208/782]: Loss 0.07698392868041992\n",
      "Training Batch [209/782]: Loss 0.02679201401770115\n",
      "Training Batch [210/782]: Loss 0.018985174596309662\n",
      "Training Batch [211/782]: Loss 0.006986070889979601\n",
      "Training Batch [212/782]: Loss 0.009693705476820469\n",
      "Training Batch [213/782]: Loss 0.042069412767887115\n",
      "Training Batch [214/782]: Loss 0.007139889523386955\n",
      "Training Batch [215/782]: Loss 0.006815769709646702\n",
      "Training Batch [216/782]: Loss 0.0893520712852478\n",
      "Training Batch [217/782]: Loss 0.04980292171239853\n",
      "Training Batch [218/782]: Loss 0.005015447735786438\n",
      "Training Batch [219/782]: Loss 0.02498008869588375\n",
      "Training Batch [220/782]: Loss 0.032078567892313004\n",
      "Training Batch [221/782]: Loss 0.08037807047367096\n",
      "Training Batch [222/782]: Loss 0.004221334122121334\n",
      "Training Batch [223/782]: Loss 0.01541663147509098\n",
      "Training Batch [224/782]: Loss 0.027841957286000252\n",
      "Training Batch [225/782]: Loss 0.009750591591000557\n",
      "Training Batch [226/782]: Loss 0.011695974506437778\n",
      "Training Batch [227/782]: Loss 0.05720284953713417\n",
      "Training Batch [228/782]: Loss 0.014267225749790668\n",
      "Training Batch [229/782]: Loss 0.004024163354188204\n",
      "Training Batch [230/782]: Loss 0.015069439075887203\n",
      "Training Batch [231/782]: Loss 0.043903447687625885\n",
      "Training Batch [232/782]: Loss 0.03998381644487381\n",
      "Training Batch [233/782]: Loss 0.01382865197956562\n",
      "Training Batch [234/782]: Loss 0.04556610435247421\n",
      "Training Batch [235/782]: Loss 0.02920876070857048\n",
      "Training Batch [236/782]: Loss 0.005596307571977377\n",
      "Training Batch [237/782]: Loss 0.006115501280874014\n",
      "Training Batch [238/782]: Loss 0.010916941799223423\n",
      "Training Batch [239/782]: Loss 0.020814215764403343\n",
      "Training Batch [240/782]: Loss 0.006241928786039352\n",
      "Training Batch [241/782]: Loss 0.024097632616758347\n",
      "Training Batch [242/782]: Loss 0.054826006293296814\n",
      "Training Batch [243/782]: Loss 0.011637433432042599\n",
      "Training Batch [244/782]: Loss 0.0013678023824468255\n",
      "Training Batch [245/782]: Loss 0.022095635533332825\n",
      "Training Batch [246/782]: Loss 0.01943076029419899\n",
      "Training Batch [247/782]: Loss 0.017762036994099617\n",
      "Training Batch [248/782]: Loss 0.016622819006443024\n",
      "Training Batch [249/782]: Loss 0.013419046998023987\n",
      "Training Batch [250/782]: Loss 0.027655908837914467\n",
      "Training Batch [251/782]: Loss 0.006614033132791519\n",
      "Training Batch [252/782]: Loss 0.00358943035826087\n",
      "Training Batch [253/782]: Loss 0.006690720561891794\n",
      "Training Batch [254/782]: Loss 0.010107207112014294\n",
      "Training Batch [255/782]: Loss 0.013279914855957031\n",
      "Training Batch [256/782]: Loss 0.033035386353731155\n",
      "Training Batch [257/782]: Loss 0.020697902888059616\n",
      "Training Batch [258/782]: Loss 0.013431455008685589\n",
      "Training Batch [259/782]: Loss 0.019156992435455322\n",
      "Training Batch [260/782]: Loss 0.04891050606966019\n",
      "Training Batch [261/782]: Loss 0.014194601215422153\n",
      "Training Batch [262/782]: Loss 0.04946738854050636\n",
      "Training Batch [263/782]: Loss 0.02661924622952938\n",
      "Training Batch [264/782]: Loss 0.018735375255346298\n",
      "Training Batch [265/782]: Loss 0.04519319161772728\n",
      "Training Batch [266/782]: Loss 0.0304759182035923\n",
      "Training Batch [267/782]: Loss 0.013406132347881794\n",
      "Training Batch [268/782]: Loss 0.012417331337928772\n",
      "Training Batch [269/782]: Loss 0.027251556515693665\n",
      "Training Batch [270/782]: Loss 0.006854405160993338\n",
      "Training Batch [271/782]: Loss 0.006676925346255302\n",
      "Training Batch [272/782]: Loss 0.007191698532551527\n",
      "Training Batch [273/782]: Loss 0.003024979494512081\n",
      "Training Batch [274/782]: Loss 0.012374155223369598\n",
      "Training Batch [275/782]: Loss 0.02339768409729004\n",
      "Training Batch [276/782]: Loss 0.03342120349407196\n",
      "Training Batch [277/782]: Loss 0.022212859243154526\n",
      "Training Batch [278/782]: Loss 0.009053532034158707\n",
      "Training Batch [279/782]: Loss 0.008978481404483318\n",
      "Training Batch [280/782]: Loss 0.035499583929777145\n",
      "Training Batch [281/782]: Loss 0.0025869058445096016\n",
      "Training Batch [282/782]: Loss 0.020233286544680595\n",
      "Training Batch [283/782]: Loss 0.013452139683067799\n",
      "Training Batch [284/782]: Loss 0.012637432664632797\n",
      "Training Batch [285/782]: Loss 0.0014254042180255055\n",
      "Training Batch [286/782]: Loss 0.11114097386598587\n",
      "Training Batch [287/782]: Loss 0.035124145448207855\n",
      "Training Batch [288/782]: Loss 0.008893352001905441\n",
      "Training Batch [289/782]: Loss 0.027639713138341904\n",
      "Training Batch [290/782]: Loss 0.012902077287435532\n",
      "Training Batch [291/782]: Loss 0.001077805645763874\n",
      "Training Batch [292/782]: Loss 0.0021847581956535578\n",
      "Training Batch [293/782]: Loss 0.01086547039449215\n",
      "Training Batch [294/782]: Loss 0.011923596262931824\n",
      "Training Batch [295/782]: Loss 0.011716149747371674\n",
      "Training Batch [296/782]: Loss 0.012263539247214794\n",
      "Training Batch [297/782]: Loss 0.001798356301151216\n",
      "Training Batch [298/782]: Loss 0.007994069717824459\n",
      "Training Batch [299/782]: Loss 0.011722723953425884\n",
      "Training Batch [300/782]: Loss 0.019038058817386627\n",
      "Training Batch [301/782]: Loss 0.0026860216166824102\n",
      "Training Batch [302/782]: Loss 0.018135905265808105\n",
      "Training Batch [303/782]: Loss 0.007572885602712631\n",
      "Training Batch [304/782]: Loss 0.02638900838792324\n",
      "Training Batch [305/782]: Loss 0.004558103624731302\n",
      "Training Batch [306/782]: Loss 0.01983608305454254\n",
      "Training Batch [307/782]: Loss 0.062061309814453125\n",
      "Training Batch [308/782]: Loss 0.01423073559999466\n",
      "Training Batch [309/782]: Loss 0.04955591633915901\n",
      "Training Batch [310/782]: Loss 0.004464054945856333\n",
      "Training Batch [311/782]: Loss 0.09793659299612045\n",
      "Training Batch [312/782]: Loss 0.007404759991914034\n",
      "Training Batch [313/782]: Loss 0.009874144569039345\n",
      "Training Batch [314/782]: Loss 0.052844610065221786\n",
      "Training Batch [315/782]: Loss 0.0028229751624166965\n",
      "Training Batch [316/782]: Loss 0.014164214953780174\n",
      "Training Batch [317/782]: Loss 0.0225776806473732\n",
      "Training Batch [318/782]: Loss 0.019142184406518936\n",
      "Training Batch [319/782]: Loss 0.028227563947439194\n",
      "Training Batch [320/782]: Loss 0.04421483352780342\n",
      "Training Batch [321/782]: Loss 0.011320230551064014\n",
      "Training Batch [322/782]: Loss 0.0591665618121624\n",
      "Training Batch [323/782]: Loss 0.051368165761232376\n",
      "Training Batch [324/782]: Loss 0.040327105671167374\n",
      "Training Batch [325/782]: Loss 0.020177612081170082\n",
      "Training Batch [326/782]: Loss 0.00604993337765336\n",
      "Training Batch [327/782]: Loss 0.012092902325093746\n",
      "Training Batch [328/782]: Loss 0.10777417570352554\n",
      "Training Batch [329/782]: Loss 0.01950323022902012\n",
      "Training Batch [330/782]: Loss 0.015756912529468536\n",
      "Training Batch [331/782]: Loss 0.01711343787610531\n",
      "Training Batch [332/782]: Loss 0.019192393869161606\n",
      "Training Batch [333/782]: Loss 0.007720291148871183\n",
      "Training Batch [334/782]: Loss 0.01207455899566412\n",
      "Training Batch [335/782]: Loss 0.022062761709094048\n",
      "Training Batch [336/782]: Loss 0.0540383905172348\n",
      "Training Batch [337/782]: Loss 0.07117430865764618\n",
      "Training Batch [338/782]: Loss 0.05415453761816025\n",
      "Training Batch [339/782]: Loss 0.023627227172255516\n",
      "Training Batch [340/782]: Loss 0.0024209199473261833\n",
      "Training Batch [341/782]: Loss 0.05248522758483887\n",
      "Training Batch [342/782]: Loss 0.036661770194768906\n",
      "Training Batch [343/782]: Loss 0.011629820801317692\n",
      "Training Batch [344/782]: Loss 0.05229341611266136\n",
      "Training Batch [345/782]: Loss 0.06718337535858154\n",
      "Training Batch [346/782]: Loss 0.1625548005104065\n",
      "Training Batch [347/782]: Loss 0.11959254741668701\n",
      "Training Batch [348/782]: Loss 0.03225777670741081\n",
      "Training Batch [349/782]: Loss 0.005519425496459007\n",
      "Training Batch [350/782]: Loss 0.0659034252166748\n",
      "Training Batch [351/782]: Loss 0.016937002539634705\n",
      "Training Batch [352/782]: Loss 0.008594686165452003\n",
      "Training Batch [353/782]: Loss 0.03670533001422882\n",
      "Training Batch [354/782]: Loss 0.05762072652578354\n",
      "Training Batch [355/782]: Loss 0.030597422271966934\n",
      "Training Batch [356/782]: Loss 0.04021598771214485\n",
      "Training Batch [357/782]: Loss 0.026074813678860664\n",
      "Training Batch [358/782]: Loss 0.1263188123703003\n",
      "Training Batch [359/782]: Loss 0.015370624139904976\n",
      "Training Batch [360/782]: Loss 0.03141683340072632\n",
      "Training Batch [361/782]: Loss 0.13441874086856842\n",
      "Training Batch [362/782]: Loss 0.017102938145399094\n",
      "Training Batch [363/782]: Loss 0.01404733955860138\n",
      "Training Batch [364/782]: Loss 0.008217725902795792\n",
      "Training Batch [365/782]: Loss 0.028103137388825417\n",
      "Training Batch [366/782]: Loss 0.16765561699867249\n",
      "Training Batch [367/782]: Loss 0.004538480658084154\n",
      "Training Batch [368/782]: Loss 0.004415052477270365\n",
      "Training Batch [369/782]: Loss 0.06100371107459068\n",
      "Training Batch [370/782]: Loss 0.04577155411243439\n",
      "Training Batch [371/782]: Loss 0.18732771277427673\n",
      "Training Batch [372/782]: Loss 0.16539224982261658\n",
      "Training Batch [373/782]: Loss 0.01542884111404419\n",
      "Training Batch [374/782]: Loss 0.016670778393745422\n",
      "Training Batch [375/782]: Loss 0.020183062180876732\n",
      "Training Batch [376/782]: Loss 0.009701610542833805\n",
      "Training Batch [377/782]: Loss 0.008472064509987831\n",
      "Training Batch [378/782]: Loss 0.01670449785888195\n",
      "Training Batch [379/782]: Loss 0.050221629440784454\n",
      "Training Batch [380/782]: Loss 0.020724838599562645\n",
      "Training Batch [381/782]: Loss 0.03567677363753319\n",
      "Training Batch [382/782]: Loss 0.20331083238124847\n",
      "Training Batch [383/782]: Loss 0.018416130915284157\n",
      "Training Batch [384/782]: Loss 0.01677824556827545\n",
      "Training Batch [385/782]: Loss 0.05354328453540802\n",
      "Training Batch [386/782]: Loss 0.022687291726469994\n",
      "Training Batch [387/782]: Loss 0.07125603407621384\n",
      "Training Batch [388/782]: Loss 0.028186876326799393\n",
      "Training Batch [389/782]: Loss 0.035538580268621445\n",
      "Training Batch [390/782]: Loss 0.04439159482717514\n",
      "Training Batch [391/782]: Loss 0.04730008542537689\n",
      "Training Batch [392/782]: Loss 0.07553423196077347\n",
      "Training Batch [393/782]: Loss 0.03332662954926491\n",
      "Training Batch [394/782]: Loss 0.004787943325936794\n",
      "Training Batch [395/782]: Loss 0.008323127403855324\n",
      "Training Batch [396/782]: Loss 0.019959254190325737\n",
      "Training Batch [397/782]: Loss 0.006226052064448595\n",
      "Training Batch [398/782]: Loss 0.105099618434906\n",
      "Training Batch [399/782]: Loss 0.17216312885284424\n",
      "Training Batch [400/782]: Loss 0.07573316246271133\n",
      "Training Batch [401/782]: Loss 0.09048104286193848\n",
      "Training Batch [402/782]: Loss 0.017876796424388885\n",
      "Training Batch [403/782]: Loss 0.010749711655080318\n",
      "Training Batch [404/782]: Loss 0.02573063038289547\n",
      "Training Batch [405/782]: Loss 0.01961445063352585\n",
      "Training Batch [406/782]: Loss 0.09317638725042343\n",
      "Training Batch [407/782]: Loss 0.05986400321125984\n",
      "Training Batch [408/782]: Loss 0.04585794731974602\n",
      "Training Batch [409/782]: Loss 0.039880283176898956\n",
      "Training Batch [410/782]: Loss 0.09044989943504333\n",
      "Training Batch [411/782]: Loss 0.09989520162343979\n",
      "Training Batch [412/782]: Loss 0.08598300814628601\n",
      "Training Batch [413/782]: Loss 0.019768867641687393\n",
      "Training Batch [414/782]: Loss 0.0690581277012825\n",
      "Training Batch [415/782]: Loss 0.09215833991765976\n",
      "Training Batch [416/782]: Loss 0.058106791228055954\n",
      "Training Batch [417/782]: Loss 0.023452360183000565\n",
      "Training Batch [418/782]: Loss 0.011971146799623966\n",
      "Training Batch [419/782]: Loss 0.010659316554665565\n",
      "Training Batch [420/782]: Loss 0.02434026263654232\n",
      "Training Batch [421/782]: Loss 0.050296783447265625\n",
      "Training Batch [422/782]: Loss 0.027188530191779137\n",
      "Training Batch [423/782]: Loss 0.013121446594595909\n",
      "Training Batch [424/782]: Loss 0.021192001178860664\n",
      "Training Batch [425/782]: Loss 0.050703346729278564\n",
      "Training Batch [426/782]: Loss 0.051738571375608444\n",
      "Training Batch [427/782]: Loss 0.09622269123792648\n",
      "Training Batch [428/782]: Loss 0.1445436030626297\n",
      "Training Batch [429/782]: Loss 0.1367977112531662\n",
      "Training Batch [430/782]: Loss 0.026378773152828217\n",
      "Training Batch [431/782]: Loss 0.04802844300866127\n",
      "Training Batch [432/782]: Loss 0.011651949025690556\n",
      "Training Batch [433/782]: Loss 0.05187688767910004\n",
      "Training Batch [434/782]: Loss 0.1332339495420456\n",
      "Training Batch [435/782]: Loss 0.026528317481279373\n",
      "Training Batch [436/782]: Loss 0.051473211497068405\n",
      "Training Batch [437/782]: Loss 0.033777009695768356\n",
      "Training Batch [438/782]: Loss 0.01652444712817669\n",
      "Training Batch [439/782]: Loss 0.018276430666446686\n",
      "Training Batch [440/782]: Loss 0.023974645882844925\n",
      "Training Batch [441/782]: Loss 0.07819397747516632\n",
      "Training Batch [442/782]: Loss 0.034284353256225586\n",
      "Training Batch [443/782]: Loss 0.054936159402132034\n",
      "Training Batch [444/782]: Loss 0.020809391513466835\n",
      "Training Batch [445/782]: Loss 0.017557619139552116\n",
      "Training Batch [446/782]: Loss 0.05652638152241707\n",
      "Training Batch [447/782]: Loss 0.03251316770911217\n",
      "Training Batch [448/782]: Loss 0.031001783907413483\n",
      "Training Batch [449/782]: Loss 0.03720783442258835\n",
      "Training Batch [450/782]: Loss 0.10664033889770508\n",
      "Training Batch [451/782]: Loss 0.022369148209691048\n",
      "Training Batch [452/782]: Loss 0.0082132238894701\n",
      "Training Batch [453/782]: Loss 0.022069785743951797\n",
      "Training Batch [454/782]: Loss 0.04194234311580658\n",
      "Training Batch [455/782]: Loss 0.07718505710363388\n",
      "Training Batch [456/782]: Loss 0.02272060699760914\n",
      "Training Batch [457/782]: Loss 0.1676199585199356\n",
      "Training Batch [458/782]: Loss 0.04900037497282028\n",
      "Training Batch [459/782]: Loss 0.012207704596221447\n",
      "Training Batch [460/782]: Loss 0.0925963744521141\n",
      "Training Batch [461/782]: Loss 0.07277825474739075\n",
      "Training Batch [462/782]: Loss 0.008378231897950172\n",
      "Training Batch [463/782]: Loss 0.003919057082384825\n",
      "Training Batch [464/782]: Loss 0.015538543462753296\n",
      "Training Batch [465/782]: Loss 0.002655577613040805\n",
      "Training Batch [466/782]: Loss 0.023646948859095573\n",
      "Training Batch [467/782]: Loss 0.01064080186188221\n",
      "Training Batch [468/782]: Loss 0.07198837399482727\n",
      "Training Batch [469/782]: Loss 0.04510798305273056\n",
      "Training Batch [470/782]: Loss 0.049258552491664886\n",
      "Training Batch [471/782]: Loss 0.06388875842094421\n",
      "Training Batch [472/782]: Loss 0.13964860141277313\n",
      "Training Batch [473/782]: Loss 0.06055949255824089\n",
      "Training Batch [474/782]: Loss 0.0662447139620781\n",
      "Training Batch [475/782]: Loss 0.02837229147553444\n",
      "Training Batch [476/782]: Loss 0.008558524772524834\n",
      "Training Batch [477/782]: Loss 0.039482392370700836\n",
      "Training Batch [478/782]: Loss 0.035470157861709595\n",
      "Training Batch [479/782]: Loss 0.1423618644475937\n",
      "Training Batch [480/782]: Loss 0.030086888000369072\n",
      "Training Batch [481/782]: Loss 0.0216493159532547\n",
      "Training Batch [482/782]: Loss 0.015400844626128674\n",
      "Training Batch [483/782]: Loss 0.012933677062392235\n",
      "Training Batch [484/782]: Loss 0.0986814796924591\n",
      "Training Batch [485/782]: Loss 0.010374077595770359\n",
      "Training Batch [486/782]: Loss 0.25663068890571594\n",
      "Training Batch [487/782]: Loss 0.010849382728338242\n",
      "Training Batch [488/782]: Loss 0.01894507370889187\n",
      "Training Batch [489/782]: Loss 0.0950026586651802\n",
      "Training Batch [490/782]: Loss 0.01988578774034977\n",
      "Training Batch [491/782]: Loss 0.03641268610954285\n",
      "Training Batch [492/782]: Loss 0.038704127073287964\n",
      "Training Batch [493/782]: Loss 0.0737600177526474\n",
      "Training Batch [494/782]: Loss 0.013404903002083302\n",
      "Training Batch [495/782]: Loss 0.01049240492284298\n",
      "Training Batch [496/782]: Loss 0.1930679976940155\n",
      "Training Batch [497/782]: Loss 0.07875730097293854\n",
      "Training Batch [498/782]: Loss 0.10977581143379211\n",
      "Training Batch [499/782]: Loss 0.008127273991703987\n",
      "Training Batch [500/782]: Loss 0.13143765926361084\n",
      "Training Batch [501/782]: Loss 0.030497992411255836\n",
      "Training Batch [502/782]: Loss 0.009974585846066475\n",
      "Training Batch [503/782]: Loss 0.07944338768720627\n",
      "Training Batch [504/782]: Loss 0.06753873080015182\n",
      "Training Batch [505/782]: Loss 0.0031546233221888542\n",
      "Training Batch [506/782]: Loss 0.0090982960537076\n",
      "Training Batch [507/782]: Loss 0.03129648417234421\n",
      "Training Batch [508/782]: Loss 0.022367043420672417\n",
      "Training Batch [509/782]: Loss 0.11377943307161331\n",
      "Training Batch [510/782]: Loss 0.034541089087724686\n",
      "Training Batch [511/782]: Loss 0.12168781459331512\n",
      "Training Batch [512/782]: Loss 0.02039499394595623\n",
      "Training Batch [513/782]: Loss 0.06834547221660614\n",
      "Training Batch [514/782]: Loss 0.08987271785736084\n",
      "Training Batch [515/782]: Loss 0.17892204225063324\n",
      "Training Batch [516/782]: Loss 0.032062288373708725\n",
      "Training Batch [517/782]: Loss 0.021626999601721764\n",
      "Training Batch [518/782]: Loss 0.02301643043756485\n",
      "Training Batch [519/782]: Loss 0.051112569868564606\n",
      "Training Batch [520/782]: Loss 0.2911023795604706\n",
      "Training Batch [521/782]: Loss 0.07509757578372955\n",
      "Training Batch [522/782]: Loss 0.09447736293077469\n",
      "Training Batch [523/782]: Loss 0.1984272301197052\n",
      "Training Batch [524/782]: Loss 0.04815957322716713\n",
      "Training Batch [525/782]: Loss 0.037948958575725555\n",
      "Training Batch [526/782]: Loss 0.12025079131126404\n",
      "Training Batch [527/782]: Loss 0.028912751004099846\n",
      "Training Batch [528/782]: Loss 0.08748948574066162\n",
      "Training Batch [529/782]: Loss 0.034759942442178726\n",
      "Training Batch [530/782]: Loss 0.07053378224372864\n",
      "Training Batch [531/782]: Loss 0.10074210166931152\n",
      "Training Batch [532/782]: Loss 0.17935258150100708\n",
      "Training Batch [533/782]: Loss 0.040463387966156006\n",
      "Training Batch [534/782]: Loss 0.07004158198833466\n",
      "Training Batch [535/782]: Loss 0.03181792050600052\n",
      "Training Batch [536/782]: Loss 0.05215069279074669\n",
      "Training Batch [537/782]: Loss 0.04332328215241432\n",
      "Training Batch [538/782]: Loss 0.06717028468847275\n",
      "Training Batch [539/782]: Loss 0.059574637562036514\n",
      "Training Batch [540/782]: Loss 0.018228109925985336\n",
      "Training Batch [541/782]: Loss 0.05484772473573685\n",
      "Training Batch [542/782]: Loss 0.04424767941236496\n",
      "Training Batch [543/782]: Loss 0.03843528777360916\n",
      "Training Batch [544/782]: Loss 0.01418671477586031\n",
      "Training Batch [545/782]: Loss 0.06149400770664215\n",
      "Training Batch [546/782]: Loss 0.03440403193235397\n",
      "Training Batch [547/782]: Loss 0.0444074310362339\n",
      "Training Batch [548/782]: Loss 0.11752559244632721\n",
      "Training Batch [549/782]: Loss 0.05914866924285889\n",
      "Training Batch [550/782]: Loss 0.05364617705345154\n",
      "Training Batch [551/782]: Loss 0.08306954801082611\n",
      "Training Batch [552/782]: Loss 0.04739483818411827\n",
      "Training Batch [553/782]: Loss 0.08624637871980667\n",
      "Training Batch [554/782]: Loss 0.03707017004489899\n",
      "Training Batch [555/782]: Loss 0.07020331919193268\n",
      "Training Batch [556/782]: Loss 0.048933014273643494\n",
      "Training Batch [557/782]: Loss 0.0916183665394783\n",
      "Training Batch [558/782]: Loss 0.08401124179363251\n",
      "Training Batch [559/782]: Loss 0.1718260943889618\n",
      "Training Batch [560/782]: Loss 0.03485461324453354\n",
      "Training Batch [561/782]: Loss 0.05984333157539368\n",
      "Training Batch [562/782]: Loss 0.03728228062391281\n",
      "Training Batch [563/782]: Loss 0.02227815054357052\n",
      "Training Batch [564/782]: Loss 0.008892147801816463\n",
      "Training Batch [565/782]: Loss 0.03462270647287369\n",
      "Training Batch [566/782]: Loss 0.01686379685997963\n",
      "Training Batch [567/782]: Loss 0.06486202031373978\n",
      "Training Batch [568/782]: Loss 0.09274309873580933\n",
      "Training Batch [569/782]: Loss 0.01374560222029686\n",
      "Training Batch [570/782]: Loss 0.020927701145410538\n",
      "Training Batch [571/782]: Loss 0.12186842411756516\n",
      "Training Batch [572/782]: Loss 0.01332588866353035\n",
      "Training Batch [573/782]: Loss 0.04916225001215935\n",
      "Training Batch [574/782]: Loss 0.08467082679271698\n",
      "Training Batch [575/782]: Loss 0.025978492572903633\n",
      "Training Batch [576/782]: Loss 0.028776107355952263\n",
      "Training Batch [577/782]: Loss 0.11500407010316849\n",
      "Training Batch [578/782]: Loss 0.033111706376075745\n",
      "Training Batch [579/782]: Loss 0.10078700631856918\n",
      "Training Batch [580/782]: Loss 0.034642744809389114\n",
      "Training Batch [581/782]: Loss 0.08177207410335541\n",
      "Training Batch [582/782]: Loss 0.011364965699613094\n",
      "Training Batch [583/782]: Loss 0.16647255420684814\n",
      "Training Batch [584/782]: Loss 0.10097990930080414\n",
      "Training Batch [585/782]: Loss 0.04914442449808121\n",
      "Training Batch [586/782]: Loss 0.05770760029554367\n",
      "Training Batch [587/782]: Loss 0.09694799780845642\n",
      "Training Batch [588/782]: Loss 0.026024991646409035\n",
      "Training Batch [589/782]: Loss 0.061347492039203644\n",
      "Training Batch [590/782]: Loss 0.008015468716621399\n",
      "Training Batch [591/782]: Loss 0.08812626451253891\n",
      "Training Batch [592/782]: Loss 0.057863641530275345\n",
      "Training Batch [593/782]: Loss 0.024362806230783463\n",
      "Training Batch [594/782]: Loss 0.031821612268686295\n",
      "Training Batch [595/782]: Loss 0.10496561974287033\n",
      "Training Batch [596/782]: Loss 0.04584367200732231\n",
      "Training Batch [597/782]: Loss 0.06585394591093063\n",
      "Training Batch [598/782]: Loss 0.15386301279067993\n",
      "Training Batch [599/782]: Loss 0.015814481303095818\n",
      "Training Batch [600/782]: Loss 0.06673422455787659\n",
      "Training Batch [601/782]: Loss 0.1361476331949234\n",
      "Training Batch [602/782]: Loss 0.06750033050775528\n",
      "Training Batch [603/782]: Loss 0.013515263795852661\n",
      "Training Batch [604/782]: Loss 0.021931255236268044\n",
      "Training Batch [605/782]: Loss 0.037699997425079346\n",
      "Training Batch [606/782]: Loss 0.20989511907100677\n",
      "Training Batch [607/782]: Loss 0.054820068180561066\n",
      "Training Batch [608/782]: Loss 0.09496720880270004\n",
      "Training Batch [609/782]: Loss 0.04106798395514488\n",
      "Training Batch [610/782]: Loss 0.13610222935676575\n",
      "Training Batch [611/782]: Loss 0.008591815829277039\n",
      "Training Batch [612/782]: Loss 0.024554390460252762\n",
      "Training Batch [613/782]: Loss 0.0760999396443367\n",
      "Training Batch [614/782]: Loss 0.11040352284908295\n",
      "Training Batch [615/782]: Loss 0.013681957498192787\n",
      "Training Batch [616/782]: Loss 0.04226556792855263\n",
      "Training Batch [617/782]: Loss 0.038368187844753265\n",
      "Training Batch [618/782]: Loss 0.07832761853933334\n",
      "Training Batch [619/782]: Loss 0.03768579661846161\n",
      "Training Batch [620/782]: Loss 0.07020401954650879\n",
      "Training Batch [621/782]: Loss 0.11431314051151276\n",
      "Training Batch [622/782]: Loss 0.10030707716941833\n",
      "Training Batch [623/782]: Loss 0.09851617366075516\n",
      "Training Batch [624/782]: Loss 0.0536649152636528\n",
      "Training Batch [625/782]: Loss 0.04956043139100075\n",
      "Training Batch [626/782]: Loss 0.05706362426280975\n",
      "Training Batch [627/782]: Loss 0.09014878422021866\n",
      "Training Batch [628/782]: Loss 0.06970995664596558\n",
      "Training Batch [629/782]: Loss 0.013423538766801357\n",
      "Training Batch [630/782]: Loss 0.03962854668498039\n",
      "Training Batch [631/782]: Loss 0.15311916172504425\n",
      "Training Batch [632/782]: Loss 0.05885706841945648\n",
      "Training Batch [633/782]: Loss 0.021784741431474686\n",
      "Training Batch [634/782]: Loss 0.027467960491776466\n",
      "Training Batch [635/782]: Loss 0.14850661158561707\n",
      "Training Batch [636/782]: Loss 0.06737819314002991\n",
      "Training Batch [637/782]: Loss 0.04519471526145935\n",
      "Training Batch [638/782]: Loss 0.005282797385007143\n",
      "Training Batch [639/782]: Loss 0.10185761749744415\n",
      "Training Batch [640/782]: Loss 0.034129250794649124\n",
      "Training Batch [641/782]: Loss 0.02234472706913948\n",
      "Training Batch [642/782]: Loss 0.02917001210153103\n",
      "Training Batch [643/782]: Loss 0.1443079113960266\n",
      "Training Batch [644/782]: Loss 0.03672982007265091\n",
      "Training Batch [645/782]: Loss 0.017425263300538063\n",
      "Training Batch [646/782]: Loss 0.055200379341840744\n",
      "Training Batch [647/782]: Loss 0.03666289150714874\n",
      "Training Batch [648/782]: Loss 0.01376749761402607\n",
      "Training Batch [649/782]: Loss 0.18237437307834625\n",
      "Training Batch [650/782]: Loss 0.08592411130666733\n",
      "Training Batch [651/782]: Loss 0.15240979194641113\n",
      "Training Batch [652/782]: Loss 0.1373567432165146\n",
      "Training Batch [653/782]: Loss 0.04117744788527489\n",
      "Training Batch [654/782]: Loss 0.10841479152441025\n",
      "Training Batch [655/782]: Loss 0.02427789196372032\n",
      "Training Batch [656/782]: Loss 0.006912318989634514\n",
      "Training Batch [657/782]: Loss 0.05881073698401451\n",
      "Training Batch [658/782]: Loss 0.013847911730408669\n",
      "Training Batch [659/782]: Loss 0.13524805009365082\n",
      "Training Batch [660/782]: Loss 0.0134227080270648\n",
      "Training Batch [661/782]: Loss 0.0630100890994072\n",
      "Training Batch [662/782]: Loss 0.09109217673540115\n",
      "Training Batch [663/782]: Loss 0.04013439267873764\n",
      "Training Batch [664/782]: Loss 0.08256913721561432\n",
      "Training Batch [665/782]: Loss 0.05907580256462097\n",
      "Training Batch [666/782]: Loss 0.09756192564964294\n",
      "Training Batch [667/782]: Loss 0.04333643615245819\n",
      "Training Batch [668/782]: Loss 0.02523154392838478\n",
      "Training Batch [669/782]: Loss 0.07031817734241486\n",
      "Training Batch [670/782]: Loss 0.09115879982709885\n",
      "Training Batch [671/782]: Loss 0.07966157793998718\n",
      "Training Batch [672/782]: Loss 0.05166669562458992\n",
      "Training Batch [673/782]: Loss 0.05615959316492081\n",
      "Training Batch [674/782]: Loss 0.0729447677731514\n",
      "Training Batch [675/782]: Loss 0.011197676882147789\n",
      "Training Batch [676/782]: Loss 0.05755312740802765\n",
      "Training Batch [677/782]: Loss 0.03994106873869896\n",
      "Training Batch [678/782]: Loss 0.09875019639730453\n",
      "Training Batch [679/782]: Loss 0.052584897726774216\n",
      "Training Batch [680/782]: Loss 0.10493849962949753\n",
      "Training Batch [681/782]: Loss 0.04147939756512642\n",
      "Training Batch [682/782]: Loss 0.05461518093943596\n",
      "Training Batch [683/782]: Loss 0.03818256035447121\n",
      "Training Batch [684/782]: Loss 0.10352126508951187\n",
      "Training Batch [685/782]: Loss 0.05377732962369919\n",
      "Training Batch [686/782]: Loss 0.04821786656975746\n",
      "Training Batch [687/782]: Loss 0.10552144050598145\n",
      "Training Batch [688/782]: Loss 0.019773924723267555\n",
      "Training Batch [689/782]: Loss 0.04079892486333847\n",
      "Training Batch [690/782]: Loss 0.12246783077716827\n",
      "Training Batch [691/782]: Loss 0.18818078935146332\n",
      "Training Batch [692/782]: Loss 0.0359775610268116\n",
      "Training Batch [693/782]: Loss 0.17509426176548004\n",
      "Training Batch [694/782]: Loss 0.06253562867641449\n",
      "Training Batch [695/782]: Loss 0.10600768029689789\n",
      "Training Batch [696/782]: Loss 0.10731491446495056\n",
      "Training Batch [697/782]: Loss 0.0031355402898043394\n",
      "Training Batch [698/782]: Loss 0.03479941189289093\n",
      "Training Batch [699/782]: Loss 0.03928693011403084\n",
      "Training Batch [700/782]: Loss 0.07168819010257721\n",
      "Training Batch [701/782]: Loss 0.10233745723962784\n",
      "Training Batch [702/782]: Loss 0.020862208679318428\n",
      "Training Batch [703/782]: Loss 0.029722269624471664\n",
      "Training Batch [704/782]: Loss 0.0047853910364210606\n",
      "Training Batch [705/782]: Loss 0.16494439542293549\n",
      "Training Batch [706/782]: Loss 0.05924787372350693\n",
      "Training Batch [707/782]: Loss 0.011684350669384003\n",
      "Training Batch [708/782]: Loss 0.0419861264526844\n",
      "Training Batch [709/782]: Loss 0.046288445591926575\n",
      "Training Batch [710/782]: Loss 0.061080411076545715\n",
      "Training Batch [711/782]: Loss 0.025934407487511635\n",
      "Training Batch [712/782]: Loss 0.06743044406175613\n",
      "Training Batch [713/782]: Loss 0.011950128711760044\n",
      "Training Batch [714/782]: Loss 0.06734912097454071\n",
      "Training Batch [715/782]: Loss 0.021339956670999527\n",
      "Training Batch [716/782]: Loss 0.05294729769229889\n",
      "Training Batch [717/782]: Loss 0.026108575984835625\n",
      "Training Batch [718/782]: Loss 0.11202292889356613\n",
      "Training Batch [719/782]: Loss 0.01345553994178772\n",
      "Training Batch [720/782]: Loss 0.0970463752746582\n",
      "Training Batch [721/782]: Loss 0.05857989192008972\n",
      "Training Batch [722/782]: Loss 0.031592946499586105\n",
      "Training Batch [723/782]: Loss 0.03433170169591904\n",
      "Training Batch [724/782]: Loss 0.046965546905994415\n",
      "Training Batch [725/782]: Loss 0.09827863425016403\n",
      "Training Batch [726/782]: Loss 0.08480852097272873\n",
      "Training Batch [727/782]: Loss 0.027989177033305168\n",
      "Training Batch [728/782]: Loss 0.08482423424720764\n",
      "Training Batch [729/782]: Loss 0.11248784512281418\n",
      "Training Batch [730/782]: Loss 0.016969691962003708\n",
      "Training Batch [731/782]: Loss 0.06389202177524567\n",
      "Training Batch [732/782]: Loss 0.02771970070898533\n",
      "Training Batch [733/782]: Loss 0.19122432172298431\n",
      "Training Batch [734/782]: Loss 0.0738062858581543\n",
      "Training Batch [735/782]: Loss 0.03813953697681427\n",
      "Training Batch [736/782]: Loss 0.05116310343146324\n",
      "Training Batch [737/782]: Loss 0.05833759903907776\n",
      "Training Batch [738/782]: Loss 0.07739880681037903\n",
      "Training Batch [739/782]: Loss 0.033361297100782394\n",
      "Training Batch [740/782]: Loss 0.13151076436042786\n",
      "Training Batch [741/782]: Loss 0.02547219581902027\n",
      "Training Batch [742/782]: Loss 0.04276726767420769\n",
      "Training Batch [743/782]: Loss 0.04955244064331055\n",
      "Training Batch [744/782]: Loss 0.014271882362663746\n",
      "Training Batch [745/782]: Loss 0.07729765772819519\n",
      "Training Batch [746/782]: Loss 0.01410422008484602\n",
      "Training Batch [747/782]: Loss 0.013851827941834927\n",
      "Training Batch [748/782]: Loss 0.026060620322823524\n",
      "Training Batch [749/782]: Loss 0.05538187921047211\n",
      "Training Batch [750/782]: Loss 0.012105925008654594\n",
      "Training Batch [751/782]: Loss 0.018182549625635147\n",
      "Training Batch [752/782]: Loss 0.11851409822702408\n",
      "Training Batch [753/782]: Loss 0.15645544230937958\n",
      "Training Batch [754/782]: Loss 0.0377223901450634\n",
      "Training Batch [755/782]: Loss 0.037293024361133575\n",
      "Training Batch [756/782]: Loss 0.012849646620452404\n",
      "Training Batch [757/782]: Loss 0.07498273253440857\n",
      "Training Batch [758/782]: Loss 0.02460741437971592\n",
      "Training Batch [759/782]: Loss 0.035786908119916916\n",
      "Training Batch [760/782]: Loss 0.020655887201428413\n",
      "Training Batch [761/782]: Loss 0.030324997380375862\n",
      "Training Batch [762/782]: Loss 0.061687346547842026\n",
      "Training Batch [763/782]: Loss 0.014335298910737038\n",
      "Training Batch [764/782]: Loss 0.07750601321458817\n",
      "Training Batch [765/782]: Loss 0.09325786679983139\n",
      "Training Batch [766/782]: Loss 0.03474731743335724\n",
      "Training Batch [767/782]: Loss 0.03181711956858635\n",
      "Training Batch [768/782]: Loss 0.07241394370794296\n",
      "Training Batch [769/782]: Loss 0.0193749088793993\n",
      "Training Batch [770/782]: Loss 0.025969762355089188\n",
      "Training Batch [771/782]: Loss 0.0412939079105854\n",
      "Training Batch [772/782]: Loss 0.04333702102303505\n",
      "Training Batch [773/782]: Loss 0.006621491629630327\n",
      "Training Batch [774/782]: Loss 0.1186043992638588\n",
      "Training Batch [775/782]: Loss 0.029251014813780785\n",
      "Training Batch [776/782]: Loss 0.025018906220793724\n",
      "Training Batch [777/782]: Loss 0.07994599640369415\n",
      "Training Batch [778/782]: Loss 0.11358021944761276\n",
      "Training Batch [779/782]: Loss 0.045389652252197266\n",
      "Training Batch [780/782]: Loss 0.07152983546257019\n",
      "Training Batch [781/782]: Loss 0.05599095672369003\n",
      "Training Batch [782/782]: Loss 0.07200173288583755\n",
      "Epoch 18 - Train Loss: 0.0456\n",
      "*********  Epoch 19/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.012963948771357536\n",
      "Training Batch [2/782]: Loss 0.06613315641880035\n",
      "Training Batch [3/782]: Loss 0.13085614144802094\n",
      "Training Batch [4/782]: Loss 0.013277006335556507\n",
      "Training Batch [5/782]: Loss 0.012540053576231003\n",
      "Training Batch [6/782]: Loss 0.007976170629262924\n",
      "Training Batch [7/782]: Loss 0.06386247277259827\n",
      "Training Batch [8/782]: Loss 0.13600537180900574\n",
      "Training Batch [9/782]: Loss 0.042025815695524216\n",
      "Training Batch [10/782]: Loss 0.02142620086669922\n",
      "Training Batch [11/782]: Loss 0.08096549659967422\n",
      "Training Batch [12/782]: Loss 0.036962106823921204\n",
      "Training Batch [13/782]: Loss 0.023336762562394142\n",
      "Training Batch [14/782]: Loss 0.03938792645931244\n",
      "Training Batch [15/782]: Loss 0.014525122940540314\n",
      "Training Batch [16/782]: Loss 0.010337842628359795\n",
      "Training Batch [17/782]: Loss 0.05611244961619377\n",
      "Training Batch [18/782]: Loss 0.062295787036418915\n",
      "Training Batch [19/782]: Loss 0.04204000160098076\n",
      "Training Batch [20/782]: Loss 0.07097294181585312\n",
      "Training Batch [21/782]: Loss 0.04944632574915886\n",
      "Training Batch [22/782]: Loss 0.025110477581620216\n",
      "Training Batch [23/782]: Loss 0.10310225933790207\n",
      "Training Batch [24/782]: Loss 0.05626412108540535\n",
      "Training Batch [25/782]: Loss 0.038861483335494995\n",
      "Training Batch [26/782]: Loss 0.027548694983124733\n",
      "Training Batch [27/782]: Loss 0.005934127606451511\n",
      "Training Batch [28/782]: Loss 0.06463623046875\n",
      "Training Batch [29/782]: Loss 0.049286358058452606\n",
      "Training Batch [30/782]: Loss 0.012333954684436321\n",
      "Training Batch [31/782]: Loss 0.026642099022865295\n",
      "Training Batch [32/782]: Loss 0.04016556218266487\n",
      "Training Batch [33/782]: Loss 0.08543316274881363\n",
      "Training Batch [34/782]: Loss 0.03732503950595856\n",
      "Training Batch [35/782]: Loss 0.09584605693817139\n",
      "Training Batch [36/782]: Loss 0.0025480580516159534\n",
      "Training Batch [37/782]: Loss 0.019459087401628494\n",
      "Training Batch [38/782]: Loss 0.009773223660886288\n",
      "Training Batch [39/782]: Loss 0.00483294203877449\n",
      "Training Batch [40/782]: Loss 0.04026707261800766\n",
      "Training Batch [41/782]: Loss 0.021519165486097336\n",
      "Training Batch [42/782]: Loss 0.02618015743792057\n",
      "Training Batch [43/782]: Loss 0.0858267992734909\n",
      "Training Batch [44/782]: Loss 0.013599744066596031\n",
      "Training Batch [45/782]: Loss 0.05775144323706627\n",
      "Training Batch [46/782]: Loss 0.11841496080160141\n",
      "Training Batch [47/782]: Loss 0.007014344446361065\n",
      "Training Batch [48/782]: Loss 0.028392303735017776\n",
      "Training Batch [49/782]: Loss 0.01063617691397667\n",
      "Training Batch [50/782]: Loss 0.02104988507926464\n",
      "Training Batch [51/782]: Loss 0.017903707921504974\n",
      "Training Batch [52/782]: Loss 0.004136297386139631\n",
      "Training Batch [53/782]: Loss 0.05576011538505554\n",
      "Training Batch [54/782]: Loss 0.0060475231148302555\n",
      "Training Batch [55/782]: Loss 0.04572576284408569\n",
      "Training Batch [56/782]: Loss 0.07140590995550156\n",
      "Training Batch [57/782]: Loss 0.08385293930768967\n",
      "Training Batch [58/782]: Loss 0.05276748538017273\n",
      "Training Batch [59/782]: Loss 0.028979578986763954\n",
      "Training Batch [60/782]: Loss 0.12643587589263916\n",
      "Training Batch [61/782]: Loss 0.03847828134894371\n",
      "Training Batch [62/782]: Loss 0.030494607985019684\n",
      "Training Batch [63/782]: Loss 0.0572434701025486\n",
      "Training Batch [64/782]: Loss 0.014643042348325253\n",
      "Training Batch [65/782]: Loss 0.040623996406793594\n",
      "Training Batch [66/782]: Loss 0.012825456447899342\n",
      "Training Batch [67/782]: Loss 0.04805796593427658\n",
      "Training Batch [68/782]: Loss 0.019049642607569695\n",
      "Training Batch [69/782]: Loss 0.04822709411382675\n",
      "Training Batch [70/782]: Loss 0.06755313277244568\n",
      "Training Batch [71/782]: Loss 0.02882573753595352\n",
      "Training Batch [72/782]: Loss 0.19919291138648987\n",
      "Training Batch [73/782]: Loss 0.021764714270830154\n",
      "Training Batch [74/782]: Loss 0.0673745647072792\n",
      "Training Batch [75/782]: Loss 0.16500252485275269\n",
      "Training Batch [76/782]: Loss 0.016376398503780365\n",
      "Training Batch [77/782]: Loss 0.024552414193749428\n",
      "Training Batch [78/782]: Loss 0.06769814342260361\n",
      "Training Batch [79/782]: Loss 0.04659191891551018\n",
      "Training Batch [80/782]: Loss 0.05391935259103775\n",
      "Training Batch [81/782]: Loss 0.008950874209403992\n",
      "Training Batch [82/782]: Loss 0.18009312450885773\n",
      "Training Batch [83/782]: Loss 0.08598312735557556\n",
      "Training Batch [84/782]: Loss 0.016870180144906044\n",
      "Training Batch [85/782]: Loss 0.0884118378162384\n",
      "Training Batch [86/782]: Loss 0.004865525756031275\n",
      "Training Batch [87/782]: Loss 0.03480939939618111\n",
      "Training Batch [88/782]: Loss 0.004002110101282597\n",
      "Training Batch [89/782]: Loss 0.018667129799723625\n",
      "Training Batch [90/782]: Loss 0.007245307322591543\n",
      "Training Batch [91/782]: Loss 0.013749977573752403\n",
      "Training Batch [92/782]: Loss 0.013956290669739246\n",
      "Training Batch [93/782]: Loss 0.02000422589480877\n",
      "Training Batch [94/782]: Loss 0.0584414079785347\n",
      "Training Batch [95/782]: Loss 0.08440466225147247\n",
      "Training Batch [96/782]: Loss 0.054880883544683456\n",
      "Training Batch [97/782]: Loss 0.01474793441593647\n",
      "Training Batch [98/782]: Loss 0.02197852171957493\n",
      "Training Batch [99/782]: Loss 0.11994146555662155\n",
      "Training Batch [100/782]: Loss 0.03221328929066658\n",
      "Training Batch [101/782]: Loss 0.010404737666249275\n",
      "Training Batch [102/782]: Loss 0.009683053940534592\n",
      "Training Batch [103/782]: Loss 0.032731588929891586\n",
      "Training Batch [104/782]: Loss 0.02035328932106495\n",
      "Training Batch [105/782]: Loss 0.07206106185913086\n",
      "Training Batch [106/782]: Loss 0.08314409852027893\n",
      "Training Batch [107/782]: Loss 0.030870333313941956\n",
      "Training Batch [108/782]: Loss 0.032495222985744476\n",
      "Training Batch [109/782]: Loss 0.06382403522729874\n",
      "Training Batch [110/782]: Loss 0.01130363903939724\n",
      "Training Batch [111/782]: Loss 0.026127230376005173\n",
      "Training Batch [112/782]: Loss 0.04906683415174484\n",
      "Training Batch [113/782]: Loss 0.0722951889038086\n",
      "Training Batch [114/782]: Loss 0.04610544070601463\n",
      "Training Batch [115/782]: Loss 0.025468820706009865\n",
      "Training Batch [116/782]: Loss 0.005893341265618801\n",
      "Training Batch [117/782]: Loss 0.004197853151708841\n",
      "Training Batch [118/782]: Loss 0.03049084171652794\n",
      "Training Batch [119/782]: Loss 0.058293942362070084\n",
      "Training Batch [120/782]: Loss 0.011890556663274765\n",
      "Training Batch [121/782]: Loss 0.11785216629505157\n",
      "Training Batch [122/782]: Loss 0.004990436602383852\n",
      "Training Batch [123/782]: Loss 0.05033448711037636\n",
      "Training Batch [124/782]: Loss 0.00900848489254713\n",
      "Training Batch [125/782]: Loss 0.1189756691455841\n",
      "Training Batch [126/782]: Loss 0.028499910607933998\n",
      "Training Batch [127/782]: Loss 0.01691007800400257\n",
      "Training Batch [128/782]: Loss 0.031164832413196564\n",
      "Training Batch [129/782]: Loss 0.005349468439817429\n",
      "Training Batch [130/782]: Loss 0.0010928018018603325\n",
      "Training Batch [131/782]: Loss 0.015473372302949429\n",
      "Training Batch [132/782]: Loss 0.025340527296066284\n",
      "Training Batch [133/782]: Loss 0.004088811110705137\n",
      "Training Batch [134/782]: Loss 0.010585537180304527\n",
      "Training Batch [135/782]: Loss 0.060814451426267624\n",
      "Training Batch [136/782]: Loss 0.004918191581964493\n",
      "Training Batch [137/782]: Loss 0.05001197010278702\n",
      "Training Batch [138/782]: Loss 0.007737868465483189\n",
      "Training Batch [139/782]: Loss 0.059417515993118286\n",
      "Training Batch [140/782]: Loss 0.02726844884455204\n",
      "Training Batch [141/782]: Loss 0.02480730228126049\n",
      "Training Batch [142/782]: Loss 0.012245293706655502\n",
      "Training Batch [143/782]: Loss 0.06740009784698486\n",
      "Training Batch [144/782]: Loss 0.005573892965912819\n",
      "Training Batch [145/782]: Loss 0.10366424173116684\n",
      "Training Batch [146/782]: Loss 0.04377727583050728\n",
      "Training Batch [147/782]: Loss 0.08127514272928238\n",
      "Training Batch [148/782]: Loss 0.01071031391620636\n",
      "Training Batch [149/782]: Loss 0.007661489304155111\n",
      "Training Batch [150/782]: Loss 0.024032775312662125\n",
      "Training Batch [151/782]: Loss 0.01564650423824787\n",
      "Training Batch [152/782]: Loss 0.09214943647384644\n",
      "Training Batch [153/782]: Loss 0.004440621938556433\n",
      "Training Batch [154/782]: Loss 0.01911284774541855\n",
      "Training Batch [155/782]: Loss 0.02402440831065178\n",
      "Training Batch [156/782]: Loss 0.028022626414895058\n",
      "Training Batch [157/782]: Loss 0.03780815750360489\n",
      "Training Batch [158/782]: Loss 0.008431236259639263\n",
      "Training Batch [159/782]: Loss 0.007506676018238068\n",
      "Training Batch [160/782]: Loss 0.037338562309741974\n",
      "Training Batch [161/782]: Loss 0.11758436262607574\n",
      "Training Batch [162/782]: Loss 0.009080839343369007\n",
      "Training Batch [163/782]: Loss 0.05691113695502281\n",
      "Training Batch [164/782]: Loss 0.012276316992938519\n",
      "Training Batch [165/782]: Loss 0.014323885552585125\n",
      "Training Batch [166/782]: Loss 0.0059250732883811\n",
      "Training Batch [167/782]: Loss 0.01734980382025242\n",
      "Training Batch [168/782]: Loss 0.0069565195590257645\n",
      "Training Batch [169/782]: Loss 0.028673237189650536\n",
      "Training Batch [170/782]: Loss 0.021749993786215782\n",
      "Training Batch [171/782]: Loss 0.11058827489614487\n",
      "Training Batch [172/782]: Loss 0.007397268898785114\n",
      "Training Batch [173/782]: Loss 0.005436531733721495\n",
      "Training Batch [174/782]: Loss 0.018740501254796982\n",
      "Training Batch [175/782]: Loss 0.030896298587322235\n",
      "Training Batch [176/782]: Loss 0.00328689138405025\n",
      "Training Batch [177/782]: Loss 0.03253212198615074\n",
      "Training Batch [178/782]: Loss 0.009668674319982529\n",
      "Training Batch [179/782]: Loss 0.01662062667310238\n",
      "Training Batch [180/782]: Loss 0.01820450648665428\n",
      "Training Batch [181/782]: Loss 0.02937271259725094\n",
      "Training Batch [182/782]: Loss 0.02551979012787342\n",
      "Training Batch [183/782]: Loss 0.009507082402706146\n",
      "Training Batch [184/782]: Loss 0.028605973348021507\n",
      "Training Batch [185/782]: Loss 0.016927266493439674\n",
      "Training Batch [186/782]: Loss 0.07198472321033478\n",
      "Training Batch [187/782]: Loss 0.0018290776060894132\n",
      "Training Batch [188/782]: Loss 0.06606262922286987\n",
      "Training Batch [189/782]: Loss 0.03728264570236206\n",
      "Training Batch [190/782]: Loss 0.012815359979867935\n",
      "Training Batch [191/782]: Loss 0.007161078974604607\n",
      "Training Batch [192/782]: Loss 0.060171570628881454\n",
      "Training Batch [193/782]: Loss 0.005440459121018648\n",
      "Training Batch [194/782]: Loss 0.010609263554215431\n",
      "Training Batch [195/782]: Loss 0.00528104929253459\n",
      "Training Batch [196/782]: Loss 0.020031627267599106\n",
      "Training Batch [197/782]: Loss 0.013811137527227402\n",
      "Training Batch [198/782]: Loss 0.015683278441429138\n",
      "Training Batch [199/782]: Loss 0.03387206420302391\n",
      "Training Batch [200/782]: Loss 0.012917147018015385\n",
      "Training Batch [201/782]: Loss 0.032242145389318466\n",
      "Training Batch [202/782]: Loss 0.027665114030241966\n",
      "Training Batch [203/782]: Loss 0.025460265576839447\n",
      "Training Batch [204/782]: Loss 0.008184487000107765\n",
      "Training Batch [205/782]: Loss 0.02580890618264675\n",
      "Training Batch [206/782]: Loss 0.038037821650505066\n",
      "Training Batch [207/782]: Loss 0.04741787165403366\n",
      "Training Batch [208/782]: Loss 0.029910484328866005\n",
      "Training Batch [209/782]: Loss 0.042597491294145584\n",
      "Training Batch [210/782]: Loss 0.006490742322057486\n",
      "Training Batch [211/782]: Loss 0.012024065479636192\n",
      "Training Batch [212/782]: Loss 0.010717827826738358\n",
      "Training Batch [213/782]: Loss 0.03125268220901489\n",
      "Training Batch [214/782]: Loss 0.042687807232141495\n",
      "Training Batch [215/782]: Loss 0.01662226766347885\n",
      "Training Batch [216/782]: Loss 0.005390019156038761\n",
      "Training Batch [217/782]: Loss 0.022069357335567474\n",
      "Training Batch [218/782]: Loss 0.07857340574264526\n",
      "Training Batch [219/782]: Loss 0.04703473299741745\n",
      "Training Batch [220/782]: Loss 0.0902002826333046\n",
      "Training Batch [221/782]: Loss 0.0593520812690258\n",
      "Training Batch [222/782]: Loss 0.16159185767173767\n",
      "Training Batch [223/782]: Loss 0.0049004750326275826\n",
      "Training Batch [224/782]: Loss 0.011680292896926403\n",
      "Training Batch [225/782]: Loss 0.05930086597800255\n",
      "Training Batch [226/782]: Loss 0.039057113230228424\n",
      "Training Batch [227/782]: Loss 0.18417973816394806\n",
      "Training Batch [228/782]: Loss 0.04885393753647804\n",
      "Training Batch [229/782]: Loss 0.027660779654979706\n",
      "Training Batch [230/782]: Loss 0.027031341567635536\n",
      "Training Batch [231/782]: Loss 0.03947847709059715\n",
      "Training Batch [232/782]: Loss 0.030590955168008804\n",
      "Training Batch [233/782]: Loss 0.010153217241168022\n",
      "Training Batch [234/782]: Loss 0.005153978243470192\n",
      "Training Batch [235/782]: Loss 0.01710929349064827\n",
      "Training Batch [236/782]: Loss 0.08974196761846542\n",
      "Training Batch [237/782]: Loss 0.007013909053057432\n",
      "Training Batch [238/782]: Loss 0.030170002952218056\n",
      "Training Batch [239/782]: Loss 0.06451433897018433\n",
      "Training Batch [240/782]: Loss 0.11185134202241898\n",
      "Training Batch [241/782]: Loss 0.0286788959056139\n",
      "Training Batch [242/782]: Loss 0.06949690729379654\n",
      "Training Batch [243/782]: Loss 0.014959599822759628\n",
      "Training Batch [244/782]: Loss 0.023098301142454147\n",
      "Training Batch [245/782]: Loss 0.03461584448814392\n",
      "Training Batch [246/782]: Loss 0.006130194757133722\n",
      "Training Batch [247/782]: Loss 0.02034764736890793\n",
      "Training Batch [248/782]: Loss 0.048319511115550995\n",
      "Training Batch [249/782]: Loss 0.07876455783843994\n",
      "Training Batch [250/782]: Loss 0.013329746201634407\n",
      "Training Batch [251/782]: Loss 0.006552412174642086\n",
      "Training Batch [252/782]: Loss 0.11934326589107513\n",
      "Training Batch [253/782]: Loss 0.0060316720046103\n",
      "Training Batch [254/782]: Loss 0.06775796413421631\n",
      "Training Batch [255/782]: Loss 0.08551397919654846\n",
      "Training Batch [256/782]: Loss 0.012720403261482716\n",
      "Training Batch [257/782]: Loss 0.018235983327031136\n",
      "Training Batch [258/782]: Loss 0.027048932388424873\n",
      "Training Batch [259/782]: Loss 0.004560923669487238\n",
      "Training Batch [260/782]: Loss 0.10828889906406403\n",
      "Training Batch [261/782]: Loss 0.01865020953118801\n",
      "Training Batch [262/782]: Loss 0.07085348665714264\n",
      "Training Batch [263/782]: Loss 0.24761536717414856\n",
      "Training Batch [264/782]: Loss 0.03208951652050018\n",
      "Training Batch [265/782]: Loss 0.13459396362304688\n",
      "Training Batch [266/782]: Loss 0.11238232254981995\n",
      "Training Batch [267/782]: Loss 0.05429050326347351\n",
      "Training Batch [268/782]: Loss 0.14278389513492584\n",
      "Training Batch [269/782]: Loss 0.07555988430976868\n",
      "Training Batch [270/782]: Loss 0.0913841724395752\n",
      "Training Batch [271/782]: Loss 0.022123470902442932\n",
      "Training Batch [272/782]: Loss 0.03482490032911301\n",
      "Training Batch [273/782]: Loss 0.06617005169391632\n",
      "Training Batch [274/782]: Loss 0.006507586222141981\n",
      "Training Batch [275/782]: Loss 0.014748538844287395\n",
      "Training Batch [276/782]: Loss 0.03520633652806282\n",
      "Training Batch [277/782]: Loss 0.11921989172697067\n",
      "Training Batch [278/782]: Loss 0.01994774304330349\n",
      "Training Batch [279/782]: Loss 0.07751980423927307\n",
      "Training Batch [280/782]: Loss 0.02365974150598049\n",
      "Training Batch [281/782]: Loss 0.08360414206981659\n",
      "Training Batch [282/782]: Loss 0.013370419852435589\n",
      "Training Batch [283/782]: Loss 0.09751912206411362\n",
      "Training Batch [284/782]: Loss 0.06599114835262299\n",
      "Training Batch [285/782]: Loss 0.018870754167437553\n",
      "Training Batch [286/782]: Loss 0.05560830235481262\n",
      "Training Batch [287/782]: Loss 0.009495266713202\n",
      "Training Batch [288/782]: Loss 0.05499163642525673\n",
      "Training Batch [289/782]: Loss 0.006870541255921125\n",
      "Training Batch [290/782]: Loss 0.06049429625272751\n",
      "Training Batch [291/782]: Loss 0.03537745773792267\n",
      "Training Batch [292/782]: Loss 0.03689923882484436\n",
      "Training Batch [293/782]: Loss 0.011102485470473766\n",
      "Training Batch [294/782]: Loss 0.012044364586472511\n",
      "Training Batch [295/782]: Loss 0.09105668216943741\n",
      "Training Batch [296/782]: Loss 0.04452892020344734\n",
      "Training Batch [297/782]: Loss 0.08095167577266693\n",
      "Training Batch [298/782]: Loss 0.09893209487199783\n",
      "Training Batch [299/782]: Loss 0.026270508766174316\n",
      "Training Batch [300/782]: Loss 0.07613327354192734\n",
      "Training Batch [301/782]: Loss 0.0038015425670892\n",
      "Training Batch [302/782]: Loss 0.09892944246530533\n",
      "Training Batch [303/782]: Loss 0.009955735877156258\n",
      "Training Batch [304/782]: Loss 0.02897215448319912\n",
      "Training Batch [305/782]: Loss 0.04279658943414688\n",
      "Training Batch [306/782]: Loss 0.01054434385150671\n",
      "Training Batch [307/782]: Loss 0.10151766240596771\n",
      "Training Batch [308/782]: Loss 0.023632995784282684\n",
      "Training Batch [309/782]: Loss 0.03388223424553871\n",
      "Training Batch [310/782]: Loss 0.010800885036587715\n",
      "Training Batch [311/782]: Loss 0.15463787317276\n",
      "Training Batch [312/782]: Loss 0.0038056503981351852\n",
      "Training Batch [313/782]: Loss 0.012275698594748974\n",
      "Training Batch [314/782]: Loss 0.17875438928604126\n",
      "Training Batch [315/782]: Loss 0.015458375215530396\n",
      "Training Batch [316/782]: Loss 0.07193600386381149\n",
      "Training Batch [317/782]: Loss 0.061197929084300995\n",
      "Training Batch [318/782]: Loss 0.12007243931293488\n",
      "Training Batch [319/782]: Loss 0.029399100691080093\n",
      "Training Batch [320/782]: Loss 0.06673406809568405\n",
      "Training Batch [321/782]: Loss 0.011165418662130833\n",
      "Training Batch [322/782]: Loss 0.04807513207197189\n",
      "Training Batch [323/782]: Loss 0.10863123834133148\n",
      "Training Batch [324/782]: Loss 0.07252278923988342\n",
      "Training Batch [325/782]: Loss 0.07418107986450195\n",
      "Training Batch [326/782]: Loss 0.01111543644219637\n",
      "Training Batch [327/782]: Loss 0.09285639226436615\n",
      "Training Batch [328/782]: Loss 0.02815200760960579\n",
      "Training Batch [329/782]: Loss 0.007415791507810354\n",
      "Training Batch [330/782]: Loss 0.0606161467730999\n",
      "Training Batch [331/782]: Loss 0.05192895978689194\n",
      "Training Batch [332/782]: Loss 0.03458325192332268\n",
      "Training Batch [333/782]: Loss 0.05703461915254593\n",
      "Training Batch [334/782]: Loss 0.03432715684175491\n",
      "Training Batch [335/782]: Loss 0.04040646553039551\n",
      "Training Batch [336/782]: Loss 0.021187715232372284\n",
      "Training Batch [337/782]: Loss 0.11388424783945084\n",
      "Training Batch [338/782]: Loss 0.006255457643419504\n",
      "Training Batch [339/782]: Loss 0.10076279938220978\n",
      "Training Batch [340/782]: Loss 0.048370007425546646\n",
      "Training Batch [341/782]: Loss 0.011535543017089367\n",
      "Training Batch [342/782]: Loss 0.05138830840587616\n",
      "Training Batch [343/782]: Loss 0.009419345296919346\n",
      "Training Batch [344/782]: Loss 0.034289199858903885\n",
      "Training Batch [345/782]: Loss 0.03963601589202881\n",
      "Training Batch [346/782]: Loss 0.01166604831814766\n",
      "Training Batch [347/782]: Loss 0.03322197124361992\n",
      "Training Batch [348/782]: Loss 0.011584078893065453\n",
      "Training Batch [349/782]: Loss 0.05954167991876602\n",
      "Training Batch [350/782]: Loss 0.02503061853349209\n",
      "Training Batch [351/782]: Loss 0.04205497354269028\n",
      "Training Batch [352/782]: Loss 0.004136824514716864\n",
      "Training Batch [353/782]: Loss 0.1376703977584839\n",
      "Training Batch [354/782]: Loss 0.01653510145843029\n",
      "Training Batch [355/782]: Loss 0.10780873894691467\n",
      "Training Batch [356/782]: Loss 0.16092832386493683\n",
      "Training Batch [357/782]: Loss 0.060696426779031754\n",
      "Training Batch [358/782]: Loss 0.037921421229839325\n",
      "Training Batch [359/782]: Loss 0.021556466817855835\n",
      "Training Batch [360/782]: Loss 0.013438407331705093\n",
      "Training Batch [361/782]: Loss 0.04236598312854767\n",
      "Training Batch [362/782]: Loss 0.02606685273349285\n",
      "Training Batch [363/782]: Loss 0.0307505764067173\n",
      "Training Batch [364/782]: Loss 0.02195798233151436\n",
      "Training Batch [365/782]: Loss 0.10440311580896378\n",
      "Training Batch [366/782]: Loss 0.006171976216137409\n",
      "Training Batch [367/782]: Loss 0.07478507608175278\n",
      "Training Batch [368/782]: Loss 0.11598969250917435\n",
      "Training Batch [369/782]: Loss 0.007134960498660803\n",
      "Training Batch [370/782]: Loss 0.03358542174100876\n",
      "Training Batch [371/782]: Loss 0.012958877719938755\n",
      "Training Batch [372/782]: Loss 0.0700293630361557\n",
      "Training Batch [373/782]: Loss 0.0918448194861412\n",
      "Training Batch [374/782]: Loss 0.15466490387916565\n",
      "Training Batch [375/782]: Loss 0.149277463555336\n",
      "Training Batch [376/782]: Loss 0.0805814266204834\n",
      "Training Batch [377/782]: Loss 0.07346528768539429\n",
      "Training Batch [378/782]: Loss 0.03151410445570946\n",
      "Training Batch [379/782]: Loss 0.03783218190073967\n",
      "Training Batch [380/782]: Loss 0.05042191594839096\n",
      "Training Batch [381/782]: Loss 0.04962068423628807\n",
      "Training Batch [382/782]: Loss 0.0024060371797531843\n",
      "Training Batch [383/782]: Loss 0.02549734339118004\n",
      "Training Batch [384/782]: Loss 0.010481755249202251\n",
      "Training Batch [385/782]: Loss 0.016112374141812325\n",
      "Training Batch [386/782]: Loss 0.0366462841629982\n",
      "Training Batch [387/782]: Loss 0.016749730333685875\n",
      "Training Batch [388/782]: Loss 0.05095478147268295\n",
      "Training Batch [389/782]: Loss 0.015370821580290794\n",
      "Training Batch [390/782]: Loss 0.053784698247909546\n",
      "Training Batch [391/782]: Loss 0.016588928177952766\n",
      "Training Batch [392/782]: Loss 0.05276523530483246\n",
      "Training Batch [393/782]: Loss 0.030695905908942223\n",
      "Training Batch [394/782]: Loss 0.02090827189385891\n",
      "Training Batch [395/782]: Loss 0.21911372244358063\n",
      "Training Batch [396/782]: Loss 0.05434701219201088\n",
      "Training Batch [397/782]: Loss 0.08294850587844849\n",
      "Training Batch [398/782]: Loss 0.03201139718294144\n",
      "Training Batch [399/782]: Loss 0.01250447891652584\n",
      "Training Batch [400/782]: Loss 0.06299220025539398\n",
      "Training Batch [401/782]: Loss 0.061535678803920746\n",
      "Training Batch [402/782]: Loss 0.02580748125910759\n",
      "Training Batch [403/782]: Loss 0.057541947811841965\n",
      "Training Batch [404/782]: Loss 0.08783450722694397\n",
      "Training Batch [405/782]: Loss 0.00678264768794179\n",
      "Training Batch [406/782]: Loss 0.04412023350596428\n",
      "Training Batch [407/782]: Loss 0.06910599768161774\n",
      "Training Batch [408/782]: Loss 0.0702439695596695\n",
      "Training Batch [409/782]: Loss 0.01527828723192215\n",
      "Training Batch [410/782]: Loss 0.056822627782821655\n",
      "Training Batch [411/782]: Loss 0.08049602061510086\n",
      "Training Batch [412/782]: Loss 0.010344392620027065\n",
      "Training Batch [413/782]: Loss 0.008820576593279839\n",
      "Training Batch [414/782]: Loss 0.20721295475959778\n",
      "Training Batch [415/782]: Loss 0.039762262254953384\n",
      "Training Batch [416/782]: Loss 0.06431043893098831\n",
      "Training Batch [417/782]: Loss 0.156602144241333\n",
      "Training Batch [418/782]: Loss 0.09117443859577179\n",
      "Training Batch [419/782]: Loss 0.0530707873404026\n",
      "Training Batch [420/782]: Loss 0.02291244827210903\n",
      "Training Batch [421/782]: Loss 0.059661366045475006\n",
      "Training Batch [422/782]: Loss 0.1194068044424057\n",
      "Training Batch [423/782]: Loss 0.09363297373056412\n",
      "Training Batch [424/782]: Loss 0.109154611825943\n",
      "Training Batch [425/782]: Loss 0.028023920953273773\n",
      "Training Batch [426/782]: Loss 0.13028062880039215\n",
      "Training Batch [427/782]: Loss 0.035942088812589645\n",
      "Training Batch [428/782]: Loss 0.07258185744285583\n",
      "Training Batch [429/782]: Loss 0.012392156757414341\n",
      "Training Batch [430/782]: Loss 0.026500195264816284\n",
      "Training Batch [431/782]: Loss 0.011543565429747105\n",
      "Training Batch [432/782]: Loss 0.022731393575668335\n",
      "Training Batch [433/782]: Loss 0.08196883648633957\n",
      "Training Batch [434/782]: Loss 0.03853803500533104\n",
      "Training Batch [435/782]: Loss 0.14224083721637726\n",
      "Training Batch [436/782]: Loss 0.02533392421901226\n",
      "Training Batch [437/782]: Loss 0.16402430832386017\n",
      "Training Batch [438/782]: Loss 0.020155007019639015\n",
      "Training Batch [439/782]: Loss 0.006496903486549854\n",
      "Training Batch [440/782]: Loss 0.00807498674839735\n",
      "Training Batch [441/782]: Loss 0.015202965587377548\n",
      "Training Batch [442/782]: Loss 0.038781095296144485\n",
      "Training Batch [443/782]: Loss 0.14884500205516815\n",
      "Training Batch [444/782]: Loss 0.011053231544792652\n",
      "Training Batch [445/782]: Loss 0.038028035312891006\n",
      "Training Batch [446/782]: Loss 0.05030188709497452\n",
      "Training Batch [447/782]: Loss 0.06947574764490128\n",
      "Training Batch [448/782]: Loss 0.010592237114906311\n",
      "Training Batch [449/782]: Loss 0.021074606105685234\n",
      "Training Batch [450/782]: Loss 0.04571947455406189\n",
      "Training Batch [451/782]: Loss 0.13087856769561768\n",
      "Training Batch [452/782]: Loss 0.06971804052591324\n",
      "Training Batch [453/782]: Loss 0.007481334265321493\n",
      "Training Batch [454/782]: Loss 0.01697070524096489\n",
      "Training Batch [455/782]: Loss 0.07135377079248428\n",
      "Training Batch [456/782]: Loss 0.0054483492858707905\n",
      "Training Batch [457/782]: Loss 0.030873341485857964\n",
      "Training Batch [458/782]: Loss 0.1397837996482849\n",
      "Training Batch [459/782]: Loss 0.1020272895693779\n",
      "Training Batch [460/782]: Loss 0.0993981808423996\n",
      "Training Batch [461/782]: Loss 0.005368590820580721\n",
      "Training Batch [462/782]: Loss 0.07872429490089417\n",
      "Training Batch [463/782]: Loss 0.06362759321928024\n",
      "Training Batch [464/782]: Loss 0.027690749615430832\n",
      "Training Batch [465/782]: Loss 0.0280313603579998\n",
      "Training Batch [466/782]: Loss 0.012851284816861153\n",
      "Training Batch [467/782]: Loss 0.018566276878118515\n",
      "Training Batch [468/782]: Loss 0.037253011018037796\n",
      "Training Batch [469/782]: Loss 0.1425660252571106\n",
      "Training Batch [470/782]: Loss 0.015567949041724205\n",
      "Training Batch [471/782]: Loss 0.1181802824139595\n",
      "Training Batch [472/782]: Loss 0.03437672182917595\n",
      "Training Batch [473/782]: Loss 0.008071129210293293\n",
      "Training Batch [474/782]: Loss 0.09409186989068985\n",
      "Training Batch [475/782]: Loss 0.14277872443199158\n",
      "Training Batch [476/782]: Loss 0.12235526740550995\n",
      "Training Batch [477/782]: Loss 0.07327580451965332\n",
      "Training Batch [478/782]: Loss 0.007759295869618654\n",
      "Training Batch [479/782]: Loss 0.03473720699548721\n",
      "Training Batch [480/782]: Loss 0.030193015933036804\n",
      "Training Batch [481/782]: Loss 0.009122366085648537\n",
      "Training Batch [482/782]: Loss 0.03107399307191372\n",
      "Training Batch [483/782]: Loss 0.08178719878196716\n",
      "Training Batch [484/782]: Loss 0.11666898429393768\n",
      "Training Batch [485/782]: Loss 0.03075995109975338\n",
      "Training Batch [486/782]: Loss 0.04269914701581001\n",
      "Training Batch [487/782]: Loss 0.004606837406754494\n",
      "Training Batch [488/782]: Loss 0.0679774209856987\n",
      "Training Batch [489/782]: Loss 0.051986370235681534\n",
      "Training Batch [490/782]: Loss 0.0544428713619709\n",
      "Training Batch [491/782]: Loss 0.23989346623420715\n",
      "Training Batch [492/782]: Loss 0.055692363530397415\n",
      "Training Batch [493/782]: Loss 0.09108129143714905\n",
      "Training Batch [494/782]: Loss 0.054980967193841934\n",
      "Training Batch [495/782]: Loss 0.017049796879291534\n",
      "Training Batch [496/782]: Loss 0.04608611389994621\n",
      "Training Batch [497/782]: Loss 0.024461353197693825\n",
      "Training Batch [498/782]: Loss 0.013613952323794365\n",
      "Training Batch [499/782]: Loss 0.024454517289996147\n",
      "Training Batch [500/782]: Loss 0.026015540584921837\n",
      "Training Batch [501/782]: Loss 0.1623631715774536\n",
      "Training Batch [502/782]: Loss 0.027861585840582848\n",
      "Training Batch [503/782]: Loss 0.05383437126874924\n",
      "Training Batch [504/782]: Loss 0.038663916289806366\n",
      "Training Batch [505/782]: Loss 0.026088017970323563\n",
      "Training Batch [506/782]: Loss 0.12837442755699158\n",
      "Training Batch [507/782]: Loss 0.06441472470760345\n",
      "Training Batch [508/782]: Loss 0.01352340541779995\n",
      "Training Batch [509/782]: Loss 0.09313133358955383\n",
      "Training Batch [510/782]: Loss 0.06197420880198479\n",
      "Training Batch [511/782]: Loss 0.0598791167140007\n",
      "Training Batch [512/782]: Loss 0.010734145529568195\n",
      "Training Batch [513/782]: Loss 0.037497628480196\n",
      "Training Batch [514/782]: Loss 0.08800824731588364\n",
      "Training Batch [515/782]: Loss 0.04124271124601364\n",
      "Training Batch [516/782]: Loss 0.0758918970823288\n",
      "Training Batch [517/782]: Loss 0.03904333710670471\n",
      "Training Batch [518/782]: Loss 0.023128096014261246\n",
      "Training Batch [519/782]: Loss 0.0309456754475832\n",
      "Training Batch [520/782]: Loss 0.07414278388023376\n",
      "Training Batch [521/782]: Loss 0.03680596873164177\n",
      "Training Batch [522/782]: Loss 0.03265954926609993\n",
      "Training Batch [523/782]: Loss 0.04921778291463852\n",
      "Training Batch [524/782]: Loss 0.07369312644004822\n",
      "Training Batch [525/782]: Loss 0.02855888195335865\n",
      "Training Batch [526/782]: Loss 0.10216708481311798\n",
      "Training Batch [527/782]: Loss 0.006023289170116186\n",
      "Training Batch [528/782]: Loss 0.027830490842461586\n",
      "Training Batch [529/782]: Loss 0.07056975364685059\n",
      "Training Batch [530/782]: Loss 0.004880000837147236\n",
      "Training Batch [531/782]: Loss 0.14698360860347748\n",
      "Training Batch [532/782]: Loss 0.09936293959617615\n",
      "Training Batch [533/782]: Loss 0.0051733991131186485\n",
      "Training Batch [534/782]: Loss 0.07822510600090027\n",
      "Training Batch [535/782]: Loss 0.14242230355739594\n",
      "Training Batch [536/782]: Loss 0.16961339116096497\n",
      "Training Batch [537/782]: Loss 0.03870202600955963\n",
      "Training Batch [538/782]: Loss 0.09113302081823349\n",
      "Training Batch [539/782]: Loss 0.05030355975031853\n",
      "Training Batch [540/782]: Loss 0.006147448904812336\n",
      "Training Batch [541/782]: Loss 0.028749169781804085\n",
      "Training Batch [542/782]: Loss 0.022703200578689575\n",
      "Training Batch [543/782]: Loss 0.032996345311403275\n",
      "Training Batch [544/782]: Loss 0.09147287905216217\n",
      "Training Batch [545/782]: Loss 0.06628265976905823\n",
      "Training Batch [546/782]: Loss 0.005390693433582783\n",
      "Training Batch [547/782]: Loss 0.055512093007564545\n",
      "Training Batch [548/782]: Loss 0.02976878546178341\n",
      "Training Batch [549/782]: Loss 0.08925404399633408\n",
      "Training Batch [550/782]: Loss 0.005339868366718292\n",
      "Training Batch [551/782]: Loss 0.001539477496407926\n",
      "Training Batch [552/782]: Loss 0.020910359919071198\n",
      "Training Batch [553/782]: Loss 0.061115704476833344\n",
      "Training Batch [554/782]: Loss 0.017532560974359512\n",
      "Training Batch [555/782]: Loss 0.01854313537478447\n",
      "Training Batch [556/782]: Loss 0.01664385013282299\n",
      "Training Batch [557/782]: Loss 0.04899696633219719\n",
      "Training Batch [558/782]: Loss 0.10388365387916565\n",
      "Training Batch [559/782]: Loss 0.01586483046412468\n",
      "Training Batch [560/782]: Loss 0.018276650458574295\n",
      "Training Batch [561/782]: Loss 0.02388174459338188\n",
      "Training Batch [562/782]: Loss 0.04269416630268097\n",
      "Training Batch [563/782]: Loss 0.014629683457314968\n",
      "Training Batch [564/782]: Loss 0.021195193752646446\n",
      "Training Batch [565/782]: Loss 0.08842415362596512\n",
      "Training Batch [566/782]: Loss 0.07476505637168884\n",
      "Training Batch [567/782]: Loss 0.0810660794377327\n",
      "Training Batch [568/782]: Loss 0.018096501007676125\n",
      "Training Batch [569/782]: Loss 0.012893588282167912\n",
      "Training Batch [570/782]: Loss 0.010027178563177586\n",
      "Training Batch [571/782]: Loss 0.010172701440751553\n",
      "Training Batch [572/782]: Loss 0.06820753961801529\n",
      "Training Batch [573/782]: Loss 0.04040044546127319\n",
      "Training Batch [574/782]: Loss 0.0022973373997956514\n",
      "Training Batch [575/782]: Loss 0.02286008931696415\n",
      "Training Batch [576/782]: Loss 0.03670595958828926\n",
      "Training Batch [577/782]: Loss 0.01348200999200344\n",
      "Training Batch [578/782]: Loss 0.005096287466585636\n",
      "Training Batch [579/782]: Loss 0.010039365850389004\n",
      "Training Batch [580/782]: Loss 0.027419792488217354\n",
      "Training Batch [581/782]: Loss 0.02953491359949112\n",
      "Training Batch [582/782]: Loss 0.05458539351820946\n",
      "Training Batch [583/782]: Loss 0.047799188643693924\n",
      "Training Batch [584/782]: Loss 0.04156823456287384\n",
      "Training Batch [585/782]: Loss 0.07744507491588593\n",
      "Training Batch [586/782]: Loss 0.03573973476886749\n",
      "Training Batch [587/782]: Loss 0.09940231591463089\n",
      "Training Batch [588/782]: Loss 0.008579812943935394\n",
      "Training Batch [589/782]: Loss 0.05115574598312378\n",
      "Training Batch [590/782]: Loss 0.12934735417366028\n",
      "Training Batch [591/782]: Loss 0.023788366466760635\n",
      "Training Batch [592/782]: Loss 0.12700504064559937\n",
      "Training Batch [593/782]: Loss 0.012535324320197105\n",
      "Training Batch [594/782]: Loss 0.06029263138771057\n",
      "Training Batch [595/782]: Loss 0.0027154700364917517\n",
      "Training Batch [596/782]: Loss 0.008203800767660141\n",
      "Training Batch [597/782]: Loss 0.05611106753349304\n",
      "Training Batch [598/782]: Loss 0.0749628096818924\n",
      "Training Batch [599/782]: Loss 0.06166260316967964\n",
      "Training Batch [600/782]: Loss 0.077921561896801\n",
      "Training Batch [601/782]: Loss 0.05219614505767822\n",
      "Training Batch [602/782]: Loss 0.020452730357646942\n",
      "Training Batch [603/782]: Loss 0.016067400574684143\n",
      "Training Batch [604/782]: Loss 0.0022082007490098476\n",
      "Training Batch [605/782]: Loss 0.08676927536725998\n",
      "Training Batch [606/782]: Loss 0.10297613590955734\n",
      "Training Batch [607/782]: Loss 0.11225009709596634\n",
      "Training Batch [608/782]: Loss 0.01817658171057701\n",
      "Training Batch [609/782]: Loss 0.05676134303212166\n",
      "Training Batch [610/782]: Loss 0.06302639842033386\n",
      "Training Batch [611/782]: Loss 0.02545098401606083\n",
      "Training Batch [612/782]: Loss 0.10113648325204849\n",
      "Training Batch [613/782]: Loss 0.019665425643324852\n",
      "Training Batch [614/782]: Loss 0.04425911232829094\n",
      "Training Batch [615/782]: Loss 0.00722148735076189\n",
      "Training Batch [616/782]: Loss 0.027755744755268097\n",
      "Training Batch [617/782]: Loss 0.012661430984735489\n",
      "Training Batch [618/782]: Loss 0.1391938477754593\n",
      "Training Batch [619/782]: Loss 0.09730974584817886\n",
      "Training Batch [620/782]: Loss 0.035134684294462204\n",
      "Training Batch [621/782]: Loss 0.05907757580280304\n",
      "Training Batch [622/782]: Loss 0.03557780385017395\n",
      "Training Batch [623/782]: Loss 0.027475109323859215\n",
      "Training Batch [624/782]: Loss 0.004716272931545973\n",
      "Training Batch [625/782]: Loss 0.016473917290568352\n",
      "Training Batch [626/782]: Loss 0.05023098737001419\n",
      "Training Batch [627/782]: Loss 0.02295139990746975\n",
      "Training Batch [628/782]: Loss 0.06241591274738312\n",
      "Training Batch [629/782]: Loss 0.022672053426504135\n",
      "Training Batch [630/782]: Loss 0.02625967562198639\n",
      "Training Batch [631/782]: Loss 0.01794160157442093\n",
      "Training Batch [632/782]: Loss 0.028146686032414436\n",
      "Training Batch [633/782]: Loss 0.00803584884852171\n",
      "Training Batch [634/782]: Loss 0.03469614312052727\n",
      "Training Batch [635/782]: Loss 0.052079033106565475\n",
      "Training Batch [636/782]: Loss 0.04570123180747032\n",
      "Training Batch [637/782]: Loss 0.003213118528947234\n",
      "Training Batch [638/782]: Loss 0.02793458290398121\n",
      "Training Batch [639/782]: Loss 0.07330628484487534\n",
      "Training Batch [640/782]: Loss 0.01928532123565674\n",
      "Training Batch [641/782]: Loss 0.09187023341655731\n",
      "Training Batch [642/782]: Loss 0.059062112122774124\n",
      "Training Batch [643/782]: Loss 0.06445597112178802\n",
      "Training Batch [644/782]: Loss 0.050549305975437164\n",
      "Training Batch [645/782]: Loss 0.003642490366473794\n",
      "Training Batch [646/782]: Loss 0.08875027298927307\n",
      "Training Batch [647/782]: Loss 0.048524778336286545\n",
      "Training Batch [648/782]: Loss 0.0162733793258667\n",
      "Training Batch [649/782]: Loss 0.023467250168323517\n",
      "Training Batch [650/782]: Loss 0.02062533237040043\n",
      "Training Batch [651/782]: Loss 0.1357087641954422\n",
      "Training Batch [652/782]: Loss 0.004710896406322718\n",
      "Training Batch [653/782]: Loss 0.0037339383270591497\n",
      "Training Batch [654/782]: Loss 0.006702537182718515\n",
      "Training Batch [655/782]: Loss 0.10412294417619705\n",
      "Training Batch [656/782]: Loss 0.020346783101558685\n",
      "Training Batch [657/782]: Loss 0.061165668070316315\n",
      "Training Batch [658/782]: Loss 0.025850746780633926\n",
      "Training Batch [659/782]: Loss 0.01908792182803154\n",
      "Training Batch [660/782]: Loss 0.014091216027736664\n",
      "Training Batch [661/782]: Loss 0.014369870536029339\n",
      "Training Batch [662/782]: Loss 0.08142019063234329\n",
      "Training Batch [663/782]: Loss 0.04164746403694153\n",
      "Training Batch [664/782]: Loss 0.024581575766205788\n",
      "Training Batch [665/782]: Loss 0.010733397677540779\n",
      "Training Batch [666/782]: Loss 0.06478649377822876\n",
      "Training Batch [667/782]: Loss 0.2160903811454773\n",
      "Training Batch [668/782]: Loss 0.02010132558643818\n",
      "Training Batch [669/782]: Loss 0.0019775948021560907\n",
      "Training Batch [670/782]: Loss 0.019437476992607117\n",
      "Training Batch [671/782]: Loss 0.20500220358371735\n",
      "Training Batch [672/782]: Loss 0.031194042414426804\n",
      "Training Batch [673/782]: Loss 0.077084481716156\n",
      "Training Batch [674/782]: Loss 0.030261004343628883\n",
      "Training Batch [675/782]: Loss 0.02917891927063465\n",
      "Training Batch [676/782]: Loss 0.05396850034594536\n",
      "Training Batch [677/782]: Loss 0.09708628803491592\n",
      "Training Batch [678/782]: Loss 0.042329587042331696\n",
      "Training Batch [679/782]: Loss 0.035309337079524994\n",
      "Training Batch [680/782]: Loss 0.056301482021808624\n",
      "Training Batch [681/782]: Loss 0.08716726303100586\n",
      "Training Batch [682/782]: Loss 0.07130312919616699\n",
      "Training Batch [683/782]: Loss 0.04978644475340843\n",
      "Training Batch [684/782]: Loss 0.07465279847383499\n",
      "Training Batch [685/782]: Loss 0.04271304979920387\n",
      "Training Batch [686/782]: Loss 0.047994647175073624\n",
      "Training Batch [687/782]: Loss 0.03471038118004799\n",
      "Training Batch [688/782]: Loss 0.06651764363050461\n",
      "Training Batch [689/782]: Loss 0.007484703790396452\n",
      "Training Batch [690/782]: Loss 0.045871756970882416\n",
      "Training Batch [691/782]: Loss 0.03454652428627014\n",
      "Training Batch [692/782]: Loss 0.10972202569246292\n",
      "Training Batch [693/782]: Loss 0.011512819677591324\n",
      "Training Batch [694/782]: Loss 0.05647622048854828\n",
      "Training Batch [695/782]: Loss 0.06977567076683044\n",
      "Training Batch [696/782]: Loss 0.010069389827549458\n",
      "Training Batch [697/782]: Loss 0.017065618187189102\n",
      "Training Batch [698/782]: Loss 0.016684144735336304\n",
      "Training Batch [699/782]: Loss 0.030299516394734383\n",
      "Training Batch [700/782]: Loss 0.010687058791518211\n",
      "Training Batch [701/782]: Loss 0.045548614114522934\n",
      "Training Batch [702/782]: Loss 0.03523463010787964\n",
      "Training Batch [703/782]: Loss 0.041674789041280746\n",
      "Training Batch [704/782]: Loss 0.07826603204011917\n",
      "Training Batch [705/782]: Loss 0.008612570352852345\n",
      "Training Batch [706/782]: Loss 0.0171902384608984\n",
      "Training Batch [707/782]: Loss 0.03959927707910538\n",
      "Training Batch [708/782]: Loss 0.015018334612250328\n",
      "Training Batch [709/782]: Loss 0.05823082849383354\n",
      "Training Batch [710/782]: Loss 0.010679767467081547\n",
      "Training Batch [711/782]: Loss 0.012810606509447098\n",
      "Training Batch [712/782]: Loss 0.03412841632962227\n",
      "Training Batch [713/782]: Loss 0.038566313683986664\n",
      "Training Batch [714/782]: Loss 0.09323287755250931\n",
      "Training Batch [715/782]: Loss 0.06545715779066086\n",
      "Training Batch [716/782]: Loss 0.004222756251692772\n",
      "Training Batch [717/782]: Loss 0.02127777598798275\n",
      "Training Batch [718/782]: Loss 0.016300298273563385\n",
      "Training Batch [719/782]: Loss 0.014321847818791866\n",
      "Training Batch [720/782]: Loss 0.019914833828806877\n",
      "Training Batch [721/782]: Loss 0.008231802843511105\n",
      "Training Batch [722/782]: Loss 0.04967806860804558\n",
      "Training Batch [723/782]: Loss 0.03727288916707039\n",
      "Training Batch [724/782]: Loss 0.012599165551364422\n",
      "Training Batch [725/782]: Loss 0.03147464990615845\n",
      "Training Batch [726/782]: Loss 0.10364554822444916\n",
      "Training Batch [727/782]: Loss 0.04934258386492729\n",
      "Training Batch [728/782]: Loss 0.03110186941921711\n",
      "Training Batch [729/782]: Loss 0.0232541523873806\n",
      "Training Batch [730/782]: Loss 0.04686397314071655\n",
      "Training Batch [731/782]: Loss 0.01794455014169216\n",
      "Training Batch [732/782]: Loss 0.01087240595370531\n",
      "Training Batch [733/782]: Loss 0.028836416080594063\n",
      "Training Batch [734/782]: Loss 0.007179970853030682\n",
      "Training Batch [735/782]: Loss 0.007315964438021183\n",
      "Training Batch [736/782]: Loss 0.014970384538173676\n",
      "Training Batch [737/782]: Loss 0.06278953701257706\n",
      "Training Batch [738/782]: Loss 0.03863519802689552\n",
      "Training Batch [739/782]: Loss 0.10412782430648804\n",
      "Training Batch [740/782]: Loss 0.0941585898399353\n",
      "Training Batch [741/782]: Loss 0.09649687260389328\n",
      "Training Batch [742/782]: Loss 0.03403754159808159\n",
      "Training Batch [743/782]: Loss 0.10268678516149521\n",
      "Training Batch [744/782]: Loss 0.01516024861484766\n",
      "Training Batch [745/782]: Loss 0.006208724807947874\n",
      "Training Batch [746/782]: Loss 0.03224927559494972\n",
      "Training Batch [747/782]: Loss 0.0072943042032420635\n",
      "Training Batch [748/782]: Loss 0.06592907011508942\n",
      "Training Batch [749/782]: Loss 0.007480425760149956\n",
      "Training Batch [750/782]: Loss 0.03887084126472473\n",
      "Training Batch [751/782]: Loss 0.11509311199188232\n",
      "Training Batch [752/782]: Loss 0.07296227663755417\n",
      "Training Batch [753/782]: Loss 0.12602455914020538\n",
      "Training Batch [754/782]: Loss 0.09927881509065628\n",
      "Training Batch [755/782]: Loss 0.018765905871987343\n",
      "Training Batch [756/782]: Loss 0.08316133916378021\n",
      "Training Batch [757/782]: Loss 0.02932295948266983\n",
      "Training Batch [758/782]: Loss 0.01623782142996788\n",
      "Training Batch [759/782]: Loss 0.041987307369709015\n",
      "Training Batch [760/782]: Loss 0.1587647944688797\n",
      "Training Batch [761/782]: Loss 0.03006226196885109\n",
      "Training Batch [762/782]: Loss 0.01310255192220211\n",
      "Training Batch [763/782]: Loss 0.09651494771242142\n",
      "Training Batch [764/782]: Loss 0.14039088785648346\n",
      "Training Batch [765/782]: Loss 0.04200492426753044\n",
      "Training Batch [766/782]: Loss 0.03601943328976631\n",
      "Training Batch [767/782]: Loss 0.006771681364625692\n",
      "Training Batch [768/782]: Loss 0.013735429383814335\n",
      "Training Batch [769/782]: Loss 0.039488784968853\n",
      "Training Batch [770/782]: Loss 0.010361570864915848\n",
      "Training Batch [771/782]: Loss 0.13763566315174103\n",
      "Training Batch [772/782]: Loss 0.08881445229053497\n",
      "Training Batch [773/782]: Loss 0.020016413182020187\n",
      "Training Batch [774/782]: Loss 0.03520382568240166\n",
      "Training Batch [775/782]: Loss 0.03454658016562462\n",
      "Training Batch [776/782]: Loss 0.22774752974510193\n",
      "Training Batch [777/782]: Loss 0.044546205550432205\n",
      "Training Batch [778/782]: Loss 0.021446237340569496\n",
      "Training Batch [779/782]: Loss 0.022450992837548256\n",
      "Training Batch [780/782]: Loss 0.006226615514606237\n",
      "Training Batch [781/782]: Loss 0.02379680424928665\n",
      "Training Batch [782/782]: Loss 0.012732619419693947\n",
      "Epoch 19 - Train Loss: 0.0466\n",
      "*********  Epoch 20/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.02037859335541725\n",
      "Training Batch [2/782]: Loss 0.033401183784008026\n",
      "Training Batch [3/782]: Loss 0.10276190936565399\n",
      "Training Batch [4/782]: Loss 0.046803299337625504\n",
      "Training Batch [5/782]: Loss 0.04034430906176567\n",
      "Training Batch [6/782]: Loss 0.046504780650138855\n",
      "Training Batch [7/782]: Loss 0.018188796937465668\n",
      "Training Batch [8/782]: Loss 0.031157031655311584\n",
      "Training Batch [9/782]: Loss 0.0359649732708931\n",
      "Training Batch [10/782]: Loss 0.008380351588129997\n",
      "Training Batch [11/782]: Loss 0.11972301453351974\n",
      "Training Batch [12/782]: Loss 0.05489032343029976\n",
      "Training Batch [13/782]: Loss 0.03166890889406204\n",
      "Training Batch [14/782]: Loss 0.04102039709687233\n",
      "Training Batch [15/782]: Loss 0.15160919725894928\n",
      "Training Batch [16/782]: Loss 0.018122857436537743\n",
      "Training Batch [17/782]: Loss 0.08849532902240753\n",
      "Training Batch [18/782]: Loss 0.05645535886287689\n",
      "Training Batch [19/782]: Loss 0.06600691378116608\n",
      "Training Batch [20/782]: Loss 0.052205536514520645\n",
      "Training Batch [21/782]: Loss 0.03739560768008232\n",
      "Training Batch [22/782]: Loss 0.06223859265446663\n",
      "Training Batch [23/782]: Loss 0.01535775139927864\n",
      "Training Batch [24/782]: Loss 0.0069188387133181095\n",
      "Training Batch [25/782]: Loss 0.007442527450621128\n",
      "Training Batch [26/782]: Loss 0.01577099785208702\n",
      "Training Batch [27/782]: Loss 0.07272828370332718\n",
      "Training Batch [28/782]: Loss 0.018474984914064407\n",
      "Training Batch [29/782]: Loss 0.11069691181182861\n",
      "Training Batch [30/782]: Loss 0.014565263874828815\n",
      "Training Batch [31/782]: Loss 0.011424770578742027\n",
      "Training Batch [32/782]: Loss 0.09656941890716553\n",
      "Training Batch [33/782]: Loss 0.02404685690999031\n",
      "Training Batch [34/782]: Loss 0.02926613576710224\n",
      "Training Batch [35/782]: Loss 0.0037728818133473396\n",
      "Training Batch [36/782]: Loss 0.07678088545799255\n",
      "Training Batch [37/782]: Loss 0.029792629182338715\n",
      "Training Batch [38/782]: Loss 0.001997845247387886\n",
      "Training Batch [39/782]: Loss 0.007517208810895681\n",
      "Training Batch [40/782]: Loss 0.074667789041996\n",
      "Training Batch [41/782]: Loss 0.0043439967557787895\n",
      "Training Batch [42/782]: Loss 0.014635794796049595\n",
      "Training Batch [43/782]: Loss 0.023348966613411903\n",
      "Training Batch [44/782]: Loss 0.038698289543390274\n",
      "Training Batch [45/782]: Loss 0.04085715487599373\n",
      "Training Batch [46/782]: Loss 0.02236820012331009\n",
      "Training Batch [47/782]: Loss 0.006406492553651333\n",
      "Training Batch [48/782]: Loss 0.013765452429652214\n",
      "Training Batch [49/782]: Loss 0.021573565900325775\n",
      "Training Batch [50/782]: Loss 0.014435221441090107\n",
      "Training Batch [51/782]: Loss 0.041404157876968384\n",
      "Training Batch [52/782]: Loss 0.06601397693157196\n",
      "Training Batch [53/782]: Loss 0.014373439364135265\n",
      "Training Batch [54/782]: Loss 0.01812298595905304\n",
      "Training Batch [55/782]: Loss 0.004353024996817112\n",
      "Training Batch [56/782]: Loss 0.027887050062417984\n",
      "Training Batch [57/782]: Loss 0.05820833891630173\n",
      "Training Batch [58/782]: Loss 0.06049731746315956\n",
      "Training Batch [59/782]: Loss 0.034699033945798874\n",
      "Training Batch [60/782]: Loss 0.0015550263924524188\n",
      "Training Batch [61/782]: Loss 0.01147555373609066\n",
      "Training Batch [62/782]: Loss 0.005217841826379299\n",
      "Training Batch [63/782]: Loss 0.004353750962764025\n",
      "Training Batch [64/782]: Loss 0.009534885175526142\n",
      "Training Batch [65/782]: Loss 0.025060148909687996\n",
      "Training Batch [66/782]: Loss 0.01340016070753336\n",
      "Training Batch [67/782]: Loss 0.01765032857656479\n",
      "Training Batch [68/782]: Loss 0.02945135533809662\n",
      "Training Batch [69/782]: Loss 0.06332963705062866\n",
      "Training Batch [70/782]: Loss 0.06932809948921204\n",
      "Training Batch [71/782]: Loss 0.054928019642829895\n",
      "Training Batch [72/782]: Loss 0.004015412647277117\n",
      "Training Batch [73/782]: Loss 0.0032288033980876207\n",
      "Training Batch [74/782]: Loss 0.03748784214258194\n",
      "Training Batch [75/782]: Loss 0.0037462604232132435\n",
      "Training Batch [76/782]: Loss 0.016730917617678642\n",
      "Training Batch [77/782]: Loss 0.004594852682203054\n",
      "Training Batch [78/782]: Loss 0.01253734901547432\n",
      "Training Batch [79/782]: Loss 0.03233008086681366\n",
      "Training Batch [80/782]: Loss 0.015286851674318314\n",
      "Training Batch [81/782]: Loss 0.04149011895060539\n",
      "Training Batch [82/782]: Loss 0.014675730839371681\n",
      "Training Batch [83/782]: Loss 0.051313627511262894\n",
      "Training Batch [84/782]: Loss 0.14749720692634583\n",
      "Training Batch [85/782]: Loss 0.032578323036432266\n",
      "Training Batch [86/782]: Loss 0.04949479177594185\n",
      "Training Batch [87/782]: Loss 0.06230897456407547\n",
      "Training Batch [88/782]: Loss 0.006705996580421925\n",
      "Training Batch [89/782]: Loss 0.0234517902135849\n",
      "Training Batch [90/782]: Loss 0.04729137569665909\n",
      "Training Batch [91/782]: Loss 0.00840884167701006\n",
      "Training Batch [92/782]: Loss 0.0032009435817599297\n",
      "Training Batch [93/782]: Loss 0.00746051874011755\n",
      "Training Batch [94/782]: Loss 0.04879084974527359\n",
      "Training Batch [95/782]: Loss 0.009481782093644142\n",
      "Training Batch [96/782]: Loss 0.021569939330220222\n",
      "Training Batch [97/782]: Loss 0.0033418117091059685\n",
      "Training Batch [98/782]: Loss 0.04192741587758064\n",
      "Training Batch [99/782]: Loss 0.04000386968255043\n",
      "Training Batch [100/782]: Loss 0.0042862435802817345\n",
      "Training Batch [101/782]: Loss 0.013472349382936954\n",
      "Training Batch [102/782]: Loss 0.03838580846786499\n",
      "Training Batch [103/782]: Loss 0.003267622319981456\n",
      "Training Batch [104/782]: Loss 0.023584416136145592\n",
      "Training Batch [105/782]: Loss 0.009245307184755802\n",
      "Training Batch [106/782]: Loss 0.0014686807990074158\n",
      "Training Batch [107/782]: Loss 0.026933081448078156\n",
      "Training Batch [108/782]: Loss 0.15385249257087708\n",
      "Training Batch [109/782]: Loss 0.0075041791424155235\n",
      "Training Batch [110/782]: Loss 0.0108602624386549\n",
      "Training Batch [111/782]: Loss 0.00904067326337099\n",
      "Training Batch [112/782]: Loss 0.024464299902319908\n",
      "Training Batch [113/782]: Loss 0.012575234286487103\n",
      "Training Batch [114/782]: Loss 0.005290291737765074\n",
      "Training Batch [115/782]: Loss 0.06559459120035172\n",
      "Training Batch [116/782]: Loss 0.01968889869749546\n",
      "Training Batch [117/782]: Loss 0.01989627629518509\n",
      "Training Batch [118/782]: Loss 0.01518384087830782\n",
      "Training Batch [119/782]: Loss 0.01252478826791048\n",
      "Training Batch [120/782]: Loss 0.027934888377785683\n",
      "Training Batch [121/782]: Loss 0.029709164053201675\n",
      "Training Batch [122/782]: Loss 0.008136820048093796\n",
      "Training Batch [123/782]: Loss 0.07640602439641953\n",
      "Training Batch [124/782]: Loss 0.01095028966665268\n",
      "Training Batch [125/782]: Loss 0.008891248144209385\n",
      "Training Batch [126/782]: Loss 0.012863107025623322\n",
      "Training Batch [127/782]: Loss 0.019547929987311363\n",
      "Training Batch [128/782]: Loss 0.001285775564610958\n",
      "Training Batch [129/782]: Loss 0.09258737415075302\n",
      "Training Batch [130/782]: Loss 0.01802567020058632\n",
      "Training Batch [131/782]: Loss 0.006114816293120384\n",
      "Training Batch [132/782]: Loss 0.009485934861004353\n",
      "Training Batch [133/782]: Loss 0.021565474569797516\n",
      "Training Batch [134/782]: Loss 0.009014373645186424\n",
      "Training Batch [135/782]: Loss 0.011475902982056141\n",
      "Training Batch [136/782]: Loss 0.014193100854754448\n",
      "Training Batch [137/782]: Loss 0.0031913924030959606\n",
      "Training Batch [138/782]: Loss 0.009969759732484818\n",
      "Training Batch [139/782]: Loss 0.06326936185359955\n",
      "Training Batch [140/782]: Loss 0.08333726227283478\n",
      "Training Batch [141/782]: Loss 0.022120807319879532\n",
      "Training Batch [142/782]: Loss 0.0031722139101475477\n",
      "Training Batch [143/782]: Loss 0.0059191519394516945\n",
      "Training Batch [144/782]: Loss 0.02680017799139023\n",
      "Training Batch [145/782]: Loss 0.04197622463107109\n",
      "Training Batch [146/782]: Loss 0.007961053401231766\n",
      "Training Batch [147/782]: Loss 0.06340545415878296\n",
      "Training Batch [148/782]: Loss 0.04997146874666214\n",
      "Training Batch [149/782]: Loss 0.009994284249842167\n",
      "Training Batch [150/782]: Loss 0.02112504467368126\n",
      "Training Batch [151/782]: Loss 0.005035739857703447\n",
      "Training Batch [152/782]: Loss 0.010703037492930889\n",
      "Training Batch [153/782]: Loss 0.021757084876298904\n",
      "Training Batch [154/782]: Loss 0.0052990964613854885\n",
      "Training Batch [155/782]: Loss 0.012464895844459534\n",
      "Training Batch [156/782]: Loss 0.04176681116223335\n",
      "Training Batch [157/782]: Loss 0.07499410957098007\n",
      "Training Batch [158/782]: Loss 0.012872958555817604\n",
      "Training Batch [159/782]: Loss 0.011467916890978813\n",
      "Training Batch [160/782]: Loss 0.008183920755982399\n",
      "Training Batch [161/782]: Loss 0.04266461730003357\n",
      "Training Batch [162/782]: Loss 0.007751487195491791\n",
      "Training Batch [163/782]: Loss 0.00871764775365591\n",
      "Training Batch [164/782]: Loss 0.006301096640527248\n",
      "Training Batch [165/782]: Loss 0.012798857875168324\n",
      "Training Batch [166/782]: Loss 0.005418835207819939\n",
      "Training Batch [167/782]: Loss 0.002322256797924638\n",
      "Training Batch [168/782]: Loss 0.004645651672035456\n",
      "Training Batch [169/782]: Loss 0.04266887530684471\n",
      "Training Batch [170/782]: Loss 0.014408129267394543\n",
      "Training Batch [171/782]: Loss 0.035535261034965515\n",
      "Training Batch [172/782]: Loss 0.05788515880703926\n",
      "Training Batch [173/782]: Loss 0.013335783034563065\n",
      "Training Batch [174/782]: Loss 0.10993916541337967\n",
      "Training Batch [175/782]: Loss 0.03519841656088829\n",
      "Training Batch [176/782]: Loss 0.010241225361824036\n",
      "Training Batch [177/782]: Loss 0.030841970816254616\n",
      "Training Batch [178/782]: Loss 0.0025569358840584755\n",
      "Training Batch [179/782]: Loss 0.020701928064227104\n",
      "Training Batch [180/782]: Loss 0.0029270420782268047\n",
      "Training Batch [181/782]: Loss 0.020624006167054176\n",
      "Training Batch [182/782]: Loss 0.008982660248875618\n",
      "Training Batch [183/782]: Loss 0.0021065622568130493\n",
      "Training Batch [184/782]: Loss 0.025641122832894325\n",
      "Training Batch [185/782]: Loss 0.044073931872844696\n",
      "Training Batch [186/782]: Loss 0.0371631383895874\n",
      "Training Batch [187/782]: Loss 0.10199317336082458\n",
      "Training Batch [188/782]: Loss 0.0006801171111874282\n",
      "Training Batch [189/782]: Loss 0.00332545954734087\n",
      "Training Batch [190/782]: Loss 0.02581091970205307\n",
      "Training Batch [191/782]: Loss 0.005063924938440323\n",
      "Training Batch [192/782]: Loss 0.01560928113758564\n",
      "Training Batch [193/782]: Loss 0.04464341327548027\n",
      "Training Batch [194/782]: Loss 0.0025431944523006678\n",
      "Training Batch [195/782]: Loss 0.023582419380545616\n",
      "Training Batch [196/782]: Loss 0.012644294649362564\n",
      "Training Batch [197/782]: Loss 0.017042767256498337\n",
      "Training Batch [198/782]: Loss 0.01407660637050867\n",
      "Training Batch [199/782]: Loss 0.003467630362138152\n",
      "Training Batch [200/782]: Loss 0.010775331407785416\n",
      "Training Batch [201/782]: Loss 0.06534702330827713\n",
      "Training Batch [202/782]: Loss 0.008374832570552826\n",
      "Training Batch [203/782]: Loss 0.03566213324666023\n",
      "Training Batch [204/782]: Loss 0.09124957770109177\n",
      "Training Batch [205/782]: Loss 0.011394402012228966\n",
      "Training Batch [206/782]: Loss 0.005041805561631918\n",
      "Training Batch [207/782]: Loss 0.05822954326868057\n",
      "Training Batch [208/782]: Loss 0.005509720183908939\n",
      "Training Batch [209/782]: Loss 0.012373842298984528\n",
      "Training Batch [210/782]: Loss 0.010277215391397476\n",
      "Training Batch [211/782]: Loss 0.12641699612140656\n",
      "Training Batch [212/782]: Loss 0.04891417920589447\n",
      "Training Batch [213/782]: Loss 0.12171244621276855\n",
      "Training Batch [214/782]: Loss 0.03556634858250618\n",
      "Training Batch [215/782]: Loss 0.04112526401877403\n",
      "Training Batch [216/782]: Loss 0.044707249850034714\n",
      "Training Batch [217/782]: Loss 0.0035310471430420876\n",
      "Training Batch [218/782]: Loss 0.02225504070520401\n",
      "Training Batch [219/782]: Loss 0.013529793359339237\n",
      "Training Batch [220/782]: Loss 0.024962784722447395\n",
      "Training Batch [221/782]: Loss 0.01681751199066639\n",
      "Training Batch [222/782]: Loss 0.041139837354421616\n",
      "Training Batch [223/782]: Loss 0.03693210706114769\n",
      "Training Batch [224/782]: Loss 0.07222165912389755\n",
      "Training Batch [225/782]: Loss 0.01418798603117466\n",
      "Training Batch [226/782]: Loss 0.00586187606677413\n",
      "Training Batch [227/782]: Loss 0.03233090043067932\n",
      "Training Batch [228/782]: Loss 0.050441574305295944\n",
      "Training Batch [229/782]: Loss 0.10931622982025146\n",
      "Training Batch [230/782]: Loss 0.00070004170993343\n",
      "Training Batch [231/782]: Loss 0.0026232758536934853\n",
      "Training Batch [232/782]: Loss 0.01911763660609722\n",
      "Training Batch [233/782]: Loss 0.028953909873962402\n",
      "Training Batch [234/782]: Loss 0.1613074541091919\n",
      "Training Batch [235/782]: Loss 0.01327615324407816\n",
      "Training Batch [236/782]: Loss 0.0042493147775530815\n",
      "Training Batch [237/782]: Loss 0.0034935090225189924\n",
      "Training Batch [238/782]: Loss 0.005752310156822205\n",
      "Training Batch [239/782]: Loss 0.07581044733524323\n",
      "Training Batch [240/782]: Loss 0.01777753047645092\n",
      "Training Batch [241/782]: Loss 0.041871968656778336\n",
      "Training Batch [242/782]: Loss 0.023686015978455544\n",
      "Training Batch [243/782]: Loss 0.0031883588526397943\n",
      "Training Batch [244/782]: Loss 0.010452339425683022\n",
      "Training Batch [245/782]: Loss 0.06781739741563797\n",
      "Training Batch [246/782]: Loss 0.06865918636322021\n",
      "Training Batch [247/782]: Loss 0.013764603063464165\n",
      "Training Batch [248/782]: Loss 0.06902902573347092\n",
      "Training Batch [249/782]: Loss 0.10410145670175552\n",
      "Training Batch [250/782]: Loss 0.010311583057045937\n",
      "Training Batch [251/782]: Loss 0.1334867775440216\n",
      "Training Batch [252/782]: Loss 0.006400653161108494\n",
      "Training Batch [253/782]: Loss 0.09919821470975876\n",
      "Training Batch [254/782]: Loss 0.01970578357577324\n",
      "Training Batch [255/782]: Loss 0.10328736156225204\n",
      "Training Batch [256/782]: Loss 0.009701318107545376\n",
      "Training Batch [257/782]: Loss 0.010754463262856007\n",
      "Training Batch [258/782]: Loss 0.08862195909023285\n",
      "Training Batch [259/782]: Loss 0.05034870654344559\n",
      "Training Batch [260/782]: Loss 0.05799673870205879\n",
      "Training Batch [261/782]: Loss 0.0025486224330961704\n",
      "Training Batch [262/782]: Loss 0.1687854379415512\n",
      "Training Batch [263/782]: Loss 0.004329937044531107\n",
      "Training Batch [264/782]: Loss 0.019885389134287834\n",
      "Training Batch [265/782]: Loss 0.03166867420077324\n",
      "Training Batch [266/782]: Loss 0.007959614507853985\n",
      "Training Batch [267/782]: Loss 0.012512491084635258\n",
      "Training Batch [268/782]: Loss 0.030182262882590294\n",
      "Training Batch [269/782]: Loss 0.0664389505982399\n",
      "Training Batch [270/782]: Loss 0.022054363042116165\n",
      "Training Batch [271/782]: Loss 0.014460965991020203\n",
      "Training Batch [272/782]: Loss 0.02331678196787834\n",
      "Training Batch [273/782]: Loss 0.06845619529485703\n",
      "Training Batch [274/782]: Loss 0.008857703767716885\n",
      "Training Batch [275/782]: Loss 0.008861597627401352\n",
      "Training Batch [276/782]: Loss 0.03652292117476463\n",
      "Training Batch [277/782]: Loss 0.05352402105927467\n",
      "Training Batch [278/782]: Loss 0.019147170707583427\n",
      "Training Batch [279/782]: Loss 0.11659222841262817\n",
      "Training Batch [280/782]: Loss 0.014668907038867474\n",
      "Training Batch [281/782]: Loss 0.029530029743909836\n",
      "Training Batch [282/782]: Loss 0.02116982638835907\n",
      "Training Batch [283/782]: Loss 0.014593325555324554\n",
      "Training Batch [284/782]: Loss 0.04642686992883682\n",
      "Training Batch [285/782]: Loss 0.007430999539792538\n",
      "Training Batch [286/782]: Loss 0.11759397387504578\n",
      "Training Batch [287/782]: Loss 0.0005547097534872591\n",
      "Training Batch [288/782]: Loss 0.006264829076826572\n",
      "Training Batch [289/782]: Loss 0.04393456503748894\n",
      "Training Batch [290/782]: Loss 0.010277153924107552\n",
      "Training Batch [291/782]: Loss 0.009968881495296955\n",
      "Training Batch [292/782]: Loss 0.017817147076129913\n",
      "Training Batch [293/782]: Loss 0.028130892664194107\n",
      "Training Batch [294/782]: Loss 0.0030884721782058477\n",
      "Training Batch [295/782]: Loss 0.00981889944523573\n",
      "Training Batch [296/782]: Loss 0.06539952754974365\n",
      "Training Batch [297/782]: Loss 0.04766913875937462\n",
      "Training Batch [298/782]: Loss 0.02814527601003647\n",
      "Training Batch [299/782]: Loss 0.12167991697788239\n",
      "Training Batch [300/782]: Loss 0.04377115145325661\n",
      "Training Batch [301/782]: Loss 0.009549567475914955\n",
      "Training Batch [302/782]: Loss 0.020972881466150284\n",
      "Training Batch [303/782]: Loss 0.05172555521130562\n",
      "Training Batch [304/782]: Loss 0.034929826855659485\n",
      "Training Batch [305/782]: Loss 0.02583671174943447\n",
      "Training Batch [306/782]: Loss 0.06045040860772133\n",
      "Training Batch [307/782]: Loss 0.06834182888269424\n",
      "Training Batch [308/782]: Loss 0.027676980942487717\n",
      "Training Batch [309/782]: Loss 0.007920435629785061\n",
      "Training Batch [310/782]: Loss 0.011886837892234325\n",
      "Training Batch [311/782]: Loss 0.007316118106245995\n",
      "Training Batch [312/782]: Loss 0.03464625030755997\n",
      "Training Batch [313/782]: Loss 0.09464004635810852\n",
      "Training Batch [314/782]: Loss 0.041504692286252975\n",
      "Training Batch [315/782]: Loss 0.020852720364928246\n",
      "Training Batch [316/782]: Loss 0.005355305038392544\n",
      "Training Batch [317/782]: Loss 0.05636901408433914\n",
      "Training Batch [318/782]: Loss 0.012580597773194313\n",
      "Training Batch [319/782]: Loss 0.0105046471580863\n",
      "Training Batch [320/782]: Loss 0.003994806669652462\n",
      "Training Batch [321/782]: Loss 0.0033037415705621243\n",
      "Training Batch [322/782]: Loss 0.013729994185268879\n",
      "Training Batch [323/782]: Loss 0.019768783822655678\n",
      "Training Batch [324/782]: Loss 0.009599082171916962\n",
      "Training Batch [325/782]: Loss 0.017100514844059944\n",
      "Training Batch [326/782]: Loss 0.015396851114928722\n",
      "Training Batch [327/782]: Loss 0.032595064491033554\n",
      "Training Batch [328/782]: Loss 0.05257898196578026\n",
      "Training Batch [329/782]: Loss 0.015917625278234482\n",
      "Training Batch [330/782]: Loss 0.009439180605113506\n",
      "Training Batch [331/782]: Loss 0.01529018022119999\n",
      "Training Batch [332/782]: Loss 0.08332444727420807\n",
      "Training Batch [333/782]: Loss 0.04985577240586281\n",
      "Training Batch [334/782]: Loss 0.06465433537960052\n",
      "Training Batch [335/782]: Loss 0.0014334750594571233\n",
      "Training Batch [336/782]: Loss 0.022806581109762192\n",
      "Training Batch [337/782]: Loss 0.035037100315093994\n",
      "Training Batch [338/782]: Loss 0.030545856803655624\n",
      "Training Batch [339/782]: Loss 0.038344744592905045\n",
      "Training Batch [340/782]: Loss 0.010006309486925602\n",
      "Training Batch [341/782]: Loss 0.018908968195319176\n",
      "Training Batch [342/782]: Loss 0.028973912820219994\n",
      "Training Batch [343/782]: Loss 0.010217300616204739\n",
      "Training Batch [344/782]: Loss 0.07280705124139786\n",
      "Training Batch [345/782]: Loss 0.01411285437643528\n",
      "Training Batch [346/782]: Loss 0.02478371188044548\n",
      "Training Batch [347/782]: Loss 0.01048622652888298\n",
      "Training Batch [348/782]: Loss 0.0019147738348692656\n",
      "Training Batch [349/782]: Loss 0.017629023641347885\n",
      "Training Batch [350/782]: Loss 0.09199987351894379\n",
      "Training Batch [351/782]: Loss 0.009912879206240177\n",
      "Training Batch [352/782]: Loss 0.043375998735427856\n",
      "Training Batch [353/782]: Loss 0.060772109776735306\n",
      "Training Batch [354/782]: Loss 0.042111773043870926\n",
      "Training Batch [355/782]: Loss 0.10169576853513718\n",
      "Training Batch [356/782]: Loss 0.02332155779004097\n",
      "Training Batch [357/782]: Loss 0.029778245836496353\n",
      "Training Batch [358/782]: Loss 0.013220832683146\n",
      "Training Batch [359/782]: Loss 0.012973080389201641\n",
      "Training Batch [360/782]: Loss 0.035517238080501556\n",
      "Training Batch [361/782]: Loss 0.03445838764309883\n",
      "Training Batch [362/782]: Loss 0.02911517582833767\n",
      "Training Batch [363/782]: Loss 0.00879854429513216\n",
      "Training Batch [364/782]: Loss 0.010636926628649235\n",
      "Training Batch [365/782]: Loss 0.16652530431747437\n",
      "Training Batch [366/782]: Loss 0.045871879905462265\n",
      "Training Batch [367/782]: Loss 0.013973294757306576\n",
      "Training Batch [368/782]: Loss 0.16126710176467896\n",
      "Training Batch [369/782]: Loss 0.031530819833278656\n",
      "Training Batch [370/782]: Loss 0.07056564092636108\n",
      "Training Batch [371/782]: Loss 0.05237790569663048\n",
      "Training Batch [372/782]: Loss 0.09941323101520538\n",
      "Training Batch [373/782]: Loss 0.0024224857334047556\n",
      "Training Batch [374/782]: Loss 0.0709262266755104\n",
      "Training Batch [375/782]: Loss 0.012027338147163391\n",
      "Training Batch [376/782]: Loss 0.06534375250339508\n",
      "Training Batch [377/782]: Loss 0.048525940626859665\n",
      "Training Batch [378/782]: Loss 0.013551470823585987\n",
      "Training Batch [379/782]: Loss 0.051154974848032\n",
      "Training Batch [380/782]: Loss 0.11915002763271332\n",
      "Training Batch [381/782]: Loss 0.016829537227749825\n",
      "Training Batch [382/782]: Loss 0.04723012447357178\n",
      "Training Batch [383/782]: Loss 0.020622067153453827\n",
      "Training Batch [384/782]: Loss 0.06712477654218674\n",
      "Training Batch [385/782]: Loss 0.11003591865301132\n",
      "Training Batch [386/782]: Loss 0.005933044478297234\n",
      "Training Batch [387/782]: Loss 0.014487124979496002\n",
      "Training Batch [388/782]: Loss 0.031747739762067795\n",
      "Training Batch [389/782]: Loss 0.028006011620163918\n",
      "Training Batch [390/782]: Loss 0.026781070977449417\n",
      "Training Batch [391/782]: Loss 0.12993250787258148\n",
      "Training Batch [392/782]: Loss 0.012135975062847137\n",
      "Training Batch [393/782]: Loss 0.12402409315109253\n",
      "Training Batch [394/782]: Loss 0.0848374143242836\n",
      "Training Batch [395/782]: Loss 0.03673188015818596\n",
      "Training Batch [396/782]: Loss 0.05892372876405716\n",
      "Training Batch [397/782]: Loss 0.08423712104558945\n",
      "Training Batch [398/782]: Loss 0.07952471822500229\n",
      "Training Batch [399/782]: Loss 0.0034397898707538843\n",
      "Training Batch [400/782]: Loss 0.017925160005688667\n",
      "Training Batch [401/782]: Loss 0.03531995788216591\n",
      "Training Batch [402/782]: Loss 0.030208507552742958\n",
      "Training Batch [403/782]: Loss 0.055896926671266556\n",
      "Training Batch [404/782]: Loss 0.037418171763420105\n",
      "Training Batch [405/782]: Loss 0.07100401818752289\n",
      "Training Batch [406/782]: Loss 0.029568547382950783\n",
      "Training Batch [407/782]: Loss 0.05058066174387932\n",
      "Training Batch [408/782]: Loss 0.072039894759655\n",
      "Training Batch [409/782]: Loss 0.0269557386636734\n",
      "Training Batch [410/782]: Loss 0.0164882093667984\n",
      "Training Batch [411/782]: Loss 0.07480448484420776\n",
      "Training Batch [412/782]: Loss 0.0236072801053524\n",
      "Training Batch [413/782]: Loss 0.008608169853687286\n",
      "Training Batch [414/782]: Loss 0.003667136188596487\n",
      "Training Batch [415/782]: Loss 0.02365340292453766\n",
      "Training Batch [416/782]: Loss 0.042454130947589874\n",
      "Training Batch [417/782]: Loss 0.0373540036380291\n",
      "Training Batch [418/782]: Loss 0.018167618662118912\n",
      "Training Batch [419/782]: Loss 0.010568294674158096\n",
      "Training Batch [420/782]: Loss 0.006461721379309893\n",
      "Training Batch [421/782]: Loss 0.05140009522438049\n",
      "Training Batch [422/782]: Loss 0.023928986862301826\n",
      "Training Batch [423/782]: Loss 0.05941237136721611\n",
      "Training Batch [424/782]: Loss 0.014453466981649399\n",
      "Training Batch [425/782]: Loss 0.031566813588142395\n",
      "Training Batch [426/782]: Loss 0.09822171181440353\n",
      "Training Batch [427/782]: Loss 0.06610141694545746\n",
      "Training Batch [428/782]: Loss 0.00936688482761383\n",
      "Training Batch [429/782]: Loss 0.025016464293003082\n",
      "Training Batch [430/782]: Loss 0.039973184466362\n",
      "Training Batch [431/782]: Loss 0.014878137037158012\n",
      "Training Batch [432/782]: Loss 0.0726570188999176\n",
      "Training Batch [433/782]: Loss 0.0383940115571022\n",
      "Training Batch [434/782]: Loss 0.039967168122529984\n",
      "Training Batch [435/782]: Loss 0.008058947511017323\n",
      "Training Batch [436/782]: Loss 0.014397691935300827\n",
      "Training Batch [437/782]: Loss 0.008004264906048775\n",
      "Training Batch [438/782]: Loss 0.03730625659227371\n",
      "Training Batch [439/782]: Loss 0.024422409012913704\n",
      "Training Batch [440/782]: Loss 0.03704630210995674\n",
      "Training Batch [441/782]: Loss 0.010154809802770615\n",
      "Training Batch [442/782]: Loss 0.014025484211742878\n",
      "Training Batch [443/782]: Loss 0.08260886371135712\n",
      "Training Batch [444/782]: Loss 0.012115871533751488\n",
      "Training Batch [445/782]: Loss 0.07352354377508163\n",
      "Training Batch [446/782]: Loss 0.025441786274313927\n",
      "Training Batch [447/782]: Loss 0.006729546003043652\n",
      "Training Batch [448/782]: Loss 0.027228359133005142\n",
      "Training Batch [449/782]: Loss 0.03579104319214821\n",
      "Training Batch [450/782]: Loss 0.04025943949818611\n",
      "Training Batch [451/782]: Loss 0.024192102253437042\n",
      "Training Batch [452/782]: Loss 0.13466693460941315\n",
      "Training Batch [453/782]: Loss 0.0028478242456912994\n",
      "Training Batch [454/782]: Loss 0.010168753564357758\n",
      "Training Batch [455/782]: Loss 0.029404547065496445\n",
      "Training Batch [456/782]: Loss 0.02705910988152027\n",
      "Training Batch [457/782]: Loss 0.10341192036867142\n",
      "Training Batch [458/782]: Loss 0.019694197922945023\n",
      "Training Batch [459/782]: Loss 0.008499887771904469\n",
      "Training Batch [460/782]: Loss 0.04170733690261841\n",
      "Training Batch [461/782]: Loss 0.0228520929813385\n",
      "Training Batch [462/782]: Loss 0.032906413078308105\n",
      "Training Batch [463/782]: Loss 0.06269542872905731\n",
      "Training Batch [464/782]: Loss 0.0072656613774597645\n",
      "Training Batch [465/782]: Loss 0.00828019343316555\n",
      "Training Batch [466/782]: Loss 0.008707940578460693\n",
      "Training Batch [467/782]: Loss 0.003934094216674566\n",
      "Training Batch [468/782]: Loss 0.021799631416797638\n",
      "Training Batch [469/782]: Loss 0.03452625125646591\n",
      "Training Batch [470/782]: Loss 0.007955244742333889\n",
      "Training Batch [471/782]: Loss 0.012794175185263157\n",
      "Training Batch [472/782]: Loss 0.05284729599952698\n",
      "Training Batch [473/782]: Loss 0.008388007059693336\n",
      "Training Batch [474/782]: Loss 0.020452359691262245\n",
      "Training Batch [475/782]: Loss 0.007842791266739368\n",
      "Training Batch [476/782]: Loss 0.007538486272096634\n",
      "Training Batch [477/782]: Loss 0.016233401373028755\n",
      "Training Batch [478/782]: Loss 0.03710424527525902\n",
      "Training Batch [479/782]: Loss 0.057148277759552\n",
      "Training Batch [480/782]: Loss 0.0054447767324745655\n",
      "Training Batch [481/782]: Loss 0.01855175755918026\n",
      "Training Batch [482/782]: Loss 0.02284986898303032\n",
      "Training Batch [483/782]: Loss 0.08572639524936676\n",
      "Training Batch [484/782]: Loss 0.015062808059155941\n",
      "Training Batch [485/782]: Loss 0.009268754161894321\n",
      "Training Batch [486/782]: Loss 0.01327326986938715\n",
      "Training Batch [487/782]: Loss 0.006570328492671251\n",
      "Training Batch [488/782]: Loss 0.060894906520843506\n",
      "Training Batch [489/782]: Loss 0.04543013870716095\n",
      "Training Batch [490/782]: Loss 0.018984073773026466\n",
      "Training Batch [491/782]: Loss 0.0057344031520187855\n",
      "Training Batch [492/782]: Loss 0.019608600065112114\n",
      "Training Batch [493/782]: Loss 0.043200958520174026\n",
      "Training Batch [494/782]: Loss 0.026819830760359764\n",
      "Training Batch [495/782]: Loss 0.03588568791747093\n",
      "Training Batch [496/782]: Loss 0.012444335967302322\n",
      "Training Batch [497/782]: Loss 0.038073472678661346\n",
      "Training Batch [498/782]: Loss 0.06509135663509369\n",
      "Training Batch [499/782]: Loss 0.03537154942750931\n",
      "Training Batch [500/782]: Loss 0.03211209550499916\n",
      "Training Batch [501/782]: Loss 0.010227838531136513\n",
      "Training Batch [502/782]: Loss 0.04558427259325981\n",
      "Training Batch [503/782]: Loss 0.012740030884742737\n",
      "Training Batch [504/782]: Loss 0.043748270720243454\n",
      "Training Batch [505/782]: Loss 0.05487574264407158\n",
      "Training Batch [506/782]: Loss 0.04092821106314659\n",
      "Training Batch [507/782]: Loss 0.045488908886909485\n",
      "Training Batch [508/782]: Loss 0.02612355351448059\n",
      "Training Batch [509/782]: Loss 0.0421517975628376\n",
      "Training Batch [510/782]: Loss 0.03317314386367798\n",
      "Training Batch [511/782]: Loss 0.009351316839456558\n",
      "Training Batch [512/782]: Loss 0.022962164133787155\n",
      "Training Batch [513/782]: Loss 0.05299410596489906\n",
      "Training Batch [514/782]: Loss 0.017540542408823967\n",
      "Training Batch [515/782]: Loss 0.0016661809058859944\n",
      "Training Batch [516/782]: Loss 0.009144740179181099\n",
      "Training Batch [517/782]: Loss 0.1013697013258934\n",
      "Training Batch [518/782]: Loss 0.01676339842379093\n",
      "Training Batch [519/782]: Loss 0.016632480546832085\n",
      "Training Batch [520/782]: Loss 0.007868096232414246\n",
      "Training Batch [521/782]: Loss 0.17390772700309753\n",
      "Training Batch [522/782]: Loss 0.10470011085271835\n",
      "Training Batch [523/782]: Loss 0.06816872954368591\n",
      "Training Batch [524/782]: Loss 0.034629710018634796\n",
      "Training Batch [525/782]: Loss 0.008240647614002228\n",
      "Training Batch [526/782]: Loss 0.020675240084528923\n",
      "Training Batch [527/782]: Loss 0.02825082279741764\n",
      "Training Batch [528/782]: Loss 0.06631417572498322\n",
      "Training Batch [529/782]: Loss 0.06606926023960114\n",
      "Training Batch [530/782]: Loss 0.016519904136657715\n",
      "Training Batch [531/782]: Loss 0.04754238203167915\n",
      "Training Batch [532/782]: Loss 0.015188801102340221\n",
      "Training Batch [533/782]: Loss 0.01708059385418892\n",
      "Training Batch [534/782]: Loss 0.004172648768872023\n",
      "Training Batch [535/782]: Loss 0.05429444834589958\n",
      "Training Batch [536/782]: Loss 0.08278905600309372\n",
      "Training Batch [537/782]: Loss 0.010884858667850494\n",
      "Training Batch [538/782]: Loss 0.010456499643623829\n",
      "Training Batch [539/782]: Loss 0.04445883631706238\n",
      "Training Batch [540/782]: Loss 0.03358389437198639\n",
      "Training Batch [541/782]: Loss 0.014158646576106548\n",
      "Training Batch [542/782]: Loss 0.1408698707818985\n",
      "Training Batch [543/782]: Loss 0.0217170100659132\n",
      "Training Batch [544/782]: Loss 0.03577365353703499\n",
      "Training Batch [545/782]: Loss 0.017055364325642586\n",
      "Training Batch [546/782]: Loss 0.019085753709077835\n",
      "Training Batch [547/782]: Loss 0.034943509846925735\n",
      "Training Batch [548/782]: Loss 0.004899703897535801\n",
      "Training Batch [549/782]: Loss 0.04787331074476242\n",
      "Training Batch [550/782]: Loss 0.1142275407910347\n",
      "Training Batch [551/782]: Loss 0.0217540692538023\n",
      "Training Batch [552/782]: Loss 0.0917576476931572\n",
      "Training Batch [553/782]: Loss 0.02500816620886326\n",
      "Training Batch [554/782]: Loss 0.012620190158486366\n",
      "Training Batch [555/782]: Loss 0.050963353365659714\n",
      "Training Batch [556/782]: Loss 0.01794080063700676\n",
      "Training Batch [557/782]: Loss 0.012717418372631073\n",
      "Training Batch [558/782]: Loss 0.03943570703268051\n",
      "Training Batch [559/782]: Loss 0.006311709992587566\n",
      "Training Batch [560/782]: Loss 0.01185943279415369\n",
      "Training Batch [561/782]: Loss 0.026430092751979828\n",
      "Training Batch [562/782]: Loss 0.07178426533937454\n",
      "Training Batch [563/782]: Loss 0.07654233276844025\n",
      "Training Batch [564/782]: Loss 0.01046112272888422\n",
      "Training Batch [565/782]: Loss 0.046507954597473145\n",
      "Training Batch [566/782]: Loss 0.15476137399673462\n",
      "Training Batch [567/782]: Loss 0.006890197284519672\n",
      "Training Batch [568/782]: Loss 0.056174054741859436\n",
      "Training Batch [569/782]: Loss 0.026870235800743103\n",
      "Training Batch [570/782]: Loss 0.07428035140037537\n",
      "Training Batch [571/782]: Loss 0.03315441310405731\n",
      "Training Batch [572/782]: Loss 0.006404337473213673\n",
      "Training Batch [573/782]: Loss 0.014634065330028534\n",
      "Training Batch [574/782]: Loss 0.015262364409863949\n",
      "Training Batch [575/782]: Loss 0.02151937410235405\n",
      "Training Batch [576/782]: Loss 0.021315505728125572\n",
      "Training Batch [577/782]: Loss 0.01805090345442295\n",
      "Training Batch [578/782]: Loss 0.004030400421470404\n",
      "Training Batch [579/782]: Loss 0.02268177643418312\n",
      "Training Batch [580/782]: Loss 0.028780078515410423\n",
      "Training Batch [581/782]: Loss 0.03952779248356819\n",
      "Training Batch [582/782]: Loss 0.043147433549165726\n",
      "Training Batch [583/782]: Loss 0.05763169005513191\n",
      "Training Batch [584/782]: Loss 0.01909371092915535\n",
      "Training Batch [585/782]: Loss 0.010970911011099815\n",
      "Training Batch [586/782]: Loss 0.009436294436454773\n",
      "Training Batch [587/782]: Loss 0.06947609037160873\n",
      "Training Batch [588/782]: Loss 0.06402190029621124\n",
      "Training Batch [589/782]: Loss 0.014084696769714355\n",
      "Training Batch [590/782]: Loss 0.058784421533346176\n",
      "Training Batch [591/782]: Loss 0.028530146926641464\n",
      "Training Batch [592/782]: Loss 0.019455626606941223\n",
      "Training Batch [593/782]: Loss 0.01106057409197092\n",
      "Training Batch [594/782]: Loss 0.08376660943031311\n",
      "Training Batch [595/782]: Loss 0.07028425484895706\n",
      "Training Batch [596/782]: Loss 0.06384092569351196\n",
      "Training Batch [597/782]: Loss 0.03364161401987076\n",
      "Training Batch [598/782]: Loss 0.051060400903224945\n",
      "Training Batch [599/782]: Loss 0.005692334845662117\n",
      "Training Batch [600/782]: Loss 0.0016879127360880375\n",
      "Training Batch [601/782]: Loss 0.07310817390680313\n",
      "Training Batch [602/782]: Loss 0.0013725371100008488\n",
      "Training Batch [603/782]: Loss 0.0029717397410422564\n",
      "Training Batch [604/782]: Loss 0.024556731805205345\n",
      "Training Batch [605/782]: Loss 0.04359807074069977\n",
      "Training Batch [606/782]: Loss 0.018682125955820084\n",
      "Training Batch [607/782]: Loss 0.009452108293771744\n",
      "Training Batch [608/782]: Loss 0.026955561712384224\n",
      "Training Batch [609/782]: Loss 0.043229714035987854\n",
      "Training Batch [610/782]: Loss 0.017253119498491287\n",
      "Training Batch [611/782]: Loss 0.023665888234972954\n",
      "Training Batch [612/782]: Loss 0.0169344712048769\n",
      "Training Batch [613/782]: Loss 0.008457017131149769\n",
      "Training Batch [614/782]: Loss 0.05250011384487152\n",
      "Training Batch [615/782]: Loss 0.02844097465276718\n",
      "Training Batch [616/782]: Loss 0.020153973251581192\n",
      "Training Batch [617/782]: Loss 0.004492573440074921\n",
      "Training Batch [618/782]: Loss 0.05987502634525299\n",
      "Training Batch [619/782]: Loss 0.008765370585024357\n",
      "Training Batch [620/782]: Loss 0.02441634051501751\n",
      "Training Batch [621/782]: Loss 0.026549212634563446\n",
      "Training Batch [622/782]: Loss 0.009933331981301308\n",
      "Training Batch [623/782]: Loss 0.016136761754751205\n",
      "Training Batch [624/782]: Loss 0.06824525445699692\n",
      "Training Batch [625/782]: Loss 0.005301437806338072\n",
      "Training Batch [626/782]: Loss 0.011540014296770096\n",
      "Training Batch [627/782]: Loss 0.06430358439683914\n",
      "Training Batch [628/782]: Loss 0.05214927718043327\n",
      "Training Batch [629/782]: Loss 0.02232682891190052\n",
      "Training Batch [630/782]: Loss 0.01183383073657751\n",
      "Training Batch [631/782]: Loss 0.0030254016164690256\n",
      "Training Batch [632/782]: Loss 0.08720757067203522\n",
      "Training Batch [633/782]: Loss 0.0333980955183506\n",
      "Training Batch [634/782]: Loss 0.004922640509903431\n",
      "Training Batch [635/782]: Loss 0.07931624352931976\n",
      "Training Batch [636/782]: Loss 0.004720574710518122\n",
      "Training Batch [637/782]: Loss 0.008569319732487202\n",
      "Training Batch [638/782]: Loss 0.00646361755207181\n",
      "Training Batch [639/782]: Loss 0.04777054861187935\n",
      "Training Batch [640/782]: Loss 0.01622314564883709\n",
      "Training Batch [641/782]: Loss 0.049137379974126816\n",
      "Training Batch [642/782]: Loss 0.011904939077794552\n",
      "Training Batch [643/782]: Loss 0.012562301009893417\n",
      "Training Batch [644/782]: Loss 0.02843383699655533\n",
      "Training Batch [645/782]: Loss 0.08861537277698517\n",
      "Training Batch [646/782]: Loss 0.07155659049749374\n",
      "Training Batch [647/782]: Loss 0.020660651847720146\n",
      "Training Batch [648/782]: Loss 0.0008960041450336576\n",
      "Training Batch [649/782]: Loss 0.19607597589492798\n",
      "Training Batch [650/782]: Loss 0.0037360070273280144\n",
      "Training Batch [651/782]: Loss 0.028753943741321564\n",
      "Training Batch [652/782]: Loss 0.06379362940788269\n",
      "Training Batch [653/782]: Loss 0.10089156031608582\n",
      "Training Batch [654/782]: Loss 0.07159809023141861\n",
      "Training Batch [655/782]: Loss 0.058235615491867065\n",
      "Training Batch [656/782]: Loss 0.021049268543720245\n",
      "Training Batch [657/782]: Loss 0.04194789007306099\n",
      "Training Batch [658/782]: Loss 0.048514652997255325\n",
      "Training Batch [659/782]: Loss 0.21329337358474731\n",
      "Training Batch [660/782]: Loss 0.028860701248049736\n",
      "Training Batch [661/782]: Loss 0.02499380148947239\n",
      "Training Batch [662/782]: Loss 0.045037176460027695\n",
      "Training Batch [663/782]: Loss 0.06534547358751297\n",
      "Training Batch [664/782]: Loss 0.08521643280982971\n",
      "Training Batch [665/782]: Loss 0.12201465666294098\n",
      "Training Batch [666/782]: Loss 0.010459099896252155\n",
      "Training Batch [667/782]: Loss 0.03184361010789871\n",
      "Training Batch [668/782]: Loss 0.025884374976158142\n",
      "Training Batch [669/782]: Loss 0.10207914561033249\n",
      "Training Batch [670/782]: Loss 0.0734088271856308\n",
      "Training Batch [671/782]: Loss 0.004842780996114016\n",
      "Training Batch [672/782]: Loss 0.02627994678914547\n",
      "Training Batch [673/782]: Loss 0.0008828118443489075\n",
      "Training Batch [674/782]: Loss 0.06705529242753983\n",
      "Training Batch [675/782]: Loss 0.09012787789106369\n",
      "Training Batch [676/782]: Loss 0.044021692126989365\n",
      "Training Batch [677/782]: Loss 0.09116668999195099\n",
      "Training Batch [678/782]: Loss 0.025355685502290726\n",
      "Training Batch [679/782]: Loss 0.13677358627319336\n",
      "Training Batch [680/782]: Loss 0.03711017593741417\n",
      "Training Batch [681/782]: Loss 0.06462952494621277\n",
      "Training Batch [682/782]: Loss 0.06816967576742172\n",
      "Training Batch [683/782]: Loss 0.022013235837221146\n",
      "Training Batch [684/782]: Loss 0.04109632223844528\n",
      "Training Batch [685/782]: Loss 0.03330782800912857\n",
      "Training Batch [686/782]: Loss 0.0602356381714344\n",
      "Training Batch [687/782]: Loss 0.05324732884764671\n",
      "Training Batch [688/782]: Loss 0.0668962150812149\n",
      "Training Batch [689/782]: Loss 0.08169423043727875\n",
      "Training Batch [690/782]: Loss 0.004243579227477312\n",
      "Training Batch [691/782]: Loss 0.011535374447703362\n",
      "Training Batch [692/782]: Loss 0.04588177800178528\n",
      "Training Batch [693/782]: Loss 0.014234542846679688\n",
      "Training Batch [694/782]: Loss 0.04556209594011307\n",
      "Training Batch [695/782]: Loss 0.015968766063451767\n",
      "Training Batch [696/782]: Loss 0.15925729274749756\n",
      "Training Batch [697/782]: Loss 0.15612059831619263\n",
      "Training Batch [698/782]: Loss 0.04279785975813866\n",
      "Training Batch [699/782]: Loss 0.028091447427868843\n",
      "Training Batch [700/782]: Loss 0.03382740542292595\n",
      "Training Batch [701/782]: Loss 0.12121040374040604\n",
      "Training Batch [702/782]: Loss 0.0024383768904954195\n",
      "Training Batch [703/782]: Loss 0.06623277068138123\n",
      "Training Batch [704/782]: Loss 0.02689354494214058\n",
      "Training Batch [705/782]: Loss 0.12675192952156067\n",
      "Training Batch [706/782]: Loss 0.02059330977499485\n",
      "Training Batch [707/782]: Loss 0.0593101792037487\n",
      "Training Batch [708/782]: Loss 0.025133665651082993\n",
      "Training Batch [709/782]: Loss 0.048443593084812164\n",
      "Training Batch [710/782]: Loss 0.07994703948497772\n",
      "Training Batch [711/782]: Loss 0.01585714891552925\n",
      "Training Batch [712/782]: Loss 0.05798353627324104\n",
      "Training Batch [713/782]: Loss 0.013902311213314533\n",
      "Training Batch [714/782]: Loss 0.041705094277858734\n",
      "Training Batch [715/782]: Loss 0.023221168667078018\n",
      "Training Batch [716/782]: Loss 0.014987783506512642\n",
      "Training Batch [717/782]: Loss 0.02306528389453888\n",
      "Training Batch [718/782]: Loss 0.019065530970692635\n",
      "Training Batch [719/782]: Loss 0.017872827127575874\n",
      "Training Batch [720/782]: Loss 0.1137857437133789\n",
      "Training Batch [721/782]: Loss 0.08065535873174667\n",
      "Training Batch [722/782]: Loss 0.017515918239951134\n",
      "Training Batch [723/782]: Loss 0.0024290336295962334\n",
      "Training Batch [724/782]: Loss 0.06252918392419815\n",
      "Training Batch [725/782]: Loss 0.020566558465361595\n",
      "Training Batch [726/782]: Loss 0.007400901522487402\n",
      "Training Batch [727/782]: Loss 0.0561070591211319\n",
      "Training Batch [728/782]: Loss 0.03863469883799553\n",
      "Training Batch [729/782]: Loss 0.03041541948914528\n",
      "Training Batch [730/782]: Loss 0.10579417645931244\n",
      "Training Batch [731/782]: Loss 0.03922335430979729\n",
      "Training Batch [732/782]: Loss 0.07144606858491898\n",
      "Training Batch [733/782]: Loss 0.04765211045742035\n",
      "Training Batch [734/782]: Loss 0.012214722111821175\n",
      "Training Batch [735/782]: Loss 0.004258469212800264\n",
      "Training Batch [736/782]: Loss 0.1217108964920044\n",
      "Training Batch [737/782]: Loss 0.006329031195491552\n",
      "Training Batch [738/782]: Loss 0.0019456959562376142\n",
      "Training Batch [739/782]: Loss 0.05153198167681694\n",
      "Training Batch [740/782]: Loss 0.004589583724737167\n",
      "Training Batch [741/782]: Loss 0.00527852401137352\n",
      "Training Batch [742/782]: Loss 0.07602999359369278\n",
      "Training Batch [743/782]: Loss 0.015711920335888863\n",
      "Training Batch [744/782]: Loss 0.062199968844652176\n",
      "Training Batch [745/782]: Loss 0.010179034434258938\n",
      "Training Batch [746/782]: Loss 0.009567784145474434\n",
      "Training Batch [747/782]: Loss 0.12168124318122864\n",
      "Training Batch [748/782]: Loss 0.05660730227828026\n",
      "Training Batch [749/782]: Loss 0.08764998614788055\n",
      "Training Batch [750/782]: Loss 0.009777888655662537\n",
      "Training Batch [751/782]: Loss 0.06096980720758438\n",
      "Training Batch [752/782]: Loss 0.009282542392611504\n",
      "Training Batch [753/782]: Loss 0.0065815323032438755\n",
      "Training Batch [754/782]: Loss 0.0213636327534914\n",
      "Training Batch [755/782]: Loss 0.006511901970952749\n",
      "Training Batch [756/782]: Loss 0.14459753036499023\n",
      "Training Batch [757/782]: Loss 0.04545678570866585\n",
      "Training Batch [758/782]: Loss 0.16281194984912872\n",
      "Training Batch [759/782]: Loss 0.007969630882143974\n",
      "Training Batch [760/782]: Loss 0.006226213648915291\n",
      "Training Batch [761/782]: Loss 0.012054717168211937\n",
      "Training Batch [762/782]: Loss 0.05724730342626572\n",
      "Training Batch [763/782]: Loss 0.027143089100718498\n",
      "Training Batch [764/782]: Loss 0.032141514122486115\n",
      "Training Batch [765/782]: Loss 0.013385750353336334\n",
      "Training Batch [766/782]: Loss 0.05075612664222717\n",
      "Training Batch [767/782]: Loss 0.06726115196943283\n",
      "Training Batch [768/782]: Loss 0.007143582217395306\n",
      "Training Batch [769/782]: Loss 0.006466298829764128\n",
      "Training Batch [770/782]: Loss 0.06885425746440887\n",
      "Training Batch [771/782]: Loss 0.05753529071807861\n",
      "Training Batch [772/782]: Loss 0.21753090620040894\n",
      "Training Batch [773/782]: Loss 0.002883073640987277\n",
      "Training Batch [774/782]: Loss 0.013345672748982906\n",
      "Training Batch [775/782]: Loss 0.055254608392715454\n",
      "Training Batch [776/782]: Loss 0.019490277394652367\n",
      "Training Batch [777/782]: Loss 0.13198623061180115\n",
      "Training Batch [778/782]: Loss 0.028067192062735558\n",
      "Training Batch [779/782]: Loss 0.03650869429111481\n",
      "Training Batch [780/782]: Loss 0.04417461156845093\n",
      "Training Batch [781/782]: Loss 0.015259490348398685\n",
      "Training Batch [782/782]: Loss 0.0006889078067615628\n",
      "Epoch 20 - Train Loss: 0.0364\n",
      "*********  Epoch 21/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.004473500419408083\n",
      "Training Batch [2/782]: Loss 0.0410652831196785\n",
      "Training Batch [3/782]: Loss 0.009227238595485687\n",
      "Training Batch [4/782]: Loss 0.027002332732081413\n",
      "Training Batch [5/782]: Loss 0.0029288125224411488\n",
      "Training Batch [6/782]: Loss 0.0252080000936985\n",
      "Training Batch [7/782]: Loss 0.01929357461631298\n",
      "Training Batch [8/782]: Loss 0.011421866714954376\n",
      "Training Batch [9/782]: Loss 0.07937122136354446\n",
      "Training Batch [10/782]: Loss 0.03670114651322365\n",
      "Training Batch [11/782]: Loss 0.09438450634479523\n",
      "Training Batch [12/782]: Loss 0.0054612900130450726\n",
      "Training Batch [13/782]: Loss 0.01893509365618229\n",
      "Training Batch [14/782]: Loss 0.00989584717899561\n",
      "Training Batch [15/782]: Loss 0.003585740691050887\n",
      "Training Batch [16/782]: Loss 0.013565833680331707\n",
      "Training Batch [17/782]: Loss 0.006182923447340727\n",
      "Training Batch [18/782]: Loss 0.016750318929553032\n",
      "Training Batch [19/782]: Loss 0.024472704157233238\n",
      "Training Batch [20/782]: Loss 0.025818470865488052\n",
      "Training Batch [21/782]: Loss 0.0296292956918478\n",
      "Training Batch [22/782]: Loss 0.03408504277467728\n",
      "Training Batch [23/782]: Loss 0.013966914266347885\n",
      "Training Batch [24/782]: Loss 0.0021266955882310867\n",
      "Training Batch [25/782]: Loss 0.0027456351090222597\n",
      "Training Batch [26/782]: Loss 0.10034289211034775\n",
      "Training Batch [27/782]: Loss 0.0032050409354269505\n",
      "Training Batch [28/782]: Loss 0.021112661808729172\n",
      "Training Batch [29/782]: Loss 0.05369364470243454\n",
      "Training Batch [30/782]: Loss 0.019812146201729774\n",
      "Training Batch [31/782]: Loss 0.008426615968346596\n",
      "Training Batch [32/782]: Loss 0.0539981909096241\n",
      "Training Batch [33/782]: Loss 0.0008858086657710373\n",
      "Training Batch [34/782]: Loss 0.05070038512349129\n",
      "Training Batch [35/782]: Loss 0.032002970576286316\n",
      "Training Batch [36/782]: Loss 0.016318727284669876\n",
      "Training Batch [37/782]: Loss 0.018656384199857712\n",
      "Training Batch [38/782]: Loss 0.0243074893951416\n",
      "Training Batch [39/782]: Loss 0.005657145753502846\n",
      "Training Batch [40/782]: Loss 0.004433148540556431\n",
      "Training Batch [41/782]: Loss 0.028591163456439972\n",
      "Training Batch [42/782]: Loss 0.0052506509236991405\n",
      "Training Batch [43/782]: Loss 0.03297276422381401\n",
      "Training Batch [44/782]: Loss 0.006099616643041372\n",
      "Training Batch [45/782]: Loss 0.055445291101932526\n",
      "Training Batch [46/782]: Loss 0.03636147081851959\n",
      "Training Batch [47/782]: Loss 0.0015650613931939006\n",
      "Training Batch [48/782]: Loss 0.02356570214033127\n",
      "Training Batch [49/782]: Loss 0.010697295889258385\n",
      "Training Batch [50/782]: Loss 0.004348482005298138\n",
      "Training Batch [51/782]: Loss 0.04677164554595947\n",
      "Training Batch [52/782]: Loss 0.007354556582868099\n",
      "Training Batch [53/782]: Loss 0.009138299152255058\n",
      "Training Batch [54/782]: Loss 0.007325788959860802\n",
      "Training Batch [55/782]: Loss 0.014288637787103653\n",
      "Training Batch [56/782]: Loss 0.027974987402558327\n",
      "Training Batch [57/782]: Loss 0.00496836518868804\n",
      "Training Batch [58/782]: Loss 0.008598863147199154\n",
      "Training Batch [59/782]: Loss 0.01666577346622944\n",
      "Training Batch [60/782]: Loss 0.009970622137188911\n",
      "Training Batch [61/782]: Loss 0.043528854846954346\n",
      "Training Batch [62/782]: Loss 0.014166020788252354\n",
      "Training Batch [63/782]: Loss 0.004069832619279623\n",
      "Training Batch [64/782]: Loss 0.030809853225946426\n",
      "Training Batch [65/782]: Loss 0.027783231809735298\n",
      "Training Batch [66/782]: Loss 0.011252548545598984\n",
      "Training Batch [67/782]: Loss 0.005558657459914684\n",
      "Training Batch [68/782]: Loss 0.008869894780218601\n",
      "Training Batch [69/782]: Loss 0.0038256808184087276\n",
      "Training Batch [70/782]: Loss 0.05479566380381584\n",
      "Training Batch [71/782]: Loss 0.00714844511821866\n",
      "Training Batch [72/782]: Loss 0.003797882702201605\n",
      "Training Batch [73/782]: Loss 0.018411992117762566\n",
      "Training Batch [74/782]: Loss 0.006394506432116032\n",
      "Training Batch [75/782]: Loss 0.0646345466375351\n",
      "Training Batch [76/782]: Loss 0.004916951525956392\n",
      "Training Batch [77/782]: Loss 0.006555354688316584\n",
      "Training Batch [78/782]: Loss 0.013962497934699059\n",
      "Training Batch [79/782]: Loss 0.04823201522231102\n",
      "Training Batch [80/782]: Loss 0.003120525972917676\n",
      "Training Batch [81/782]: Loss 0.009300880134105682\n",
      "Training Batch [82/782]: Loss 0.014015245251357555\n",
      "Training Batch [83/782]: Loss 0.01610734313726425\n",
      "Training Batch [84/782]: Loss 0.034334953874349594\n",
      "Training Batch [85/782]: Loss 0.00311288982629776\n",
      "Training Batch [86/782]: Loss 0.03677569329738617\n",
      "Training Batch [87/782]: Loss 0.024715352803468704\n",
      "Training Batch [88/782]: Loss 0.04796856641769409\n",
      "Training Batch [89/782]: Loss 0.010750380344688892\n",
      "Training Batch [90/782]: Loss 0.038293253630399704\n",
      "Training Batch [91/782]: Loss 0.003006744896993041\n",
      "Training Batch [92/782]: Loss 0.004593957215547562\n",
      "Training Batch [93/782]: Loss 0.00430498830974102\n",
      "Training Batch [94/782]: Loss 0.006221093703061342\n",
      "Training Batch [95/782]: Loss 0.023106280714273453\n",
      "Training Batch [96/782]: Loss 0.03304954990744591\n",
      "Training Batch [97/782]: Loss 0.012290926650166512\n",
      "Training Batch [98/782]: Loss 0.010005636140704155\n",
      "Training Batch [99/782]: Loss 0.00418658833950758\n",
      "Training Batch [100/782]: Loss 0.0021254224702715874\n",
      "Training Batch [101/782]: Loss 0.00421109888702631\n",
      "Training Batch [102/782]: Loss 0.0032124286517500877\n",
      "Training Batch [103/782]: Loss 0.02593725547194481\n",
      "Training Batch [104/782]: Loss 0.0041859871707856655\n",
      "Training Batch [105/782]: Loss 0.03039560653269291\n",
      "Training Batch [106/782]: Loss 0.05103755742311478\n",
      "Training Batch [107/782]: Loss 0.0074776094406843185\n",
      "Training Batch [108/782]: Loss 0.0015005632303655148\n",
      "Training Batch [109/782]: Loss 0.012664278037846088\n",
      "Training Batch [110/782]: Loss 0.020274484530091286\n",
      "Training Batch [111/782]: Loss 0.016961155459284782\n",
      "Training Batch [112/782]: Loss 0.009154477156698704\n",
      "Training Batch [113/782]: Loss 0.007913256995379925\n",
      "Training Batch [114/782]: Loss 0.018411673605442047\n",
      "Training Batch [115/782]: Loss 0.008603209629654884\n",
      "Training Batch [116/782]: Loss 0.050095073878765106\n",
      "Training Batch [117/782]: Loss 0.011660820804536343\n",
      "Training Batch [118/782]: Loss 0.02855379320681095\n",
      "Training Batch [119/782]: Loss 0.0091271186247468\n",
      "Training Batch [120/782]: Loss 0.020804090425372124\n",
      "Training Batch [121/782]: Loss 0.007979758083820343\n",
      "Training Batch [122/782]: Loss 0.007505168206989765\n",
      "Training Batch [123/782]: Loss 0.033344801515340805\n",
      "Training Batch [124/782]: Loss 0.01833360455930233\n",
      "Training Batch [125/782]: Loss 0.06840438395738602\n",
      "Training Batch [126/782]: Loss 0.012160958722233772\n",
      "Training Batch [127/782]: Loss 0.006593611091375351\n",
      "Training Batch [128/782]: Loss 0.009881386533379555\n",
      "Training Batch [129/782]: Loss 0.05059114471077919\n",
      "Training Batch [130/782]: Loss 0.031712036579847336\n",
      "Training Batch [131/782]: Loss 0.004554287530481815\n",
      "Training Batch [132/782]: Loss 0.05159730464220047\n",
      "Training Batch [133/782]: Loss 0.010251134634017944\n",
      "Training Batch [134/782]: Loss 0.04158451035618782\n",
      "Training Batch [135/782]: Loss 0.04308121278882027\n",
      "Training Batch [136/782]: Loss 0.004287684336304665\n",
      "Training Batch [137/782]: Loss 0.011445020325481892\n",
      "Training Batch [138/782]: Loss 0.09940885007381439\n",
      "Training Batch [139/782]: Loss 0.009603653103113174\n",
      "Training Batch [140/782]: Loss 0.0023446553386747837\n",
      "Training Batch [141/782]: Loss 0.0018975784769281745\n",
      "Training Batch [142/782]: Loss 0.01837565377354622\n",
      "Training Batch [143/782]: Loss 0.03325219824910164\n",
      "Training Batch [144/782]: Loss 0.01210761722177267\n",
      "Training Batch [145/782]: Loss 0.015543491579592228\n",
      "Training Batch [146/782]: Loss 0.00048034245264716446\n",
      "Training Batch [147/782]: Loss 0.05631565302610397\n",
      "Training Batch [148/782]: Loss 0.02997639961540699\n",
      "Training Batch [149/782]: Loss 0.0013344446197152138\n",
      "Training Batch [150/782]: Loss 0.0028730742633342743\n",
      "Training Batch [151/782]: Loss 0.015341667458415031\n",
      "Training Batch [152/782]: Loss 0.006414422765374184\n",
      "Training Batch [153/782]: Loss 0.007368716411292553\n",
      "Training Batch [154/782]: Loss 0.021557288244366646\n",
      "Training Batch [155/782]: Loss 0.003924780525267124\n",
      "Training Batch [156/782]: Loss 0.009730268269777298\n",
      "Training Batch [157/782]: Loss 0.0014659304870292544\n",
      "Training Batch [158/782]: Loss 0.026852410286664963\n",
      "Training Batch [159/782]: Loss 0.04738284647464752\n",
      "Training Batch [160/782]: Loss 0.008149600587785244\n",
      "Training Batch [161/782]: Loss 0.002378563629463315\n",
      "Training Batch [162/782]: Loss 0.004261135123670101\n",
      "Training Batch [163/782]: Loss 0.004992540925741196\n",
      "Training Batch [164/782]: Loss 0.04464646056294441\n",
      "Training Batch [165/782]: Loss 0.0025391806848347187\n",
      "Training Batch [166/782]: Loss 0.022789010778069496\n",
      "Training Batch [167/782]: Loss 0.02649565413594246\n",
      "Training Batch [168/782]: Loss 0.004409454297274351\n",
      "Training Batch [169/782]: Loss 0.016553593799471855\n",
      "Training Batch [170/782]: Loss 0.005122118629515171\n",
      "Training Batch [171/782]: Loss 0.017787275835871696\n",
      "Training Batch [172/782]: Loss 0.0012534941779449582\n",
      "Training Batch [173/782]: Loss 0.004910436924546957\n",
      "Training Batch [174/782]: Loss 0.010641560889780521\n",
      "Training Batch [175/782]: Loss 0.010099113918840885\n",
      "Training Batch [176/782]: Loss 0.015980210155248642\n",
      "Training Batch [177/782]: Loss 0.010594921186566353\n",
      "Training Batch [178/782]: Loss 0.0047235325910151005\n",
      "Training Batch [179/782]: Loss 0.01340657938271761\n",
      "Training Batch [180/782]: Loss 0.002060448518022895\n",
      "Training Batch [181/782]: Loss 0.010858104564249516\n",
      "Training Batch [182/782]: Loss 0.01239047572016716\n",
      "Training Batch [183/782]: Loss 0.10987807810306549\n",
      "Training Batch [184/782]: Loss 0.02866942808032036\n",
      "Training Batch [185/782]: Loss 0.013964423909783363\n",
      "Training Batch [186/782]: Loss 0.030088433995842934\n",
      "Training Batch [187/782]: Loss 0.004712093621492386\n",
      "Training Batch [188/782]: Loss 0.0038556749932467937\n",
      "Training Batch [189/782]: Loss 0.003145603695884347\n",
      "Training Batch [190/782]: Loss 0.021570200100541115\n",
      "Training Batch [191/782]: Loss 0.022690678015351295\n",
      "Training Batch [192/782]: Loss 0.0022824350744485855\n",
      "Training Batch [193/782]: Loss 0.0033491451758891344\n",
      "Training Batch [194/782]: Loss 0.017517274245619774\n",
      "Training Batch [195/782]: Loss 0.005392185412347317\n",
      "Training Batch [196/782]: Loss 0.06599399447441101\n",
      "Training Batch [197/782]: Loss 0.05007261410355568\n",
      "Training Batch [198/782]: Loss 0.007909929379820824\n",
      "Training Batch [199/782]: Loss 0.008330249227583408\n",
      "Training Batch [200/782]: Loss 0.0005929960170760751\n",
      "Training Batch [201/782]: Loss 0.045092109590768814\n",
      "Training Batch [202/782]: Loss 0.025943847373127937\n",
      "Training Batch [203/782]: Loss 0.002533216029405594\n",
      "Training Batch [204/782]: Loss 0.0018219660269096494\n",
      "Training Batch [205/782]: Loss 0.0034837755374610424\n",
      "Training Batch [206/782]: Loss 0.03163829818367958\n",
      "Training Batch [207/782]: Loss 0.003946228418499231\n",
      "Training Batch [208/782]: Loss 0.05857456848025322\n",
      "Training Batch [209/782]: Loss 0.02816496044397354\n",
      "Training Batch [210/782]: Loss 0.033078186213970184\n",
      "Training Batch [211/782]: Loss 0.015033564530313015\n",
      "Training Batch [212/782]: Loss 0.003539497498422861\n",
      "Training Batch [213/782]: Loss 0.0551532618701458\n",
      "Training Batch [214/782]: Loss 0.007346173282712698\n",
      "Training Batch [215/782]: Loss 0.019337361678481102\n",
      "Training Batch [216/782]: Loss 0.013619731180369854\n",
      "Training Batch [217/782]: Loss 0.02154162898659706\n",
      "Training Batch [218/782]: Loss 0.00597861735150218\n",
      "Training Batch [219/782]: Loss 0.02785237692296505\n",
      "Training Batch [220/782]: Loss 0.045490656048059464\n",
      "Training Batch [221/782]: Loss 0.0018271779408678412\n",
      "Training Batch [222/782]: Loss 0.009212618693709373\n",
      "Training Batch [223/782]: Loss 0.018819507211446762\n",
      "Training Batch [224/782]: Loss 0.010352008044719696\n",
      "Training Batch [225/782]: Loss 0.009015943855047226\n",
      "Training Batch [226/782]: Loss 0.05868285894393921\n",
      "Training Batch [227/782]: Loss 0.005868572276085615\n",
      "Training Batch [228/782]: Loss 0.004536178428679705\n",
      "Training Batch [229/782]: Loss 0.00855848379433155\n",
      "Training Batch [230/782]: Loss 0.0038496453780680895\n",
      "Training Batch [231/782]: Loss 0.007640373893082142\n",
      "Training Batch [232/782]: Loss 0.01401732861995697\n",
      "Training Batch [233/782]: Loss 0.00391651364043355\n",
      "Training Batch [234/782]: Loss 0.011863886378705502\n",
      "Training Batch [235/782]: Loss 0.05683793127536774\n",
      "Training Batch [236/782]: Loss 0.006048953626304865\n",
      "Training Batch [237/782]: Loss 0.010370887815952301\n",
      "Training Batch [238/782]: Loss 0.006738713942468166\n",
      "Training Batch [239/782]: Loss 0.010309912264347076\n",
      "Training Batch [240/782]: Loss 0.02310539036989212\n",
      "Training Batch [241/782]: Loss 0.0009484444744884968\n",
      "Training Batch [242/782]: Loss 0.004407947417348623\n",
      "Training Batch [243/782]: Loss 0.0033192916307598352\n",
      "Training Batch [244/782]: Loss 0.04242826998233795\n",
      "Training Batch [245/782]: Loss 0.18954913318157196\n",
      "Training Batch [246/782]: Loss 0.023560035973787308\n",
      "Training Batch [247/782]: Loss 0.0792943462729454\n",
      "Training Batch [248/782]: Loss 0.0061573064886033535\n",
      "Training Batch [249/782]: Loss 0.04720320552587509\n",
      "Training Batch [250/782]: Loss 0.013368875719606876\n",
      "Training Batch [251/782]: Loss 0.009119516238570213\n",
      "Training Batch [252/782]: Loss 0.08326217532157898\n",
      "Training Batch [253/782]: Loss 0.011682316660881042\n",
      "Training Batch [254/782]: Loss 0.0070188818499445915\n",
      "Training Batch [255/782]: Loss 0.11415290832519531\n",
      "Training Batch [256/782]: Loss 0.0016713174991309643\n",
      "Training Batch [257/782]: Loss 0.043927744030952454\n",
      "Training Batch [258/782]: Loss 0.002268388867378235\n",
      "Training Batch [259/782]: Loss 0.026538120582699776\n",
      "Training Batch [260/782]: Loss 0.027339788153767586\n",
      "Training Batch [261/782]: Loss 0.03052963688969612\n",
      "Training Batch [262/782]: Loss 0.0009602014324627817\n",
      "Training Batch [263/782]: Loss 0.003682843642309308\n",
      "Training Batch [264/782]: Loss 0.030412955209612846\n",
      "Training Batch [265/782]: Loss 0.06543466448783875\n",
      "Training Batch [266/782]: Loss 0.031118493527173996\n",
      "Training Batch [267/782]: Loss 0.014919016510248184\n",
      "Training Batch [268/782]: Loss 0.052159346640110016\n",
      "Training Batch [269/782]: Loss 0.0766429603099823\n",
      "Training Batch [270/782]: Loss 0.030419155955314636\n",
      "Training Batch [271/782]: Loss 0.03456223011016846\n",
      "Training Batch [272/782]: Loss 0.022741146385669708\n",
      "Training Batch [273/782]: Loss 0.00851584505289793\n",
      "Training Batch [274/782]: Loss 0.016579732298851013\n",
      "Training Batch [275/782]: Loss 0.007225054316222668\n",
      "Training Batch [276/782]: Loss 0.009214255958795547\n",
      "Training Batch [277/782]: Loss 0.012184666469693184\n",
      "Training Batch [278/782]: Loss 0.07554616034030914\n",
      "Training Batch [279/782]: Loss 0.03778664022684097\n",
      "Training Batch [280/782]: Loss 0.045803047716617584\n",
      "Training Batch [281/782]: Loss 0.008528406731784344\n",
      "Training Batch [282/782]: Loss 0.07696551084518433\n",
      "Training Batch [283/782]: Loss 0.021659115329384804\n",
      "Training Batch [284/782]: Loss 0.03207983076572418\n",
      "Training Batch [285/782]: Loss 0.01397759560495615\n",
      "Training Batch [286/782]: Loss 0.03865736350417137\n",
      "Training Batch [287/782]: Loss 0.01699129492044449\n",
      "Training Batch [288/782]: Loss 0.008845225907862186\n",
      "Training Batch [289/782]: Loss 0.02169135957956314\n",
      "Training Batch [290/782]: Loss 0.01735079474747181\n",
      "Training Batch [291/782]: Loss 0.014971048571169376\n",
      "Training Batch [292/782]: Loss 0.0009483913891017437\n",
      "Training Batch [293/782]: Loss 0.005266948603093624\n",
      "Training Batch [294/782]: Loss 0.005357265938073397\n",
      "Training Batch [295/782]: Loss 0.006901133339852095\n",
      "Training Batch [296/782]: Loss 0.03998207300901413\n",
      "Training Batch [297/782]: Loss 0.057077210396528244\n",
      "Training Batch [298/782]: Loss 0.006896744482219219\n",
      "Training Batch [299/782]: Loss 0.01370652299374342\n",
      "Training Batch [300/782]: Loss 0.003058637725189328\n",
      "Training Batch [301/782]: Loss 0.015152751468122005\n",
      "Training Batch [302/782]: Loss 0.040859512984752655\n",
      "Training Batch [303/782]: Loss 0.0070997136645019054\n",
      "Training Batch [304/782]: Loss 0.0030472681391984224\n",
      "Training Batch [305/782]: Loss 0.014697805978357792\n",
      "Training Batch [306/782]: Loss 0.08681473135948181\n",
      "Training Batch [307/782]: Loss 0.0017881753155961633\n",
      "Training Batch [308/782]: Loss 0.018507245928049088\n",
      "Training Batch [309/782]: Loss 0.03967426344752312\n",
      "Training Batch [310/782]: Loss 0.09894102811813354\n",
      "Training Batch [311/782]: Loss 0.07152155786752701\n",
      "Training Batch [312/782]: Loss 0.056813694536685944\n",
      "Training Batch [313/782]: Loss 0.004938637372106314\n",
      "Training Batch [314/782]: Loss 0.017031896859407425\n",
      "Training Batch [315/782]: Loss 0.014159579761326313\n",
      "Training Batch [316/782]: Loss 0.05589984357357025\n",
      "Training Batch [317/782]: Loss 0.010434024035930634\n",
      "Training Batch [318/782]: Loss 0.0609077624976635\n",
      "Training Batch [319/782]: Loss 0.014786986634135246\n",
      "Training Batch [320/782]: Loss 0.013127230107784271\n",
      "Training Batch [321/782]: Loss 0.0755520612001419\n",
      "Training Batch [322/782]: Loss 0.035206835716962814\n",
      "Training Batch [323/782]: Loss 0.016327179968357086\n",
      "Training Batch [324/782]: Loss 0.012971240095794201\n",
      "Training Batch [325/782]: Loss 0.00985995214432478\n",
      "Training Batch [326/782]: Loss 0.012326877564191818\n",
      "Training Batch [327/782]: Loss 0.012788091786205769\n",
      "Training Batch [328/782]: Loss 0.005715192295610905\n",
      "Training Batch [329/782]: Loss 0.07818450778722763\n",
      "Training Batch [330/782]: Loss 0.004942435305565596\n",
      "Training Batch [331/782]: Loss 0.03190045803785324\n",
      "Training Batch [332/782]: Loss 0.007251916918903589\n",
      "Training Batch [333/782]: Loss 0.008893570862710476\n",
      "Training Batch [334/782]: Loss 0.02537173591554165\n",
      "Training Batch [335/782]: Loss 0.0012114535784348845\n",
      "Training Batch [336/782]: Loss 0.0017183361342176795\n",
      "Training Batch [337/782]: Loss 0.00875126663595438\n",
      "Training Batch [338/782]: Loss 0.049691975116729736\n",
      "Training Batch [339/782]: Loss 0.006963497027754784\n",
      "Training Batch [340/782]: Loss 0.06440620869398117\n",
      "Training Batch [341/782]: Loss 0.022955872118473053\n",
      "Training Batch [342/782]: Loss 0.019391942769289017\n",
      "Training Batch [343/782]: Loss 0.06080550700426102\n",
      "Training Batch [344/782]: Loss 0.003594682551920414\n",
      "Training Batch [345/782]: Loss 0.0179996807128191\n",
      "Training Batch [346/782]: Loss 0.00524830911308527\n",
      "Training Batch [347/782]: Loss 0.012467838823795319\n",
      "Training Batch [348/782]: Loss 0.011822307482361794\n",
      "Training Batch [349/782]: Loss 0.029159128665924072\n",
      "Training Batch [350/782]: Loss 0.028848832473158836\n",
      "Training Batch [351/782]: Loss 0.009706657379865646\n",
      "Training Batch [352/782]: Loss 0.018815170973539352\n",
      "Training Batch [353/782]: Loss 0.08310993760824203\n",
      "Training Batch [354/782]: Loss 0.009344082325696945\n",
      "Training Batch [355/782]: Loss 0.03548924997448921\n",
      "Training Batch [356/782]: Loss 0.0028560240752995014\n",
      "Training Batch [357/782]: Loss 0.0022005680948495865\n",
      "Training Batch [358/782]: Loss 0.003623287659138441\n",
      "Training Batch [359/782]: Loss 0.05950015410780907\n",
      "Training Batch [360/782]: Loss 0.019110126420855522\n",
      "Training Batch [361/782]: Loss 0.054680656641721725\n",
      "Training Batch [362/782]: Loss 0.011405928060412407\n",
      "Training Batch [363/782]: Loss 0.009841988794505596\n",
      "Training Batch [364/782]: Loss 0.07346007227897644\n",
      "Training Batch [365/782]: Loss 0.008408581838011742\n",
      "Training Batch [366/782]: Loss 0.0044992114417254925\n",
      "Training Batch [367/782]: Loss 0.020567942410707474\n",
      "Training Batch [368/782]: Loss 0.02348840795457363\n",
      "Training Batch [369/782]: Loss 0.014024355448782444\n",
      "Training Batch [370/782]: Loss 0.01923530362546444\n",
      "Training Batch [371/782]: Loss 0.03372185304760933\n",
      "Training Batch [372/782]: Loss 0.014261871576309204\n",
      "Training Batch [373/782]: Loss 0.004442915320396423\n",
      "Training Batch [374/782]: Loss 0.003841052297502756\n",
      "Training Batch [375/782]: Loss 0.015480506233870983\n",
      "Training Batch [376/782]: Loss 0.044571030884981155\n",
      "Training Batch [377/782]: Loss 0.00919802300632\n",
      "Training Batch [378/782]: Loss 0.016399012878537178\n",
      "Training Batch [379/782]: Loss 0.037040311843156815\n",
      "Training Batch [380/782]: Loss 0.014522252604365349\n",
      "Training Batch [381/782]: Loss 0.09937600046396255\n",
      "Training Batch [382/782]: Loss 0.008998819626867771\n",
      "Training Batch [383/782]: Loss 0.0033226190134882927\n",
      "Training Batch [384/782]: Loss 0.02140938490629196\n",
      "Training Batch [385/782]: Loss 0.0018913005478680134\n",
      "Training Batch [386/782]: Loss 0.011621348559856415\n",
      "Training Batch [387/782]: Loss 0.007301953621208668\n",
      "Training Batch [388/782]: Loss 0.0030566060449928045\n",
      "Training Batch [389/782]: Loss 0.0018148556118831038\n",
      "Training Batch [390/782]: Loss 0.008230982348322868\n",
      "Training Batch [391/782]: Loss 0.006331889424473047\n",
      "Training Batch [392/782]: Loss 0.014681228436529636\n",
      "Training Batch [393/782]: Loss 0.07014783471822739\n",
      "Training Batch [394/782]: Loss 0.00458240881562233\n",
      "Training Batch [395/782]: Loss 0.009387397207319736\n",
      "Training Batch [396/782]: Loss 0.1355658322572708\n",
      "Training Batch [397/782]: Loss 0.042607009410858154\n",
      "Training Batch [398/782]: Loss 0.009982253424823284\n",
      "Training Batch [399/782]: Loss 0.04355591908097267\n",
      "Training Batch [400/782]: Loss 0.021280057728290558\n",
      "Training Batch [401/782]: Loss 0.04366486519575119\n",
      "Training Batch [402/782]: Loss 0.003995325416326523\n",
      "Training Batch [403/782]: Loss 0.025796545669436455\n",
      "Training Batch [404/782]: Loss 0.05763261765241623\n",
      "Training Batch [405/782]: Loss 0.004526123870164156\n",
      "Training Batch [406/782]: Loss 0.020067226141691208\n",
      "Training Batch [407/782]: Loss 0.014842619188129902\n",
      "Training Batch [408/782]: Loss 0.05676627531647682\n",
      "Training Batch [409/782]: Loss 0.0010256991954520345\n",
      "Training Batch [410/782]: Loss 0.015526744537055492\n",
      "Training Batch [411/782]: Loss 0.02710103429853916\n",
      "Training Batch [412/782]: Loss 0.02002636156976223\n",
      "Training Batch [413/782]: Loss 0.025101635605096817\n",
      "Training Batch [414/782]: Loss 0.00978332944214344\n",
      "Training Batch [415/782]: Loss 0.03423285111784935\n",
      "Training Batch [416/782]: Loss 0.05373014509677887\n",
      "Training Batch [417/782]: Loss 0.010772300884127617\n",
      "Training Batch [418/782]: Loss 0.09577945619821548\n",
      "Training Batch [419/782]: Loss 0.09359939396381378\n",
      "Training Batch [420/782]: Loss 0.10909456759691238\n",
      "Training Batch [421/782]: Loss 0.031941358000040054\n",
      "Training Batch [422/782]: Loss 0.009467012248933315\n",
      "Training Batch [423/782]: Loss 0.015359185636043549\n",
      "Training Batch [424/782]: Loss 0.035593174397945404\n",
      "Training Batch [425/782]: Loss 0.016684340313076973\n",
      "Training Batch [426/782]: Loss 0.04204145073890686\n",
      "Training Batch [427/782]: Loss 0.08879759907722473\n",
      "Training Batch [428/782]: Loss 0.011939169839024544\n",
      "Training Batch [429/782]: Loss 0.009682463482022285\n",
      "Training Batch [430/782]: Loss 0.012034602463245392\n",
      "Training Batch [431/782]: Loss 0.015137864276766777\n",
      "Training Batch [432/782]: Loss 0.0024552391842007637\n",
      "Training Batch [433/782]: Loss 0.015683596953749657\n",
      "Training Batch [434/782]: Loss 0.019681332632899284\n",
      "Training Batch [435/782]: Loss 0.10813528299331665\n",
      "Training Batch [436/782]: Loss 0.08925870805978775\n",
      "Training Batch [437/782]: Loss 0.0256660133600235\n",
      "Training Batch [438/782]: Loss 0.007928202860057354\n",
      "Training Batch [439/782]: Loss 0.029321372509002686\n",
      "Training Batch [440/782]: Loss 0.025402942672371864\n",
      "Training Batch [441/782]: Loss 0.018638161942362785\n",
      "Training Batch [442/782]: Loss 0.029758941382169724\n",
      "Training Batch [443/782]: Loss 0.04510612413287163\n",
      "Training Batch [444/782]: Loss 0.0035269539803266525\n",
      "Training Batch [445/782]: Loss 0.008912354707717896\n",
      "Training Batch [446/782]: Loss 0.04153389856219292\n",
      "Training Batch [447/782]: Loss 0.04548642784357071\n",
      "Training Batch [448/782]: Loss 0.037011757493019104\n",
      "Training Batch [449/782]: Loss 0.08691302686929703\n",
      "Training Batch [450/782]: Loss 0.0031350310891866684\n",
      "Training Batch [451/782]: Loss 0.05523302033543587\n",
      "Training Batch [452/782]: Loss 0.005060452036559582\n",
      "Training Batch [453/782]: Loss 0.07564026117324829\n",
      "Training Batch [454/782]: Loss 0.012945719994604588\n",
      "Training Batch [455/782]: Loss 0.008413095958530903\n",
      "Training Batch [456/782]: Loss 0.028215212747454643\n",
      "Training Batch [457/782]: Loss 0.016066549345850945\n",
      "Training Batch [458/782]: Loss 0.10594677180051804\n",
      "Training Batch [459/782]: Loss 0.015184734016656876\n",
      "Training Batch [460/782]: Loss 0.03701763600111008\n",
      "Training Batch [461/782]: Loss 0.02208661660552025\n",
      "Training Batch [462/782]: Loss 0.0236241165548563\n",
      "Training Batch [463/782]: Loss 0.005836809985339642\n",
      "Training Batch [464/782]: Loss 0.06577529013156891\n",
      "Training Batch [465/782]: Loss 0.018425606191158295\n",
      "Training Batch [466/782]: Loss 0.010681208223104477\n",
      "Training Batch [467/782]: Loss 0.018655091524124146\n",
      "Training Batch [468/782]: Loss 0.10551867634057999\n",
      "Training Batch [469/782]: Loss 0.050016265362501144\n",
      "Training Batch [470/782]: Loss 0.05534755438566208\n",
      "Training Batch [471/782]: Loss 0.011205503717064857\n",
      "Training Batch [472/782]: Loss 0.10170891135931015\n",
      "Training Batch [473/782]: Loss 0.059121452271938324\n",
      "Training Batch [474/782]: Loss 0.03883346542716026\n",
      "Training Batch [475/782]: Loss 0.10602216422557831\n",
      "Training Batch [476/782]: Loss 0.013911453075706959\n",
      "Training Batch [477/782]: Loss 0.015621587634086609\n",
      "Training Batch [478/782]: Loss 0.007815783843398094\n",
      "Training Batch [479/782]: Loss 0.11499105393886566\n",
      "Training Batch [480/782]: Loss 0.007938538677990437\n",
      "Training Batch [481/782]: Loss 0.055733390152454376\n",
      "Training Batch [482/782]: Loss 0.01685342565178871\n",
      "Training Batch [483/782]: Loss 0.0659068152308464\n",
      "Training Batch [484/782]: Loss 0.024099886417388916\n",
      "Training Batch [485/782]: Loss 0.04371068254113197\n",
      "Training Batch [486/782]: Loss 0.01871235854923725\n",
      "Training Batch [487/782]: Loss 0.02678670920431614\n",
      "Training Batch [488/782]: Loss 0.018862616270780563\n",
      "Training Batch [489/782]: Loss 0.011157476343214512\n",
      "Training Batch [490/782]: Loss 0.07513003051280975\n",
      "Training Batch [491/782]: Loss 0.041446566581726074\n",
      "Training Batch [492/782]: Loss 0.06915922462940216\n",
      "Training Batch [493/782]: Loss 0.10172085464000702\n",
      "Training Batch [494/782]: Loss 0.009803959168493748\n",
      "Training Batch [495/782]: Loss 0.008111669681966305\n",
      "Training Batch [496/782]: Loss 0.03508896380662918\n",
      "Training Batch [497/782]: Loss 0.03127457574009895\n",
      "Training Batch [498/782]: Loss 0.015806376934051514\n",
      "Training Batch [499/782]: Loss 0.024245155975222588\n",
      "Training Batch [500/782]: Loss 0.0264961626380682\n",
      "Training Batch [501/782]: Loss 0.06959507614374161\n",
      "Training Batch [502/782]: Loss 0.01900465413928032\n",
      "Training Batch [503/782]: Loss 0.006099274847656488\n",
      "Training Batch [504/782]: Loss 0.007263937499374151\n",
      "Training Batch [505/782]: Loss 0.028595447540283203\n",
      "Training Batch [506/782]: Loss 0.015557519160211086\n",
      "Training Batch [507/782]: Loss 0.055004723370075226\n",
      "Training Batch [508/782]: Loss 0.027044711634516716\n",
      "Training Batch [509/782]: Loss 0.004715082701295614\n",
      "Training Batch [510/782]: Loss 0.017751960083842278\n",
      "Training Batch [511/782]: Loss 0.08789024502038956\n",
      "Training Batch [512/782]: Loss 0.02967049740254879\n",
      "Training Batch [513/782]: Loss 0.013518619351089\n",
      "Training Batch [514/782]: Loss 0.005979382432997227\n",
      "Training Batch [515/782]: Loss 0.0028416148852556944\n",
      "Training Batch [516/782]: Loss 0.0223983321338892\n",
      "Training Batch [517/782]: Loss 0.013215054757893085\n",
      "Training Batch [518/782]: Loss 0.03142476826906204\n",
      "Training Batch [519/782]: Loss 0.008446350693702698\n",
      "Training Batch [520/782]: Loss 0.1193169504404068\n",
      "Training Batch [521/782]: Loss 0.21018515527248383\n",
      "Training Batch [522/782]: Loss 0.026306459680199623\n",
      "Training Batch [523/782]: Loss 0.04072381556034088\n",
      "Training Batch [524/782]: Loss 0.13224591314792633\n",
      "Training Batch [525/782]: Loss 0.009090550243854523\n",
      "Training Batch [526/782]: Loss 0.05961588770151138\n",
      "Training Batch [527/782]: Loss 0.012675002217292786\n",
      "Training Batch [528/782]: Loss 0.010324656963348389\n",
      "Training Batch [529/782]: Loss 0.06349726021289825\n",
      "Training Batch [530/782]: Loss 0.043183837085962296\n",
      "Training Batch [531/782]: Loss 0.08137640357017517\n",
      "Training Batch [532/782]: Loss 0.1005033329129219\n",
      "Training Batch [533/782]: Loss 0.09342332184314728\n",
      "Training Batch [534/782]: Loss 0.059042636305093765\n",
      "Training Batch [535/782]: Loss 0.01826540380716324\n",
      "Training Batch [536/782]: Loss 0.03716472536325455\n",
      "Training Batch [537/782]: Loss 0.05028224736452103\n",
      "Training Batch [538/782]: Loss 0.02098884992301464\n",
      "Training Batch [539/782]: Loss 0.03157484903931618\n",
      "Training Batch [540/782]: Loss 0.023527905344963074\n",
      "Training Batch [541/782]: Loss 0.1626664698123932\n",
      "Training Batch [542/782]: Loss 0.011769747361540794\n",
      "Training Batch [543/782]: Loss 0.04408003389835358\n",
      "Training Batch [544/782]: Loss 0.07318732887506485\n",
      "Training Batch [545/782]: Loss 0.006585665047168732\n",
      "Training Batch [546/782]: Loss 0.03891780972480774\n",
      "Training Batch [547/782]: Loss 0.012901990674436092\n",
      "Training Batch [548/782]: Loss 0.025387773290276527\n",
      "Training Batch [549/782]: Loss 0.1103239580988884\n",
      "Training Batch [550/782]: Loss 0.011723743751645088\n",
      "Training Batch [551/782]: Loss 0.13699163496494293\n",
      "Training Batch [552/782]: Loss 0.011751827783882618\n",
      "Training Batch [553/782]: Loss 0.04899892210960388\n",
      "Training Batch [554/782]: Loss 0.032903701066970825\n",
      "Training Batch [555/782]: Loss 0.03600522503256798\n",
      "Training Batch [556/782]: Loss 0.04192551597952843\n",
      "Training Batch [557/782]: Loss 0.03429014980792999\n",
      "Training Batch [558/782]: Loss 0.04551312327384949\n",
      "Training Batch [559/782]: Loss 0.10074149072170258\n",
      "Training Batch [560/782]: Loss 0.030061345547437668\n",
      "Training Batch [561/782]: Loss 0.1090872660279274\n",
      "Training Batch [562/782]: Loss 0.011974959634244442\n",
      "Training Batch [563/782]: Loss 0.07880061864852905\n",
      "Training Batch [564/782]: Loss 0.0055881282314658165\n",
      "Training Batch [565/782]: Loss 0.004049208015203476\n",
      "Training Batch [566/782]: Loss 0.01408846490085125\n",
      "Training Batch [567/782]: Loss 0.027668947353959084\n",
      "Training Batch [568/782]: Loss 0.059614717960357666\n",
      "Training Batch [569/782]: Loss 0.08137815445661545\n",
      "Training Batch [570/782]: Loss 0.044951096177101135\n",
      "Training Batch [571/782]: Loss 0.011826524510979652\n",
      "Training Batch [572/782]: Loss 0.006873356644064188\n",
      "Training Batch [573/782]: Loss 0.05953335016965866\n",
      "Training Batch [574/782]: Loss 0.008772705681622028\n",
      "Training Batch [575/782]: Loss 0.10274018347263336\n",
      "Training Batch [576/782]: Loss 0.0026830919086933136\n",
      "Training Batch [577/782]: Loss 0.02952205203473568\n",
      "Training Batch [578/782]: Loss 0.027354875579476357\n",
      "Training Batch [579/782]: Loss 0.04116484150290489\n",
      "Training Batch [580/782]: Loss 0.018221449106931686\n",
      "Training Batch [581/782]: Loss 0.0159723199903965\n",
      "Training Batch [582/782]: Loss 0.03145607188344002\n",
      "Training Batch [583/782]: Loss 0.05770207941532135\n",
      "Training Batch [584/782]: Loss 0.0015364171704277396\n",
      "Training Batch [585/782]: Loss 0.04730182886123657\n",
      "Training Batch [586/782]: Loss 0.028988473117351532\n",
      "Training Batch [587/782]: Loss 0.005126045551151037\n",
      "Training Batch [588/782]: Loss 0.06156979128718376\n",
      "Training Batch [589/782]: Loss 0.07761047035455704\n",
      "Training Batch [590/782]: Loss 0.017708614468574524\n",
      "Training Batch [591/782]: Loss 0.010740783996880054\n",
      "Training Batch [592/782]: Loss 0.0086271483451128\n",
      "Training Batch [593/782]: Loss 0.04840356111526489\n",
      "Training Batch [594/782]: Loss 0.05649196356534958\n",
      "Training Batch [595/782]: Loss 0.07543312013149261\n",
      "Training Batch [596/782]: Loss 0.017762551084160805\n",
      "Training Batch [597/782]: Loss 0.08943053334951401\n",
      "Training Batch [598/782]: Loss 0.03174775466322899\n",
      "Training Batch [599/782]: Loss 0.032913871109485626\n",
      "Training Batch [600/782]: Loss 0.013563604094088078\n",
      "Training Batch [601/782]: Loss 0.07012340426445007\n",
      "Training Batch [602/782]: Loss 0.007275800686329603\n",
      "Training Batch [603/782]: Loss 0.011706847697496414\n",
      "Training Batch [604/782]: Loss 0.21963296830654144\n",
      "Training Batch [605/782]: Loss 0.05101571977138519\n",
      "Training Batch [606/782]: Loss 0.05189892649650574\n",
      "Training Batch [607/782]: Loss 0.10381697863340378\n",
      "Training Batch [608/782]: Loss 0.029874255880713463\n",
      "Training Batch [609/782]: Loss 0.00215909443795681\n",
      "Training Batch [610/782]: Loss 0.04196891561150551\n",
      "Training Batch [611/782]: Loss 0.02888357825577259\n",
      "Training Batch [612/782]: Loss 0.021619480103254318\n",
      "Training Batch [613/782]: Loss 0.024968236684799194\n",
      "Training Batch [614/782]: Loss 0.009544393047690392\n",
      "Training Batch [615/782]: Loss 0.0845268964767456\n",
      "Training Batch [616/782]: Loss 0.019690943881869316\n",
      "Training Batch [617/782]: Loss 0.06645660102367401\n",
      "Training Batch [618/782]: Loss 0.06488021463155746\n",
      "Training Batch [619/782]: Loss 0.06347210705280304\n",
      "Training Batch [620/782]: Loss 0.11315727233886719\n",
      "Training Batch [621/782]: Loss 0.013576009310781956\n",
      "Training Batch [622/782]: Loss 0.007337958551943302\n",
      "Training Batch [623/782]: Loss 0.0021863915026187897\n",
      "Training Batch [624/782]: Loss 0.00707179494202137\n",
      "Training Batch [625/782]: Loss 0.019287362694740295\n",
      "Training Batch [626/782]: Loss 0.039086807519197464\n",
      "Training Batch [627/782]: Loss 0.021568549796938896\n",
      "Training Batch [628/782]: Loss 0.014330064877867699\n",
      "Training Batch [629/782]: Loss 0.0053946212865412235\n",
      "Training Batch [630/782]: Loss 0.04676959291100502\n",
      "Training Batch [631/782]: Loss 0.036241549998521805\n",
      "Training Batch [632/782]: Loss 0.09365766495466232\n",
      "Training Batch [633/782]: Loss 0.06126409396529198\n",
      "Training Batch [634/782]: Loss 0.03898162394762039\n",
      "Training Batch [635/782]: Loss 0.00825879443436861\n",
      "Training Batch [636/782]: Loss 0.015968142077326775\n",
      "Training Batch [637/782]: Loss 0.11481703817844391\n",
      "Training Batch [638/782]: Loss 0.06353417038917542\n",
      "Training Batch [639/782]: Loss 0.06210070103406906\n",
      "Training Batch [640/782]: Loss 0.05328654497861862\n",
      "Training Batch [641/782]: Loss 0.007661090232431889\n",
      "Training Batch [642/782]: Loss 0.11415911465883255\n",
      "Training Batch [643/782]: Loss 0.06903031468391418\n",
      "Training Batch [644/782]: Loss 0.10259444266557693\n",
      "Training Batch [645/782]: Loss 0.06119297817349434\n",
      "Training Batch [646/782]: Loss 0.14689511060714722\n",
      "Training Batch [647/782]: Loss 0.10184872895479202\n",
      "Training Batch [648/782]: Loss 0.01442169863730669\n",
      "Training Batch [649/782]: Loss 0.013631482608616352\n",
      "Training Batch [650/782]: Loss 0.06897927820682526\n",
      "Training Batch [651/782]: Loss 0.015135636553168297\n",
      "Training Batch [652/782]: Loss 0.007934722118079662\n",
      "Training Batch [653/782]: Loss 0.02927468530833721\n",
      "Training Batch [654/782]: Loss 0.04257580637931824\n",
      "Training Batch [655/782]: Loss 0.0981435626745224\n",
      "Training Batch [656/782]: Loss 0.025743696838617325\n",
      "Training Batch [657/782]: Loss 0.24478289484977722\n",
      "Training Batch [658/782]: Loss 0.06862600892782211\n",
      "Training Batch [659/782]: Loss 0.009524594992399216\n",
      "Training Batch [660/782]: Loss 0.006383553147315979\n",
      "Training Batch [661/782]: Loss 0.05159655213356018\n",
      "Training Batch [662/782]: Loss 0.009899097494781017\n",
      "Training Batch [663/782]: Loss 0.38837534189224243\n",
      "Training Batch [664/782]: Loss 0.024052074179053307\n",
      "Training Batch [665/782]: Loss 0.01332292053848505\n",
      "Training Batch [666/782]: Loss 0.009810716845095158\n",
      "Training Batch [667/782]: Loss 0.011080184020102024\n",
      "Training Batch [668/782]: Loss 0.01741666905581951\n",
      "Training Batch [669/782]: Loss 0.05725780501961708\n",
      "Training Batch [670/782]: Loss 0.06301836669445038\n",
      "Training Batch [671/782]: Loss 0.08188915997743607\n",
      "Training Batch [672/782]: Loss 0.03601190447807312\n",
      "Training Batch [673/782]: Loss 0.03823009878396988\n",
      "Training Batch [674/782]: Loss 0.028504349291324615\n",
      "Training Batch [675/782]: Loss 0.009561928920447826\n",
      "Training Batch [676/782]: Loss 0.05247297137975693\n",
      "Training Batch [677/782]: Loss 0.09494291245937347\n",
      "Training Batch [678/782]: Loss 0.021591374650597572\n",
      "Training Batch [679/782]: Loss 0.08196770399808884\n",
      "Training Batch [680/782]: Loss 0.010567260906100273\n",
      "Training Batch [681/782]: Loss 0.02886463887989521\n",
      "Training Batch [682/782]: Loss 0.02203836664557457\n",
      "Training Batch [683/782]: Loss 0.003061742754653096\n",
      "Training Batch [684/782]: Loss 0.026501035317778587\n",
      "Training Batch [685/782]: Loss 0.010791873559355736\n",
      "Training Batch [686/782]: Loss 0.009027132764458656\n",
      "Training Batch [687/782]: Loss 0.017422884702682495\n",
      "Training Batch [688/782]: Loss 0.03737271949648857\n",
      "Training Batch [689/782]: Loss 0.00928163155913353\n",
      "Training Batch [690/782]: Loss 0.16082797944545746\n",
      "Training Batch [691/782]: Loss 0.04260813072323799\n",
      "Training Batch [692/782]: Loss 0.13591480255126953\n",
      "Training Batch [693/782]: Loss 0.043425340205430984\n",
      "Training Batch [694/782]: Loss 0.10453454405069351\n",
      "Training Batch [695/782]: Loss 0.05951457843184471\n",
      "Training Batch [696/782]: Loss 0.1504613310098648\n",
      "Training Batch [697/782]: Loss 0.03505457565188408\n",
      "Training Batch [698/782]: Loss 0.1583891361951828\n",
      "Training Batch [699/782]: Loss 0.057934943586587906\n",
      "Training Batch [700/782]: Loss 0.025907685980200768\n",
      "Training Batch [701/782]: Loss 0.03034863993525505\n",
      "Training Batch [702/782]: Loss 0.02896241471171379\n",
      "Training Batch [703/782]: Loss 0.05652930587530136\n",
      "Training Batch [704/782]: Loss 0.013962442986667156\n",
      "Training Batch [705/782]: Loss 0.036923687905073166\n",
      "Training Batch [706/782]: Loss 0.23146533966064453\n",
      "Training Batch [707/782]: Loss 0.04739650711417198\n",
      "Training Batch [708/782]: Loss 0.026565346866846085\n",
      "Training Batch [709/782]: Loss 0.13981986045837402\n",
      "Training Batch [710/782]: Loss 0.010722914710640907\n",
      "Training Batch [711/782]: Loss 0.03570723906159401\n",
      "Training Batch [712/782]: Loss 0.032877687364816666\n",
      "Training Batch [713/782]: Loss 0.0914740040898323\n",
      "Training Batch [714/782]: Loss 0.01618563011288643\n",
      "Training Batch [715/782]: Loss 0.010878851637244225\n",
      "Training Batch [716/782]: Loss 0.16410952806472778\n",
      "Training Batch [717/782]: Loss 0.03305390849709511\n",
      "Training Batch [718/782]: Loss 0.011045037768781185\n",
      "Training Batch [719/782]: Loss 0.036433588713407516\n",
      "Training Batch [720/782]: Loss 0.08788028359413147\n",
      "Training Batch [721/782]: Loss 0.10800168663263321\n",
      "Training Batch [722/782]: Loss 0.04151366651058197\n",
      "Training Batch [723/782]: Loss 0.00935972947627306\n",
      "Training Batch [724/782]: Loss 0.04175799340009689\n",
      "Training Batch [725/782]: Loss 0.04705612733960152\n",
      "Training Batch [726/782]: Loss 0.014734329655766487\n",
      "Training Batch [727/782]: Loss 0.008832423016428947\n",
      "Training Batch [728/782]: Loss 0.03739377111196518\n",
      "Training Batch [729/782]: Loss 0.024756627157330513\n",
      "Training Batch [730/782]: Loss 0.012429510243237019\n",
      "Training Batch [731/782]: Loss 0.01420464739203453\n",
      "Training Batch [732/782]: Loss 0.01428659725934267\n",
      "Training Batch [733/782]: Loss 0.01145769003778696\n",
      "Training Batch [734/782]: Loss 0.037220925092697144\n",
      "Training Batch [735/782]: Loss 0.013322312384843826\n",
      "Training Batch [736/782]: Loss 0.03215811401605606\n",
      "Training Batch [737/782]: Loss 0.06507039070129395\n",
      "Training Batch [738/782]: Loss 0.15974555909633636\n",
      "Training Batch [739/782]: Loss 0.007116276305168867\n",
      "Training Batch [740/782]: Loss 0.03473435714840889\n",
      "Training Batch [741/782]: Loss 0.10023054480552673\n",
      "Training Batch [742/782]: Loss 0.07493851333856583\n",
      "Training Batch [743/782]: Loss 0.03906947001814842\n",
      "Training Batch [744/782]: Loss 0.017695879563689232\n",
      "Training Batch [745/782]: Loss 0.12892432510852814\n",
      "Training Batch [746/782]: Loss 0.016867022961378098\n",
      "Training Batch [747/782]: Loss 0.007443588692694902\n",
      "Training Batch [748/782]: Loss 0.04457191377878189\n",
      "Training Batch [749/782]: Loss 0.025188354775309563\n",
      "Training Batch [750/782]: Loss 0.017886171117424965\n",
      "Training Batch [751/782]: Loss 0.004515442997217178\n",
      "Training Batch [752/782]: Loss 0.02136383205652237\n",
      "Training Batch [753/782]: Loss 0.012440946884453297\n",
      "Training Batch [754/782]: Loss 0.015813734382390976\n",
      "Training Batch [755/782]: Loss 0.03232584893703461\n",
      "Training Batch [756/782]: Loss 0.12997226417064667\n",
      "Training Batch [757/782]: Loss 0.016671333461999893\n",
      "Training Batch [758/782]: Loss 0.043981000781059265\n",
      "Training Batch [759/782]: Loss 0.0092781912535429\n",
      "Training Batch [760/782]: Loss 0.11570659279823303\n",
      "Training Batch [761/782]: Loss 0.06147146597504616\n",
      "Training Batch [762/782]: Loss 0.00920229684561491\n",
      "Training Batch [763/782]: Loss 0.038173649460077286\n",
      "Training Batch [764/782]: Loss 0.05794331058859825\n",
      "Training Batch [765/782]: Loss 0.10549885779619217\n",
      "Training Batch [766/782]: Loss 0.050175461918115616\n",
      "Training Batch [767/782]: Loss 0.05670158937573433\n",
      "Training Batch [768/782]: Loss 0.1265454888343811\n",
      "Training Batch [769/782]: Loss 0.0045411111786961555\n",
      "Training Batch [770/782]: Loss 0.01632118597626686\n",
      "Training Batch [771/782]: Loss 0.00551030645146966\n",
      "Training Batch [772/782]: Loss 0.01934698224067688\n",
      "Training Batch [773/782]: Loss 0.02921956218779087\n",
      "Training Batch [774/782]: Loss 0.04453407600522041\n",
      "Training Batch [775/782]: Loss 0.036569610238075256\n",
      "Training Batch [776/782]: Loss 0.01520941499620676\n",
      "Training Batch [777/782]: Loss 0.08270323276519775\n",
      "Training Batch [778/782]: Loss 0.03079124726355076\n",
      "Training Batch [779/782]: Loss 0.062391411513090134\n",
      "Training Batch [780/782]: Loss 0.05764543265104294\n",
      "Training Batch [781/782]: Loss 0.012904059141874313\n",
      "Training Batch [782/782]: Loss 0.014251419343054295\n",
      "Epoch 21 - Train Loss: 0.0328\n",
      "*********  Epoch 22/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.00183367810677737\n",
      "Training Batch [2/782]: Loss 0.07709392160177231\n",
      "Training Batch [3/782]: Loss 0.048565302044153214\n",
      "Training Batch [4/782]: Loss 0.060181643813848495\n",
      "Training Batch [5/782]: Loss 0.11595075577497482\n",
      "Training Batch [6/782]: Loss 0.03432484716176987\n",
      "Training Batch [7/782]: Loss 0.021548563614487648\n",
      "Training Batch [8/782]: Loss 0.01598474197089672\n",
      "Training Batch [9/782]: Loss 0.0054685319773852825\n",
      "Training Batch [10/782]: Loss 0.018182557076215744\n",
      "Training Batch [11/782]: Loss 0.024511225521564484\n",
      "Training Batch [12/782]: Loss 0.0036322695668786764\n",
      "Training Batch [13/782]: Loss 0.05984019488096237\n",
      "Training Batch [14/782]: Loss 0.00967406202107668\n",
      "Training Batch [15/782]: Loss 0.03205100819468498\n",
      "Training Batch [16/782]: Loss 0.041306909173727036\n",
      "Training Batch [17/782]: Loss 0.04879220575094223\n",
      "Training Batch [18/782]: Loss 0.007318316958844662\n",
      "Training Batch [19/782]: Loss 0.005270066671073437\n",
      "Training Batch [20/782]: Loss 0.004969984758645296\n",
      "Training Batch [21/782]: Loss 0.012699280865490437\n",
      "Training Batch [22/782]: Loss 0.0034169938880950212\n",
      "Training Batch [23/782]: Loss 0.015120995230972767\n",
      "Training Batch [24/782]: Loss 0.08268000185489655\n",
      "Training Batch [25/782]: Loss 0.017079129815101624\n",
      "Training Batch [26/782]: Loss 0.004496564622968435\n",
      "Training Batch [27/782]: Loss 0.0032987238373607397\n",
      "Training Batch [28/782]: Loss 0.023119566962122917\n",
      "Training Batch [29/782]: Loss 0.0073577468283474445\n",
      "Training Batch [30/782]: Loss 0.016628969460725784\n",
      "Training Batch [31/782]: Loss 0.016852127388119698\n",
      "Training Batch [32/782]: Loss 0.015062225051224232\n",
      "Training Batch [33/782]: Loss 0.02580999583005905\n",
      "Training Batch [34/782]: Loss 0.008131776005029678\n",
      "Training Batch [35/782]: Loss 0.033423859626054764\n",
      "Training Batch [36/782]: Loss 0.006406095344573259\n",
      "Training Batch [37/782]: Loss 0.028882533311843872\n",
      "Training Batch [38/782]: Loss 0.004156231414526701\n",
      "Training Batch [39/782]: Loss 0.20510601997375488\n",
      "Training Batch [40/782]: Loss 0.004519432783126831\n",
      "Training Batch [41/782]: Loss 0.019930467009544373\n",
      "Training Batch [42/782]: Loss 0.01789494790136814\n",
      "Training Batch [43/782]: Loss 0.004671404138207436\n",
      "Training Batch [44/782]: Loss 0.001839471748098731\n",
      "Training Batch [45/782]: Loss 0.004381377249956131\n",
      "Training Batch [46/782]: Loss 0.030672531574964523\n",
      "Training Batch [47/782]: Loss 0.022172652184963226\n",
      "Training Batch [48/782]: Loss 0.009614645503461361\n",
      "Training Batch [49/782]: Loss 0.011212729848921299\n",
      "Training Batch [50/782]: Loss 0.011909116990864277\n",
      "Training Batch [51/782]: Loss 0.0276371780782938\n",
      "Training Batch [52/782]: Loss 0.07048168033361435\n",
      "Training Batch [53/782]: Loss 0.06282205134630203\n",
      "Training Batch [54/782]: Loss 0.015987642109394073\n",
      "Training Batch [55/782]: Loss 0.0062245712615549564\n",
      "Training Batch [56/782]: Loss 0.005560746416449547\n",
      "Training Batch [57/782]: Loss 0.002704466227442026\n",
      "Training Batch [58/782]: Loss 0.0067129069939255714\n",
      "Training Batch [59/782]: Loss 0.0021805933210998774\n",
      "Training Batch [60/782]: Loss 0.048796817660331726\n",
      "Training Batch [61/782]: Loss 0.009970053099095821\n",
      "Training Batch [62/782]: Loss 0.037671223282814026\n",
      "Training Batch [63/782]: Loss 0.010651101358234882\n",
      "Training Batch [64/782]: Loss 0.03315681591629982\n",
      "Training Batch [65/782]: Loss 0.026454688981175423\n",
      "Training Batch [66/782]: Loss 0.0013147237477824092\n",
      "Training Batch [67/782]: Loss 0.024413440376520157\n",
      "Training Batch [68/782]: Loss 0.006721834652125835\n",
      "Training Batch [69/782]: Loss 0.009773565456271172\n",
      "Training Batch [70/782]: Loss 0.007280141580849886\n",
      "Training Batch [71/782]: Loss 0.002107340609654784\n",
      "Training Batch [72/782]: Loss 0.004253762774169445\n",
      "Training Batch [73/782]: Loss 0.002488562371581793\n",
      "Training Batch [74/782]: Loss 0.02605101838707924\n",
      "Training Batch [75/782]: Loss 0.004956295248121023\n",
      "Training Batch [76/782]: Loss 0.015221619047224522\n",
      "Training Batch [77/782]: Loss 0.01315082237124443\n",
      "Training Batch [78/782]: Loss 0.024681759998202324\n",
      "Training Batch [79/782]: Loss 0.005934419576078653\n",
      "Training Batch [80/782]: Loss 0.015124999918043613\n",
      "Training Batch [81/782]: Loss 0.04118834808468819\n",
      "Training Batch [82/782]: Loss 0.08158507198095322\n",
      "Training Batch [83/782]: Loss 0.02748056873679161\n",
      "Training Batch [84/782]: Loss 0.0032821320928633213\n",
      "Training Batch [85/782]: Loss 0.02173253707587719\n",
      "Training Batch [86/782]: Loss 0.05420895293354988\n",
      "Training Batch [87/782]: Loss 0.003378636436536908\n",
      "Training Batch [88/782]: Loss 0.012556750327348709\n",
      "Training Batch [89/782]: Loss 0.02471330016851425\n",
      "Training Batch [90/782]: Loss 0.0018341883551329374\n",
      "Training Batch [91/782]: Loss 0.011664782650768757\n",
      "Training Batch [92/782]: Loss 0.002640578430145979\n",
      "Training Batch [93/782]: Loss 0.04768530651926994\n",
      "Training Batch [94/782]: Loss 0.002422802150249481\n",
      "Training Batch [95/782]: Loss 0.00338570773601532\n",
      "Training Batch [96/782]: Loss 0.01340056024491787\n",
      "Training Batch [97/782]: Loss 0.0027460830751806498\n",
      "Training Batch [98/782]: Loss 0.00199179258197546\n",
      "Training Batch [99/782]: Loss 0.031524408608675\n",
      "Training Batch [100/782]: Loss 0.012390529736876488\n",
      "Training Batch [101/782]: Loss 0.005656985100358725\n",
      "Training Batch [102/782]: Loss 0.01066751778125763\n",
      "Training Batch [103/782]: Loss 0.01635749824345112\n",
      "Training Batch [104/782]: Loss 0.016154523938894272\n",
      "Training Batch [105/782]: Loss 0.01871364191174507\n",
      "Training Batch [106/782]: Loss 0.02671947330236435\n",
      "Training Batch [107/782]: Loss 0.03372279927134514\n",
      "Training Batch [108/782]: Loss 0.018317285925149918\n",
      "Training Batch [109/782]: Loss 0.00493526179343462\n",
      "Training Batch [110/782]: Loss 0.002093088813126087\n",
      "Training Batch [111/782]: Loss 0.023598257452249527\n",
      "Training Batch [112/782]: Loss 0.012745684944093227\n",
      "Training Batch [113/782]: Loss 0.004003745038062334\n",
      "Training Batch [114/782]: Loss 0.014558590017259121\n",
      "Training Batch [115/782]: Loss 0.018477613106369972\n",
      "Training Batch [116/782]: Loss 0.013084557838737965\n",
      "Training Batch [117/782]: Loss 0.041595522314310074\n",
      "Training Batch [118/782]: Loss 0.005743762478232384\n",
      "Training Batch [119/782]: Loss 0.01022367924451828\n",
      "Training Batch [120/782]: Loss 0.007320827804505825\n",
      "Training Batch [121/782]: Loss 0.004369763191789389\n",
      "Training Batch [122/782]: Loss 0.007767948321998119\n",
      "Training Batch [123/782]: Loss 0.005934957414865494\n",
      "Training Batch [124/782]: Loss 0.0008875831263139844\n",
      "Training Batch [125/782]: Loss 0.02051834762096405\n",
      "Training Batch [126/782]: Loss 0.003545440034940839\n",
      "Training Batch [127/782]: Loss 0.003742409171536565\n",
      "Training Batch [128/782]: Loss 0.01764330081641674\n",
      "Training Batch [129/782]: Loss 0.01093066856265068\n",
      "Training Batch [130/782]: Loss 0.0007276813266798854\n",
      "Training Batch [131/782]: Loss 0.01952064409852028\n",
      "Training Batch [132/782]: Loss 0.01763969287276268\n",
      "Training Batch [133/782]: Loss 0.017708921805024147\n",
      "Training Batch [134/782]: Loss 0.007413272745907307\n",
      "Training Batch [135/782]: Loss 0.004661861341446638\n",
      "Training Batch [136/782]: Loss 0.0224483460187912\n",
      "Training Batch [137/782]: Loss 0.0027430495247244835\n",
      "Training Batch [138/782]: Loss 0.0012914091348648071\n",
      "Training Batch [139/782]: Loss 0.06001895293593407\n",
      "Training Batch [140/782]: Loss 0.006325382273644209\n",
      "Training Batch [141/782]: Loss 0.02519341930747032\n",
      "Training Batch [142/782]: Loss 0.002374450210481882\n",
      "Training Batch [143/782]: Loss 0.006085413973778486\n",
      "Training Batch [144/782]: Loss 0.0015216961037367582\n",
      "Training Batch [145/782]: Loss 0.020634949207305908\n",
      "Training Batch [146/782]: Loss 0.09675764292478561\n",
      "Training Batch [147/782]: Loss 0.01667867973446846\n",
      "Training Batch [148/782]: Loss 0.004199061077088118\n",
      "Training Batch [149/782]: Loss 0.0052079446613788605\n",
      "Training Batch [150/782]: Loss 0.07586067169904709\n",
      "Training Batch [151/782]: Loss 0.0201958566904068\n",
      "Training Batch [152/782]: Loss 0.043807171285152435\n",
      "Training Batch [153/782]: Loss 0.05442428961396217\n",
      "Training Batch [154/782]: Loss 0.0414200983941555\n",
      "Training Batch [155/782]: Loss 0.002100857673212886\n",
      "Training Batch [156/782]: Loss 0.0070213680155575275\n",
      "Training Batch [157/782]: Loss 0.0037075630389153957\n",
      "Training Batch [158/782]: Loss 0.04368489980697632\n",
      "Training Batch [159/782]: Loss 0.0031085603404790163\n",
      "Training Batch [160/782]: Loss 0.0910697877407074\n",
      "Training Batch [161/782]: Loss 0.016042543575167656\n",
      "Training Batch [162/782]: Loss 0.00873330794274807\n",
      "Training Batch [163/782]: Loss 0.036797504872083664\n",
      "Training Batch [164/782]: Loss 0.003543842351064086\n",
      "Training Batch [165/782]: Loss 0.003313482040539384\n",
      "Training Batch [166/782]: Loss 0.013773403130471706\n",
      "Training Batch [167/782]: Loss 0.028455611318349838\n",
      "Training Batch [168/782]: Loss 0.0024157646112143993\n",
      "Training Batch [169/782]: Loss 0.025904860347509384\n",
      "Training Batch [170/782]: Loss 0.04473644122481346\n",
      "Training Batch [171/782]: Loss 0.007332088891416788\n",
      "Training Batch [172/782]: Loss 0.018460242077708244\n",
      "Training Batch [173/782]: Loss 0.003986719995737076\n",
      "Training Batch [174/782]: Loss 0.04595839977264404\n",
      "Training Batch [175/782]: Loss 0.04568648710846901\n",
      "Training Batch [176/782]: Loss 0.0022436482831835747\n",
      "Training Batch [177/782]: Loss 0.008091030642390251\n",
      "Training Batch [178/782]: Loss 0.020900430157780647\n",
      "Training Batch [179/782]: Loss 0.004731802269816399\n",
      "Training Batch [180/782]: Loss 0.012614228762686253\n",
      "Training Batch [181/782]: Loss 0.008182385005056858\n",
      "Training Batch [182/782]: Loss 0.010642738081514835\n",
      "Training Batch [183/782]: Loss 0.015172585844993591\n",
      "Training Batch [184/782]: Loss 0.006555754691362381\n",
      "Training Batch [185/782]: Loss 0.00356113538146019\n",
      "Training Batch [186/782]: Loss 0.027342841029167175\n",
      "Training Batch [187/782]: Loss 0.013309716247022152\n",
      "Training Batch [188/782]: Loss 0.051973890513181686\n",
      "Training Batch [189/782]: Loss 0.032310716807842255\n",
      "Training Batch [190/782]: Loss 0.011739150620996952\n",
      "Training Batch [191/782]: Loss 0.000862040207721293\n",
      "Training Batch [192/782]: Loss 0.016260193660855293\n",
      "Training Batch [193/782]: Loss 0.002248887438327074\n",
      "Training Batch [194/782]: Loss 0.02391846664249897\n",
      "Training Batch [195/782]: Loss 0.022115979343652725\n",
      "Training Batch [196/782]: Loss 0.00518440967425704\n",
      "Training Batch [197/782]: Loss 0.025608249008655548\n",
      "Training Batch [198/782]: Loss 0.03397149592638016\n",
      "Training Batch [199/782]: Loss 0.00559790525585413\n",
      "Training Batch [200/782]: Loss 0.015110759995877743\n",
      "Training Batch [201/782]: Loss 0.01596696488559246\n",
      "Training Batch [202/782]: Loss 0.028451893478631973\n",
      "Training Batch [203/782]: Loss 0.04623427242040634\n",
      "Training Batch [204/782]: Loss 0.0032313179690390825\n",
      "Training Batch [205/782]: Loss 0.0221053808927536\n",
      "Training Batch [206/782]: Loss 0.007944357581436634\n",
      "Training Batch [207/782]: Loss 0.00786251574754715\n",
      "Training Batch [208/782]: Loss 0.006018775049597025\n",
      "Training Batch [209/782]: Loss 0.0488324835896492\n",
      "Training Batch [210/782]: Loss 0.021689051762223244\n",
      "Training Batch [211/782]: Loss 0.017553655430674553\n",
      "Training Batch [212/782]: Loss 0.005551923532038927\n",
      "Training Batch [213/782]: Loss 0.00830826722085476\n",
      "Training Batch [214/782]: Loss 0.0016601094976067543\n",
      "Training Batch [215/782]: Loss 0.005741977132856846\n",
      "Training Batch [216/782]: Loss 0.022088058292865753\n",
      "Training Batch [217/782]: Loss 0.021670682355761528\n",
      "Training Batch [218/782]: Loss 0.005859976634383202\n",
      "Training Batch [219/782]: Loss 0.032616518437862396\n",
      "Training Batch [220/782]: Loss 0.004618446342647076\n",
      "Training Batch [221/782]: Loss 0.08985290676355362\n",
      "Training Batch [222/782]: Loss 0.011287658475339413\n",
      "Training Batch [223/782]: Loss 0.002385072410106659\n",
      "Training Batch [224/782]: Loss 0.0017503193812444806\n",
      "Training Batch [225/782]: Loss 0.07774730771780014\n",
      "Training Batch [226/782]: Loss 0.12109273672103882\n",
      "Training Batch [227/782]: Loss 0.04540342092514038\n",
      "Training Batch [228/782]: Loss 0.025582600384950638\n",
      "Training Batch [229/782]: Loss 0.012866314500570297\n",
      "Training Batch [230/782]: Loss 0.0028958728071302176\n",
      "Training Batch [231/782]: Loss 0.06710078567266464\n",
      "Training Batch [232/782]: Loss 0.0727766752243042\n",
      "Training Batch [233/782]: Loss 0.007740573026239872\n",
      "Training Batch [234/782]: Loss 0.002999562071636319\n",
      "Training Batch [235/782]: Loss 0.0025503772776573896\n",
      "Training Batch [236/782]: Loss 0.06193460524082184\n",
      "Training Batch [237/782]: Loss 0.14438192546367645\n",
      "Training Batch [238/782]: Loss 0.007405951619148254\n",
      "Training Batch [239/782]: Loss 0.0057237595319747925\n",
      "Training Batch [240/782]: Loss 0.005447041243314743\n",
      "Training Batch [241/782]: Loss 0.005085620563477278\n",
      "Training Batch [242/782]: Loss 0.0019651949405670166\n",
      "Training Batch [243/782]: Loss 0.002612991724163294\n",
      "Training Batch [244/782]: Loss 0.005890551023185253\n",
      "Training Batch [245/782]: Loss 0.11550741642713547\n",
      "Training Batch [246/782]: Loss 0.01658693328499794\n",
      "Training Batch [247/782]: Loss 0.004200113005936146\n",
      "Training Batch [248/782]: Loss 0.06555624306201935\n",
      "Training Batch [249/782]: Loss 0.008881568908691406\n",
      "Training Batch [250/782]: Loss 0.02130032144486904\n",
      "Training Batch [251/782]: Loss 0.04462460055947304\n",
      "Training Batch [252/782]: Loss 0.048572055995464325\n",
      "Training Batch [253/782]: Loss 0.05149044841527939\n",
      "Training Batch [254/782]: Loss 0.019284261390566826\n",
      "Training Batch [255/782]: Loss 0.006332822144031525\n",
      "Training Batch [256/782]: Loss 0.0179890263825655\n",
      "Training Batch [257/782]: Loss 0.002690583234652877\n",
      "Training Batch [258/782]: Loss 0.010184811428189278\n",
      "Training Batch [259/782]: Loss 0.002542258705943823\n",
      "Training Batch [260/782]: Loss 0.006146618630737066\n",
      "Training Batch [261/782]: Loss 0.003786982037127018\n",
      "Training Batch [262/782]: Loss 0.07081723213195801\n",
      "Training Batch [263/782]: Loss 0.019741714000701904\n",
      "Training Batch [264/782]: Loss 0.0031670944299548864\n",
      "Training Batch [265/782]: Loss 0.0036851121112704277\n",
      "Training Batch [266/782]: Loss 0.05355357378721237\n",
      "Training Batch [267/782]: Loss 0.004271551501005888\n",
      "Training Batch [268/782]: Loss 0.055614862591028214\n",
      "Training Batch [269/782]: Loss 0.03498942777514458\n",
      "Training Batch [270/782]: Loss 0.021331489086151123\n",
      "Training Batch [271/782]: Loss 0.010492119938135147\n",
      "Training Batch [272/782]: Loss 0.006680657621473074\n",
      "Training Batch [273/782]: Loss 0.011930576525628567\n",
      "Training Batch [274/782]: Loss 0.04057673364877701\n",
      "Training Batch [275/782]: Loss 0.001488684443756938\n",
      "Training Batch [276/782]: Loss 0.040499214082956314\n",
      "Training Batch [277/782]: Loss 0.0020072644110769033\n",
      "Training Batch [278/782]: Loss 0.013413991779088974\n",
      "Training Batch [279/782]: Loss 0.03309628367424011\n",
      "Training Batch [280/782]: Loss 0.02557600475847721\n",
      "Training Batch [281/782]: Loss 0.028976939618587494\n",
      "Training Batch [282/782]: Loss 0.0024743706453591585\n",
      "Training Batch [283/782]: Loss 0.05631004646420479\n",
      "Training Batch [284/782]: Loss 0.04256736487150192\n",
      "Training Batch [285/782]: Loss 0.18552696704864502\n",
      "Training Batch [286/782]: Loss 0.0014990638010203838\n",
      "Training Batch [287/782]: Loss 0.010516512207686901\n",
      "Training Batch [288/782]: Loss 0.0052698091603815556\n",
      "Training Batch [289/782]: Loss 0.02836667373776436\n",
      "Training Batch [290/782]: Loss 0.0054648518562316895\n",
      "Training Batch [291/782]: Loss 0.028096236288547516\n",
      "Training Batch [292/782]: Loss 0.005287277977913618\n",
      "Training Batch [293/782]: Loss 0.03162916749715805\n",
      "Training Batch [294/782]: Loss 0.08670343458652496\n",
      "Training Batch [295/782]: Loss 0.0014223576290532947\n",
      "Training Batch [296/782]: Loss 0.019097447395324707\n",
      "Training Batch [297/782]: Loss 0.002786961616948247\n",
      "Training Batch [298/782]: Loss 0.03433099761605263\n",
      "Training Batch [299/782]: Loss 0.009174964390695095\n",
      "Training Batch [300/782]: Loss 0.09510289877653122\n",
      "Training Batch [301/782]: Loss 0.08468714356422424\n",
      "Training Batch [302/782]: Loss 0.03294925391674042\n",
      "Training Batch [303/782]: Loss 0.06476307660341263\n",
      "Training Batch [304/782]: Loss 0.018383795395493507\n",
      "Training Batch [305/782]: Loss 0.01182815432548523\n",
      "Training Batch [306/782]: Loss 0.007156536914408207\n",
      "Training Batch [307/782]: Loss 0.010740319266915321\n",
      "Training Batch [308/782]: Loss 0.03299446031451225\n",
      "Training Batch [309/782]: Loss 0.09802275896072388\n",
      "Training Batch [310/782]: Loss 0.02858724072575569\n",
      "Training Batch [311/782]: Loss 0.009299554862082005\n",
      "Training Batch [312/782]: Loss 0.01730174943804741\n",
      "Training Batch [313/782]: Loss 0.027135366573929787\n",
      "Training Batch [314/782]: Loss 0.0014452929608523846\n",
      "Training Batch [315/782]: Loss 0.00498555414378643\n",
      "Training Batch [316/782]: Loss 0.022067446261644363\n",
      "Training Batch [317/782]: Loss 0.005436541046947241\n",
      "Training Batch [318/782]: Loss 0.025893542915582657\n",
      "Training Batch [319/782]: Loss 0.012319902889430523\n",
      "Training Batch [320/782]: Loss 0.005393728613853455\n",
      "Training Batch [321/782]: Loss 0.0028524938970804214\n",
      "Training Batch [322/782]: Loss 0.007964015007019043\n",
      "Training Batch [323/782]: Loss 0.0033078016713261604\n",
      "Training Batch [324/782]: Loss 0.0038403624203056097\n",
      "Training Batch [325/782]: Loss 0.0016270012129098177\n",
      "Training Batch [326/782]: Loss 0.012018728069961071\n",
      "Training Batch [327/782]: Loss 0.06928003579378128\n",
      "Training Batch [328/782]: Loss 0.02220637910068035\n",
      "Training Batch [329/782]: Loss 0.00812604185193777\n",
      "Training Batch [330/782]: Loss 0.0055645122192800045\n",
      "Training Batch [331/782]: Loss 0.029007568955421448\n",
      "Training Batch [332/782]: Loss 0.007153850048780441\n",
      "Training Batch [333/782]: Loss 0.021800203248858452\n",
      "Training Batch [334/782]: Loss 0.04461405426263809\n",
      "Training Batch [335/782]: Loss 0.009032538160681725\n",
      "Training Batch [336/782]: Loss 0.026042094454169273\n",
      "Training Batch [337/782]: Loss 0.016956457868218422\n",
      "Training Batch [338/782]: Loss 0.0031087547540664673\n",
      "Training Batch [339/782]: Loss 0.02720242738723755\n",
      "Training Batch [340/782]: Loss 0.0322120301425457\n",
      "Training Batch [341/782]: Loss 0.010260120034217834\n",
      "Training Batch [342/782]: Loss 0.03054494969546795\n",
      "Training Batch [343/782]: Loss 0.0022213386837393045\n",
      "Training Batch [344/782]: Loss 0.0028300751000642776\n",
      "Training Batch [345/782]: Loss 0.0032430419232696295\n",
      "Training Batch [346/782]: Loss 0.004735806491225958\n",
      "Training Batch [347/782]: Loss 0.001565661863423884\n",
      "Training Batch [348/782]: Loss 0.012791812419891357\n",
      "Training Batch [349/782]: Loss 0.002075220923870802\n",
      "Training Batch [350/782]: Loss 0.10617407411336899\n",
      "Training Batch [351/782]: Loss 0.05960163474082947\n",
      "Training Batch [352/782]: Loss 0.004529004916548729\n",
      "Training Batch [353/782]: Loss 0.011540823616087437\n",
      "Training Batch [354/782]: Loss 0.008650233037769794\n",
      "Training Batch [355/782]: Loss 0.002363876672461629\n",
      "Training Batch [356/782]: Loss 0.0038712862879037857\n",
      "Training Batch [357/782]: Loss 0.010988469235599041\n",
      "Training Batch [358/782]: Loss 0.0064802635461091995\n",
      "Training Batch [359/782]: Loss 0.008661329746246338\n",
      "Training Batch [360/782]: Loss 0.03062444180250168\n",
      "Training Batch [361/782]: Loss 0.0892077386379242\n",
      "Training Batch [362/782]: Loss 0.07726132869720459\n",
      "Training Batch [363/782]: Loss 0.1285391002893448\n",
      "Training Batch [364/782]: Loss 0.008148499764502048\n",
      "Training Batch [365/782]: Loss 0.007260414306074381\n",
      "Training Batch [366/782]: Loss 0.0034530689008533955\n",
      "Training Batch [367/782]: Loss 0.009047350846230984\n",
      "Training Batch [368/782]: Loss 0.01597866602241993\n",
      "Training Batch [369/782]: Loss 0.009389782324433327\n",
      "Training Batch [370/782]: Loss 0.003438269719481468\n",
      "Training Batch [371/782]: Loss 0.006266254931688309\n",
      "Training Batch [372/782]: Loss 0.025823721662163734\n",
      "Training Batch [373/782]: Loss 0.001037105219438672\n",
      "Training Batch [374/782]: Loss 0.023992711678147316\n",
      "Training Batch [375/782]: Loss 0.003916400950402021\n",
      "Training Batch [376/782]: Loss 0.037883784621953964\n",
      "Training Batch [377/782]: Loss 0.006718205288052559\n",
      "Training Batch [378/782]: Loss 0.02719179540872574\n",
      "Training Batch [379/782]: Loss 0.04838743805885315\n",
      "Training Batch [380/782]: Loss 0.009599914774298668\n",
      "Training Batch [381/782]: Loss 0.03278718143701553\n",
      "Training Batch [382/782]: Loss 0.024693086743354797\n",
      "Training Batch [383/782]: Loss 0.05285998806357384\n",
      "Training Batch [384/782]: Loss 0.00559769943356514\n",
      "Training Batch [385/782]: Loss 0.017437204718589783\n",
      "Training Batch [386/782]: Loss 0.12959761917591095\n",
      "Training Batch [387/782]: Loss 0.029231185093522072\n",
      "Training Batch [388/782]: Loss 0.014602399431169033\n",
      "Training Batch [389/782]: Loss 0.0008620676235295832\n",
      "Training Batch [390/782]: Loss 0.004912225529551506\n",
      "Training Batch [391/782]: Loss 0.004564681556075811\n",
      "Training Batch [392/782]: Loss 0.010673332959413528\n",
      "Training Batch [393/782]: Loss 0.016027038916945457\n",
      "Training Batch [394/782]: Loss 0.12649717926979065\n",
      "Training Batch [395/782]: Loss 0.022884508594870567\n",
      "Training Batch [396/782]: Loss 0.020185215398669243\n",
      "Training Batch [397/782]: Loss 0.037735696882009506\n",
      "Training Batch [398/782]: Loss 0.009788818657398224\n",
      "Training Batch [399/782]: Loss 0.011179635301232338\n",
      "Training Batch [400/782]: Loss 0.017256777733564377\n",
      "Training Batch [401/782]: Loss 0.010831663385033607\n",
      "Training Batch [402/782]: Loss 0.005442775320261717\n",
      "Training Batch [403/782]: Loss 0.012546541169285774\n",
      "Training Batch [404/782]: Loss 0.0018101646564900875\n",
      "Training Batch [405/782]: Loss 0.007407947909086943\n",
      "Training Batch [406/782]: Loss 0.012219675816595554\n",
      "Training Batch [407/782]: Loss 0.012515299022197723\n",
      "Training Batch [408/782]: Loss 0.010063903406262398\n",
      "Training Batch [409/782]: Loss 0.00709878234192729\n",
      "Training Batch [410/782]: Loss 0.0017029816517606378\n",
      "Training Batch [411/782]: Loss 0.0023315218277275562\n",
      "Training Batch [412/782]: Loss 0.02956153266131878\n",
      "Training Batch [413/782]: Loss 0.024290747940540314\n",
      "Training Batch [414/782]: Loss 0.06770922243595123\n",
      "Training Batch [415/782]: Loss 0.038727156817913055\n",
      "Training Batch [416/782]: Loss 0.03811373934149742\n",
      "Training Batch [417/782]: Loss 0.0032417057082057\n",
      "Training Batch [418/782]: Loss 0.023700153455138206\n",
      "Training Batch [419/782]: Loss 0.00805038120597601\n",
      "Training Batch [420/782]: Loss 0.0063850050792098045\n",
      "Training Batch [421/782]: Loss 0.005904278252273798\n",
      "Training Batch [422/782]: Loss 0.003900213399901986\n",
      "Training Batch [423/782]: Loss 0.021868078038096428\n",
      "Training Batch [424/782]: Loss 0.003734107594937086\n",
      "Training Batch [425/782]: Loss 0.02079332247376442\n",
      "Training Batch [426/782]: Loss 0.0453450046479702\n",
      "Training Batch [427/782]: Loss 0.01656891219317913\n",
      "Training Batch [428/782]: Loss 0.004405759274959564\n",
      "Training Batch [429/782]: Loss 0.021080348640680313\n",
      "Training Batch [430/782]: Loss 0.017791176214814186\n",
      "Training Batch [431/782]: Loss 0.033358894288539886\n",
      "Training Batch [432/782]: Loss 0.04203301668167114\n",
      "Training Batch [433/782]: Loss 0.005004641599953175\n",
      "Training Batch [434/782]: Loss 0.02556905336678028\n",
      "Training Batch [435/782]: Loss 0.008070293813943863\n",
      "Training Batch [436/782]: Loss 0.04924020171165466\n",
      "Training Batch [437/782]: Loss 0.002920997329056263\n",
      "Training Batch [438/782]: Loss 0.11299247294664383\n",
      "Training Batch [439/782]: Loss 0.027357006445527077\n",
      "Training Batch [440/782]: Loss 0.007350096479058266\n",
      "Training Batch [441/782]: Loss 0.004025160800665617\n",
      "Training Batch [442/782]: Loss 0.015329576097428799\n",
      "Training Batch [443/782]: Loss 0.018840579316020012\n",
      "Training Batch [444/782]: Loss 0.018679799512028694\n",
      "Training Batch [445/782]: Loss 0.0038640769198536873\n",
      "Training Batch [446/782]: Loss 0.006256654392927885\n",
      "Training Batch [447/782]: Loss 0.031918223947286606\n",
      "Training Batch [448/782]: Loss 0.008185651153326035\n",
      "Training Batch [449/782]: Loss 0.007343440316617489\n",
      "Training Batch [450/782]: Loss 0.011406263336539268\n",
      "Training Batch [451/782]: Loss 0.006094148848205805\n",
      "Training Batch [452/782]: Loss 0.017840728163719177\n",
      "Training Batch [453/782]: Loss 0.03425842896103859\n",
      "Training Batch [454/782]: Loss 0.0232084933668375\n",
      "Training Batch [455/782]: Loss 0.029929544776678085\n",
      "Training Batch [456/782]: Loss 0.06368900835514069\n",
      "Training Batch [457/782]: Loss 0.03833134099841118\n",
      "Training Batch [458/782]: Loss 0.0009112099651247263\n",
      "Training Batch [459/782]: Loss 0.008811727166175842\n",
      "Training Batch [460/782]: Loss 0.014024434611201286\n",
      "Training Batch [461/782]: Loss 0.02124391309916973\n",
      "Training Batch [462/782]: Loss 0.025446517392992973\n",
      "Training Batch [463/782]: Loss 0.014490658417344093\n",
      "Training Batch [464/782]: Loss 0.05262429267168045\n",
      "Training Batch [465/782]: Loss 0.06638453900814056\n",
      "Training Batch [466/782]: Loss 0.01235363818705082\n",
      "Training Batch [467/782]: Loss 0.008369122631847858\n",
      "Training Batch [468/782]: Loss 0.0016367282951250672\n",
      "Training Batch [469/782]: Loss 0.030257826671004295\n",
      "Training Batch [470/782]: Loss 0.01042996160686016\n",
      "Training Batch [471/782]: Loss 0.01175196934491396\n",
      "Training Batch [472/782]: Loss 0.019066382199525833\n",
      "Training Batch [473/782]: Loss 0.01488013006746769\n",
      "Training Batch [474/782]: Loss 0.05769047141075134\n",
      "Training Batch [475/782]: Loss 0.01835859753191471\n",
      "Training Batch [476/782]: Loss 0.02375907264649868\n",
      "Training Batch [477/782]: Loss 0.0019184062257409096\n",
      "Training Batch [478/782]: Loss 0.03846064954996109\n",
      "Training Batch [479/782]: Loss 0.0013922502985224128\n",
      "Training Batch [480/782]: Loss 0.25312110781669617\n",
      "Training Batch [481/782]: Loss 0.023036299273371696\n",
      "Training Batch [482/782]: Loss 0.009474734775722027\n",
      "Training Batch [483/782]: Loss 0.012296522036194801\n",
      "Training Batch [484/782]: Loss 0.09014243632555008\n",
      "Training Batch [485/782]: Loss 0.021061675623059273\n",
      "Training Batch [486/782]: Loss 0.009419994428753853\n",
      "Training Batch [487/782]: Loss 0.1609714925289154\n",
      "Training Batch [488/782]: Loss 0.021486882120370865\n",
      "Training Batch [489/782]: Loss 0.010983160696923733\n",
      "Training Batch [490/782]: Loss 0.01935705915093422\n",
      "Training Batch [491/782]: Loss 0.01287807710468769\n",
      "Training Batch [492/782]: Loss 0.011166311800479889\n",
      "Training Batch [493/782]: Loss 0.05309302359819412\n",
      "Training Batch [494/782]: Loss 0.0019150988664478064\n",
      "Training Batch [495/782]: Loss 0.1411750465631485\n",
      "Training Batch [496/782]: Loss 0.056447695940732956\n",
      "Training Batch [497/782]: Loss 0.03227299451828003\n",
      "Training Batch [498/782]: Loss 0.0011482634581625462\n",
      "Training Batch [499/782]: Loss 0.03880719840526581\n",
      "Training Batch [500/782]: Loss 0.015078218653798103\n",
      "Training Batch [501/782]: Loss 0.04481378570199013\n",
      "Training Batch [502/782]: Loss 0.03778519108891487\n",
      "Training Batch [503/782]: Loss 0.035676393657922745\n",
      "Training Batch [504/782]: Loss 0.09867128729820251\n",
      "Training Batch [505/782]: Loss 0.03146366402506828\n",
      "Training Batch [506/782]: Loss 0.02091064862906933\n",
      "Training Batch [507/782]: Loss 0.03231673687696457\n",
      "Training Batch [508/782]: Loss 0.06289343535900116\n",
      "Training Batch [509/782]: Loss 0.018722975626587868\n",
      "Training Batch [510/782]: Loss 0.002645718166604638\n",
      "Training Batch [511/782]: Loss 0.0011444062693044543\n",
      "Training Batch [512/782]: Loss 0.030684908851981163\n",
      "Training Batch [513/782]: Loss 0.00247869361191988\n",
      "Training Batch [514/782]: Loss 0.011248239316046238\n",
      "Training Batch [515/782]: Loss 0.032880999147892\n",
      "Training Batch [516/782]: Loss 0.006003816146403551\n",
      "Training Batch [517/782]: Loss 0.01863205060362816\n",
      "Training Batch [518/782]: Loss 0.008860040456056595\n",
      "Training Batch [519/782]: Loss 0.011942300945520401\n",
      "Training Batch [520/782]: Loss 0.0032132654450833797\n",
      "Training Batch [521/782]: Loss 0.057558152824640274\n",
      "Training Batch [522/782]: Loss 0.029788631945848465\n",
      "Training Batch [523/782]: Loss 0.016848601400852203\n",
      "Training Batch [524/782]: Loss 0.01081996038556099\n",
      "Training Batch [525/782]: Loss 0.002317986683920026\n",
      "Training Batch [526/782]: Loss 0.022743167355656624\n",
      "Training Batch [527/782]: Loss 0.01661965809762478\n",
      "Training Batch [528/782]: Loss 0.005852636881172657\n",
      "Training Batch [529/782]: Loss 0.028761981055140495\n",
      "Training Batch [530/782]: Loss 0.0018362316768616438\n",
      "Training Batch [531/782]: Loss 0.009309506043791771\n",
      "Training Batch [532/782]: Loss 0.004892510827630758\n",
      "Training Batch [533/782]: Loss 0.04587889090180397\n",
      "Training Batch [534/782]: Loss 0.032306838780641556\n",
      "Training Batch [535/782]: Loss 0.007558788172900677\n",
      "Training Batch [536/782]: Loss 0.002644255291670561\n",
      "Training Batch [537/782]: Loss 0.0016769969370216131\n",
      "Training Batch [538/782]: Loss 0.04008205980062485\n",
      "Training Batch [539/782]: Loss 0.0025192080065608025\n",
      "Training Batch [540/782]: Loss 0.018233248963952065\n",
      "Training Batch [541/782]: Loss 0.007853953167796135\n",
      "Training Batch [542/782]: Loss 0.0023683637846261263\n",
      "Training Batch [543/782]: Loss 0.004094914998859167\n",
      "Training Batch [544/782]: Loss 0.013815728016197681\n",
      "Training Batch [545/782]: Loss 0.0033787963911890984\n",
      "Training Batch [546/782]: Loss 0.03939094394445419\n",
      "Training Batch [547/782]: Loss 0.016654513776302338\n",
      "Training Batch [548/782]: Loss 0.005909041967242956\n",
      "Training Batch [549/782]: Loss 0.02031095139682293\n",
      "Training Batch [550/782]: Loss 0.019411198794841766\n",
      "Training Batch [551/782]: Loss 0.004832175560295582\n",
      "Training Batch [552/782]: Loss 0.05254615470767021\n",
      "Training Batch [553/782]: Loss 0.011593681760132313\n",
      "Training Batch [554/782]: Loss 0.008169043809175491\n",
      "Training Batch [555/782]: Loss 0.00707987742498517\n",
      "Training Batch [556/782]: Loss 0.053206950426101685\n",
      "Training Batch [557/782]: Loss 0.027215853333473206\n",
      "Training Batch [558/782]: Loss 0.054458536207675934\n",
      "Training Batch [559/782]: Loss 0.05254156142473221\n",
      "Training Batch [560/782]: Loss 0.006147036794573069\n",
      "Training Batch [561/782]: Loss 0.001911626197397709\n",
      "Training Batch [562/782]: Loss 0.007684019859880209\n",
      "Training Batch [563/782]: Loss 0.005986711010336876\n",
      "Training Batch [564/782]: Loss 0.008292872458696365\n",
      "Training Batch [565/782]: Loss 0.11296017467975616\n",
      "Training Batch [566/782]: Loss 0.031483229249715805\n",
      "Training Batch [567/782]: Loss 0.005030431784689426\n",
      "Training Batch [568/782]: Loss 0.017442328855395317\n",
      "Training Batch [569/782]: Loss 0.025792157277464867\n",
      "Training Batch [570/782]: Loss 0.02742413803935051\n",
      "Training Batch [571/782]: Loss 0.04585038870573044\n",
      "Training Batch [572/782]: Loss 0.0035698837600648403\n",
      "Training Batch [573/782]: Loss 0.014277546666562557\n",
      "Training Batch [574/782]: Loss 0.024685805663466454\n",
      "Training Batch [575/782]: Loss 0.006158114410936832\n",
      "Training Batch [576/782]: Loss 0.004235692787915468\n",
      "Training Batch [577/782]: Loss 0.014148716814815998\n",
      "Training Batch [578/782]: Loss 0.021628279238939285\n",
      "Training Batch [579/782]: Loss 0.007826630026102066\n",
      "Training Batch [580/782]: Loss 0.013148728758096695\n",
      "Training Batch [581/782]: Loss 0.057024870067834854\n",
      "Training Batch [582/782]: Loss 0.04584993049502373\n",
      "Training Batch [583/782]: Loss 0.03583763167262077\n",
      "Training Batch [584/782]: Loss 0.02594711259007454\n",
      "Training Batch [585/782]: Loss 0.010779935866594315\n",
      "Training Batch [586/782]: Loss 0.02514774538576603\n",
      "Training Batch [587/782]: Loss 0.07229148596525192\n",
      "Training Batch [588/782]: Loss 0.0016728962073102593\n",
      "Training Batch [589/782]: Loss 0.0025487930979579687\n",
      "Training Batch [590/782]: Loss 0.014941333793103695\n",
      "Training Batch [591/782]: Loss 0.04906902834773064\n",
      "Training Batch [592/782]: Loss 0.026171023026108742\n",
      "Training Batch [593/782]: Loss 0.011353380978107452\n",
      "Training Batch [594/782]: Loss 0.00688159791752696\n",
      "Training Batch [595/782]: Loss 0.059940408915281296\n",
      "Training Batch [596/782]: Loss 0.013482745736837387\n",
      "Training Batch [597/782]: Loss 0.007756058592349291\n",
      "Training Batch [598/782]: Loss 0.014055603183805943\n",
      "Training Batch [599/782]: Loss 0.005618765950202942\n",
      "Training Batch [600/782]: Loss 0.08103495836257935\n",
      "Training Batch [601/782]: Loss 0.009176929481327534\n",
      "Training Batch [602/782]: Loss 0.0019188865553587675\n",
      "Training Batch [603/782]: Loss 0.0498504713177681\n",
      "Training Batch [604/782]: Loss 0.11335788667201996\n",
      "Training Batch [605/782]: Loss 0.010214628651738167\n",
      "Training Batch [606/782]: Loss 0.008511792868375778\n",
      "Training Batch [607/782]: Loss 0.03427230939269066\n",
      "Training Batch [608/782]: Loss 0.007323241326957941\n",
      "Training Batch [609/782]: Loss 0.029298553243279457\n",
      "Training Batch [610/782]: Loss 0.012997996062040329\n",
      "Training Batch [611/782]: Loss 0.023157993331551552\n",
      "Training Batch [612/782]: Loss 0.01161144021898508\n",
      "Training Batch [613/782]: Loss 0.004675605800002813\n",
      "Training Batch [614/782]: Loss 0.022977683693170547\n",
      "Training Batch [615/782]: Loss 0.014844493009150028\n",
      "Training Batch [616/782]: Loss 0.0023367551621049643\n",
      "Training Batch [617/782]: Loss 0.00465261097997427\n",
      "Training Batch [618/782]: Loss 0.006955825723707676\n",
      "Training Batch [619/782]: Loss 0.016735253855586052\n",
      "Training Batch [620/782]: Loss 0.01787399686872959\n",
      "Training Batch [621/782]: Loss 0.03856827691197395\n",
      "Training Batch [622/782]: Loss 0.019479693844914436\n",
      "Training Batch [623/782]: Loss 0.0030957316048443317\n",
      "Training Batch [624/782]: Loss 0.0004785490164067596\n",
      "Training Batch [625/782]: Loss 0.05394111946225166\n",
      "Training Batch [626/782]: Loss 0.029737718403339386\n",
      "Training Batch [627/782]: Loss 0.0056729926727712154\n",
      "Training Batch [628/782]: Loss 0.0036408621817827225\n",
      "Training Batch [629/782]: Loss 0.0015403545694425702\n",
      "Training Batch [630/782]: Loss 0.2108408510684967\n",
      "Training Batch [631/782]: Loss 0.07214183360338211\n",
      "Training Batch [632/782]: Loss 0.002131609246134758\n",
      "Training Batch [633/782]: Loss 0.085271455347538\n",
      "Training Batch [634/782]: Loss 0.006711642257869244\n",
      "Training Batch [635/782]: Loss 0.013572494499385357\n",
      "Training Batch [636/782]: Loss 0.010438323020935059\n",
      "Training Batch [637/782]: Loss 0.07567054778337479\n",
      "Training Batch [638/782]: Loss 0.008370584808290005\n",
      "Training Batch [639/782]: Loss 0.08231053501367569\n",
      "Training Batch [640/782]: Loss 0.010336749255657196\n",
      "Training Batch [641/782]: Loss 0.00936728622764349\n",
      "Training Batch [642/782]: Loss 0.05595693737268448\n",
      "Training Batch [643/782]: Loss 0.04638250172138214\n",
      "Training Batch [644/782]: Loss 0.008639064617455006\n",
      "Training Batch [645/782]: Loss 0.0043049887754023075\n",
      "Training Batch [646/782]: Loss 0.004880440421402454\n",
      "Training Batch [647/782]: Loss 0.024478748440742493\n",
      "Training Batch [648/782]: Loss 0.07294910401105881\n",
      "Training Batch [649/782]: Loss 0.0090596042573452\n",
      "Training Batch [650/782]: Loss 0.007111695595085621\n",
      "Training Batch [651/782]: Loss 0.007894809357821941\n",
      "Training Batch [652/782]: Loss 0.0026354147121310234\n",
      "Training Batch [653/782]: Loss 0.057948898524045944\n",
      "Training Batch [654/782]: Loss 0.01168476790189743\n",
      "Training Batch [655/782]: Loss 0.02875525690615177\n",
      "Training Batch [656/782]: Loss 0.014606970362365246\n",
      "Training Batch [657/782]: Loss 0.006499901879578829\n",
      "Training Batch [658/782]: Loss 0.14071246981620789\n",
      "Training Batch [659/782]: Loss 0.0012956135906279087\n",
      "Training Batch [660/782]: Loss 0.009604374878108501\n",
      "Training Batch [661/782]: Loss 0.02872864156961441\n",
      "Training Batch [662/782]: Loss 0.07703991234302521\n",
      "Training Batch [663/782]: Loss 0.013333232142031193\n",
      "Training Batch [664/782]: Loss 0.002613138873130083\n",
      "Training Batch [665/782]: Loss 0.06121109798550606\n",
      "Training Batch [666/782]: Loss 0.08486499637365341\n",
      "Training Batch [667/782]: Loss 0.008240871131420135\n",
      "Training Batch [668/782]: Loss 0.008050542324781418\n",
      "Training Batch [669/782]: Loss 0.03370671346783638\n",
      "Training Batch [670/782]: Loss 0.011465456336736679\n",
      "Training Batch [671/782]: Loss 0.002553213620558381\n",
      "Training Batch [672/782]: Loss 0.005654364824295044\n",
      "Training Batch [673/782]: Loss 0.010753205977380276\n",
      "Training Batch [674/782]: Loss 0.010465799830853939\n",
      "Training Batch [675/782]: Loss 0.0030726382974535227\n",
      "Training Batch [676/782]: Loss 0.014229848049581051\n",
      "Training Batch [677/782]: Loss 0.02343904599547386\n",
      "Training Batch [678/782]: Loss 0.014695136807858944\n",
      "Training Batch [679/782]: Loss 0.010952714830636978\n",
      "Training Batch [680/782]: Loss 0.009741964749991894\n",
      "Training Batch [681/782]: Loss 0.016723157837986946\n",
      "Training Batch [682/782]: Loss 0.011371200904250145\n",
      "Training Batch [683/782]: Loss 0.0037415458355098963\n",
      "Training Batch [684/782]: Loss 0.0059610214084386826\n",
      "Training Batch [685/782]: Loss 0.024418693035840988\n",
      "Training Batch [686/782]: Loss 0.04426301643252373\n",
      "Training Batch [687/782]: Loss 0.003284306265413761\n",
      "Training Batch [688/782]: Loss 0.07079318165779114\n",
      "Training Batch [689/782]: Loss 0.03319023177027702\n",
      "Training Batch [690/782]: Loss 0.06300881505012512\n",
      "Training Batch [691/782]: Loss 0.03840799629688263\n",
      "Training Batch [692/782]: Loss 0.03601881489157677\n",
      "Training Batch [693/782]: Loss 0.04043357074260712\n",
      "Training Batch [694/782]: Loss 0.04382991045713425\n",
      "Training Batch [695/782]: Loss 0.025096403434872627\n",
      "Training Batch [696/782]: Loss 0.003997857216745615\n",
      "Training Batch [697/782]: Loss 0.0032676050905138254\n",
      "Training Batch [698/782]: Loss 0.01217503659427166\n",
      "Training Batch [699/782]: Loss 0.03737511858344078\n",
      "Training Batch [700/782]: Loss 0.07978401333093643\n",
      "Training Batch [701/782]: Loss 0.003000526688992977\n",
      "Training Batch [702/782]: Loss 0.029032578691840172\n",
      "Training Batch [703/782]: Loss 0.004900529980659485\n",
      "Training Batch [704/782]: Loss 0.008843282237648964\n",
      "Training Batch [705/782]: Loss 0.026437468826770782\n",
      "Training Batch [706/782]: Loss 0.006070188712328672\n",
      "Training Batch [707/782]: Loss 0.0027256545145064592\n",
      "Training Batch [708/782]: Loss 0.06331358850002289\n",
      "Training Batch [709/782]: Loss 0.018496477976441383\n",
      "Training Batch [710/782]: Loss 0.017995527014136314\n",
      "Training Batch [711/782]: Loss 0.0020141215063631535\n",
      "Training Batch [712/782]: Loss 0.04161906614899635\n",
      "Training Batch [713/782]: Loss 0.008504225872457027\n",
      "Training Batch [714/782]: Loss 0.0028962106443941593\n",
      "Training Batch [715/782]: Loss 0.0023279674351215363\n",
      "Training Batch [716/782]: Loss 0.08634672313928604\n",
      "Training Batch [717/782]: Loss 0.1092931255698204\n",
      "Training Batch [718/782]: Loss 0.02285050041973591\n",
      "Training Batch [719/782]: Loss 0.022246867418289185\n",
      "Training Batch [720/782]: Loss 0.03664704039692879\n",
      "Training Batch [721/782]: Loss 0.010317746549844742\n",
      "Training Batch [722/782]: Loss 0.00973617471754551\n",
      "Training Batch [723/782]: Loss 0.04531492292881012\n",
      "Training Batch [724/782]: Loss 0.011980297975242138\n",
      "Training Batch [725/782]: Loss 0.018709449097514153\n",
      "Training Batch [726/782]: Loss 0.0034049225505441427\n",
      "Training Batch [727/782]: Loss 0.0048262933269143105\n",
      "Training Batch [728/782]: Loss 0.0005994954262860119\n",
      "Training Batch [729/782]: Loss 0.012067087925970554\n",
      "Training Batch [730/782]: Loss 0.06972990185022354\n",
      "Training Batch [731/782]: Loss 0.02312266081571579\n",
      "Training Batch [732/782]: Loss 0.008725732564926147\n",
      "Training Batch [733/782]: Loss 0.02175089158117771\n",
      "Training Batch [734/782]: Loss 0.004410360939800739\n",
      "Training Batch [735/782]: Loss 0.0010330774821341038\n",
      "Training Batch [736/782]: Loss 0.02480684220790863\n",
      "Training Batch [737/782]: Loss 0.019147930666804314\n",
      "Training Batch [738/782]: Loss 0.05157363787293434\n",
      "Training Batch [739/782]: Loss 0.0044964090920984745\n",
      "Training Batch [740/782]: Loss 0.018390899524092674\n",
      "Training Batch [741/782]: Loss 0.010715541429817677\n",
      "Training Batch [742/782]: Loss 0.09524098038673401\n",
      "Training Batch [743/782]: Loss 0.003364067990332842\n",
      "Training Batch [744/782]: Loss 0.01791505143046379\n",
      "Training Batch [745/782]: Loss 0.024199416860938072\n",
      "Training Batch [746/782]: Loss 0.06002168357372284\n",
      "Training Batch [747/782]: Loss 0.017362389713525772\n",
      "Training Batch [748/782]: Loss 0.005948154721409082\n",
      "Training Batch [749/782]: Loss 0.010646592825651169\n",
      "Training Batch [750/782]: Loss 0.06808260828256607\n",
      "Training Batch [751/782]: Loss 0.15932844579219818\n",
      "Training Batch [752/782]: Loss 0.053774308413267136\n",
      "Training Batch [753/782]: Loss 0.039907366037368774\n",
      "Training Batch [754/782]: Loss 0.003085352946072817\n",
      "Training Batch [755/782]: Loss 0.00898595992475748\n",
      "Training Batch [756/782]: Loss 0.019385643303394318\n",
      "Training Batch [757/782]: Loss 0.07632550597190857\n",
      "Training Batch [758/782]: Loss 0.026030579581856728\n",
      "Training Batch [759/782]: Loss 0.0029048901051282883\n",
      "Training Batch [760/782]: Loss 0.009883943945169449\n",
      "Training Batch [761/782]: Loss 0.0337604321539402\n",
      "Training Batch [762/782]: Loss 0.07269609719514847\n",
      "Training Batch [763/782]: Loss 0.08837158232927322\n",
      "Training Batch [764/782]: Loss 0.021768249571323395\n",
      "Training Batch [765/782]: Loss 0.018254496157169342\n",
      "Training Batch [766/782]: Loss 0.05072891712188721\n",
      "Training Batch [767/782]: Loss 0.007058336865156889\n",
      "Training Batch [768/782]: Loss 0.0032413743901997805\n",
      "Training Batch [769/782]: Loss 0.003917510621249676\n",
      "Training Batch [770/782]: Loss 0.0021210492122918367\n",
      "Training Batch [771/782]: Loss 0.0665348619222641\n",
      "Training Batch [772/782]: Loss 0.001861166674643755\n",
      "Training Batch [773/782]: Loss 0.019111214205622673\n",
      "Training Batch [774/782]: Loss 0.009487494826316833\n",
      "Training Batch [775/782]: Loss 0.02749420702457428\n",
      "Training Batch [776/782]: Loss 0.04839730262756348\n",
      "Training Batch [777/782]: Loss 0.030225226655602455\n",
      "Training Batch [778/782]: Loss 0.042872194200754166\n",
      "Training Batch [779/782]: Loss 0.026833122596144676\n",
      "Training Batch [780/782]: Loss 0.04314595088362694\n",
      "Training Batch [781/782]: Loss 0.14206625521183014\n",
      "Training Batch [782/782]: Loss 0.012880316935479641\n",
      "Epoch 22 - Train Loss: 0.0242\n",
      "*********  Epoch 23/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.0145140765234828\n",
      "Training Batch [2/782]: Loss 0.0013894952135160565\n",
      "Training Batch [3/782]: Loss 0.017292054370045662\n",
      "Training Batch [4/782]: Loss 0.013316278345882893\n",
      "Training Batch [5/782]: Loss 0.011312225833535194\n",
      "Training Batch [6/782]: Loss 0.007698098197579384\n",
      "Training Batch [7/782]: Loss 0.008922107517719269\n",
      "Training Batch [8/782]: Loss 0.005012991838157177\n",
      "Training Batch [9/782]: Loss 0.0010921551147475839\n",
      "Training Batch [10/782]: Loss 0.010113932192325592\n",
      "Training Batch [11/782]: Loss 0.007872013375163078\n",
      "Training Batch [12/782]: Loss 0.01010710559785366\n",
      "Training Batch [13/782]: Loss 0.001880443305708468\n",
      "Training Batch [14/782]: Loss 0.00543553801253438\n",
      "Training Batch [15/782]: Loss 0.010335858911275864\n",
      "Training Batch [16/782]: Loss 0.037918370217084885\n",
      "Training Batch [17/782]: Loss 0.008979527279734612\n",
      "Training Batch [18/782]: Loss 0.047072261571884155\n",
      "Training Batch [19/782]: Loss 0.021940261125564575\n",
      "Training Batch [20/782]: Loss 0.004908327478915453\n",
      "Training Batch [21/782]: Loss 0.013357680290937424\n",
      "Training Batch [22/782]: Loss 0.0003625599492806941\n",
      "Training Batch [23/782]: Loss 0.02874474599957466\n",
      "Training Batch [24/782]: Loss 0.0005972885992377996\n",
      "Training Batch [25/782]: Loss 0.02636352740228176\n",
      "Training Batch [26/782]: Loss 0.007940590381622314\n",
      "Training Batch [27/782]: Loss 0.012465662322938442\n",
      "Training Batch [28/782]: Loss 0.05334309861063957\n",
      "Training Batch [29/782]: Loss 0.00317047699354589\n",
      "Training Batch [30/782]: Loss 0.062461744993925095\n",
      "Training Batch [31/782]: Loss 0.021904919296503067\n",
      "Training Batch [32/782]: Loss 0.045709278434515\n",
      "Training Batch [33/782]: Loss 0.003611483611166477\n",
      "Training Batch [34/782]: Loss 0.05277523770928383\n",
      "Training Batch [35/782]: Loss 0.07207094132900238\n",
      "Training Batch [36/782]: Loss 0.01956007443368435\n",
      "Training Batch [37/782]: Loss 0.009213202632963657\n",
      "Training Batch [38/782]: Loss 0.036844491958618164\n",
      "Training Batch [39/782]: Loss 0.002984835999086499\n",
      "Training Batch [40/782]: Loss 0.043613217771053314\n",
      "Training Batch [41/782]: Loss 0.08544647693634033\n",
      "Training Batch [42/782]: Loss 0.016047172248363495\n",
      "Training Batch [43/782]: Loss 0.012190666049718857\n",
      "Training Batch [44/782]: Loss 0.011639224365353584\n",
      "Training Batch [45/782]: Loss 0.1087133139371872\n",
      "Training Batch [46/782]: Loss 0.004159046337008476\n",
      "Training Batch [47/782]: Loss 0.017280524596571922\n",
      "Training Batch [48/782]: Loss 0.03647911921143532\n",
      "Training Batch [49/782]: Loss 0.06515508145093918\n",
      "Training Batch [50/782]: Loss 0.01840483583509922\n",
      "Training Batch [51/782]: Loss 0.005262096878141165\n",
      "Training Batch [52/782]: Loss 0.020727893337607384\n",
      "Training Batch [53/782]: Loss 0.008993718773126602\n",
      "Training Batch [54/782]: Loss 0.007366672158241272\n",
      "Training Batch [55/782]: Loss 0.054316695779561996\n",
      "Training Batch [56/782]: Loss 0.008214125409722328\n",
      "Training Batch [57/782]: Loss 0.005766667891293764\n",
      "Training Batch [58/782]: Loss 0.005997803527861834\n",
      "Training Batch [59/782]: Loss 0.01857246272265911\n",
      "Training Batch [60/782]: Loss 0.03429148718714714\n",
      "Training Batch [61/782]: Loss 0.04980017989873886\n",
      "Training Batch [62/782]: Loss 0.01365498173981905\n",
      "Training Batch [63/782]: Loss 0.006578790955245495\n",
      "Training Batch [64/782]: Loss 0.007596707437187433\n",
      "Training Batch [65/782]: Loss 0.002816962543874979\n",
      "Training Batch [66/782]: Loss 0.0022866816725581884\n",
      "Training Batch [67/782]: Loss 0.013660158962011337\n",
      "Training Batch [68/782]: Loss 0.004331415519118309\n",
      "Training Batch [69/782]: Loss 0.01955142244696617\n",
      "Training Batch [70/782]: Loss 0.00338574661873281\n",
      "Training Batch [71/782]: Loss 0.003364007920026779\n",
      "Training Batch [72/782]: Loss 0.004763387609273195\n",
      "Training Batch [73/782]: Loss 0.000987222883850336\n",
      "Training Batch [74/782]: Loss 0.0038655083626508713\n",
      "Training Batch [75/782]: Loss 0.003037804737687111\n",
      "Training Batch [76/782]: Loss 0.0471491776406765\n",
      "Training Batch [77/782]: Loss 0.01076733972877264\n",
      "Training Batch [78/782]: Loss 0.003907298669219017\n",
      "Training Batch [79/782]: Loss 0.02775338850915432\n",
      "Training Batch [80/782]: Loss 0.005505961365997791\n",
      "Training Batch [81/782]: Loss 0.0029516289941966534\n",
      "Training Batch [82/782]: Loss 0.014574945904314518\n",
      "Training Batch [83/782]: Loss 0.059892620891332626\n",
      "Training Batch [84/782]: Loss 0.03452131152153015\n",
      "Training Batch [85/782]: Loss 0.006669878959655762\n",
      "Training Batch [86/782]: Loss 0.021016400307416916\n",
      "Training Batch [87/782]: Loss 0.0038699470460414886\n",
      "Training Batch [88/782]: Loss 0.01268412359058857\n",
      "Training Batch [89/782]: Loss 0.00401132320985198\n",
      "Training Batch [90/782]: Loss 0.002799586160108447\n",
      "Training Batch [91/782]: Loss 0.044176992028951645\n",
      "Training Batch [92/782]: Loss 0.059219516813755035\n",
      "Training Batch [93/782]: Loss 0.016044981777668\n",
      "Training Batch [94/782]: Loss 0.005715507082641125\n",
      "Training Batch [95/782]: Loss 0.044300779700279236\n",
      "Training Batch [96/782]: Loss 0.010746576823294163\n",
      "Training Batch [97/782]: Loss 0.0038814861327409744\n",
      "Training Batch [98/782]: Loss 0.0006500718300230801\n",
      "Training Batch [99/782]: Loss 0.03657154366374016\n",
      "Training Batch [100/782]: Loss 0.027259603142738342\n",
      "Training Batch [101/782]: Loss 0.004049768205732107\n",
      "Training Batch [102/782]: Loss 0.0067656319588422775\n",
      "Training Batch [103/782]: Loss 0.02397882379591465\n",
      "Training Batch [104/782]: Loss 0.03478628396987915\n",
      "Training Batch [105/782]: Loss 0.027567556127905846\n",
      "Training Batch [106/782]: Loss 0.028130456805229187\n",
      "Training Batch [107/782]: Loss 0.0040705460123717785\n",
      "Training Batch [108/782]: Loss 0.00042112593655474484\n",
      "Training Batch [109/782]: Loss 0.04540041834115982\n",
      "Training Batch [110/782]: Loss 0.04316672310233116\n",
      "Training Batch [111/782]: Loss 0.003969176206737757\n",
      "Training Batch [112/782]: Loss 0.09488126635551453\n",
      "Training Batch [113/782]: Loss 0.017159543931484222\n",
      "Training Batch [114/782]: Loss 0.022297600284218788\n",
      "Training Batch [115/782]: Loss 0.035540346056222916\n",
      "Training Batch [116/782]: Loss 0.016677359119057655\n",
      "Training Batch [117/782]: Loss 0.007700042333453894\n",
      "Training Batch [118/782]: Loss 0.0017536195227876306\n",
      "Training Batch [119/782]: Loss 0.028280744329094887\n",
      "Training Batch [120/782]: Loss 0.012112993746995926\n",
      "Training Batch [121/782]: Loss 0.046535346657037735\n",
      "Training Batch [122/782]: Loss 0.004287798888981342\n",
      "Training Batch [123/782]: Loss 0.006144344806671143\n",
      "Training Batch [124/782]: Loss 0.03794997185468674\n",
      "Training Batch [125/782]: Loss 0.0047755492851138115\n",
      "Training Batch [126/782]: Loss 0.01104339212179184\n",
      "Training Batch [127/782]: Loss 0.004520724527537823\n",
      "Training Batch [128/782]: Loss 0.0175003819167614\n",
      "Training Batch [129/782]: Loss 0.015619839541614056\n",
      "Training Batch [130/782]: Loss 0.0032799888867884874\n",
      "Training Batch [131/782]: Loss 0.029505690559744835\n",
      "Training Batch [132/782]: Loss 0.005867844447493553\n",
      "Training Batch [133/782]: Loss 0.0013662190176546574\n",
      "Training Batch [134/782]: Loss 0.0020412809681147337\n",
      "Training Batch [135/782]: Loss 0.002283424837514758\n",
      "Training Batch [136/782]: Loss 0.003766688983887434\n",
      "Training Batch [137/782]: Loss 0.02522527240216732\n",
      "Training Batch [138/782]: Loss 0.009129177778959274\n",
      "Training Batch [139/782]: Loss 0.0017794922459870577\n",
      "Training Batch [140/782]: Loss 0.07517159730195999\n",
      "Training Batch [141/782]: Loss 0.004722845274955034\n",
      "Training Batch [142/782]: Loss 0.01083617378026247\n",
      "Training Batch [143/782]: Loss 0.11112874001264572\n",
      "Training Batch [144/782]: Loss 0.02702714502811432\n",
      "Training Batch [145/782]: Loss 0.010380160063505173\n",
      "Training Batch [146/782]: Loss 0.08698287606239319\n",
      "Training Batch [147/782]: Loss 0.032713282853364944\n",
      "Training Batch [148/782]: Loss 0.0009638539049774408\n",
      "Training Batch [149/782]: Loss 0.0045393118634819984\n",
      "Training Batch [150/782]: Loss 0.007352773565798998\n",
      "Training Batch [151/782]: Loss 0.0027593092527240515\n",
      "Training Batch [152/782]: Loss 0.0011450480669736862\n",
      "Training Batch [153/782]: Loss 0.039304789155721664\n",
      "Training Batch [154/782]: Loss 0.009820451959967613\n",
      "Training Batch [155/782]: Loss 0.015558084473013878\n",
      "Training Batch [156/782]: Loss 0.0018801818368956447\n",
      "Training Batch [157/782]: Loss 0.0020176158286631107\n",
      "Training Batch [158/782]: Loss 0.00586731918156147\n",
      "Training Batch [159/782]: Loss 0.0023953262716531754\n",
      "Training Batch [160/782]: Loss 0.008264693431556225\n",
      "Training Batch [161/782]: Loss 0.008969128131866455\n",
      "Training Batch [162/782]: Loss 0.0003902225289493799\n",
      "Training Batch [163/782]: Loss 0.00514049781486392\n",
      "Training Batch [164/782]: Loss 0.012754194438457489\n",
      "Training Batch [165/782]: Loss 0.02069205977022648\n",
      "Training Batch [166/782]: Loss 0.019222386181354523\n",
      "Training Batch [167/782]: Loss 0.028907567262649536\n",
      "Training Batch [168/782]: Loss 0.0016025869408622384\n",
      "Training Batch [169/782]: Loss 0.0037471193354576826\n",
      "Training Batch [170/782]: Loss 0.006540191359817982\n",
      "Training Batch [171/782]: Loss 0.0045107267796993256\n",
      "Training Batch [172/782]: Loss 0.010006998665630817\n",
      "Training Batch [173/782]: Loss 0.0053052231669425964\n",
      "Training Batch [174/782]: Loss 0.0007649731705896556\n",
      "Training Batch [175/782]: Loss 0.002029413590207696\n",
      "Training Batch [176/782]: Loss 0.0011785101378336549\n",
      "Training Batch [177/782]: Loss 0.0019099140772596002\n",
      "Training Batch [178/782]: Loss 0.03872932121157646\n",
      "Training Batch [179/782]: Loss 0.0013686909805983305\n",
      "Training Batch [180/782]: Loss 0.010914882645010948\n",
      "Training Batch [181/782]: Loss 0.006780835334211588\n",
      "Training Batch [182/782]: Loss 0.0007130776648409665\n",
      "Training Batch [183/782]: Loss 0.0005219397717155516\n",
      "Training Batch [184/782]: Loss 0.013205774128437042\n",
      "Training Batch [185/782]: Loss 0.008098932914435863\n",
      "Training Batch [186/782]: Loss 0.002697853371500969\n",
      "Training Batch [187/782]: Loss 0.005708958022296429\n",
      "Training Batch [188/782]: Loss 0.004575611092150211\n",
      "Training Batch [189/782]: Loss 0.006611839402467012\n",
      "Training Batch [190/782]: Loss 0.0023138215765357018\n",
      "Training Batch [191/782]: Loss 0.01358261052519083\n",
      "Training Batch [192/782]: Loss 0.008251006714999676\n",
      "Training Batch [193/782]: Loss 0.006641712971031666\n",
      "Training Batch [194/782]: Loss 0.020356472581624985\n",
      "Training Batch [195/782]: Loss 0.0007571608875878155\n",
      "Training Batch [196/782]: Loss 0.001288273953832686\n",
      "Training Batch [197/782]: Loss 0.0032522878609597683\n",
      "Training Batch [198/782]: Loss 0.003150509437546134\n",
      "Training Batch [199/782]: Loss 0.006587618961930275\n",
      "Training Batch [200/782]: Loss 0.04053692892193794\n",
      "Training Batch [201/782]: Loss 0.004032678436487913\n",
      "Training Batch [202/782]: Loss 0.0004599287058226764\n",
      "Training Batch [203/782]: Loss 0.003793462412431836\n",
      "Training Batch [204/782]: Loss 0.0008371846051886678\n",
      "Training Batch [205/782]: Loss 0.0012449654750525951\n",
      "Training Batch [206/782]: Loss 0.0006717282230965793\n",
      "Training Batch [207/782]: Loss 0.0014232390094548464\n",
      "Training Batch [208/782]: Loss 0.0038296154234558344\n",
      "Training Batch [209/782]: Loss 0.0022965185344219208\n",
      "Training Batch [210/782]: Loss 0.001404103939421475\n",
      "Training Batch [211/782]: Loss 0.00745808333158493\n",
      "Training Batch [212/782]: Loss 0.001258055679500103\n",
      "Training Batch [213/782]: Loss 0.003738576779142022\n",
      "Training Batch [214/782]: Loss 0.004170625936239958\n",
      "Training Batch [215/782]: Loss 0.03768724203109741\n",
      "Training Batch [216/782]: Loss 0.0019860914908349514\n",
      "Training Batch [217/782]: Loss 0.001770627684891224\n",
      "Training Batch [218/782]: Loss 0.0011456453939899802\n",
      "Training Batch [219/782]: Loss 0.000588244991376996\n",
      "Training Batch [220/782]: Loss 0.00111674633808434\n",
      "Training Batch [221/782]: Loss 0.0016397391445934772\n",
      "Training Batch [222/782]: Loss 0.07529538124799728\n",
      "Training Batch [223/782]: Loss 0.009112279862165451\n",
      "Training Batch [224/782]: Loss 0.011232234537601471\n",
      "Training Batch [225/782]: Loss 0.0509142242372036\n",
      "Training Batch [226/782]: Loss 0.0019511154387146235\n",
      "Training Batch [227/782]: Loss 0.048881012946367264\n",
      "Training Batch [228/782]: Loss 0.0010925217065960169\n",
      "Training Batch [229/782]: Loss 0.010921859182417393\n",
      "Training Batch [230/782]: Loss 0.038048356771469116\n",
      "Training Batch [231/782]: Loss 0.00436024833470583\n",
      "Training Batch [232/782]: Loss 0.0020685766357928514\n",
      "Training Batch [233/782]: Loss 0.007866018451750278\n",
      "Training Batch [234/782]: Loss 0.00411564763635397\n",
      "Training Batch [235/782]: Loss 0.003672380931675434\n",
      "Training Batch [236/782]: Loss 0.003592502558603883\n",
      "Training Batch [237/782]: Loss 0.00177067203912884\n",
      "Training Batch [238/782]: Loss 0.0027038143016397953\n",
      "Training Batch [239/782]: Loss 0.0037680123932659626\n",
      "Training Batch [240/782]: Loss 0.07243021577596664\n",
      "Training Batch [241/782]: Loss 0.018951334059238434\n",
      "Training Batch [242/782]: Loss 0.003338068723678589\n",
      "Training Batch [243/782]: Loss 0.04862522333860397\n",
      "Training Batch [244/782]: Loss 0.006296265870332718\n",
      "Training Batch [245/782]: Loss 0.05589304119348526\n",
      "Training Batch [246/782]: Loss 0.005504038650542498\n",
      "Training Batch [247/782]: Loss 0.002356208162382245\n",
      "Training Batch [248/782]: Loss 0.003095318330451846\n",
      "Training Batch [249/782]: Loss 0.0009930531959980726\n",
      "Training Batch [250/782]: Loss 0.015704181045293808\n",
      "Training Batch [251/782]: Loss 0.022429028525948524\n",
      "Training Batch [252/782]: Loss 0.007055620197206736\n",
      "Training Batch [253/782]: Loss 0.0032171811908483505\n",
      "Training Batch [254/782]: Loss 0.0015105977654457092\n",
      "Training Batch [255/782]: Loss 0.0009069856605492532\n",
      "Training Batch [256/782]: Loss 0.02978944592177868\n",
      "Training Batch [257/782]: Loss 0.003382166149094701\n",
      "Training Batch [258/782]: Loss 0.02557445876300335\n",
      "Training Batch [259/782]: Loss 0.012914318591356277\n",
      "Training Batch [260/782]: Loss 0.0053793867118656635\n",
      "Training Batch [261/782]: Loss 0.04903886839747429\n",
      "Training Batch [262/782]: Loss 0.037482619285583496\n",
      "Training Batch [263/782]: Loss 0.03162434324622154\n",
      "Training Batch [264/782]: Loss 0.0009072893881238997\n",
      "Training Batch [265/782]: Loss 0.008406848646700382\n",
      "Training Batch [266/782]: Loss 0.008212185464799404\n",
      "Training Batch [267/782]: Loss 0.02700904943048954\n",
      "Training Batch [268/782]: Loss 0.0014292382402345538\n",
      "Training Batch [269/782]: Loss 0.006838314235210419\n",
      "Training Batch [270/782]: Loss 0.010576467029750347\n",
      "Training Batch [271/782]: Loss 0.0020139338448643684\n",
      "Training Batch [272/782]: Loss 0.008954968303442001\n",
      "Training Batch [273/782]: Loss 0.00652316864579916\n",
      "Training Batch [274/782]: Loss 0.0023748655803501606\n",
      "Training Batch [275/782]: Loss 0.019028685986995697\n",
      "Training Batch [276/782]: Loss 0.023563314229249954\n",
      "Training Batch [277/782]: Loss 0.004905230365693569\n",
      "Training Batch [278/782]: Loss 0.0017974106594920158\n",
      "Training Batch [279/782]: Loss 0.047817740589380264\n",
      "Training Batch [280/782]: Loss 0.00880548357963562\n",
      "Training Batch [281/782]: Loss 0.003565381048247218\n",
      "Training Batch [282/782]: Loss 0.001982057001441717\n",
      "Training Batch [283/782]: Loss 0.0015514654805883765\n",
      "Training Batch [284/782]: Loss 0.007800607942044735\n",
      "Training Batch [285/782]: Loss 0.021010257303714752\n",
      "Training Batch [286/782]: Loss 0.012585515156388283\n",
      "Training Batch [287/782]: Loss 0.0074669476598501205\n",
      "Training Batch [288/782]: Loss 0.06847376376390457\n",
      "Training Batch [289/782]: Loss 0.0030626561492681503\n",
      "Training Batch [290/782]: Loss 0.0016380323795601726\n",
      "Training Batch [291/782]: Loss 0.0058695003390312195\n",
      "Training Batch [292/782]: Loss 0.02213071845471859\n",
      "Training Batch [293/782]: Loss 0.02618861384689808\n",
      "Training Batch [294/782]: Loss 0.01590711437165737\n",
      "Training Batch [295/782]: Loss 0.0034519697073847055\n",
      "Training Batch [296/782]: Loss 0.001118933199904859\n",
      "Training Batch [297/782]: Loss 0.005454438738524914\n",
      "Training Batch [298/782]: Loss 0.0010065651731565595\n",
      "Training Batch [299/782]: Loss 0.03254162147641182\n",
      "Training Batch [300/782]: Loss 0.002278759377077222\n",
      "Training Batch [301/782]: Loss 0.0011218877043575048\n",
      "Training Batch [302/782]: Loss 0.003763563232496381\n",
      "Training Batch [303/782]: Loss 0.031490858644247055\n",
      "Training Batch [304/782]: Loss 0.0023130038753151894\n",
      "Training Batch [305/782]: Loss 0.004348432645201683\n",
      "Training Batch [306/782]: Loss 0.006052583456039429\n",
      "Training Batch [307/782]: Loss 0.010040546767413616\n",
      "Training Batch [308/782]: Loss 0.0008434485644102097\n",
      "Training Batch [309/782]: Loss 0.04349139705300331\n",
      "Training Batch [310/782]: Loss 0.09448739141225815\n",
      "Training Batch [311/782]: Loss 0.0012541261967271566\n",
      "Training Batch [312/782]: Loss 0.005844406317919493\n",
      "Training Batch [313/782]: Loss 0.061823178082704544\n",
      "Training Batch [314/782]: Loss 0.04298713058233261\n",
      "Training Batch [315/782]: Loss 0.0247508455067873\n",
      "Training Batch [316/782]: Loss 0.0047624255530536175\n",
      "Training Batch [317/782]: Loss 0.0018347820732742548\n",
      "Training Batch [318/782]: Loss 0.001192591618746519\n",
      "Training Batch [319/782]: Loss 0.0013617330696433783\n",
      "Training Batch [320/782]: Loss 0.03827705234289169\n",
      "Training Batch [321/782]: Loss 0.03939790278673172\n",
      "Training Batch [322/782]: Loss 0.000861103180795908\n",
      "Training Batch [323/782]: Loss 0.020363332703709602\n",
      "Training Batch [324/782]: Loss 0.018357906490564346\n",
      "Training Batch [325/782]: Loss 0.040650397539138794\n",
      "Training Batch [326/782]: Loss 0.02176901139318943\n",
      "Training Batch [327/782]: Loss 0.014152145013213158\n",
      "Training Batch [328/782]: Loss 0.024834593757987022\n",
      "Training Batch [329/782]: Loss 0.005800296552479267\n",
      "Training Batch [330/782]: Loss 0.008353136479854584\n",
      "Training Batch [331/782]: Loss 0.07097268849611282\n",
      "Training Batch [332/782]: Loss 0.03195825219154358\n",
      "Training Batch [333/782]: Loss 0.007662887219339609\n",
      "Training Batch [334/782]: Loss 0.012077839113771915\n",
      "Training Batch [335/782]: Loss 0.028202181681990623\n",
      "Training Batch [336/782]: Loss 0.002942140679806471\n",
      "Training Batch [337/782]: Loss 0.03545455262064934\n",
      "Training Batch [338/782]: Loss 0.008985524997115135\n",
      "Training Batch [339/782]: Loss 0.10237717628479004\n",
      "Training Batch [340/782]: Loss 0.002706278581172228\n",
      "Training Batch [341/782]: Loss 0.0009812067728489637\n",
      "Training Batch [342/782]: Loss 0.018111485987901688\n",
      "Training Batch [343/782]: Loss 0.020505663007497787\n",
      "Training Batch [344/782]: Loss 0.009553659707307816\n",
      "Training Batch [345/782]: Loss 0.0163926649838686\n",
      "Training Batch [346/782]: Loss 0.0390002578496933\n",
      "Training Batch [347/782]: Loss 0.00859716534614563\n",
      "Training Batch [348/782]: Loss 0.0009101944742724299\n",
      "Training Batch [349/782]: Loss 0.05448969081044197\n",
      "Training Batch [350/782]: Loss 0.010632505640387535\n",
      "Training Batch [351/782]: Loss 0.031000137329101562\n",
      "Training Batch [352/782]: Loss 0.0029654912650585175\n",
      "Training Batch [353/782]: Loss 0.025267500430345535\n",
      "Training Batch [354/782]: Loss 0.026237167418003082\n",
      "Training Batch [355/782]: Loss 0.005068864207714796\n",
      "Training Batch [356/782]: Loss 0.07713771611452103\n",
      "Training Batch [357/782]: Loss 0.11443476378917694\n",
      "Training Batch [358/782]: Loss 0.003825817257165909\n",
      "Training Batch [359/782]: Loss 0.005275927018374205\n",
      "Training Batch [360/782]: Loss 0.0013854169519618154\n",
      "Training Batch [361/782]: Loss 0.027253448963165283\n",
      "Training Batch [362/782]: Loss 0.10724490880966187\n",
      "Training Batch [363/782]: Loss 0.002379582030698657\n",
      "Training Batch [364/782]: Loss 0.03192390128970146\n",
      "Training Batch [365/782]: Loss 0.004717772826552391\n",
      "Training Batch [366/782]: Loss 0.06329602748155594\n",
      "Training Batch [367/782]: Loss 0.06826597452163696\n",
      "Training Batch [368/782]: Loss 0.0015795108629390597\n",
      "Training Batch [369/782]: Loss 0.016594570130109787\n",
      "Training Batch [370/782]: Loss 0.025300955399870872\n",
      "Training Batch [371/782]: Loss 0.06817389279603958\n",
      "Training Batch [372/782]: Loss 0.0006598323816433549\n",
      "Training Batch [373/782]: Loss 0.005307405721396208\n",
      "Training Batch [374/782]: Loss 0.031236832961440086\n",
      "Training Batch [375/782]: Loss 0.00977600272744894\n",
      "Training Batch [376/782]: Loss 0.016228284686803818\n",
      "Training Batch [377/782]: Loss 0.08513551205396652\n",
      "Training Batch [378/782]: Loss 0.0025853444822132587\n",
      "Training Batch [379/782]: Loss 0.001959843561053276\n",
      "Training Batch [380/782]: Loss 0.030904458835721016\n",
      "Training Batch [381/782]: Loss 0.014441905543208122\n",
      "Training Batch [382/782]: Loss 0.00468556908890605\n",
      "Training Batch [383/782]: Loss 0.011627981439232826\n",
      "Training Batch [384/782]: Loss 0.07875131070613861\n",
      "Training Batch [385/782]: Loss 0.018294841051101685\n",
      "Training Batch [386/782]: Loss 0.016532637178897858\n",
      "Training Batch [387/782]: Loss 0.008571181446313858\n",
      "Training Batch [388/782]: Loss 0.021647443994879723\n",
      "Training Batch [389/782]: Loss 0.0027518614660948515\n",
      "Training Batch [390/782]: Loss 0.0037028880324214697\n",
      "Training Batch [391/782]: Loss 0.05060543119907379\n",
      "Training Batch [392/782]: Loss 0.016196468845009804\n",
      "Training Batch [393/782]: Loss 0.018175244331359863\n",
      "Training Batch [394/782]: Loss 0.0093685956671834\n",
      "Training Batch [395/782]: Loss 0.038708582520484924\n",
      "Training Batch [396/782]: Loss 0.048561256378889084\n",
      "Training Batch [397/782]: Loss 0.0024200421757996082\n",
      "Training Batch [398/782]: Loss 0.0011064704740419984\n",
      "Training Batch [399/782]: Loss 0.10303042083978653\n",
      "Training Batch [400/782]: Loss 0.004329019691795111\n",
      "Training Batch [401/782]: Loss 0.030952610075473785\n",
      "Training Batch [402/782]: Loss 0.012670658528804779\n",
      "Training Batch [403/782]: Loss 0.0013140644878149033\n",
      "Training Batch [404/782]: Loss 0.006644018460065126\n",
      "Training Batch [405/782]: Loss 0.0107024060562253\n",
      "Training Batch [406/782]: Loss 0.039555683732032776\n",
      "Training Batch [407/782]: Loss 0.007015329785645008\n",
      "Training Batch [408/782]: Loss 0.015294644050300121\n",
      "Training Batch [409/782]: Loss 0.0421297624707222\n",
      "Training Batch [410/782]: Loss 0.0016343449242413044\n",
      "Training Batch [411/782]: Loss 0.020656457170844078\n",
      "Training Batch [412/782]: Loss 0.027185289189219475\n",
      "Training Batch [413/782]: Loss 0.009304237551987171\n",
      "Training Batch [414/782]: Loss 0.0037785645108669996\n",
      "Training Batch [415/782]: Loss 0.003728872397914529\n",
      "Training Batch [416/782]: Loss 0.0021753956098109484\n",
      "Training Batch [417/782]: Loss 0.0030027893371880054\n",
      "Training Batch [418/782]: Loss 0.007610870525240898\n",
      "Training Batch [419/782]: Loss 0.0047986991703510284\n",
      "Training Batch [420/782]: Loss 0.010948075912892818\n",
      "Training Batch [421/782]: Loss 0.08238547295331955\n",
      "Training Batch [422/782]: Loss 0.0276216808706522\n",
      "Training Batch [423/782]: Loss 0.0033503836020827293\n",
      "Training Batch [424/782]: Loss 0.07713097333908081\n",
      "Training Batch [425/782]: Loss 0.022745303809642792\n",
      "Training Batch [426/782]: Loss 0.000998800154775381\n",
      "Training Batch [427/782]: Loss 0.010295597836375237\n",
      "Training Batch [428/782]: Loss 0.008202548138797283\n",
      "Training Batch [429/782]: Loss 0.004780794493854046\n",
      "Training Batch [430/782]: Loss 0.05931217223405838\n",
      "Training Batch [431/782]: Loss 0.003910381812602282\n",
      "Training Batch [432/782]: Loss 0.07822650671005249\n",
      "Training Batch [433/782]: Loss 0.001890423707664013\n",
      "Training Batch [434/782]: Loss 0.008560932241380215\n",
      "Training Batch [435/782]: Loss 0.0027541944291442633\n",
      "Training Batch [436/782]: Loss 0.02199750766158104\n",
      "Training Batch [437/782]: Loss 0.03267106041312218\n",
      "Training Batch [438/782]: Loss 0.06890582293272018\n",
      "Training Batch [439/782]: Loss 0.09416401386260986\n",
      "Training Batch [440/782]: Loss 0.004125891253352165\n",
      "Training Batch [441/782]: Loss 0.0005877984804101288\n",
      "Training Batch [442/782]: Loss 0.03025127574801445\n",
      "Training Batch [443/782]: Loss 0.06273307651281357\n",
      "Training Batch [444/782]: Loss 0.08101323992013931\n",
      "Training Batch [445/782]: Loss 0.05339711159467697\n",
      "Training Batch [446/782]: Loss 0.0071638296358287334\n",
      "Training Batch [447/782]: Loss 0.0025939769111573696\n",
      "Training Batch [448/782]: Loss 0.022142687812447548\n",
      "Training Batch [449/782]: Loss 0.009413493797183037\n",
      "Training Batch [450/782]: Loss 0.008894299156963825\n",
      "Training Batch [451/782]: Loss 0.034235186874866486\n",
      "Training Batch [452/782]: Loss 0.11176914721727371\n",
      "Training Batch [453/782]: Loss 0.008675678633153439\n",
      "Training Batch [454/782]: Loss 0.04492763802409172\n",
      "Training Batch [455/782]: Loss 0.00699104368686676\n",
      "Training Batch [456/782]: Loss 0.004888167604804039\n",
      "Training Batch [457/782]: Loss 0.01256483606994152\n",
      "Training Batch [458/782]: Loss 0.048894476145505905\n",
      "Training Batch [459/782]: Loss 0.011762118898332119\n",
      "Training Batch [460/782]: Loss 0.006149143911898136\n",
      "Training Batch [461/782]: Loss 0.00905599258840084\n",
      "Training Batch [462/782]: Loss 0.01630593277513981\n",
      "Training Batch [463/782]: Loss 0.009798738174140453\n",
      "Training Batch [464/782]: Loss 0.0030721225775778294\n",
      "Training Batch [465/782]: Loss 0.01851087622344494\n",
      "Training Batch [466/782]: Loss 0.006877511274069548\n",
      "Training Batch [467/782]: Loss 0.03426431491971016\n",
      "Training Batch [468/782]: Loss 0.02914615347981453\n",
      "Training Batch [469/782]: Loss 0.016820335760712624\n",
      "Training Batch [470/782]: Loss 0.020700741559267044\n",
      "Training Batch [471/782]: Loss 0.04350905120372772\n",
      "Training Batch [472/782]: Loss 0.10936558991670609\n",
      "Training Batch [473/782]: Loss 0.018674427643418312\n",
      "Training Batch [474/782]: Loss 0.07759588956832886\n",
      "Training Batch [475/782]: Loss 0.010658646933734417\n",
      "Training Batch [476/782]: Loss 0.01711062714457512\n",
      "Training Batch [477/782]: Loss 0.04718666523694992\n",
      "Training Batch [478/782]: Loss 0.007555972784757614\n",
      "Training Batch [479/782]: Loss 0.0014305789954960346\n",
      "Training Batch [480/782]: Loss 0.005740816239267588\n",
      "Training Batch [481/782]: Loss 0.024510469287633896\n",
      "Training Batch [482/782]: Loss 0.016672074794769287\n",
      "Training Batch [483/782]: Loss 0.0657404288649559\n",
      "Training Batch [484/782]: Loss 0.007514827884733677\n",
      "Training Batch [485/782]: Loss 0.018454445526003838\n",
      "Training Batch [486/782]: Loss 0.02462867833673954\n",
      "Training Batch [487/782]: Loss 0.02742847241461277\n",
      "Training Batch [488/782]: Loss 0.020984765142202377\n",
      "Training Batch [489/782]: Loss 0.03142513334751129\n",
      "Training Batch [490/782]: Loss 0.0030624063219875097\n",
      "Training Batch [491/782]: Loss 0.11768728494644165\n",
      "Training Batch [492/782]: Loss 0.017107393592596054\n",
      "Training Batch [493/782]: Loss 0.006089778617024422\n",
      "Training Batch [494/782]: Loss 0.01402345672249794\n",
      "Training Batch [495/782]: Loss 0.0021447446197271347\n",
      "Training Batch [496/782]: Loss 0.04326143488287926\n",
      "Training Batch [497/782]: Loss 0.011956962756812572\n",
      "Training Batch [498/782]: Loss 0.0087179746478796\n",
      "Training Batch [499/782]: Loss 0.03236907720565796\n",
      "Training Batch [500/782]: Loss 0.004824524279683828\n",
      "Training Batch [501/782]: Loss 0.020452355965971947\n",
      "Training Batch [502/782]: Loss 0.06385423243045807\n",
      "Training Batch [503/782]: Loss 0.029507800936698914\n",
      "Training Batch [504/782]: Loss 0.037620726972818375\n",
      "Training Batch [505/782]: Loss 0.09157846122980118\n",
      "Training Batch [506/782]: Loss 0.0022514578886330128\n",
      "Training Batch [507/782]: Loss 0.06596401333808899\n",
      "Training Batch [508/782]: Loss 0.008140504360198975\n",
      "Training Batch [509/782]: Loss 0.08907487988471985\n",
      "Training Batch [510/782]: Loss 0.005194767843931913\n",
      "Training Batch [511/782]: Loss 0.014134355820715427\n",
      "Training Batch [512/782]: Loss 0.007666225545108318\n",
      "Training Batch [513/782]: Loss 0.02163819782435894\n",
      "Training Batch [514/782]: Loss 0.001140784122981131\n",
      "Training Batch [515/782]: Loss 0.059202246367931366\n",
      "Training Batch [516/782]: Loss 0.028460318222641945\n",
      "Training Batch [517/782]: Loss 0.010844536125659943\n",
      "Training Batch [518/782]: Loss 0.008020197041332722\n",
      "Training Batch [519/782]: Loss 0.005734413396567106\n",
      "Training Batch [520/782]: Loss 0.008424094878137112\n",
      "Training Batch [521/782]: Loss 0.0023994173388928175\n",
      "Training Batch [522/782]: Loss 0.05667562782764435\n",
      "Training Batch [523/782]: Loss 0.01147589273750782\n",
      "Training Batch [524/782]: Loss 0.07266609370708466\n",
      "Training Batch [525/782]: Loss 0.018281681463122368\n",
      "Training Batch [526/782]: Loss 0.004719414748251438\n",
      "Training Batch [527/782]: Loss 0.014965539798140526\n",
      "Training Batch [528/782]: Loss 0.009298212826251984\n",
      "Training Batch [529/782]: Loss 0.008882187306880951\n",
      "Training Batch [530/782]: Loss 0.008638535626232624\n",
      "Training Batch [531/782]: Loss 0.09097705036401749\n",
      "Training Batch [532/782]: Loss 0.029892979189753532\n",
      "Training Batch [533/782]: Loss 0.0073754750192165375\n",
      "Training Batch [534/782]: Loss 0.002831741701811552\n",
      "Training Batch [535/782]: Loss 0.007671711966395378\n",
      "Training Batch [536/782]: Loss 0.020176956430077553\n",
      "Training Batch [537/782]: Loss 0.01803787797689438\n",
      "Training Batch [538/782]: Loss 0.027102774009108543\n",
      "Training Batch [539/782]: Loss 0.004380309022963047\n",
      "Training Batch [540/782]: Loss 0.0058907088823616505\n",
      "Training Batch [541/782]: Loss 0.010772021487355232\n",
      "Training Batch [542/782]: Loss 0.010564056225121021\n",
      "Training Batch [543/782]: Loss 0.005056892056018114\n",
      "Training Batch [544/782]: Loss 0.005595373921096325\n",
      "Training Batch [545/782]: Loss 0.02108428068459034\n",
      "Training Batch [546/782]: Loss 0.05101816728711128\n",
      "Training Batch [547/782]: Loss 0.052076466381549835\n",
      "Training Batch [548/782]: Loss 0.005313130561262369\n",
      "Training Batch [549/782]: Loss 0.04887198284268379\n",
      "Training Batch [550/782]: Loss 0.00713608181104064\n",
      "Training Batch [551/782]: Loss 0.009828850626945496\n",
      "Training Batch [552/782]: Loss 0.0009355196380056441\n",
      "Training Batch [553/782]: Loss 0.0911279246211052\n",
      "Training Batch [554/782]: Loss 0.0036383799742907286\n",
      "Training Batch [555/782]: Loss 0.004051537252962589\n",
      "Training Batch [556/782]: Loss 0.00447916379198432\n",
      "Training Batch [557/782]: Loss 0.008932363241910934\n",
      "Training Batch [558/782]: Loss 0.025027459487318993\n",
      "Training Batch [559/782]: Loss 0.058456260710954666\n",
      "Training Batch [560/782]: Loss 0.043228812515735626\n",
      "Training Batch [561/782]: Loss 0.013187527656555176\n",
      "Training Batch [562/782]: Loss 0.03787555918097496\n",
      "Training Batch [563/782]: Loss 0.07750479876995087\n",
      "Training Batch [564/782]: Loss 0.10099965333938599\n",
      "Training Batch [565/782]: Loss 0.008339675143361092\n",
      "Training Batch [566/782]: Loss 0.033810924738645554\n",
      "Training Batch [567/782]: Loss 0.004650619346648455\n",
      "Training Batch [568/782]: Loss 0.016027318313717842\n",
      "Training Batch [569/782]: Loss 0.035852499306201935\n",
      "Training Batch [570/782]: Loss 0.001168054062873125\n",
      "Training Batch [571/782]: Loss 0.003671832149848342\n",
      "Training Batch [572/782]: Loss 0.03908741846680641\n",
      "Training Batch [573/782]: Loss 0.031635940074920654\n",
      "Training Batch [574/782]: Loss 0.047745782881975174\n",
      "Training Batch [575/782]: Loss 0.03042592667043209\n",
      "Training Batch [576/782]: Loss 0.020081540569663048\n",
      "Training Batch [577/782]: Loss 0.0775904506444931\n",
      "Training Batch [578/782]: Loss 0.045090265572071075\n",
      "Training Batch [579/782]: Loss 0.03519129380583763\n",
      "Training Batch [580/782]: Loss 0.0365854874253273\n",
      "Training Batch [581/782]: Loss 0.05165203660726547\n",
      "Training Batch [582/782]: Loss 0.021475572139024734\n",
      "Training Batch [583/782]: Loss 0.013120493851602077\n",
      "Training Batch [584/782]: Loss 0.0030701651703566313\n",
      "Training Batch [585/782]: Loss 0.04835004732012749\n",
      "Training Batch [586/782]: Loss 0.047472331672906876\n",
      "Training Batch [587/782]: Loss 0.032858237624168396\n",
      "Training Batch [588/782]: Loss 0.005024101585149765\n",
      "Training Batch [589/782]: Loss 0.04248228669166565\n",
      "Training Batch [590/782]: Loss 0.05753522366285324\n",
      "Training Batch [591/782]: Loss 0.04076971113681793\n",
      "Training Batch [592/782]: Loss 0.10101263970136642\n",
      "Training Batch [593/782]: Loss 0.051751647144556046\n",
      "Training Batch [594/782]: Loss 0.00713981781154871\n",
      "Training Batch [595/782]: Loss 0.016663165763020515\n",
      "Training Batch [596/782]: Loss 0.007283920422196388\n",
      "Training Batch [597/782]: Loss 0.00617130845785141\n",
      "Training Batch [598/782]: Loss 0.014954058453440666\n",
      "Training Batch [599/782]: Loss 0.018943732604384422\n",
      "Training Batch [600/782]: Loss 0.04893781989812851\n",
      "Training Batch [601/782]: Loss 0.01975376531481743\n",
      "Training Batch [602/782]: Loss 0.07965481281280518\n",
      "Training Batch [603/782]: Loss 0.07643847167491913\n",
      "Training Batch [604/782]: Loss 0.009460854344069958\n",
      "Training Batch [605/782]: Loss 0.030376875773072243\n",
      "Training Batch [606/782]: Loss 0.023183804005384445\n",
      "Training Batch [607/782]: Loss 0.02272122912108898\n",
      "Training Batch [608/782]: Loss 0.013067414052784443\n",
      "Training Batch [609/782]: Loss 0.06727085262537003\n",
      "Training Batch [610/782]: Loss 0.012571245431900024\n",
      "Training Batch [611/782]: Loss 0.023277686908841133\n",
      "Training Batch [612/782]: Loss 0.02609858103096485\n",
      "Training Batch [613/782]: Loss 0.045828819274902344\n",
      "Training Batch [614/782]: Loss 0.0031981600914150476\n",
      "Training Batch [615/782]: Loss 0.03299747034907341\n",
      "Training Batch [616/782]: Loss 0.23794515430927277\n",
      "Training Batch [617/782]: Loss 0.03767586499452591\n",
      "Training Batch [618/782]: Loss 0.06424368172883987\n",
      "Training Batch [619/782]: Loss 0.03623264282941818\n",
      "Training Batch [620/782]: Loss 0.018634343519806862\n",
      "Training Batch [621/782]: Loss 0.017278073355555534\n",
      "Training Batch [622/782]: Loss 0.03428027033805847\n",
      "Training Batch [623/782]: Loss 0.08577927201986313\n",
      "Training Batch [624/782]: Loss 0.00927604641765356\n",
      "Training Batch [625/782]: Loss 0.03820556774735451\n",
      "Training Batch [626/782]: Loss 0.0012332372134551406\n",
      "Training Batch [627/782]: Loss 0.09217390418052673\n",
      "Training Batch [628/782]: Loss 0.04081251472234726\n",
      "Training Batch [629/782]: Loss 0.029584452509880066\n",
      "Training Batch [630/782]: Loss 0.08728798478841782\n",
      "Training Batch [631/782]: Loss 0.04388933628797531\n",
      "Training Batch [632/782]: Loss 0.06579416990280151\n",
      "Training Batch [633/782]: Loss 0.012831189669668674\n",
      "Training Batch [634/782]: Loss 0.007942973636090755\n",
      "Training Batch [635/782]: Loss 0.002591040451079607\n",
      "Training Batch [636/782]: Loss 0.025730224326252937\n",
      "Training Batch [637/782]: Loss 0.049245186150074005\n",
      "Training Batch [638/782]: Loss 0.19694821536540985\n",
      "Training Batch [639/782]: Loss 0.01977628655731678\n",
      "Training Batch [640/782]: Loss 0.059759072959423065\n",
      "Training Batch [641/782]: Loss 0.006567200645804405\n",
      "Training Batch [642/782]: Loss 0.1477181315422058\n",
      "Training Batch [643/782]: Loss 0.03717649355530739\n",
      "Training Batch [644/782]: Loss 0.004362002015113831\n",
      "Training Batch [645/782]: Loss 0.02973272278904915\n",
      "Training Batch [646/782]: Loss 0.08378628641366959\n",
      "Training Batch [647/782]: Loss 0.04906952753663063\n",
      "Training Batch [648/782]: Loss 0.03553919121623039\n",
      "Training Batch [649/782]: Loss 0.1027384102344513\n",
      "Training Batch [650/782]: Loss 0.011311537586152554\n",
      "Training Batch [651/782]: Loss 0.02574138529598713\n",
      "Training Batch [652/782]: Loss 0.008525148034095764\n",
      "Training Batch [653/782]: Loss 0.005192344542592764\n",
      "Training Batch [654/782]: Loss 0.0054755499586462975\n",
      "Training Batch [655/782]: Loss 0.0027433878276497126\n",
      "Training Batch [656/782]: Loss 0.006720216013491154\n",
      "Training Batch [657/782]: Loss 0.04930858314037323\n",
      "Training Batch [658/782]: Loss 0.017785094678401947\n",
      "Training Batch [659/782]: Loss 0.04234021157026291\n",
      "Training Batch [660/782]: Loss 0.005104338750243187\n",
      "Training Batch [661/782]: Loss 0.008791355416178703\n",
      "Training Batch [662/782]: Loss 0.014780828729271889\n",
      "Training Batch [663/782]: Loss 0.052545879036188126\n",
      "Training Batch [664/782]: Loss 0.018131960183382034\n",
      "Training Batch [665/782]: Loss 0.006712033413350582\n",
      "Training Batch [666/782]: Loss 0.1099688857793808\n",
      "Training Batch [667/782]: Loss 0.022728437557816505\n",
      "Training Batch [668/782]: Loss 0.08849895745515823\n",
      "Training Batch [669/782]: Loss 0.04269161820411682\n",
      "Training Batch [670/782]: Loss 0.04957232624292374\n",
      "Training Batch [671/782]: Loss 0.04513950273394585\n",
      "Training Batch [672/782]: Loss 0.016453268006443977\n",
      "Training Batch [673/782]: Loss 0.009365382604300976\n",
      "Training Batch [674/782]: Loss 0.048516687005758286\n",
      "Training Batch [675/782]: Loss 0.004613557364791632\n",
      "Training Batch [676/782]: Loss 0.05866307020187378\n",
      "Training Batch [677/782]: Loss 0.004219505470246077\n",
      "Training Batch [678/782]: Loss 0.010026096366345882\n",
      "Training Batch [679/782]: Loss 0.027874000370502472\n",
      "Training Batch [680/782]: Loss 0.03221670165657997\n",
      "Training Batch [681/782]: Loss 0.11553402245044708\n",
      "Training Batch [682/782]: Loss 0.002034191507846117\n",
      "Training Batch [683/782]: Loss 0.07933887839317322\n",
      "Training Batch [684/782]: Loss 0.0047216033563017845\n",
      "Training Batch [685/782]: Loss 0.04600285738706589\n",
      "Training Batch [686/782]: Loss 0.011268450878560543\n",
      "Training Batch [687/782]: Loss 0.009608482010662556\n",
      "Training Batch [688/782]: Loss 0.024710167199373245\n",
      "Training Batch [689/782]: Loss 0.08565860986709595\n",
      "Training Batch [690/782]: Loss 0.03540772199630737\n",
      "Training Batch [691/782]: Loss 0.02163512445986271\n",
      "Training Batch [692/782]: Loss 0.017891064286231995\n",
      "Training Batch [693/782]: Loss 0.10879085958003998\n",
      "Training Batch [694/782]: Loss 0.062102898955345154\n",
      "Training Batch [695/782]: Loss 0.006011271849274635\n",
      "Training Batch [696/782]: Loss 0.029539478942751884\n",
      "Training Batch [697/782]: Loss 0.19291505217552185\n",
      "Training Batch [698/782]: Loss 0.0021554860286414623\n",
      "Training Batch [699/782]: Loss 0.07235981523990631\n",
      "Training Batch [700/782]: Loss 0.015765482559800148\n",
      "Training Batch [701/782]: Loss 0.010277987457811832\n",
      "Training Batch [702/782]: Loss 0.039009470492601395\n",
      "Training Batch [703/782]: Loss 0.009191978722810745\n",
      "Training Batch [704/782]: Loss 0.0698685497045517\n",
      "Training Batch [705/782]: Loss 0.14246322214603424\n",
      "Training Batch [706/782]: Loss 0.04984337463974953\n",
      "Training Batch [707/782]: Loss 0.0030208369717001915\n",
      "Training Batch [708/782]: Loss 0.024403657764196396\n",
      "Training Batch [709/782]: Loss 0.04405225068330765\n",
      "Training Batch [710/782]: Loss 0.13503210246562958\n",
      "Training Batch [711/782]: Loss 0.04100758954882622\n",
      "Training Batch [712/782]: Loss 0.09568268805742264\n",
      "Training Batch [713/782]: Loss 0.014092440716922283\n",
      "Training Batch [714/782]: Loss 0.04321761801838875\n",
      "Training Batch [715/782]: Loss 0.010673790238797665\n",
      "Training Batch [716/782]: Loss 0.04991341009736061\n",
      "Training Batch [717/782]: Loss 0.09636813402175903\n",
      "Training Batch [718/782]: Loss 0.002984376857057214\n",
      "Training Batch [719/782]: Loss 0.010634451173245907\n",
      "Training Batch [720/782]: Loss 0.021792490035295486\n",
      "Training Batch [721/782]: Loss 0.007628095801919699\n",
      "Training Batch [722/782]: Loss 0.026399072259664536\n",
      "Training Batch [723/782]: Loss 0.03389882668852806\n",
      "Training Batch [724/782]: Loss 0.021106941625475883\n",
      "Training Batch [725/782]: Loss 0.004646169021725655\n",
      "Training Batch [726/782]: Loss 0.003270099638029933\n",
      "Training Batch [727/782]: Loss 0.04193602502346039\n",
      "Training Batch [728/782]: Loss 0.08142516762018204\n",
      "Training Batch [729/782]: Loss 0.08081139624118805\n",
      "Training Batch [730/782]: Loss 0.05706819146871567\n",
      "Training Batch [731/782]: Loss 0.022994128987193108\n",
      "Training Batch [732/782]: Loss 0.032145269215106964\n",
      "Training Batch [733/782]: Loss 0.004701061639934778\n",
      "Training Batch [734/782]: Loss 0.08289562165737152\n",
      "Training Batch [735/782]: Loss 0.005597979761660099\n",
      "Training Batch [736/782]: Loss 0.012548284605145454\n",
      "Training Batch [737/782]: Loss 0.030054103583097458\n",
      "Training Batch [738/782]: Loss 0.0521598756313324\n",
      "Training Batch [739/782]: Loss 0.00288210972212255\n",
      "Training Batch [740/782]: Loss 0.05420568585395813\n",
      "Training Batch [741/782]: Loss 0.016992898657917976\n",
      "Training Batch [742/782]: Loss 0.010942935943603516\n",
      "Training Batch [743/782]: Loss 0.01464290451258421\n",
      "Training Batch [744/782]: Loss 0.011559943668544292\n",
      "Training Batch [745/782]: Loss 0.0077330670319497585\n",
      "Training Batch [746/782]: Loss 0.020849457010626793\n",
      "Training Batch [747/782]: Loss 0.0684833750128746\n",
      "Training Batch [748/782]: Loss 0.1157773807644844\n",
      "Training Batch [749/782]: Loss 0.05650463327765465\n",
      "Training Batch [750/782]: Loss 0.09680032730102539\n",
      "Training Batch [751/782]: Loss 0.09178932011127472\n",
      "Training Batch [752/782]: Loss 0.03655191883444786\n",
      "Training Batch [753/782]: Loss 0.014828601852059364\n",
      "Training Batch [754/782]: Loss 0.04349193722009659\n",
      "Training Batch [755/782]: Loss 0.01912626065313816\n",
      "Training Batch [756/782]: Loss 0.16874158382415771\n",
      "Training Batch [757/782]: Loss 0.09548578411340714\n",
      "Training Batch [758/782]: Loss 0.03347448632121086\n",
      "Training Batch [759/782]: Loss 0.0028789115604013205\n",
      "Training Batch [760/782]: Loss 0.05468631908297539\n",
      "Training Batch [761/782]: Loss 0.07107334583997726\n",
      "Training Batch [762/782]: Loss 0.11159902065992355\n",
      "Training Batch [763/782]: Loss 0.10789890587329865\n",
      "Training Batch [764/782]: Loss 0.06146220490336418\n",
      "Training Batch [765/782]: Loss 0.024132173508405685\n",
      "Training Batch [766/782]: Loss 0.0121979471296072\n",
      "Training Batch [767/782]: Loss 0.041439685970544815\n",
      "Training Batch [768/782]: Loss 0.08001799881458282\n",
      "Training Batch [769/782]: Loss 0.022465892136096954\n",
      "Training Batch [770/782]: Loss 0.015066970139741898\n",
      "Training Batch [771/782]: Loss 0.0368044413626194\n",
      "Training Batch [772/782]: Loss 0.001876522321254015\n",
      "Training Batch [773/782]: Loss 0.004096992313861847\n",
      "Training Batch [774/782]: Loss 0.05441160872578621\n",
      "Training Batch [775/782]: Loss 0.038820862770080566\n",
      "Training Batch [776/782]: Loss 0.007938715629279613\n",
      "Training Batch [777/782]: Loss 0.07027020305395126\n",
      "Training Batch [778/782]: Loss 0.04105164483189583\n",
      "Training Batch [779/782]: Loss 0.024063430726528168\n",
      "Training Batch [780/782]: Loss 0.009745724499225616\n",
      "Training Batch [781/782]: Loss 0.046511027961969376\n",
      "Training Batch [782/782]: Loss 0.05972462520003319\n",
      "Epoch 23 - Train Loss: 0.0255\n",
      "*********  Epoch 24/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.010740618221461773\n",
      "Training Batch [2/782]: Loss 0.008239414542913437\n",
      "Training Batch [3/782]: Loss 0.012857564724981785\n",
      "Training Batch [4/782]: Loss 0.041625503450632095\n",
      "Training Batch [5/782]: Loss 0.00942261517047882\n",
      "Training Batch [6/782]: Loss 0.009045598097145557\n",
      "Training Batch [7/782]: Loss 0.006548373959958553\n",
      "Training Batch [8/782]: Loss 0.007540639024227858\n",
      "Training Batch [9/782]: Loss 0.002324010245501995\n",
      "Training Batch [10/782]: Loss 0.014732945710420609\n",
      "Training Batch [11/782]: Loss 0.06335888057947159\n",
      "Training Batch [12/782]: Loss 0.0007520922226831317\n",
      "Training Batch [13/782]: Loss 0.010242531076073647\n",
      "Training Batch [14/782]: Loss 0.03222712129354477\n",
      "Training Batch [15/782]: Loss 0.01452160906046629\n",
      "Training Batch [16/782]: Loss 0.007123912218958139\n",
      "Training Batch [17/782]: Loss 0.019879499450325966\n",
      "Training Batch [18/782]: Loss 0.04900846257805824\n",
      "Training Batch [19/782]: Loss 0.007131134159862995\n",
      "Training Batch [20/782]: Loss 0.09707891941070557\n",
      "Training Batch [21/782]: Loss 0.05823637545108795\n",
      "Training Batch [22/782]: Loss 0.05474843457341194\n",
      "Training Batch [23/782]: Loss 0.0014012085739523172\n",
      "Training Batch [24/782]: Loss 0.006355321034789085\n",
      "Training Batch [25/782]: Loss 0.008536511100828648\n",
      "Training Batch [26/782]: Loss 0.005067677237093449\n",
      "Training Batch [27/782]: Loss 0.033614542335271835\n",
      "Training Batch [28/782]: Loss 0.004739772528409958\n",
      "Training Batch [29/782]: Loss 0.007788470946252346\n",
      "Training Batch [30/782]: Loss 0.036580365151166916\n",
      "Training Batch [31/782]: Loss 0.08690886944532394\n",
      "Training Batch [32/782]: Loss 0.10863681137561798\n",
      "Training Batch [33/782]: Loss 0.10187344253063202\n",
      "Training Batch [34/782]: Loss 0.019395796582102776\n",
      "Training Batch [35/782]: Loss 0.02559291385114193\n",
      "Training Batch [36/782]: Loss 0.05160602927207947\n",
      "Training Batch [37/782]: Loss 0.039051711559295654\n",
      "Training Batch [38/782]: Loss 0.005956481676548719\n",
      "Training Batch [39/782]: Loss 0.0109822703525424\n",
      "Training Batch [40/782]: Loss 0.06666567176580429\n",
      "Training Batch [41/782]: Loss 0.004890795331448317\n",
      "Training Batch [42/782]: Loss 0.10558081418275833\n",
      "Training Batch [43/782]: Loss 0.03307337313890457\n",
      "Training Batch [44/782]: Loss 0.0065784999169409275\n",
      "Training Batch [45/782]: Loss 0.015430998057126999\n",
      "Training Batch [46/782]: Loss 0.015252301469445229\n",
      "Training Batch [47/782]: Loss 0.006369014736264944\n",
      "Training Batch [48/782]: Loss 0.004617277067154646\n",
      "Training Batch [49/782]: Loss 0.002057319274172187\n",
      "Training Batch [50/782]: Loss 0.0053961253724992275\n",
      "Training Batch [51/782]: Loss 0.016909556463360786\n",
      "Training Batch [52/782]: Loss 0.03576723113656044\n",
      "Training Batch [53/782]: Loss 0.004849833901971579\n",
      "Training Batch [54/782]: Loss 0.03039262443780899\n",
      "Training Batch [55/782]: Loss 0.04665421321988106\n",
      "Training Batch [56/782]: Loss 0.007124932948499918\n",
      "Training Batch [57/782]: Loss 0.009284032508730888\n",
      "Training Batch [58/782]: Loss 0.011020508594810963\n",
      "Training Batch [59/782]: Loss 0.046245988458395004\n",
      "Training Batch [60/782]: Loss 0.003931125625967979\n",
      "Training Batch [61/782]: Loss 0.017476532608270645\n",
      "Training Batch [62/782]: Loss 0.01819279044866562\n",
      "Training Batch [63/782]: Loss 0.003489793511107564\n",
      "Training Batch [64/782]: Loss 0.0011609034845605493\n",
      "Training Batch [65/782]: Loss 0.009372290223836899\n",
      "Training Batch [66/782]: Loss 0.014769733883440495\n",
      "Training Batch [67/782]: Loss 0.015434681437909603\n",
      "Training Batch [68/782]: Loss 0.040475085377693176\n",
      "Training Batch [69/782]: Loss 0.025231005623936653\n",
      "Training Batch [70/782]: Loss 0.018666867166757584\n",
      "Training Batch [71/782]: Loss 0.005029140505939722\n",
      "Training Batch [72/782]: Loss 0.006783957127481699\n",
      "Training Batch [73/782]: Loss 0.05009656772017479\n",
      "Training Batch [74/782]: Loss 0.16682100296020508\n",
      "Training Batch [75/782]: Loss 0.0042483508586883545\n",
      "Training Batch [76/782]: Loss 0.00606546550989151\n",
      "Training Batch [77/782]: Loss 0.0023747931700199842\n",
      "Training Batch [78/782]: Loss 0.005911918822675943\n",
      "Training Batch [79/782]: Loss 0.05234821140766144\n",
      "Training Batch [80/782]: Loss 0.002069432521238923\n",
      "Training Batch [81/782]: Loss 0.0022090317215770483\n",
      "Training Batch [82/782]: Loss 0.0040487609803676605\n",
      "Training Batch [83/782]: Loss 0.008301553316414356\n",
      "Training Batch [84/782]: Loss 0.02063547819852829\n",
      "Training Batch [85/782]: Loss 0.005443214438855648\n",
      "Training Batch [86/782]: Loss 0.0022381809540092945\n",
      "Training Batch [87/782]: Loss 0.010694689117372036\n",
      "Training Batch [88/782]: Loss 0.04552791267633438\n",
      "Training Batch [89/782]: Loss 0.02041207253932953\n",
      "Training Batch [90/782]: Loss 0.009280886501073837\n",
      "Training Batch [91/782]: Loss 0.0031632068566977978\n",
      "Training Batch [92/782]: Loss 0.047190189361572266\n",
      "Training Batch [93/782]: Loss 0.004570304416120052\n",
      "Training Batch [94/782]: Loss 0.005681361071765423\n",
      "Training Batch [95/782]: Loss 0.013923010788857937\n",
      "Training Batch [96/782]: Loss 0.0019364663166925311\n",
      "Training Batch [97/782]: Loss 0.04836742952466011\n",
      "Training Batch [98/782]: Loss 0.006045123562216759\n",
      "Training Batch [99/782]: Loss 0.0017083205748349428\n",
      "Training Batch [100/782]: Loss 0.012496567331254482\n",
      "Training Batch [101/782]: Loss 0.01636059209704399\n",
      "Training Batch [102/782]: Loss 0.005183121655136347\n",
      "Training Batch [103/782]: Loss 0.015505798161029816\n",
      "Training Batch [104/782]: Loss 0.06807716190814972\n",
      "Training Batch [105/782]: Loss 0.014958666637539864\n",
      "Training Batch [106/782]: Loss 0.008108140900731087\n",
      "Training Batch [107/782]: Loss 0.0017832351149991155\n",
      "Training Batch [108/782]: Loss 0.005965252406895161\n",
      "Training Batch [109/782]: Loss 0.004548309370875359\n",
      "Training Batch [110/782]: Loss 0.0017102346755564213\n",
      "Training Batch [111/782]: Loss 0.005011078901588917\n",
      "Training Batch [112/782]: Loss 0.0071497587487101555\n",
      "Training Batch [113/782]: Loss 0.001498003606684506\n",
      "Training Batch [114/782]: Loss 0.007549978792667389\n",
      "Training Batch [115/782]: Loss 0.0076143573969602585\n",
      "Training Batch [116/782]: Loss 0.005232328549027443\n",
      "Training Batch [117/782]: Loss 0.02936398983001709\n",
      "Training Batch [118/782]: Loss 0.007999252527952194\n",
      "Training Batch [119/782]: Loss 0.008703064173460007\n",
      "Training Batch [120/782]: Loss 0.007214748300611973\n",
      "Training Batch [121/782]: Loss 0.004011752083897591\n",
      "Training Batch [122/782]: Loss 0.0007729519857093692\n",
      "Training Batch [123/782]: Loss 0.0018934115068987012\n",
      "Training Batch [124/782]: Loss 0.01827828399837017\n",
      "Training Batch [125/782]: Loss 0.006626872345805168\n",
      "Training Batch [126/782]: Loss 0.004582686349749565\n",
      "Training Batch [127/782]: Loss 0.0021390521433204412\n",
      "Training Batch [128/782]: Loss 0.02156287245452404\n",
      "Training Batch [129/782]: Loss 0.007212074939161539\n",
      "Training Batch [130/782]: Loss 0.0010587236611172557\n",
      "Training Batch [131/782]: Loss 0.028661545366048813\n",
      "Training Batch [132/782]: Loss 0.01030053198337555\n",
      "Training Batch [133/782]: Loss 0.016186578199267387\n",
      "Training Batch [134/782]: Loss 0.0007074923487380147\n",
      "Training Batch [135/782]: Loss 0.05291209742426872\n",
      "Training Batch [136/782]: Loss 0.00023366005916614085\n",
      "Training Batch [137/782]: Loss 0.0006409497582353652\n",
      "Training Batch [138/782]: Loss 0.0003910320810973644\n",
      "Training Batch [139/782]: Loss 0.0006438043201342225\n",
      "Training Batch [140/782]: Loss 0.01696578972041607\n",
      "Training Batch [141/782]: Loss 0.01600002311170101\n",
      "Training Batch [142/782]: Loss 0.003992720972746611\n",
      "Training Batch [143/782]: Loss 0.005286792758852243\n",
      "Training Batch [144/782]: Loss 0.009590444155037403\n",
      "Training Batch [145/782]: Loss 0.0005096277454867959\n",
      "Training Batch [146/782]: Loss 0.010721460916101933\n",
      "Training Batch [147/782]: Loss 0.004642040468752384\n",
      "Training Batch [148/782]: Loss 0.006672155112028122\n",
      "Training Batch [149/782]: Loss 0.011362116783857346\n",
      "Training Batch [150/782]: Loss 0.0019076366443186998\n",
      "Training Batch [151/782]: Loss 0.005105959251523018\n",
      "Training Batch [152/782]: Loss 0.010376346297562122\n",
      "Training Batch [153/782]: Loss 0.0020080814138054848\n",
      "Training Batch [154/782]: Loss 0.0020412267185747623\n",
      "Training Batch [155/782]: Loss 0.006801539100706577\n",
      "Training Batch [156/782]: Loss 0.027600742876529694\n",
      "Training Batch [157/782]: Loss 0.018392330035567284\n",
      "Training Batch [158/782]: Loss 0.0032800028566271067\n",
      "Training Batch [159/782]: Loss 0.0018769910093396902\n",
      "Training Batch [160/782]: Loss 0.0031587660778313875\n",
      "Training Batch [161/782]: Loss 0.0014274920104071498\n",
      "Training Batch [162/782]: Loss 0.026713747531175613\n",
      "Training Batch [163/782]: Loss 0.010805195197463036\n",
      "Training Batch [164/782]: Loss 0.008790110237896442\n",
      "Training Batch [165/782]: Loss 0.011958770453929901\n",
      "Training Batch [166/782]: Loss 0.004669152665883303\n",
      "Training Batch [167/782]: Loss 0.059441257268190384\n",
      "Training Batch [168/782]: Loss 0.008416237309575081\n",
      "Training Batch [169/782]: Loss 0.07445766031742096\n",
      "Training Batch [170/782]: Loss 0.0009513322729617357\n",
      "Training Batch [171/782]: Loss 0.007561651058495045\n",
      "Training Batch [172/782]: Loss 0.047289226204156876\n",
      "Training Batch [173/782]: Loss 0.011137260124087334\n",
      "Training Batch [174/782]: Loss 0.004774906672537327\n",
      "Training Batch [175/782]: Loss 0.0023289336822927\n",
      "Training Batch [176/782]: Loss 0.002407881896942854\n",
      "Training Batch [177/782]: Loss 0.014606418088078499\n",
      "Training Batch [178/782]: Loss 0.001134334597736597\n",
      "Training Batch [179/782]: Loss 0.005289293825626373\n",
      "Training Batch [180/782]: Loss 0.009742001071572304\n",
      "Training Batch [181/782]: Loss 0.0012240847572684288\n",
      "Training Batch [182/782]: Loss 0.017117939889431\n",
      "Training Batch [183/782]: Loss 0.009009395726025105\n",
      "Training Batch [184/782]: Loss 0.011905240826308727\n",
      "Training Batch [185/782]: Loss 0.046404313296079636\n",
      "Training Batch [186/782]: Loss 0.0020162297878414392\n",
      "Training Batch [187/782]: Loss 0.007227230817079544\n",
      "Training Batch [188/782]: Loss 0.0021842659916728735\n",
      "Training Batch [189/782]: Loss 0.01728767715394497\n",
      "Training Batch [190/782]: Loss 0.0032353540882468224\n",
      "Training Batch [191/782]: Loss 0.0009618689655326307\n",
      "Training Batch [192/782]: Loss 0.03871692717075348\n",
      "Training Batch [193/782]: Loss 0.027363860979676247\n",
      "Training Batch [194/782]: Loss 0.10968420654535294\n",
      "Training Batch [195/782]: Loss 0.0012684324756264687\n",
      "Training Batch [196/782]: Loss 0.039693575352430344\n",
      "Training Batch [197/782]: Loss 0.007266858126968145\n",
      "Training Batch [198/782]: Loss 0.0038417500909417868\n",
      "Training Batch [199/782]: Loss 0.003822609083727002\n",
      "Training Batch [200/782]: Loss 0.0031350699719041586\n",
      "Training Batch [201/782]: Loss 0.027880584821105003\n",
      "Training Batch [202/782]: Loss 0.021755583584308624\n",
      "Training Batch [203/782]: Loss 0.020004592835903168\n",
      "Training Batch [204/782]: Loss 0.008409432135522366\n",
      "Training Batch [205/782]: Loss 0.013385455124080181\n",
      "Training Batch [206/782]: Loss 0.027225278317928314\n",
      "Training Batch [207/782]: Loss 0.11899325996637344\n",
      "Training Batch [208/782]: Loss 0.07786011695861816\n",
      "Training Batch [209/782]: Loss 0.06570509076118469\n",
      "Training Batch [210/782]: Loss 0.010453201830387115\n",
      "Training Batch [211/782]: Loss 0.06846453994512558\n",
      "Training Batch [212/782]: Loss 0.0016674301587045193\n",
      "Training Batch [213/782]: Loss 0.03378399834036827\n",
      "Training Batch [214/782]: Loss 0.004194481763988733\n",
      "Training Batch [215/782]: Loss 0.005349199287593365\n",
      "Training Batch [216/782]: Loss 0.00904226116836071\n",
      "Training Batch [217/782]: Loss 0.005828323308378458\n",
      "Training Batch [218/782]: Loss 0.020986979827284813\n",
      "Training Batch [219/782]: Loss 0.0061673433519899845\n",
      "Training Batch [220/782]: Loss 0.014338864013552666\n",
      "Training Batch [221/782]: Loss 0.02261383645236492\n",
      "Training Batch [222/782]: Loss 0.012651164084672928\n",
      "Training Batch [223/782]: Loss 0.002784229815006256\n",
      "Training Batch [224/782]: Loss 0.008342250250279903\n",
      "Training Batch [225/782]: Loss 0.10685982555150986\n",
      "Training Batch [226/782]: Loss 0.030942372977733612\n",
      "Training Batch [227/782]: Loss 0.015504213981330395\n",
      "Training Batch [228/782]: Loss 0.021450510248541832\n",
      "Training Batch [229/782]: Loss 0.006265538278967142\n",
      "Training Batch [230/782]: Loss 0.07969357818365097\n",
      "Training Batch [231/782]: Loss 0.006975764408707619\n",
      "Training Batch [232/782]: Loss 0.008659242652356625\n",
      "Training Batch [233/782]: Loss 0.014805462211370468\n",
      "Training Batch [234/782]: Loss 0.028015701100230217\n",
      "Training Batch [235/782]: Loss 0.005021731834858656\n",
      "Training Batch [236/782]: Loss 0.07112430781126022\n",
      "Training Batch [237/782]: Loss 0.010349729098379612\n",
      "Training Batch [238/782]: Loss 0.0010074229212477803\n",
      "Training Batch [239/782]: Loss 0.01484184805303812\n",
      "Training Batch [240/782]: Loss 0.00030671441345475614\n",
      "Training Batch [241/782]: Loss 0.011775012128055096\n",
      "Training Batch [242/782]: Loss 0.0016738204285502434\n",
      "Training Batch [243/782]: Loss 0.05072876811027527\n",
      "Training Batch [244/782]: Loss 0.0006954629207029939\n",
      "Training Batch [245/782]: Loss 0.009902884252369404\n",
      "Training Batch [246/782]: Loss 0.006668601650744677\n",
      "Training Batch [247/782]: Loss 0.02135515585541725\n",
      "Training Batch [248/782]: Loss 0.004970600362867117\n",
      "Training Batch [249/782]: Loss 0.0024837141390889883\n",
      "Training Batch [250/782]: Loss 0.005853020586073399\n",
      "Training Batch [251/782]: Loss 0.019688373431563377\n",
      "Training Batch [252/782]: Loss 0.006041812710464001\n",
      "Training Batch [253/782]: Loss 0.007446258794516325\n",
      "Training Batch [254/782]: Loss 0.026995889842510223\n",
      "Training Batch [255/782]: Loss 0.005247175693511963\n",
      "Training Batch [256/782]: Loss 0.0026879007928073406\n",
      "Training Batch [257/782]: Loss 0.00209008133970201\n",
      "Training Batch [258/782]: Loss 0.0028568797279149294\n",
      "Training Batch [259/782]: Loss 0.04310132563114166\n",
      "Training Batch [260/782]: Loss 0.007366321515291929\n",
      "Training Batch [261/782]: Loss 0.01185244508087635\n",
      "Training Batch [262/782]: Loss 0.0018677221378311515\n",
      "Training Batch [263/782]: Loss 0.008266577497124672\n",
      "Training Batch [264/782]: Loss 0.003395223757252097\n",
      "Training Batch [265/782]: Loss 0.04365880414843559\n",
      "Training Batch [266/782]: Loss 0.00393104599788785\n",
      "Training Batch [267/782]: Loss 0.007808692287653685\n",
      "Training Batch [268/782]: Loss 0.01705820858478546\n",
      "Training Batch [269/782]: Loss 0.003497305791825056\n",
      "Training Batch [270/782]: Loss 0.01995670609176159\n",
      "Training Batch [271/782]: Loss 0.026238713413476944\n",
      "Training Batch [272/782]: Loss 0.00307250814512372\n",
      "Training Batch [273/782]: Loss 0.0006630463758483529\n",
      "Training Batch [274/782]: Loss 0.05096232891082764\n",
      "Training Batch [275/782]: Loss 0.043801113963127136\n",
      "Training Batch [276/782]: Loss 0.0669364482164383\n",
      "Training Batch [277/782]: Loss 0.01000975538045168\n",
      "Training Batch [278/782]: Loss 0.004978328011929989\n",
      "Training Batch [279/782]: Loss 0.004281461704522371\n",
      "Training Batch [280/782]: Loss 0.0026520192623138428\n",
      "Training Batch [281/782]: Loss 0.010694057680666447\n",
      "Training Batch [282/782]: Loss 0.007642408832907677\n",
      "Training Batch [283/782]: Loss 0.005066591314971447\n",
      "Training Batch [284/782]: Loss 0.0031010780949145555\n",
      "Training Batch [285/782]: Loss 0.05757478252053261\n",
      "Training Batch [286/782]: Loss 0.007051472552120686\n",
      "Training Batch [287/782]: Loss 0.005962597671896219\n",
      "Training Batch [288/782]: Loss 0.0016905105439946055\n",
      "Training Batch [289/782]: Loss 0.0021268026903271675\n",
      "Training Batch [290/782]: Loss 0.001891353982500732\n",
      "Training Batch [291/782]: Loss 0.0065526943653821945\n",
      "Training Batch [292/782]: Loss 0.0012615211308002472\n",
      "Training Batch [293/782]: Loss 0.018680749461054802\n",
      "Training Batch [294/782]: Loss 0.0101842749863863\n",
      "Training Batch [295/782]: Loss 0.04403030127286911\n",
      "Training Batch [296/782]: Loss 0.007749670650810003\n",
      "Training Batch [297/782]: Loss 0.014914135448634624\n",
      "Training Batch [298/782]: Loss 0.008835726417601109\n",
      "Training Batch [299/782]: Loss 0.040611788630485535\n",
      "Training Batch [300/782]: Loss 0.006509705446660519\n",
      "Training Batch [301/782]: Loss 0.03741239383816719\n",
      "Training Batch [302/782]: Loss 0.0025379941798746586\n",
      "Training Batch [303/782]: Loss 0.0037492604460567236\n",
      "Training Batch [304/782]: Loss 0.006789867766201496\n",
      "Training Batch [305/782]: Loss 0.005584210157394409\n",
      "Training Batch [306/782]: Loss 0.006844384130090475\n",
      "Training Batch [307/782]: Loss 0.008921321481466293\n",
      "Training Batch [308/782]: Loss 0.0013192488113418221\n",
      "Training Batch [309/782]: Loss 0.01839923858642578\n",
      "Training Batch [310/782]: Loss 0.003790495917201042\n",
      "Training Batch [311/782]: Loss 0.0020838635973632336\n",
      "Training Batch [312/782]: Loss 0.02733626402914524\n",
      "Training Batch [313/782]: Loss 0.006066663656383753\n",
      "Training Batch [314/782]: Loss 0.019538376480340958\n",
      "Training Batch [315/782]: Loss 0.005619802046567202\n",
      "Training Batch [316/782]: Loss 0.024788152426481247\n",
      "Training Batch [317/782]: Loss 0.003742989618331194\n",
      "Training Batch [318/782]: Loss 0.002605210989713669\n",
      "Training Batch [319/782]: Loss 0.0033289375714957714\n",
      "Training Batch [320/782]: Loss 0.0012528974330052733\n",
      "Training Batch [321/782]: Loss 0.05877137929201126\n",
      "Training Batch [322/782]: Loss 0.0014101584674790502\n",
      "Training Batch [323/782]: Loss 0.009416050277650356\n",
      "Training Batch [324/782]: Loss 0.012106313370168209\n",
      "Training Batch [325/782]: Loss 0.006253186613321304\n",
      "Training Batch [326/782]: Loss 0.001046515884809196\n",
      "Training Batch [327/782]: Loss 0.005158042069524527\n",
      "Training Batch [328/782]: Loss 0.015726329758763313\n",
      "Training Batch [329/782]: Loss 0.0007967503624968231\n",
      "Training Batch [330/782]: Loss 0.02074853517115116\n",
      "Training Batch [331/782]: Loss 0.008348598144948483\n",
      "Training Batch [332/782]: Loss 0.004760837182402611\n",
      "Training Batch [333/782]: Loss 0.00874667800962925\n",
      "Training Batch [334/782]: Loss 0.05232870951294899\n",
      "Training Batch [335/782]: Loss 0.009221840649843216\n",
      "Training Batch [336/782]: Loss 0.002875174395740032\n",
      "Training Batch [337/782]: Loss 0.08506131172180176\n",
      "Training Batch [338/782]: Loss 0.030244341120123863\n",
      "Training Batch [339/782]: Loss 0.012232623994350433\n",
      "Training Batch [340/782]: Loss 0.0043411883525550365\n",
      "Training Batch [341/782]: Loss 0.031907275319099426\n",
      "Training Batch [342/782]: Loss 0.027458686381578445\n",
      "Training Batch [343/782]: Loss 0.0035083703696727753\n",
      "Training Batch [344/782]: Loss 0.03754042088985443\n",
      "Training Batch [345/782]: Loss 0.024838918820023537\n",
      "Training Batch [346/782]: Loss 0.03094427101314068\n",
      "Training Batch [347/782]: Loss 0.011152013204991817\n",
      "Training Batch [348/782]: Loss 0.008563422597944736\n",
      "Training Batch [349/782]: Loss 0.003351167542859912\n",
      "Training Batch [350/782]: Loss 0.001914962544105947\n",
      "Training Batch [351/782]: Loss 0.14492285251617432\n",
      "Training Batch [352/782]: Loss 0.08016844838857651\n",
      "Training Batch [353/782]: Loss 0.027860796079039574\n",
      "Training Batch [354/782]: Loss 0.0069470782764256\n",
      "Training Batch [355/782]: Loss 0.005547851324081421\n",
      "Training Batch [356/782]: Loss 0.016881592571735382\n",
      "Training Batch [357/782]: Loss 0.026825396344065666\n",
      "Training Batch [358/782]: Loss 0.016890836879611015\n",
      "Training Batch [359/782]: Loss 0.007667798548936844\n",
      "Training Batch [360/782]: Loss 0.0013864105567336082\n",
      "Training Batch [361/782]: Loss 0.019069984555244446\n",
      "Training Batch [362/782]: Loss 0.01150194089859724\n",
      "Training Batch [363/782]: Loss 0.015137330628931522\n",
      "Training Batch [364/782]: Loss 0.013909228146076202\n",
      "Training Batch [365/782]: Loss 0.01589122973382473\n",
      "Training Batch [366/782]: Loss 0.0036312886513769627\n",
      "Training Batch [367/782]: Loss 0.004052110947668552\n",
      "Training Batch [368/782]: Loss 0.014828238636255264\n",
      "Training Batch [369/782]: Loss 0.06611839681863785\n",
      "Training Batch [370/782]: Loss 0.010417885147035122\n",
      "Training Batch [371/782]: Loss 0.02450472302734852\n",
      "Training Batch [372/782]: Loss 0.015786062926054\n",
      "Training Batch [373/782]: Loss 0.09443559497594833\n",
      "Training Batch [374/782]: Loss 0.009156391955912113\n",
      "Training Batch [375/782]: Loss 0.0007994353654794395\n",
      "Training Batch [376/782]: Loss 0.00295245717279613\n",
      "Training Batch [377/782]: Loss 0.012302356772124767\n",
      "Training Batch [378/782]: Loss 0.01256492268294096\n",
      "Training Batch [379/782]: Loss 0.006864096503704786\n",
      "Training Batch [380/782]: Loss 0.0033845200669020414\n",
      "Training Batch [381/782]: Loss 0.004700291436165571\n",
      "Training Batch [382/782]: Loss 0.01248658262193203\n",
      "Training Batch [383/782]: Loss 0.026122335344552994\n",
      "Training Batch [384/782]: Loss 0.01920420303940773\n",
      "Training Batch [385/782]: Loss 0.020681921392679214\n",
      "Training Batch [386/782]: Loss 0.008093319833278656\n",
      "Training Batch [387/782]: Loss 0.006652011536061764\n",
      "Training Batch [388/782]: Loss 0.009791507385671139\n",
      "Training Batch [389/782]: Loss 0.007654586806893349\n",
      "Training Batch [390/782]: Loss 0.03685573861002922\n",
      "Training Batch [391/782]: Loss 0.01295471005141735\n",
      "Training Batch [392/782]: Loss 0.004971825983375311\n",
      "Training Batch [393/782]: Loss 0.0009297750657424331\n",
      "Training Batch [394/782]: Loss 0.001918983063660562\n",
      "Training Batch [395/782]: Loss 0.018373090773820877\n",
      "Training Batch [396/782]: Loss 0.10385517030954361\n",
      "Training Batch [397/782]: Loss 0.02871018648147583\n",
      "Training Batch [398/782]: Loss 0.0013057678006589413\n",
      "Training Batch [399/782]: Loss 0.01905226707458496\n",
      "Training Batch [400/782]: Loss 0.007969465106725693\n",
      "Training Batch [401/782]: Loss 0.017805160954594612\n",
      "Training Batch [402/782]: Loss 0.0017921319231390953\n",
      "Training Batch [403/782]: Loss 0.004746166989207268\n",
      "Training Batch [404/782]: Loss 0.001838333671912551\n",
      "Training Batch [405/782]: Loss 0.0023042906541377306\n",
      "Training Batch [406/782]: Loss 0.023286841809749603\n",
      "Training Batch [407/782]: Loss 0.008441534824669361\n",
      "Training Batch [408/782]: Loss 0.0025011489633470774\n",
      "Training Batch [409/782]: Loss 0.007037952076643705\n",
      "Training Batch [410/782]: Loss 0.00566801056265831\n",
      "Training Batch [411/782]: Loss 0.0011877573560923338\n",
      "Training Batch [412/782]: Loss 0.004640727303922176\n",
      "Training Batch [413/782]: Loss 0.0013116877526044846\n",
      "Training Batch [414/782]: Loss 0.0032355019357055426\n",
      "Training Batch [415/782]: Loss 0.01190263032913208\n",
      "Training Batch [416/782]: Loss 0.009832223877310753\n",
      "Training Batch [417/782]: Loss 0.06600470840930939\n",
      "Training Batch [418/782]: Loss 0.02153732441365719\n",
      "Training Batch [419/782]: Loss 0.009407092817127705\n",
      "Training Batch [420/782]: Loss 0.005869671236723661\n",
      "Training Batch [421/782]: Loss 0.004952782765030861\n",
      "Training Batch [422/782]: Loss 0.012723511084914207\n",
      "Training Batch [423/782]: Loss 0.0009373003267683089\n",
      "Training Batch [424/782]: Loss 0.0013378168223425746\n",
      "Training Batch [425/782]: Loss 0.00661459332332015\n",
      "Training Batch [426/782]: Loss 0.06499801576137543\n",
      "Training Batch [427/782]: Loss 0.025782082229852676\n",
      "Training Batch [428/782]: Loss 0.04939315840601921\n",
      "Training Batch [429/782]: Loss 0.003689558245241642\n",
      "Training Batch [430/782]: Loss 0.0027016461826860905\n",
      "Training Batch [431/782]: Loss 0.0022029015235602856\n",
      "Training Batch [432/782]: Loss 0.003771561197936535\n",
      "Training Batch [433/782]: Loss 0.005663673859089613\n",
      "Training Batch [434/782]: Loss 0.009961569681763649\n",
      "Training Batch [435/782]: Loss 0.0036679867189377546\n",
      "Training Batch [436/782]: Loss 0.033975981175899506\n",
      "Training Batch [437/782]: Loss 0.03296227008104324\n",
      "Training Batch [438/782]: Loss 0.0026038014329969883\n",
      "Training Batch [439/782]: Loss 0.04852895438671112\n",
      "Training Batch [440/782]: Loss 0.001852823537774384\n",
      "Training Batch [441/782]: Loss 0.010086351074278355\n",
      "Training Batch [442/782]: Loss 0.02630259469151497\n",
      "Training Batch [443/782]: Loss 0.005047036334872246\n",
      "Training Batch [444/782]: Loss 0.01511798519641161\n",
      "Training Batch [445/782]: Loss 0.0002906674926634878\n",
      "Training Batch [446/782]: Loss 0.0033190788235515356\n",
      "Training Batch [447/782]: Loss 0.038618676364421844\n",
      "Training Batch [448/782]: Loss 0.015722256153821945\n",
      "Training Batch [449/782]: Loss 0.017370710149407387\n",
      "Training Batch [450/782]: Loss 0.028925491496920586\n",
      "Training Batch [451/782]: Loss 0.02599157765507698\n",
      "Training Batch [452/782]: Loss 0.013828528113663197\n",
      "Training Batch [453/782]: Loss 0.004100164398550987\n",
      "Training Batch [454/782]: Loss 0.010839160531759262\n",
      "Training Batch [455/782]: Loss 0.0009645211976021528\n",
      "Training Batch [456/782]: Loss 0.008985567837953568\n",
      "Training Batch [457/782]: Loss 0.0010749936336651444\n",
      "Training Batch [458/782]: Loss 0.021912939846515656\n",
      "Training Batch [459/782]: Loss 0.024446900933980942\n",
      "Training Batch [460/782]: Loss 0.01253887265920639\n",
      "Training Batch [461/782]: Loss 0.0037320582196116447\n",
      "Training Batch [462/782]: Loss 0.01043940894305706\n",
      "Training Batch [463/782]: Loss 0.010922537185251713\n",
      "Training Batch [464/782]: Loss 0.0017026038840413094\n",
      "Training Batch [465/782]: Loss 0.01448460016399622\n",
      "Training Batch [466/782]: Loss 0.017232319340109825\n",
      "Training Batch [467/782]: Loss 0.0033561401069164276\n",
      "Training Batch [468/782]: Loss 0.0018410722259432077\n",
      "Training Batch [469/782]: Loss 0.06496329605579376\n",
      "Training Batch [470/782]: Loss 0.030840756371617317\n",
      "Training Batch [471/782]: Loss 0.00335385138168931\n",
      "Training Batch [472/782]: Loss 0.0066531505435705185\n",
      "Training Batch [473/782]: Loss 0.021544313058257103\n",
      "Training Batch [474/782]: Loss 0.005277038551867008\n",
      "Training Batch [475/782]: Loss 0.0019503278890624642\n",
      "Training Batch [476/782]: Loss 0.0027613320853561163\n",
      "Training Batch [477/782]: Loss 0.0065412069670856\n",
      "Training Batch [478/782]: Loss 0.028244489803910255\n",
      "Training Batch [479/782]: Loss 0.013734336942434311\n",
      "Training Batch [480/782]: Loss 0.0008986163302324712\n",
      "Training Batch [481/782]: Loss 0.00181284558493644\n",
      "Training Batch [482/782]: Loss 0.011870433576405048\n",
      "Training Batch [483/782]: Loss 0.008519384078681469\n",
      "Training Batch [484/782]: Loss 0.0008959429687820375\n",
      "Training Batch [485/782]: Loss 0.010023131035268307\n",
      "Training Batch [486/782]: Loss 0.002585536800324917\n",
      "Training Batch [487/782]: Loss 0.0006761295953765512\n",
      "Training Batch [488/782]: Loss 0.02176113799214363\n",
      "Training Batch [489/782]: Loss 0.003837170545011759\n",
      "Training Batch [490/782]: Loss 0.005349763203412294\n",
      "Training Batch [491/782]: Loss 0.027258796617388725\n",
      "Training Batch [492/782]: Loss 0.0007783991168253124\n",
      "Training Batch [493/782]: Loss 0.0382801853120327\n",
      "Training Batch [494/782]: Loss 0.0015451171202585101\n",
      "Training Batch [495/782]: Loss 0.02063620463013649\n",
      "Training Batch [496/782]: Loss 0.008850468322634697\n",
      "Training Batch [497/782]: Loss 0.021896202117204666\n",
      "Training Batch [498/782]: Loss 0.0010201595723628998\n",
      "Training Batch [499/782]: Loss 0.007220560684800148\n",
      "Training Batch [500/782]: Loss 0.004664253443479538\n",
      "Training Batch [501/782]: Loss 0.007727987132966518\n",
      "Training Batch [502/782]: Loss 0.0148137416690588\n",
      "Training Batch [503/782]: Loss 0.01984500139951706\n",
      "Training Batch [504/782]: Loss 0.009823846630752087\n",
      "Training Batch [505/782]: Loss 0.0592588372528553\n",
      "Training Batch [506/782]: Loss 0.02422504313290119\n",
      "Training Batch [507/782]: Loss 0.02564053423702717\n",
      "Training Batch [508/782]: Loss 0.006993134506046772\n",
      "Training Batch [509/782]: Loss 0.04501825198531151\n",
      "Training Batch [510/782]: Loss 0.01032546628266573\n",
      "Training Batch [511/782]: Loss 0.08564603328704834\n",
      "Training Batch [512/782]: Loss 0.005149252712726593\n",
      "Training Batch [513/782]: Loss 0.010705625638365746\n",
      "Training Batch [514/782]: Loss 0.006225626450031996\n",
      "Training Batch [515/782]: Loss 0.021249640733003616\n",
      "Training Batch [516/782]: Loss 0.013982163742184639\n",
      "Training Batch [517/782]: Loss 0.0016041365452110767\n",
      "Training Batch [518/782]: Loss 0.005634871311485767\n",
      "Training Batch [519/782]: Loss 0.004987743217498064\n",
      "Training Batch [520/782]: Loss 0.023934783414006233\n",
      "Training Batch [521/782]: Loss 0.04179777204990387\n",
      "Training Batch [522/782]: Loss 0.005665203556418419\n",
      "Training Batch [523/782]: Loss 0.0007476775208488107\n",
      "Training Batch [524/782]: Loss 0.01295873336493969\n",
      "Training Batch [525/782]: Loss 0.030560150742530823\n",
      "Training Batch [526/782]: Loss 0.014629950746893883\n",
      "Training Batch [527/782]: Loss 0.004322963301092386\n",
      "Training Batch [528/782]: Loss 0.023064211010932922\n",
      "Training Batch [529/782]: Loss 0.03999761492013931\n",
      "Training Batch [530/782]: Loss 0.0031758341938257217\n",
      "Training Batch [531/782]: Loss 0.0041154250502586365\n",
      "Training Batch [532/782]: Loss 0.02741442434489727\n",
      "Training Batch [533/782]: Loss 0.02167726308107376\n",
      "Training Batch [534/782]: Loss 0.0076704323291778564\n",
      "Training Batch [535/782]: Loss 0.004172291606664658\n",
      "Training Batch [536/782]: Loss 0.017478182911872864\n",
      "Training Batch [537/782]: Loss 0.004591763485223055\n",
      "Training Batch [538/782]: Loss 0.0017045063432306051\n",
      "Training Batch [539/782]: Loss 0.013500513508915901\n",
      "Training Batch [540/782]: Loss 0.001341594965197146\n",
      "Training Batch [541/782]: Loss 0.007022445555776358\n",
      "Training Batch [542/782]: Loss 0.00462972279638052\n",
      "Training Batch [543/782]: Loss 0.004875995684415102\n",
      "Training Batch [544/782]: Loss 0.04656263068318367\n",
      "Training Batch [545/782]: Loss 0.05971632897853851\n",
      "Training Batch [546/782]: Loss 0.0106278695166111\n",
      "Training Batch [547/782]: Loss 0.002866537543013692\n",
      "Training Batch [548/782]: Loss 0.012834254652261734\n",
      "Training Batch [549/782]: Loss 0.006769511848688126\n",
      "Training Batch [550/782]: Loss 0.0013587819412350655\n",
      "Training Batch [551/782]: Loss 0.04210376739501953\n",
      "Training Batch [552/782]: Loss 0.003309498308226466\n",
      "Training Batch [553/782]: Loss 0.0012644043890759349\n",
      "Training Batch [554/782]: Loss 0.0074983700178563595\n",
      "Training Batch [555/782]: Loss 0.004387429915368557\n",
      "Training Batch [556/782]: Loss 0.00208719028159976\n",
      "Training Batch [557/782]: Loss 0.0041789100505411625\n",
      "Training Batch [558/782]: Loss 0.010722131468355656\n",
      "Training Batch [559/782]: Loss 0.009776342660188675\n",
      "Training Batch [560/782]: Loss 0.03528212010860443\n",
      "Training Batch [561/782]: Loss 0.008444069884717464\n",
      "Training Batch [562/782]: Loss 0.0059233070351183414\n",
      "Training Batch [563/782]: Loss 0.001801287173293531\n",
      "Training Batch [564/782]: Loss 0.044312745332717896\n",
      "Training Batch [565/782]: Loss 0.007034851238131523\n",
      "Training Batch [566/782]: Loss 0.014283226802945137\n",
      "Training Batch [567/782]: Loss 0.03497350588440895\n",
      "Training Batch [568/782]: Loss 0.0013541987864300609\n",
      "Training Batch [569/782]: Loss 0.006010045763105154\n",
      "Training Batch [570/782]: Loss 0.06633107364177704\n",
      "Training Batch [571/782]: Loss 0.007863172329962254\n",
      "Training Batch [572/782]: Loss 0.017955979332327843\n",
      "Training Batch [573/782]: Loss 0.020000971853733063\n",
      "Training Batch [574/782]: Loss 0.0362078994512558\n",
      "Training Batch [575/782]: Loss 0.029686175286769867\n",
      "Training Batch [576/782]: Loss 0.004778294358402491\n",
      "Training Batch [577/782]: Loss 0.027699021622538567\n",
      "Training Batch [578/782]: Loss 0.004878781735897064\n",
      "Training Batch [579/782]: Loss 0.0016249341424554586\n",
      "Training Batch [580/782]: Loss 0.0028846857603639364\n",
      "Training Batch [581/782]: Loss 0.030725959688425064\n",
      "Training Batch [582/782]: Loss 0.012386158108711243\n",
      "Training Batch [583/782]: Loss 0.00540930638089776\n",
      "Training Batch [584/782]: Loss 0.0012389050098136067\n",
      "Training Batch [585/782]: Loss 0.005924962926656008\n",
      "Training Batch [586/782]: Loss 0.0022365464828908443\n",
      "Training Batch [587/782]: Loss 0.002585711656138301\n",
      "Training Batch [588/782]: Loss 0.016071239486336708\n",
      "Training Batch [589/782]: Loss 0.00376434950158\n",
      "Training Batch [590/782]: Loss 0.025837713852524757\n",
      "Training Batch [591/782]: Loss 0.020707620307803154\n",
      "Training Batch [592/782]: Loss 0.0021128705702722073\n",
      "Training Batch [593/782]: Loss 0.009696179069578648\n",
      "Training Batch [594/782]: Loss 0.007257716730237007\n",
      "Training Batch [595/782]: Loss 0.051894623786211014\n",
      "Training Batch [596/782]: Loss 0.03169785812497139\n",
      "Training Batch [597/782]: Loss 0.0189447533339262\n",
      "Training Batch [598/782]: Loss 0.0007644016295671463\n",
      "Training Batch [599/782]: Loss 0.0067202202044427395\n",
      "Training Batch [600/782]: Loss 0.002197097521275282\n",
      "Training Batch [601/782]: Loss 0.0030277695041149855\n",
      "Training Batch [602/782]: Loss 0.0037808362394571304\n",
      "Training Batch [603/782]: Loss 0.006475075148046017\n",
      "Training Batch [604/782]: Loss 0.02426484227180481\n",
      "Training Batch [605/782]: Loss 0.006851765792816877\n",
      "Training Batch [606/782]: Loss 0.020886145532131195\n",
      "Training Batch [607/782]: Loss 0.00851964671164751\n",
      "Training Batch [608/782]: Loss 0.07733721286058426\n",
      "Training Batch [609/782]: Loss 0.00791533850133419\n",
      "Training Batch [610/782]: Loss 0.025196654722094536\n",
      "Training Batch [611/782]: Loss 0.06282080709934235\n",
      "Training Batch [612/782]: Loss 0.0026341804768890142\n",
      "Training Batch [613/782]: Loss 0.005775464698672295\n",
      "Training Batch [614/782]: Loss 0.005214162170886993\n",
      "Training Batch [615/782]: Loss 0.006583583541214466\n",
      "Training Batch [616/782]: Loss 0.005592403467744589\n",
      "Training Batch [617/782]: Loss 0.014139801263809204\n",
      "Training Batch [618/782]: Loss 0.0033576316200196743\n",
      "Training Batch [619/782]: Loss 0.0009142784983851016\n",
      "Training Batch [620/782]: Loss 0.00849652849137783\n",
      "Training Batch [621/782]: Loss 0.010259045287966728\n",
      "Training Batch [622/782]: Loss 0.007928525097668171\n",
      "Training Batch [623/782]: Loss 0.007822047919034958\n",
      "Training Batch [624/782]: Loss 0.020233461633324623\n",
      "Training Batch [625/782]: Loss 0.009807195514440536\n",
      "Training Batch [626/782]: Loss 0.016608886420726776\n",
      "Training Batch [627/782]: Loss 0.005953704006969929\n",
      "Training Batch [628/782]: Loss 0.006998779252171516\n",
      "Training Batch [629/782]: Loss 0.0031938112806528807\n",
      "Training Batch [630/782]: Loss 0.010918054729700089\n",
      "Training Batch [631/782]: Loss 0.0021557118743658066\n",
      "Training Batch [632/782]: Loss 0.005866114981472492\n",
      "Training Batch [633/782]: Loss 0.027437303215265274\n",
      "Training Batch [634/782]: Loss 0.0019978482741862535\n",
      "Training Batch [635/782]: Loss 0.0054016076028347015\n",
      "Training Batch [636/782]: Loss 0.006904319394379854\n",
      "Training Batch [637/782]: Loss 0.002615524223074317\n",
      "Training Batch [638/782]: Loss 0.007406487595289946\n",
      "Training Batch [639/782]: Loss 0.01902337558567524\n",
      "Training Batch [640/782]: Loss 0.021905379369854927\n",
      "Training Batch [641/782]: Loss 0.003911854699254036\n",
      "Training Batch [642/782]: Loss 0.0018848637118935585\n",
      "Training Batch [643/782]: Loss 0.004402700811624527\n",
      "Training Batch [644/782]: Loss 0.0027862999122589827\n",
      "Training Batch [645/782]: Loss 0.01684064045548439\n",
      "Training Batch [646/782]: Loss 0.008936988189816475\n",
      "Training Batch [647/782]: Loss 0.03004361130297184\n",
      "Training Batch [648/782]: Loss 0.003712861333042383\n",
      "Training Batch [649/782]: Loss 0.019836625084280968\n",
      "Training Batch [650/782]: Loss 0.008523386903107166\n",
      "Training Batch [651/782]: Loss 0.01722688041627407\n",
      "Training Batch [652/782]: Loss 0.038166262209415436\n",
      "Training Batch [653/782]: Loss 0.0015843616565689445\n",
      "Training Batch [654/782]: Loss 0.009005443193018436\n",
      "Training Batch [655/782]: Loss 0.012318212538957596\n",
      "Training Batch [656/782]: Loss 0.0033144881017506123\n",
      "Training Batch [657/782]: Loss 0.029814008623361588\n",
      "Training Batch [658/782]: Loss 0.00972048845142126\n",
      "Training Batch [659/782]: Loss 0.0016423608176410198\n",
      "Training Batch [660/782]: Loss 0.05414740741252899\n",
      "Training Batch [661/782]: Loss 0.0008340636268258095\n",
      "Training Batch [662/782]: Loss 0.008294173516333103\n",
      "Training Batch [663/782]: Loss 0.00029890405130572617\n",
      "Training Batch [664/782]: Loss 0.0016089614946395159\n",
      "Training Batch [665/782]: Loss 0.0027228284161537886\n",
      "Training Batch [666/782]: Loss 0.0038718590512871742\n",
      "Training Batch [667/782]: Loss 0.0008547660545445979\n",
      "Training Batch [668/782]: Loss 0.03151135519146919\n",
      "Training Batch [669/782]: Loss 0.009327072650194168\n",
      "Training Batch [670/782]: Loss 0.06490876525640488\n",
      "Training Batch [671/782]: Loss 0.011361368000507355\n",
      "Training Batch [672/782]: Loss 0.011981367133557796\n",
      "Training Batch [673/782]: Loss 0.011938688345253468\n",
      "Training Batch [674/782]: Loss 0.0428231842815876\n",
      "Training Batch [675/782]: Loss 0.03522277623414993\n",
      "Training Batch [676/782]: Loss 0.012098529376089573\n",
      "Training Batch [677/782]: Loss 0.007531620562076569\n",
      "Training Batch [678/782]: Loss 0.07072502374649048\n",
      "Training Batch [679/782]: Loss 0.005592196248471737\n",
      "Training Batch [680/782]: Loss 0.0029362274799495935\n",
      "Training Batch [681/782]: Loss 0.02022268809378147\n",
      "Training Batch [682/782]: Loss 0.008635293692350388\n",
      "Training Batch [683/782]: Loss 0.0296039879322052\n",
      "Training Batch [684/782]: Loss 0.0009611508576199412\n",
      "Training Batch [685/782]: Loss 0.016164639964699745\n",
      "Training Batch [686/782]: Loss 0.0018311303574591875\n",
      "Training Batch [687/782]: Loss 0.01896805129945278\n",
      "Training Batch [688/782]: Loss 0.00019881875778082758\n",
      "Training Batch [689/782]: Loss 0.0010439465986564755\n",
      "Training Batch [690/782]: Loss 0.003877712646499276\n",
      "Training Batch [691/782]: Loss 0.0009662520023994148\n",
      "Training Batch [692/782]: Loss 0.0008836527704261243\n",
      "Training Batch [693/782]: Loss 0.011746963486075401\n",
      "Training Batch [694/782]: Loss 0.012769105844199657\n",
      "Training Batch [695/782]: Loss 0.004056897945702076\n",
      "Training Batch [696/782]: Loss 0.005705310497432947\n",
      "Training Batch [697/782]: Loss 0.04172547161579132\n",
      "Training Batch [698/782]: Loss 0.0006846708711236715\n",
      "Training Batch [699/782]: Loss 0.0025630907621234655\n",
      "Training Batch [700/782]: Loss 0.0189512912184\n",
      "Training Batch [701/782]: Loss 0.011385503225028515\n",
      "Training Batch [702/782]: Loss 0.033836741000413895\n",
      "Training Batch [703/782]: Loss 0.0051880464889109135\n",
      "Training Batch [704/782]: Loss 0.02097429521381855\n",
      "Training Batch [705/782]: Loss 0.0055453162640333176\n",
      "Training Batch [706/782]: Loss 0.004736932925879955\n",
      "Training Batch [707/782]: Loss 0.02643418498337269\n",
      "Training Batch [708/782]: Loss 0.0037349911872297525\n",
      "Training Batch [709/782]: Loss 0.0008081943960860372\n",
      "Training Batch [710/782]: Loss 0.000498722365591675\n",
      "Training Batch [711/782]: Loss 0.025050237774848938\n",
      "Training Batch [712/782]: Loss 0.018208904191851616\n",
      "Training Batch [713/782]: Loss 0.011031660251319408\n",
      "Training Batch [714/782]: Loss 0.010054086335003376\n",
      "Training Batch [715/782]: Loss 0.009057071059942245\n",
      "Training Batch [716/782]: Loss 0.03005100227892399\n",
      "Training Batch [717/782]: Loss 0.007308470085263252\n",
      "Training Batch [718/782]: Loss 0.015576883219182491\n",
      "Training Batch [719/782]: Loss 0.009394285269081593\n",
      "Training Batch [720/782]: Loss 0.006199015770107508\n",
      "Training Batch [721/782]: Loss 0.0019644969142973423\n",
      "Training Batch [722/782]: Loss 0.024478456005454063\n",
      "Training Batch [723/782]: Loss 0.027808040380477905\n",
      "Training Batch [724/782]: Loss 0.0034615364857017994\n",
      "Training Batch [725/782]: Loss 0.02907460182905197\n",
      "Training Batch [726/782]: Loss 0.0029862034134566784\n",
      "Training Batch [727/782]: Loss 0.00264053069986403\n",
      "Training Batch [728/782]: Loss 0.0038887029513716698\n",
      "Training Batch [729/782]: Loss 0.0026042144745588303\n",
      "Training Batch [730/782]: Loss 0.0011590918293222785\n",
      "Training Batch [731/782]: Loss 0.0010188308078795671\n",
      "Training Batch [732/782]: Loss 0.0024942131713032722\n",
      "Training Batch [733/782]: Loss 0.10249003767967224\n",
      "Training Batch [734/782]: Loss 0.0021783241536468267\n",
      "Training Batch [735/782]: Loss 0.0075911832973361015\n",
      "Training Batch [736/782]: Loss 0.007115890737622976\n",
      "Training Batch [737/782]: Loss 0.0015373073983937502\n",
      "Training Batch [738/782]: Loss 0.0016976568149402738\n",
      "Training Batch [739/782]: Loss 0.0007163910777308047\n",
      "Training Batch [740/782]: Loss 0.00403802702203393\n",
      "Training Batch [741/782]: Loss 0.0023896140046417713\n",
      "Training Batch [742/782]: Loss 0.06601472944021225\n",
      "Training Batch [743/782]: Loss 0.008820845745503902\n",
      "Training Batch [744/782]: Loss 0.005326490383595228\n",
      "Training Batch [745/782]: Loss 0.0060921781696379185\n",
      "Training Batch [746/782]: Loss 0.06651127338409424\n",
      "Training Batch [747/782]: Loss 0.011024589650332928\n",
      "Training Batch [748/782]: Loss 0.007514597848057747\n",
      "Training Batch [749/782]: Loss 0.0033885964658111334\n",
      "Training Batch [750/782]: Loss 0.00026830093702301383\n",
      "Training Batch [751/782]: Loss 0.004699259996414185\n",
      "Training Batch [752/782]: Loss 0.06608424335718155\n",
      "Training Batch [753/782]: Loss 0.003545899875462055\n",
      "Training Batch [754/782]: Loss 0.0009911980014294386\n",
      "Training Batch [755/782]: Loss 0.0004981657257303596\n",
      "Training Batch [756/782]: Loss 0.06181972846388817\n",
      "Training Batch [757/782]: Loss 0.0016466763336211443\n",
      "Training Batch [758/782]: Loss 0.015211392194032669\n",
      "Training Batch [759/782]: Loss 0.004093740601092577\n",
      "Training Batch [760/782]: Loss 0.0811481848359108\n",
      "Training Batch [761/782]: Loss 0.0010975113837048411\n",
      "Training Batch [762/782]: Loss 0.026637010276317596\n",
      "Training Batch [763/782]: Loss 0.0006586487870663404\n",
      "Training Batch [764/782]: Loss 0.02292921021580696\n",
      "Training Batch [765/782]: Loss 0.021685941144824028\n",
      "Training Batch [766/782]: Loss 0.001961928326636553\n",
      "Training Batch [767/782]: Loss 0.004724308382719755\n",
      "Training Batch [768/782]: Loss 0.0075601679272949696\n",
      "Training Batch [769/782]: Loss 0.08482130616903305\n",
      "Training Batch [770/782]: Loss 0.0020696173887699842\n",
      "Training Batch [771/782]: Loss 0.0032381433993577957\n",
      "Training Batch [772/782]: Loss 0.018066108226776123\n",
      "Training Batch [773/782]: Loss 0.013674696907401085\n",
      "Training Batch [774/782]: Loss 0.010263039730489254\n",
      "Training Batch [775/782]: Loss 0.0006545398500747979\n",
      "Training Batch [776/782]: Loss 0.0453694686293602\n",
      "Training Batch [777/782]: Loss 0.01050474215298891\n",
      "Training Batch [778/782]: Loss 0.027694061398506165\n",
      "Training Batch [779/782]: Loss 0.0008933302597142756\n",
      "Training Batch [780/782]: Loss 0.005835489835590124\n",
      "Training Batch [781/782]: Loss 0.0179297998547554\n",
      "Training Batch [782/782]: Loss 0.00023300117754843086\n",
      "Epoch 24 - Train Loss: 0.0161\n",
      "*********  Epoch 25/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.002505883574485779\n",
      "Training Batch [2/782]: Loss 0.03664118051528931\n",
      "Training Batch [3/782]: Loss 0.005122596397995949\n",
      "Training Batch [4/782]: Loss 0.004466942977160215\n",
      "Training Batch [5/782]: Loss 0.00396021967753768\n",
      "Training Batch [6/782]: Loss 0.004210576880723238\n",
      "Training Batch [7/782]: Loss 0.00026046097627840936\n",
      "Training Batch [8/782]: Loss 0.0005286703235469759\n",
      "Training Batch [9/782]: Loss 0.03241587430238724\n",
      "Training Batch [10/782]: Loss 0.002946763299405575\n",
      "Training Batch [11/782]: Loss 0.0022048125974833965\n",
      "Training Batch [12/782]: Loss 0.008179845288395882\n",
      "Training Batch [13/782]: Loss 0.0031548941042274237\n",
      "Training Batch [14/782]: Loss 0.0397786870598793\n",
      "Training Batch [15/782]: Loss 0.11070617288351059\n",
      "Training Batch [16/782]: Loss 0.0018212248105555773\n",
      "Training Batch [17/782]: Loss 0.014694703742861748\n",
      "Training Batch [18/782]: Loss 0.07366973161697388\n",
      "Training Batch [19/782]: Loss 0.07047471404075623\n",
      "Training Batch [20/782]: Loss 0.004161199554800987\n",
      "Training Batch [21/782]: Loss 0.027986319735646248\n",
      "Training Batch [22/782]: Loss 0.0013546132249757648\n",
      "Training Batch [23/782]: Loss 0.04110872000455856\n",
      "Training Batch [24/782]: Loss 0.003676604712381959\n",
      "Training Batch [25/782]: Loss 0.0015621185302734375\n",
      "Training Batch [26/782]: Loss 0.0047687445767223835\n",
      "Training Batch [27/782]: Loss 0.0005254801944829524\n",
      "Training Batch [28/782]: Loss 0.11807816475629807\n",
      "Training Batch [29/782]: Loss 0.023488745093345642\n",
      "Training Batch [30/782]: Loss 0.006856079213321209\n",
      "Training Batch [31/782]: Loss 0.026415901258587837\n",
      "Training Batch [32/782]: Loss 0.012035376392304897\n",
      "Training Batch [33/782]: Loss 0.0009762459667399526\n",
      "Training Batch [34/782]: Loss 0.0009331168257631361\n",
      "Training Batch [35/782]: Loss 0.001145566115155816\n",
      "Training Batch [36/782]: Loss 0.0014305291697382927\n",
      "Training Batch [37/782]: Loss 0.00663610827177763\n",
      "Training Batch [38/782]: Loss 0.004135692026466131\n",
      "Training Batch [39/782]: Loss 0.0437876433134079\n",
      "Training Batch [40/782]: Loss 0.0021721282973885536\n",
      "Training Batch [41/782]: Loss 0.00913815014064312\n",
      "Training Batch [42/782]: Loss 0.003285531420260668\n",
      "Training Batch [43/782]: Loss 0.0023651255760341883\n",
      "Training Batch [44/782]: Loss 0.0005535499658435583\n",
      "Training Batch [45/782]: Loss 0.016272978857159615\n",
      "Training Batch [46/782]: Loss 0.008505966514348984\n",
      "Training Batch [47/782]: Loss 0.011486981064081192\n",
      "Training Batch [48/782]: Loss 0.010423626750707626\n",
      "Training Batch [49/782]: Loss 0.0068881516344845295\n",
      "Training Batch [50/782]: Loss 0.004192389082163572\n",
      "Training Batch [51/782]: Loss 0.0049230544827878475\n",
      "Training Batch [52/782]: Loss 0.001009480794891715\n",
      "Training Batch [53/782]: Loss 0.014456940814852715\n",
      "Training Batch [54/782]: Loss 0.0005954969674348831\n",
      "Training Batch [55/782]: Loss 0.008815242908895016\n",
      "Training Batch [56/782]: Loss 0.004208325874060392\n",
      "Training Batch [57/782]: Loss 0.001318576862104237\n",
      "Training Batch [58/782]: Loss 0.0007908565457910299\n",
      "Training Batch [59/782]: Loss 0.0006420142599381506\n",
      "Training Batch [60/782]: Loss 0.0005096921231597662\n",
      "Training Batch [61/782]: Loss 0.007114937994629145\n",
      "Training Batch [62/782]: Loss 0.00027822316042147577\n",
      "Training Batch [63/782]: Loss 0.0006655851611867547\n",
      "Training Batch [64/782]: Loss 0.0018442444270476699\n",
      "Training Batch [65/782]: Loss 0.006585205439478159\n",
      "Training Batch [66/782]: Loss 0.006528646685183048\n",
      "Training Batch [67/782]: Loss 0.0007053158478811383\n",
      "Training Batch [68/782]: Loss 0.004384033381938934\n",
      "Training Batch [69/782]: Loss 0.006240434944629669\n",
      "Training Batch [70/782]: Loss 0.0060532838106155396\n",
      "Training Batch [71/782]: Loss 0.0010246546007692814\n",
      "Training Batch [72/782]: Loss 0.001728105591610074\n",
      "Training Batch [73/782]: Loss 0.001712145283818245\n",
      "Training Batch [74/782]: Loss 0.0007330142543651164\n",
      "Training Batch [75/782]: Loss 0.0013086175313219428\n",
      "Training Batch [76/782]: Loss 0.021090351045131683\n",
      "Training Batch [77/782]: Loss 0.0018732097232714295\n",
      "Training Batch [78/782]: Loss 0.0018465823959559202\n",
      "Training Batch [79/782]: Loss 0.002102898433804512\n",
      "Training Batch [80/782]: Loss 0.0045087155885994434\n",
      "Training Batch [81/782]: Loss 0.0031478467863053083\n",
      "Training Batch [82/782]: Loss 0.001240918762050569\n",
      "Training Batch [83/782]: Loss 0.012456932105123997\n",
      "Training Batch [84/782]: Loss 0.057203784584999084\n",
      "Training Batch [85/782]: Loss 0.0395328588783741\n",
      "Training Batch [86/782]: Loss 0.0021835167426615953\n",
      "Training Batch [87/782]: Loss 0.0016487276880070567\n",
      "Training Batch [88/782]: Loss 0.002264162292703986\n",
      "Training Batch [89/782]: Loss 0.0023618259001523256\n",
      "Training Batch [90/782]: Loss 0.002839010674506426\n",
      "Training Batch [91/782]: Loss 0.008596101775765419\n",
      "Training Batch [92/782]: Loss 0.0002442366676405072\n",
      "Training Batch [93/782]: Loss 0.08236858248710632\n",
      "Training Batch [94/782]: Loss 0.02074737660586834\n",
      "Training Batch [95/782]: Loss 0.001342492876574397\n",
      "Training Batch [96/782]: Loss 0.0011038880329579115\n",
      "Training Batch [97/782]: Loss 0.011651243083178997\n",
      "Training Batch [98/782]: Loss 0.005212973803281784\n",
      "Training Batch [99/782]: Loss 0.0004247166507411748\n",
      "Training Batch [100/782]: Loss 0.0019295421661809087\n",
      "Training Batch [101/782]: Loss 0.0015684958780184388\n",
      "Training Batch [102/782]: Loss 0.006698375567793846\n",
      "Training Batch [103/782]: Loss 0.001205743057653308\n",
      "Training Batch [104/782]: Loss 0.0138549180701375\n",
      "Training Batch [105/782]: Loss 0.004556318745017052\n",
      "Training Batch [106/782]: Loss 0.0021953994873911142\n",
      "Training Batch [107/782]: Loss 0.006502833683043718\n",
      "Training Batch [108/782]: Loss 0.0018530383240431547\n",
      "Training Batch [109/782]: Loss 0.0013926002429798245\n",
      "Training Batch [110/782]: Loss 0.002274963539093733\n",
      "Training Batch [111/782]: Loss 0.003504470456391573\n",
      "Training Batch [112/782]: Loss 0.004755332134664059\n",
      "Training Batch [113/782]: Loss 0.015048886649310589\n",
      "Training Batch [114/782]: Loss 0.04893225058913231\n",
      "Training Batch [115/782]: Loss 0.05198995769023895\n",
      "Training Batch [116/782]: Loss 0.002382130129262805\n",
      "Training Batch [117/782]: Loss 0.013838396407663822\n",
      "Training Batch [118/782]: Loss 0.015534316189587116\n",
      "Training Batch [119/782]: Loss 0.0008251487161032856\n",
      "Training Batch [120/782]: Loss 0.0009259479702450335\n",
      "Training Batch [121/782]: Loss 0.0005323717487044632\n",
      "Training Batch [122/782]: Loss 0.004010353237390518\n",
      "Training Batch [123/782]: Loss 0.0032002145890146494\n",
      "Training Batch [124/782]: Loss 0.0033262285869568586\n",
      "Training Batch [125/782]: Loss 0.007459515240043402\n",
      "Training Batch [126/782]: Loss 0.01130048930644989\n",
      "Training Batch [127/782]: Loss 0.0005360573995858431\n",
      "Training Batch [128/782]: Loss 0.031052377074956894\n",
      "Training Batch [129/782]: Loss 0.019495531916618347\n",
      "Training Batch [130/782]: Loss 0.0005181941669434309\n",
      "Training Batch [131/782]: Loss 0.019154895097017288\n",
      "Training Batch [132/782]: Loss 0.0010893071303144097\n",
      "Training Batch [133/782]: Loss 0.013656188733875751\n",
      "Training Batch [134/782]: Loss 0.021171726286411285\n",
      "Training Batch [135/782]: Loss 0.025361713021993637\n",
      "Training Batch [136/782]: Loss 0.007015889510512352\n",
      "Training Batch [137/782]: Loss 0.009842071682214737\n",
      "Training Batch [138/782]: Loss 0.0027221643831580877\n",
      "Training Batch [139/782]: Loss 0.0003256825730204582\n",
      "Training Batch [140/782]: Loss 0.0007194631034508348\n",
      "Training Batch [141/782]: Loss 0.000594516983255744\n",
      "Training Batch [142/782]: Loss 0.0016299114795401692\n",
      "Training Batch [143/782]: Loss 0.0022624186240136623\n",
      "Training Batch [144/782]: Loss 0.03592117875814438\n",
      "Training Batch [145/782]: Loss 0.001990631455555558\n",
      "Training Batch [146/782]: Loss 0.004122751299291849\n",
      "Training Batch [147/782]: Loss 0.050996359437704086\n",
      "Training Batch [148/782]: Loss 0.0057187387719750404\n",
      "Training Batch [149/782]: Loss 0.05246372148394585\n",
      "Training Batch [150/782]: Loss 0.0020020168740302324\n",
      "Training Batch [151/782]: Loss 0.007748628035187721\n",
      "Training Batch [152/782]: Loss 0.039417900145053864\n",
      "Training Batch [153/782]: Loss 0.0023426818661391735\n",
      "Training Batch [154/782]: Loss 0.015690622851252556\n",
      "Training Batch [155/782]: Loss 0.001604430261068046\n",
      "Training Batch [156/782]: Loss 0.004476896021515131\n",
      "Training Batch [157/782]: Loss 0.06950342655181885\n",
      "Training Batch [158/782]: Loss 0.001860242336988449\n",
      "Training Batch [159/782]: Loss 0.0017432362074032426\n",
      "Training Batch [160/782]: Loss 0.004030647687613964\n",
      "Training Batch [161/782]: Loss 0.003788137808442116\n",
      "Training Batch [162/782]: Loss 0.023148052394390106\n",
      "Training Batch [163/782]: Loss 0.006708140950649977\n",
      "Training Batch [164/782]: Loss 0.0037434077821671963\n",
      "Training Batch [165/782]: Loss 0.0010006279917433858\n",
      "Training Batch [166/782]: Loss 0.020061461254954338\n",
      "Training Batch [167/782]: Loss 0.008808543905615807\n",
      "Training Batch [168/782]: Loss 0.002257621381431818\n",
      "Training Batch [169/782]: Loss 0.02341308258473873\n",
      "Training Batch [170/782]: Loss 0.03886818885803223\n",
      "Training Batch [171/782]: Loss 0.04033563286066055\n",
      "Training Batch [172/782]: Loss 0.005444897338747978\n",
      "Training Batch [173/782]: Loss 0.016046451404690742\n",
      "Training Batch [174/782]: Loss 0.003630378283560276\n",
      "Training Batch [175/782]: Loss 0.0064582196064293385\n",
      "Training Batch [176/782]: Loss 0.01049908809363842\n",
      "Training Batch [177/782]: Loss 0.01284507755190134\n",
      "Training Batch [178/782]: Loss 0.0029064943082630634\n",
      "Training Batch [179/782]: Loss 0.0004549589357338846\n",
      "Training Batch [180/782]: Loss 0.031815893948078156\n",
      "Training Batch [181/782]: Loss 0.0020168658811599016\n",
      "Training Batch [182/782]: Loss 0.01228419877588749\n",
      "Training Batch [183/782]: Loss 0.002247450640425086\n",
      "Training Batch [184/782]: Loss 0.014993024058640003\n",
      "Training Batch [185/782]: Loss 0.007314634509384632\n",
      "Training Batch [186/782]: Loss 0.001081762369722128\n",
      "Training Batch [187/782]: Loss 0.0043944502249360085\n",
      "Training Batch [188/782]: Loss 0.002986210398375988\n",
      "Training Batch [189/782]: Loss 0.010978172533214092\n",
      "Training Batch [190/782]: Loss 0.03766435384750366\n",
      "Training Batch [191/782]: Loss 0.025314193218946457\n",
      "Training Batch [192/782]: Loss 0.07572636008262634\n",
      "Training Batch [193/782]: Loss 0.005390466190874577\n",
      "Training Batch [194/782]: Loss 0.0007004840299487114\n",
      "Training Batch [195/782]: Loss 0.02178749442100525\n",
      "Training Batch [196/782]: Loss 0.00037534753209911287\n",
      "Training Batch [197/782]: Loss 0.0022212034091353416\n",
      "Training Batch [198/782]: Loss 0.0022611988242715597\n",
      "Training Batch [199/782]: Loss 0.0009598091710358858\n",
      "Training Batch [200/782]: Loss 0.000524875009432435\n",
      "Training Batch [201/782]: Loss 0.025068385526537895\n",
      "Training Batch [202/782]: Loss 0.012849627994000912\n",
      "Training Batch [203/782]: Loss 0.0077502066269516945\n",
      "Training Batch [204/782]: Loss 0.05746578797698021\n",
      "Training Batch [205/782]: Loss 0.05674988403916359\n",
      "Training Batch [206/782]: Loss 0.0021675857715308666\n",
      "Training Batch [207/782]: Loss 0.0013120328076183796\n",
      "Training Batch [208/782]: Loss 0.017866775393486023\n",
      "Training Batch [209/782]: Loss 0.026646794751286507\n",
      "Training Batch [210/782]: Loss 0.0005855185445398092\n",
      "Training Batch [211/782]: Loss 0.00599631667137146\n",
      "Training Batch [212/782]: Loss 0.00926146749407053\n",
      "Training Batch [213/782]: Loss 0.00521555170416832\n",
      "Training Batch [214/782]: Loss 0.014821961522102356\n",
      "Training Batch [215/782]: Loss 0.002130812732502818\n",
      "Training Batch [216/782]: Loss 0.0012463724706321955\n",
      "Training Batch [217/782]: Loss 0.0470900684595108\n",
      "Training Batch [218/782]: Loss 0.006629914976656437\n",
      "Training Batch [219/782]: Loss 0.0012713364558294415\n",
      "Training Batch [220/782]: Loss 0.003933773376047611\n",
      "Training Batch [221/782]: Loss 0.061543114483356476\n",
      "Training Batch [222/782]: Loss 0.01383997593075037\n",
      "Training Batch [223/782]: Loss 0.13931630551815033\n",
      "Training Batch [224/782]: Loss 0.014056392945349216\n",
      "Training Batch [225/782]: Loss 0.006667450536042452\n",
      "Training Batch [226/782]: Loss 0.0023173228837549686\n",
      "Training Batch [227/782]: Loss 0.0914836972951889\n",
      "Training Batch [228/782]: Loss 0.012117386795580387\n",
      "Training Batch [229/782]: Loss 0.04187415540218353\n",
      "Training Batch [230/782]: Loss 0.002816090127453208\n",
      "Training Batch [231/782]: Loss 0.015182442963123322\n",
      "Training Batch [232/782]: Loss 0.0021289237774908543\n",
      "Training Batch [233/782]: Loss 0.02488473244011402\n",
      "Training Batch [234/782]: Loss 0.008175143040716648\n",
      "Training Batch [235/782]: Loss 0.006294905208051205\n",
      "Training Batch [236/782]: Loss 0.009114480577409267\n",
      "Training Batch [237/782]: Loss 0.004567777272313833\n",
      "Training Batch [238/782]: Loss 0.08512304723262787\n",
      "Training Batch [239/782]: Loss 0.03570181876420975\n",
      "Training Batch [240/782]: Loss 0.036610692739486694\n",
      "Training Batch [241/782]: Loss 0.0042963651940226555\n",
      "Training Batch [242/782]: Loss 0.021726999431848526\n",
      "Training Batch [243/782]: Loss 0.007604643236845732\n",
      "Training Batch [244/782]: Loss 0.0009690958540886641\n",
      "Training Batch [245/782]: Loss 0.012505574151873589\n",
      "Training Batch [246/782]: Loss 0.09897775948047638\n",
      "Training Batch [247/782]: Loss 0.006142072379589081\n",
      "Training Batch [248/782]: Loss 0.0024099668953567743\n",
      "Training Batch [249/782]: Loss 0.04691087082028389\n",
      "Training Batch [250/782]: Loss 0.0009630793356336653\n",
      "Training Batch [251/782]: Loss 0.009838909842073917\n",
      "Training Batch [252/782]: Loss 0.0013769755605608225\n",
      "Training Batch [253/782]: Loss 0.012138302437961102\n",
      "Training Batch [254/782]: Loss 0.04504060372710228\n",
      "Training Batch [255/782]: Loss 0.016469014808535576\n",
      "Training Batch [256/782]: Loss 0.0024944229517132044\n",
      "Training Batch [257/782]: Loss 0.003928858786821365\n",
      "Training Batch [258/782]: Loss 0.004942783620208502\n",
      "Training Batch [259/782]: Loss 0.010389143601059914\n",
      "Training Batch [260/782]: Loss 0.014930724166333675\n",
      "Training Batch [261/782]: Loss 0.002880360931158066\n",
      "Training Batch [262/782]: Loss 0.014780950732529163\n",
      "Training Batch [263/782]: Loss 0.004241121932864189\n",
      "Training Batch [264/782]: Loss 0.00257106707431376\n",
      "Training Batch [265/782]: Loss 0.0035617095418274403\n",
      "Training Batch [266/782]: Loss 0.011684360913932323\n",
      "Training Batch [267/782]: Loss 0.0815928503870964\n",
      "Training Batch [268/782]: Loss 0.0010162953985854983\n",
      "Training Batch [269/782]: Loss 0.0024430223274976015\n",
      "Training Batch [270/782]: Loss 0.051246073096990585\n",
      "Training Batch [271/782]: Loss 0.01280200481414795\n",
      "Training Batch [272/782]: Loss 0.0009082278702408075\n",
      "Training Batch [273/782]: Loss 0.0008623651810921729\n",
      "Training Batch [274/782]: Loss 0.004024798050522804\n",
      "Training Batch [275/782]: Loss 0.02645510621368885\n",
      "Training Batch [276/782]: Loss 0.0032914679031819105\n",
      "Training Batch [277/782]: Loss 0.001012440538033843\n",
      "Training Batch [278/782]: Loss 0.03969068452715874\n",
      "Training Batch [279/782]: Loss 0.018208274617791176\n",
      "Training Batch [280/782]: Loss 0.002277741674333811\n",
      "Training Batch [281/782]: Loss 0.01958959735929966\n",
      "Training Batch [282/782]: Loss 0.008317932486534119\n",
      "Training Batch [283/782]: Loss 0.015006095170974731\n",
      "Training Batch [284/782]: Loss 0.001706971088424325\n",
      "Training Batch [285/782]: Loss 0.0320696160197258\n",
      "Training Batch [286/782]: Loss 0.0041405269876122475\n",
      "Training Batch [287/782]: Loss 0.0042244927026331425\n",
      "Training Batch [288/782]: Loss 0.021216487511992455\n",
      "Training Batch [289/782]: Loss 0.042343270033597946\n",
      "Training Batch [290/782]: Loss 0.06765998899936676\n",
      "Training Batch [291/782]: Loss 0.0043967426754534245\n",
      "Training Batch [292/782]: Loss 0.011346271261572838\n",
      "Training Batch [293/782]: Loss 0.05744065344333649\n",
      "Training Batch [294/782]: Loss 0.0005014785565435886\n",
      "Training Batch [295/782]: Loss 0.003592425724491477\n",
      "Training Batch [296/782]: Loss 0.003052138490602374\n",
      "Training Batch [297/782]: Loss 0.002852650824934244\n",
      "Training Batch [298/782]: Loss 0.01630067639052868\n",
      "Training Batch [299/782]: Loss 0.01766500622034073\n",
      "Training Batch [300/782]: Loss 0.00815708376467228\n",
      "Training Batch [301/782]: Loss 0.00172791862860322\n",
      "Training Batch [302/782]: Loss 0.013175633735954762\n",
      "Training Batch [303/782]: Loss 0.003651941427960992\n",
      "Training Batch [304/782]: Loss 0.001990995369851589\n",
      "Training Batch [305/782]: Loss 0.07237991690635681\n",
      "Training Batch [306/782]: Loss 0.049463171511888504\n",
      "Training Batch [307/782]: Loss 0.005954167805612087\n",
      "Training Batch [308/782]: Loss 0.0015478316927328706\n",
      "Training Batch [309/782]: Loss 0.017302170395851135\n",
      "Training Batch [310/782]: Loss 0.031217137351632118\n",
      "Training Batch [311/782]: Loss 0.00046481925528496504\n",
      "Training Batch [312/782]: Loss 0.004754131659865379\n",
      "Training Batch [313/782]: Loss 0.0011456668144091964\n",
      "Training Batch [314/782]: Loss 0.022580107674002647\n",
      "Training Batch [315/782]: Loss 0.012632057070732117\n",
      "Training Batch [316/782]: Loss 0.020727043971419334\n",
      "Training Batch [317/782]: Loss 0.14647626876831055\n",
      "Training Batch [318/782]: Loss 0.008200223557651043\n",
      "Training Batch [319/782]: Loss 0.010112108662724495\n",
      "Training Batch [320/782]: Loss 0.010152338072657585\n",
      "Training Batch [321/782]: Loss 0.005598516669124365\n",
      "Training Batch [322/782]: Loss 0.0017402928788214922\n",
      "Training Batch [323/782]: Loss 0.00392040703445673\n",
      "Training Batch [324/782]: Loss 0.0010673287324607372\n",
      "Training Batch [325/782]: Loss 0.05149221420288086\n",
      "Training Batch [326/782]: Loss 0.0011836116900667548\n",
      "Training Batch [327/782]: Loss 0.0008335215970873833\n",
      "Training Batch [328/782]: Loss 0.011219187639653683\n",
      "Training Batch [329/782]: Loss 0.0025818326976150274\n",
      "Training Batch [330/782]: Loss 0.0044976030476391315\n",
      "Training Batch [331/782]: Loss 0.000881286570802331\n",
      "Training Batch [332/782]: Loss 0.005986533127725124\n",
      "Training Batch [333/782]: Loss 0.048528555780649185\n",
      "Training Batch [334/782]: Loss 0.005801919847726822\n",
      "Training Batch [335/782]: Loss 0.007793974131345749\n",
      "Training Batch [336/782]: Loss 0.007851305417716503\n",
      "Training Batch [337/782]: Loss 0.00449818279594183\n",
      "Training Batch [338/782]: Loss 0.003156924620270729\n",
      "Training Batch [339/782]: Loss 0.04262533411383629\n",
      "Training Batch [340/782]: Loss 0.06562554091215134\n",
      "Training Batch [341/782]: Loss 0.07610505819320679\n",
      "Training Batch [342/782]: Loss 0.007715772371739149\n",
      "Training Batch [343/782]: Loss 0.152471125125885\n",
      "Training Batch [344/782]: Loss 0.0016769481590017676\n",
      "Training Batch [345/782]: Loss 0.0015302106039598584\n",
      "Training Batch [346/782]: Loss 0.0023714653216302395\n",
      "Training Batch [347/782]: Loss 0.002759794471785426\n",
      "Training Batch [348/782]: Loss 0.007669875398278236\n",
      "Training Batch [349/782]: Loss 0.007542061619460583\n",
      "Training Batch [350/782]: Loss 0.005974613595753908\n",
      "Training Batch [351/782]: Loss 0.05987130105495453\n",
      "Training Batch [352/782]: Loss 0.04085748270153999\n",
      "Training Batch [353/782]: Loss 0.0036265014205127954\n",
      "Training Batch [354/782]: Loss 0.004178591538220644\n",
      "Training Batch [355/782]: Loss 0.0015121294418349862\n",
      "Training Batch [356/782]: Loss 0.003948824945837259\n",
      "Training Batch [357/782]: Loss 0.04070625826716423\n",
      "Training Batch [358/782]: Loss 0.08381661027669907\n",
      "Training Batch [359/782]: Loss 0.09993243962526321\n",
      "Training Batch [360/782]: Loss 0.026326682418584824\n",
      "Training Batch [361/782]: Loss 0.0014949188334867358\n",
      "Training Batch [362/782]: Loss 0.011707942001521587\n",
      "Training Batch [363/782]: Loss 0.006744420621544123\n",
      "Training Batch [364/782]: Loss 0.09148767590522766\n",
      "Training Batch [365/782]: Loss 0.006290373392403126\n",
      "Training Batch [366/782]: Loss 0.004777788184583187\n",
      "Training Batch [367/782]: Loss 0.05015337094664574\n",
      "Training Batch [368/782]: Loss 0.01547984965145588\n",
      "Training Batch [369/782]: Loss 0.08312950283288956\n",
      "Training Batch [370/782]: Loss 0.009310957975685596\n",
      "Training Batch [371/782]: Loss 0.03015649877488613\n",
      "Training Batch [372/782]: Loss 0.014179813675582409\n",
      "Training Batch [373/782]: Loss 0.0006321445689536631\n",
      "Training Batch [374/782]: Loss 0.004732733592391014\n",
      "Training Batch [375/782]: Loss 0.05813157185912132\n",
      "Training Batch [376/782]: Loss 0.008442318998277187\n",
      "Training Batch [377/782]: Loss 0.014388810843229294\n",
      "Training Batch [378/782]: Loss 0.0031876573339104652\n",
      "Training Batch [379/782]: Loss 0.006575817707926035\n",
      "Training Batch [380/782]: Loss 0.06937091797590256\n",
      "Training Batch [381/782]: Loss 0.011007624678313732\n",
      "Training Batch [382/782]: Loss 0.0048770965076982975\n",
      "Training Batch [383/782]: Loss 0.011922174133360386\n",
      "Training Batch [384/782]: Loss 0.032788731157779694\n",
      "Training Batch [385/782]: Loss 0.018162930384278297\n",
      "Training Batch [386/782]: Loss 0.027379294857382774\n",
      "Training Batch [387/782]: Loss 0.0029739229939877987\n",
      "Training Batch [388/782]: Loss 0.05053134262561798\n",
      "Training Batch [389/782]: Loss 0.004218064248561859\n",
      "Training Batch [390/782]: Loss 0.006851447746157646\n",
      "Training Batch [391/782]: Loss 0.008422456681728363\n",
      "Training Batch [392/782]: Loss 0.09547235816717148\n",
      "Training Batch [393/782]: Loss 0.04183291643857956\n",
      "Training Batch [394/782]: Loss 0.009655999019742012\n",
      "Training Batch [395/782]: Loss 0.0020234936382621527\n",
      "Training Batch [396/782]: Loss 0.0827692300081253\n",
      "Training Batch [397/782]: Loss 0.01046530157327652\n",
      "Training Batch [398/782]: Loss 0.005168866366147995\n",
      "Training Batch [399/782]: Loss 0.032214466482400894\n",
      "Training Batch [400/782]: Loss 0.007566031068563461\n",
      "Training Batch [401/782]: Loss 0.038176871836185455\n",
      "Training Batch [402/782]: Loss 0.027801135554909706\n",
      "Training Batch [403/782]: Loss 0.023268509656190872\n",
      "Training Batch [404/782]: Loss 0.004435243550688028\n",
      "Training Batch [405/782]: Loss 0.047402117401361465\n",
      "Training Batch [406/782]: Loss 0.005385593976825476\n",
      "Training Batch [407/782]: Loss 0.04223083704710007\n",
      "Training Batch [408/782]: Loss 0.013384840451180935\n",
      "Training Batch [409/782]: Loss 0.027618328109383583\n",
      "Training Batch [410/782]: Loss 0.0021365294232964516\n",
      "Training Batch [411/782]: Loss 0.01796061173081398\n",
      "Training Batch [412/782]: Loss 0.011431684717535973\n",
      "Training Batch [413/782]: Loss 0.10089096426963806\n",
      "Training Batch [414/782]: Loss 0.0022628370206803083\n",
      "Training Batch [415/782]: Loss 0.05478496477007866\n",
      "Training Batch [416/782]: Loss 0.005735017824918032\n",
      "Training Batch [417/782]: Loss 0.022596271708607674\n",
      "Training Batch [418/782]: Loss 0.006582790054380894\n",
      "Training Batch [419/782]: Loss 0.0692908987402916\n",
      "Training Batch [420/782]: Loss 0.006928295828402042\n",
      "Training Batch [421/782]: Loss 0.02860839292407036\n",
      "Training Batch [422/782]: Loss 0.022916732355952263\n",
      "Training Batch [423/782]: Loss 0.001997561426833272\n",
      "Training Batch [424/782]: Loss 0.008464867249131203\n",
      "Training Batch [425/782]: Loss 0.005524014122784138\n",
      "Training Batch [426/782]: Loss 0.022067904472351074\n",
      "Training Batch [427/782]: Loss 0.03557008132338524\n",
      "Training Batch [428/782]: Loss 0.06855729967355728\n",
      "Training Batch [429/782]: Loss 0.08424659073352814\n",
      "Training Batch [430/782]: Loss 0.038056302815675735\n",
      "Training Batch [431/782]: Loss 0.04658662527799606\n",
      "Training Batch [432/782]: Loss 0.017540011554956436\n",
      "Training Batch [433/782]: Loss 0.048524536192417145\n",
      "Training Batch [434/782]: Loss 0.06576870381832123\n",
      "Training Batch [435/782]: Loss 0.006518363952636719\n",
      "Training Batch [436/782]: Loss 0.030602682381868362\n",
      "Training Batch [437/782]: Loss 0.014503644779324532\n",
      "Training Batch [438/782]: Loss 0.005047671031206846\n",
      "Training Batch [439/782]: Loss 0.005283536855131388\n",
      "Training Batch [440/782]: Loss 0.001913251937367022\n",
      "Training Batch [441/782]: Loss 0.02359646186232567\n",
      "Training Batch [442/782]: Loss 0.010829440318048\n",
      "Training Batch [443/782]: Loss 0.0034544242080301046\n",
      "Training Batch [444/782]: Loss 0.019395550712943077\n",
      "Training Batch [445/782]: Loss 0.025178583338856697\n",
      "Training Batch [446/782]: Loss 0.0030251629650592804\n",
      "Training Batch [447/782]: Loss 0.01192169077694416\n",
      "Training Batch [448/782]: Loss 0.04716462641954422\n",
      "Training Batch [449/782]: Loss 0.009587528184056282\n",
      "Training Batch [450/782]: Loss 0.041710156947374344\n",
      "Training Batch [451/782]: Loss 0.027257895097136497\n",
      "Training Batch [452/782]: Loss 0.004630233161151409\n",
      "Training Batch [453/782]: Loss 0.010668136179447174\n",
      "Training Batch [454/782]: Loss 0.028298214077949524\n",
      "Training Batch [455/782]: Loss 0.07754100859165192\n",
      "Training Batch [456/782]: Loss 0.053194839507341385\n",
      "Training Batch [457/782]: Loss 0.00291250366717577\n",
      "Training Batch [458/782]: Loss 0.012531999498605728\n",
      "Training Batch [459/782]: Loss 0.01269184798002243\n",
      "Training Batch [460/782]: Loss 0.012014774605631828\n",
      "Training Batch [461/782]: Loss 0.015259587205946445\n",
      "Training Batch [462/782]: Loss 0.02090328559279442\n",
      "Training Batch [463/782]: Loss 0.0007732686353847384\n",
      "Training Batch [464/782]: Loss 0.018180981278419495\n",
      "Training Batch [465/782]: Loss 0.008150585927069187\n",
      "Training Batch [466/782]: Loss 0.003654648084193468\n",
      "Training Batch [467/782]: Loss 0.09212541580200195\n",
      "Training Batch [468/782]: Loss 0.18120089173316956\n",
      "Training Batch [469/782]: Loss 0.05226742848753929\n",
      "Training Batch [470/782]: Loss 0.003236643970012665\n",
      "Training Batch [471/782]: Loss 0.0012922912137582898\n",
      "Training Batch [472/782]: Loss 0.011170942336320877\n",
      "Training Batch [473/782]: Loss 0.01007099263370037\n",
      "Training Batch [474/782]: Loss 0.0022898931056261063\n",
      "Training Batch [475/782]: Loss 0.09113110601902008\n",
      "Training Batch [476/782]: Loss 0.0024880666751414537\n",
      "Training Batch [477/782]: Loss 0.1177101731300354\n",
      "Training Batch [478/782]: Loss 0.02110542170703411\n",
      "Training Batch [479/782]: Loss 0.002044814871624112\n",
      "Training Batch [480/782]: Loss 0.04899207502603531\n",
      "Training Batch [481/782]: Loss 0.002122902311384678\n",
      "Training Batch [482/782]: Loss 0.07822471112012863\n",
      "Training Batch [483/782]: Loss 0.03434605151414871\n",
      "Training Batch [484/782]: Loss 0.06631331890821457\n",
      "Training Batch [485/782]: Loss 0.0010742524173110723\n",
      "Training Batch [486/782]: Loss 0.0034635120537132025\n",
      "Training Batch [487/782]: Loss 0.01874520815908909\n",
      "Training Batch [488/782]: Loss 0.002969475695863366\n",
      "Training Batch [489/782]: Loss 0.0003920183517038822\n",
      "Training Batch [490/782]: Loss 0.024517469108104706\n",
      "Training Batch [491/782]: Loss 0.02676929347217083\n",
      "Training Batch [492/782]: Loss 0.00036749651189893484\n",
      "Training Batch [493/782]: Loss 0.1429114043712616\n",
      "Training Batch [494/782]: Loss 0.02035881206393242\n",
      "Training Batch [495/782]: Loss 0.014655954204499722\n",
      "Training Batch [496/782]: Loss 0.004572784993797541\n",
      "Training Batch [497/782]: Loss 0.011555768549442291\n",
      "Training Batch [498/782]: Loss 0.03625732660293579\n",
      "Training Batch [499/782]: Loss 0.004950768314301968\n",
      "Training Batch [500/782]: Loss 0.014097091741859913\n",
      "Training Batch [501/782]: Loss 0.021714070811867714\n",
      "Training Batch [502/782]: Loss 0.010951108299195766\n",
      "Training Batch [503/782]: Loss 0.030754616484045982\n",
      "Training Batch [504/782]: Loss 0.008903896436095238\n",
      "Training Batch [505/782]: Loss 0.008941572159528732\n",
      "Training Batch [506/782]: Loss 0.03966342657804489\n",
      "Training Batch [507/782]: Loss 0.0034316410310566425\n",
      "Training Batch [508/782]: Loss 0.013221574015915394\n",
      "Training Batch [509/782]: Loss 0.026560647413134575\n",
      "Training Batch [510/782]: Loss 0.026288526132702827\n",
      "Training Batch [511/782]: Loss 0.02923755906522274\n",
      "Training Batch [512/782]: Loss 0.001707939663901925\n",
      "Training Batch [513/782]: Loss 0.0006687800050713122\n",
      "Training Batch [514/782]: Loss 0.01876339688897133\n",
      "Training Batch [515/782]: Loss 0.030944138765335083\n",
      "Training Batch [516/782]: Loss 0.0014454228803515434\n",
      "Training Batch [517/782]: Loss 0.004432496149092913\n",
      "Training Batch [518/782]: Loss 0.007925564423203468\n",
      "Training Batch [519/782]: Loss 0.004350816830992699\n",
      "Training Batch [520/782]: Loss 0.0016420237952843308\n",
      "Training Batch [521/782]: Loss 0.00832113716751337\n",
      "Training Batch [522/782]: Loss 0.016958096995949745\n",
      "Training Batch [523/782]: Loss 0.00045413715997710824\n",
      "Training Batch [524/782]: Loss 0.002818488050252199\n",
      "Training Batch [525/782]: Loss 0.02137584239244461\n",
      "Training Batch [526/782]: Loss 0.02239900454878807\n",
      "Training Batch [527/782]: Loss 0.021802067756652832\n",
      "Training Batch [528/782]: Loss 0.0018990059616044164\n",
      "Training Batch [529/782]: Loss 0.025491904467344284\n",
      "Training Batch [530/782]: Loss 0.008464138954877853\n",
      "Training Batch [531/782]: Loss 0.027153408154845238\n",
      "Training Batch [532/782]: Loss 0.009592028334736824\n",
      "Training Batch [533/782]: Loss 0.0025664782151579857\n",
      "Training Batch [534/782]: Loss 0.0043559991754591465\n",
      "Training Batch [535/782]: Loss 0.0025551174767315388\n",
      "Training Batch [536/782]: Loss 0.009009507484734058\n",
      "Training Batch [537/782]: Loss 0.05049121007323265\n",
      "Training Batch [538/782]: Loss 0.04160548001527786\n",
      "Training Batch [539/782]: Loss 0.004358250647783279\n",
      "Training Batch [540/782]: Loss 0.005647912621498108\n",
      "Training Batch [541/782]: Loss 0.03749502822756767\n",
      "Training Batch [542/782]: Loss 0.001487818663008511\n",
      "Training Batch [543/782]: Loss 0.00900963507592678\n",
      "Training Batch [544/782]: Loss 0.010841120034456253\n",
      "Training Batch [545/782]: Loss 0.0021560601890087128\n",
      "Training Batch [546/782]: Loss 0.007014918141067028\n",
      "Training Batch [547/782]: Loss 0.0037799531128257513\n",
      "Training Batch [548/782]: Loss 0.0011652400717139244\n",
      "Training Batch [549/782]: Loss 0.004865463823080063\n",
      "Training Batch [550/782]: Loss 0.03856556490063667\n",
      "Training Batch [551/782]: Loss 0.05137711018323898\n",
      "Training Batch [552/782]: Loss 0.0024602720513939857\n",
      "Training Batch [553/782]: Loss 0.047752585262060165\n",
      "Training Batch [554/782]: Loss 0.0018669393612071872\n",
      "Training Batch [555/782]: Loss 0.00332551053725183\n",
      "Training Batch [556/782]: Loss 0.006866544019430876\n",
      "Training Batch [557/782]: Loss 0.00925204437226057\n",
      "Training Batch [558/782]: Loss 0.015184963122010231\n",
      "Training Batch [559/782]: Loss 0.0034969414118677378\n",
      "Training Batch [560/782]: Loss 0.0070859212428331375\n",
      "Training Batch [561/782]: Loss 0.012240514159202576\n",
      "Training Batch [562/782]: Loss 0.011672320775687695\n",
      "Training Batch [563/782]: Loss 0.011803656816482544\n",
      "Training Batch [564/782]: Loss 0.04050615429878235\n",
      "Training Batch [565/782]: Loss 0.05509086698293686\n",
      "Training Batch [566/782]: Loss 0.015387401916086674\n",
      "Training Batch [567/782]: Loss 0.0010733818635344505\n",
      "Training Batch [568/782]: Loss 0.00553680956363678\n",
      "Training Batch [569/782]: Loss 0.004183793440461159\n",
      "Training Batch [570/782]: Loss 0.0026899187359958887\n",
      "Training Batch [571/782]: Loss 0.014462119899690151\n",
      "Training Batch [572/782]: Loss 0.0017540398985147476\n",
      "Training Batch [573/782]: Loss 0.010132045485079288\n",
      "Training Batch [574/782]: Loss 0.004717082250863314\n",
      "Training Batch [575/782]: Loss 0.006649523042142391\n",
      "Training Batch [576/782]: Loss 0.002532660961151123\n",
      "Training Batch [577/782]: Loss 0.04621944576501846\n",
      "Training Batch [578/782]: Loss 0.008901257067918777\n",
      "Training Batch [579/782]: Loss 0.012344649992883205\n",
      "Training Batch [580/782]: Loss 0.005469330586493015\n",
      "Training Batch [581/782]: Loss 0.05534863471984863\n",
      "Training Batch [582/782]: Loss 0.001973676262423396\n",
      "Training Batch [583/782]: Loss 0.0023877141065895557\n",
      "Training Batch [584/782]: Loss 0.023427484557032585\n",
      "Training Batch [585/782]: Loss 0.018321406096220016\n",
      "Training Batch [586/782]: Loss 0.0031466486398130655\n",
      "Training Batch [587/782]: Loss 0.023296501487493515\n",
      "Training Batch [588/782]: Loss 0.0014844940742477775\n",
      "Training Batch [589/782]: Loss 0.011960036121308804\n",
      "Training Batch [590/782]: Loss 0.02212357148528099\n",
      "Training Batch [591/782]: Loss 0.0032340586185455322\n",
      "Training Batch [592/782]: Loss 0.04163222014904022\n",
      "Training Batch [593/782]: Loss 0.0023937528021633625\n",
      "Training Batch [594/782]: Loss 0.002230505459010601\n",
      "Training Batch [595/782]: Loss 0.01107168011367321\n",
      "Training Batch [596/782]: Loss 0.05592700093984604\n",
      "Training Batch [597/782]: Loss 0.012759964913129807\n",
      "Training Batch [598/782]: Loss 0.035534653812646866\n",
      "Training Batch [599/782]: Loss 0.006873722188174725\n",
      "Training Batch [600/782]: Loss 0.00021275559265632182\n",
      "Training Batch [601/782]: Loss 0.03318917378783226\n",
      "Training Batch [602/782]: Loss 0.0031262985430657864\n",
      "Training Batch [603/782]: Loss 0.011094028130173683\n",
      "Training Batch [604/782]: Loss 0.002946494845673442\n",
      "Training Batch [605/782]: Loss 0.00031084517831914127\n",
      "Training Batch [606/782]: Loss 0.008129140362143517\n",
      "Training Batch [607/782]: Loss 0.02755534090101719\n",
      "Training Batch [608/782]: Loss 0.00818521436303854\n",
      "Training Batch [609/782]: Loss 0.003515631426125765\n",
      "Training Batch [610/782]: Loss 0.026206769049167633\n",
      "Training Batch [611/782]: Loss 0.006819451227784157\n",
      "Training Batch [612/782]: Loss 0.0013740884605795145\n",
      "Training Batch [613/782]: Loss 0.007191400043666363\n",
      "Training Batch [614/782]: Loss 0.027376297861337662\n",
      "Training Batch [615/782]: Loss 0.010657686740159988\n",
      "Training Batch [616/782]: Loss 0.025219418108463287\n",
      "Training Batch [617/782]: Loss 0.023697059601545334\n",
      "Training Batch [618/782]: Loss 0.0011573361698538065\n",
      "Training Batch [619/782]: Loss 0.005853111855685711\n",
      "Training Batch [620/782]: Loss 0.0014883982948958874\n",
      "Training Batch [621/782]: Loss 0.01810494251549244\n",
      "Training Batch [622/782]: Loss 0.006438452284783125\n",
      "Training Batch [623/782]: Loss 0.003131883218884468\n",
      "Training Batch [624/782]: Loss 0.006010955199599266\n",
      "Training Batch [625/782]: Loss 0.0002885258581954986\n",
      "Training Batch [626/782]: Loss 0.0006328219315037131\n",
      "Training Batch [627/782]: Loss 0.03198836371302605\n",
      "Training Batch [628/782]: Loss 0.02610168047249317\n",
      "Training Batch [629/782]: Loss 0.006157819647341967\n",
      "Training Batch [630/782]: Loss 0.0014133882941678166\n",
      "Training Batch [631/782]: Loss 0.0032748794183135033\n",
      "Training Batch [632/782]: Loss 0.002025121357291937\n",
      "Training Batch [633/782]: Loss 0.06620866060256958\n",
      "Training Batch [634/782]: Loss 0.009169552475214005\n",
      "Training Batch [635/782]: Loss 0.007315024733543396\n",
      "Training Batch [636/782]: Loss 0.013575741089880466\n",
      "Training Batch [637/782]: Loss 0.0033374689519405365\n",
      "Training Batch [638/782]: Loss 0.0013585995184257627\n",
      "Training Batch [639/782]: Loss 0.02621307037770748\n",
      "Training Batch [640/782]: Loss 0.10225115716457367\n",
      "Training Batch [641/782]: Loss 0.004400182515382767\n",
      "Training Batch [642/782]: Loss 0.00765810115262866\n",
      "Training Batch [643/782]: Loss 0.0069379364140331745\n",
      "Training Batch [644/782]: Loss 0.00490586319938302\n",
      "Training Batch [645/782]: Loss 0.0008710221736691892\n",
      "Training Batch [646/782]: Loss 0.0003657281049527228\n",
      "Training Batch [647/782]: Loss 0.0020920448005199432\n",
      "Training Batch [648/782]: Loss 0.005300820339471102\n",
      "Training Batch [649/782]: Loss 0.07295822352170944\n",
      "Training Batch [650/782]: Loss 0.011133247055113316\n",
      "Training Batch [651/782]: Loss 0.002468878636136651\n",
      "Training Batch [652/782]: Loss 0.03576982393860817\n",
      "Training Batch [653/782]: Loss 0.003055746201425791\n",
      "Training Batch [654/782]: Loss 0.007193460129201412\n",
      "Training Batch [655/782]: Loss 0.00179089920129627\n",
      "Training Batch [656/782]: Loss 0.002786295022815466\n",
      "Training Batch [657/782]: Loss 0.06194859743118286\n",
      "Training Batch [658/782]: Loss 0.0052602351643145084\n",
      "Training Batch [659/782]: Loss 0.0184571985155344\n",
      "Training Batch [660/782]: Loss 0.02897840179502964\n",
      "Training Batch [661/782]: Loss 0.04074094444513321\n",
      "Training Batch [662/782]: Loss 0.006880457047373056\n",
      "Training Batch [663/782]: Loss 0.0071539427153766155\n",
      "Training Batch [664/782]: Loss 0.0009294626070186496\n",
      "Training Batch [665/782]: Loss 0.016128143295645714\n",
      "Training Batch [666/782]: Loss 0.016255440190434456\n",
      "Training Batch [667/782]: Loss 0.0883965939283371\n",
      "Training Batch [668/782]: Loss 0.01987166330218315\n",
      "Training Batch [669/782]: Loss 0.004608591552823782\n",
      "Training Batch [670/782]: Loss 0.003771491115912795\n",
      "Training Batch [671/782]: Loss 0.00914506334811449\n",
      "Training Batch [672/782]: Loss 0.02229124680161476\n",
      "Training Batch [673/782]: Loss 0.005833229050040245\n",
      "Training Batch [674/782]: Loss 0.004990399815142155\n",
      "Training Batch [675/782]: Loss 0.0011043231934309006\n",
      "Training Batch [676/782]: Loss 0.005366128869354725\n",
      "Training Batch [677/782]: Loss 0.00236026500351727\n",
      "Training Batch [678/782]: Loss 0.009647187776863575\n",
      "Training Batch [679/782]: Loss 0.02731584943830967\n",
      "Training Batch [680/782]: Loss 0.01881733536720276\n",
      "Training Batch [681/782]: Loss 0.009007417596876621\n",
      "Training Batch [682/782]: Loss 0.002116446616128087\n",
      "Training Batch [683/782]: Loss 0.00820377841591835\n",
      "Training Batch [684/782]: Loss 0.002234614221379161\n",
      "Training Batch [685/782]: Loss 0.004542369395494461\n",
      "Training Batch [686/782]: Loss 0.06793563812971115\n",
      "Training Batch [687/782]: Loss 0.0292540080845356\n",
      "Training Batch [688/782]: Loss 0.0022802362218499184\n",
      "Training Batch [689/782]: Loss 0.02682916820049286\n",
      "Training Batch [690/782]: Loss 0.0025575063191354275\n",
      "Training Batch [691/782]: Loss 0.0029420643113553524\n",
      "Training Batch [692/782]: Loss 0.0007576434873044491\n",
      "Training Batch [693/782]: Loss 0.010573889128863811\n",
      "Training Batch [694/782]: Loss 0.042112935334444046\n",
      "Training Batch [695/782]: Loss 0.05518999695777893\n",
      "Training Batch [696/782]: Loss 0.030132411047816277\n",
      "Training Batch [697/782]: Loss 0.0012218795018270612\n",
      "Training Batch [698/782]: Loss 0.05844751372933388\n",
      "Training Batch [699/782]: Loss 0.0011542887659743428\n",
      "Training Batch [700/782]: Loss 0.005343280732631683\n",
      "Training Batch [701/782]: Loss 0.022073041647672653\n",
      "Training Batch [702/782]: Loss 0.0015400382690131664\n",
      "Training Batch [703/782]: Loss 0.007788354065269232\n",
      "Training Batch [704/782]: Loss 0.001238752738572657\n",
      "Training Batch [705/782]: Loss 0.018470928072929382\n",
      "Training Batch [706/782]: Loss 0.011802772991359234\n",
      "Training Batch [707/782]: Loss 0.0002735862508416176\n",
      "Training Batch [708/782]: Loss 0.004056725651025772\n",
      "Training Batch [709/782]: Loss 0.005881900433450937\n",
      "Training Batch [710/782]: Loss 0.01223940122872591\n",
      "Training Batch [711/782]: Loss 0.0007549749570898712\n",
      "Training Batch [712/782]: Loss 0.03168974444270134\n",
      "Training Batch [713/782]: Loss 0.02157377079129219\n",
      "Training Batch [714/782]: Loss 0.06246941164135933\n",
      "Training Batch [715/782]: Loss 0.00981278344988823\n",
      "Training Batch [716/782]: Loss 0.016891388222575188\n",
      "Training Batch [717/782]: Loss 0.005567830987274647\n",
      "Training Batch [718/782]: Loss 0.03208238258957863\n",
      "Training Batch [719/782]: Loss 0.04648607224225998\n",
      "Training Batch [720/782]: Loss 0.0007262525614351034\n",
      "Training Batch [721/782]: Loss 0.016902917996048927\n",
      "Training Batch [722/782]: Loss 0.00539351487532258\n",
      "Training Batch [723/782]: Loss 0.006150954402983189\n",
      "Training Batch [724/782]: Loss 0.0017452731262892485\n",
      "Training Batch [725/782]: Loss 0.0011803389061242342\n",
      "Training Batch [726/782]: Loss 0.002547007752582431\n",
      "Training Batch [727/782]: Loss 0.02999192848801613\n",
      "Training Batch [728/782]: Loss 0.006227691657841206\n",
      "Training Batch [729/782]: Loss 0.05008278787136078\n",
      "Training Batch [730/782]: Loss 0.03513781353831291\n",
      "Training Batch [731/782]: Loss 0.01572972722351551\n",
      "Training Batch [732/782]: Loss 0.0135751161724329\n",
      "Training Batch [733/782]: Loss 0.0031372036319226027\n",
      "Training Batch [734/782]: Loss 0.07786329090595245\n",
      "Training Batch [735/782]: Loss 0.00783644337207079\n",
      "Training Batch [736/782]: Loss 0.004204464145004749\n",
      "Training Batch [737/782]: Loss 0.001566687016747892\n",
      "Training Batch [738/782]: Loss 0.010901090689003468\n",
      "Training Batch [739/782]: Loss 0.011502253822982311\n",
      "Training Batch [740/782]: Loss 0.015424857847392559\n",
      "Training Batch [741/782]: Loss 0.0009710583835840225\n",
      "Training Batch [742/782]: Loss 0.00668551167473197\n",
      "Training Batch [743/782]: Loss 0.015098359435796738\n",
      "Training Batch [744/782]: Loss 0.005143162794411182\n",
      "Training Batch [745/782]: Loss 0.00866902619600296\n",
      "Training Batch [746/782]: Loss 0.0067099654115736485\n",
      "Training Batch [747/782]: Loss 0.07132542133331299\n",
      "Training Batch [748/782]: Loss 0.006696094758808613\n",
      "Training Batch [749/782]: Loss 0.0015121989417821169\n",
      "Training Batch [750/782]: Loss 0.023374635726213455\n",
      "Training Batch [751/782]: Loss 0.0040294816717505455\n",
      "Training Batch [752/782]: Loss 0.01310118380934\n",
      "Training Batch [753/782]: Loss 0.0010279947891831398\n",
      "Training Batch [754/782]: Loss 0.0027701559010893106\n",
      "Training Batch [755/782]: Loss 0.018163612112402916\n",
      "Training Batch [756/782]: Loss 0.01111854612827301\n",
      "Training Batch [757/782]: Loss 0.03986387327313423\n",
      "Training Batch [758/782]: Loss 0.004590502008795738\n",
      "Training Batch [759/782]: Loss 0.0009950472740456462\n",
      "Training Batch [760/782]: Loss 0.004848137963563204\n",
      "Training Batch [761/782]: Loss 0.0930686891078949\n",
      "Training Batch [762/782]: Loss 0.005752419587224722\n",
      "Training Batch [763/782]: Loss 0.0027234703302383423\n",
      "Training Batch [764/782]: Loss 0.009969256818294525\n",
      "Training Batch [765/782]: Loss 0.020079584792256355\n",
      "Training Batch [766/782]: Loss 0.0650731697678566\n",
      "Training Batch [767/782]: Loss 0.0032698765862733126\n",
      "Training Batch [768/782]: Loss 0.011431638151407242\n",
      "Training Batch [769/782]: Loss 0.005271582398563623\n",
      "Training Batch [770/782]: Loss 0.06012599170207977\n",
      "Training Batch [771/782]: Loss 0.008843425661325455\n",
      "Training Batch [772/782]: Loss 0.002258055144920945\n",
      "Training Batch [773/782]: Loss 0.018386099487543106\n",
      "Training Batch [774/782]: Loss 0.01180871482938528\n",
      "Training Batch [775/782]: Loss 0.11387865990400314\n",
      "Training Batch [776/782]: Loss 0.0027724141255021095\n",
      "Training Batch [777/782]: Loss 0.001675416249781847\n",
      "Training Batch [778/782]: Loss 0.011082607321441174\n",
      "Training Batch [779/782]: Loss 0.09520331770181656\n",
      "Training Batch [780/782]: Loss 0.013057582080364227\n",
      "Training Batch [781/782]: Loss 0.024759970605373383\n",
      "Training Batch [782/782]: Loss 0.10510899871587753\n",
      "Epoch 25 - Train Loss: 0.0177\n",
      "*********  Epoch 26/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.0017147427424788475\n",
      "Training Batch [2/782]: Loss 0.015194966457784176\n",
      "Training Batch [3/782]: Loss 0.04524771496653557\n",
      "Training Batch [4/782]: Loss 0.009092356078326702\n",
      "Training Batch [5/782]: Loss 0.00449894554913044\n",
      "Training Batch [6/782]: Loss 0.2617761492729187\n",
      "Training Batch [7/782]: Loss 0.017431208863854408\n",
      "Training Batch [8/782]: Loss 0.08070462942123413\n",
      "Training Batch [9/782]: Loss 0.012455705553293228\n",
      "Training Batch [10/782]: Loss 0.003313937457278371\n",
      "Training Batch [11/782]: Loss 0.004743857774883509\n",
      "Training Batch [12/782]: Loss 0.03690560907125473\n",
      "Training Batch [13/782]: Loss 0.023382507264614105\n",
      "Training Batch [14/782]: Loss 0.008761873468756676\n",
      "Training Batch [15/782]: Loss 0.04639717563986778\n",
      "Training Batch [16/782]: Loss 0.006473974324762821\n",
      "Training Batch [17/782]: Loss 0.00593446334823966\n",
      "Training Batch [18/782]: Loss 0.05132071301341057\n",
      "Training Batch [19/782]: Loss 0.004588640294969082\n",
      "Training Batch [20/782]: Loss 0.0019382464233785868\n",
      "Training Batch [21/782]: Loss 0.0017932180780917406\n",
      "Training Batch [22/782]: Loss 0.020248956978321075\n",
      "Training Batch [23/782]: Loss 0.014279630966484547\n",
      "Training Batch [24/782]: Loss 0.08671151101589203\n",
      "Training Batch [25/782]: Loss 0.006393211428076029\n",
      "Training Batch [26/782]: Loss 0.012752441689372063\n",
      "Training Batch [27/782]: Loss 0.01698852702975273\n",
      "Training Batch [28/782]: Loss 0.014663233421742916\n",
      "Training Batch [29/782]: Loss 0.006528552621603012\n",
      "Training Batch [30/782]: Loss 0.005024666897952557\n",
      "Training Batch [31/782]: Loss 0.026444826275110245\n",
      "Training Batch [32/782]: Loss 0.1409640610218048\n",
      "Training Batch [33/782]: Loss 0.08637112379074097\n",
      "Training Batch [34/782]: Loss 0.2343839406967163\n",
      "Training Batch [35/782]: Loss 0.005597326438874006\n",
      "Training Batch [36/782]: Loss 0.0073955063708126545\n",
      "Training Batch [37/782]: Loss 0.049002278596162796\n",
      "Training Batch [38/782]: Loss 0.007466196548193693\n",
      "Training Batch [39/782]: Loss 0.018304159864783287\n",
      "Training Batch [40/782]: Loss 0.03325030952692032\n",
      "Training Batch [41/782]: Loss 0.0033624954521656036\n",
      "Training Batch [42/782]: Loss 0.008351695723831654\n",
      "Training Batch [43/782]: Loss 0.02610965259373188\n",
      "Training Batch [44/782]: Loss 0.01848416030406952\n",
      "Training Batch [45/782]: Loss 0.048860467970371246\n",
      "Training Batch [46/782]: Loss 0.009799265302717686\n",
      "Training Batch [47/782]: Loss 0.006230111233890057\n",
      "Training Batch [48/782]: Loss 0.00402594730257988\n",
      "Training Batch [49/782]: Loss 0.012523510493338108\n",
      "Training Batch [50/782]: Loss 0.015131823718547821\n",
      "Training Batch [51/782]: Loss 0.03195697441697121\n",
      "Training Batch [52/782]: Loss 0.0019187465077266097\n",
      "Training Batch [53/782]: Loss 0.015987325459718704\n",
      "Training Batch [54/782]: Loss 0.00827946700155735\n",
      "Training Batch [55/782]: Loss 0.01257220096886158\n",
      "Training Batch [56/782]: Loss 0.004477841779589653\n",
      "Training Batch [57/782]: Loss 0.012328465469181538\n",
      "Training Batch [58/782]: Loss 0.030078347772359848\n",
      "Training Batch [59/782]: Loss 0.0037549627013504505\n",
      "Training Batch [60/782]: Loss 0.0054802680388092995\n",
      "Training Batch [61/782]: Loss 0.03877925127744675\n",
      "Training Batch [62/782]: Loss 0.014025921002030373\n",
      "Training Batch [63/782]: Loss 0.003144785761833191\n",
      "Training Batch [64/782]: Loss 0.019120480865240097\n",
      "Training Batch [65/782]: Loss 0.0749850645661354\n",
      "Training Batch [66/782]: Loss 0.08282657712697983\n",
      "Training Batch [67/782]: Loss 0.0033822376281023026\n",
      "Training Batch [68/782]: Loss 0.0733206570148468\n",
      "Training Batch [69/782]: Loss 0.03187982738018036\n",
      "Training Batch [70/782]: Loss 0.07111995667219162\n",
      "Training Batch [71/782]: Loss 0.1515892893075943\n",
      "Training Batch [72/782]: Loss 0.014752917923033237\n",
      "Training Batch [73/782]: Loss 0.003757213708013296\n",
      "Training Batch [74/782]: Loss 0.004371239338070154\n",
      "Training Batch [75/782]: Loss 0.08682398498058319\n",
      "Training Batch [76/782]: Loss 0.057443853467702866\n",
      "Training Batch [77/782]: Loss 0.019384877756237984\n",
      "Training Batch [78/782]: Loss 0.0014623343013226986\n",
      "Training Batch [79/782]: Loss 0.004124501254409552\n",
      "Training Batch [80/782]: Loss 0.03769410774111748\n",
      "Training Batch [81/782]: Loss 0.0028435015119612217\n",
      "Training Batch [82/782]: Loss 0.03190867230296135\n",
      "Training Batch [83/782]: Loss 0.011426853947341442\n",
      "Training Batch [84/782]: Loss 0.002064416417852044\n",
      "Training Batch [85/782]: Loss 0.017085228115320206\n",
      "Training Batch [86/782]: Loss 0.021561510860919952\n",
      "Training Batch [87/782]: Loss 0.004372749011963606\n",
      "Training Batch [88/782]: Loss 0.01230529323220253\n",
      "Training Batch [89/782]: Loss 0.00622231001034379\n",
      "Training Batch [90/782]: Loss 0.01822827197611332\n",
      "Training Batch [91/782]: Loss 0.002940853824838996\n",
      "Training Batch [92/782]: Loss 0.00810870062559843\n",
      "Training Batch [93/782]: Loss 0.0013685643207281828\n",
      "Training Batch [94/782]: Loss 0.03083028644323349\n",
      "Training Batch [95/782]: Loss 0.005454741418361664\n",
      "Training Batch [96/782]: Loss 0.025944385677576065\n",
      "Training Batch [97/782]: Loss 0.013271993026137352\n",
      "Training Batch [98/782]: Loss 0.028650544583797455\n",
      "Training Batch [99/782]: Loss 0.010233930312097073\n",
      "Training Batch [100/782]: Loss 0.0029409301932901144\n",
      "Training Batch [101/782]: Loss 0.0010098663624376059\n",
      "Training Batch [102/782]: Loss 0.06673561036586761\n",
      "Training Batch [103/782]: Loss 0.04677283763885498\n",
      "Training Batch [104/782]: Loss 0.020453190430998802\n",
      "Training Batch [105/782]: Loss 0.026070334017276764\n",
      "Training Batch [106/782]: Loss 0.11926797777414322\n",
      "Training Batch [107/782]: Loss 0.02754686214029789\n",
      "Training Batch [108/782]: Loss 0.005213464144617319\n",
      "Training Batch [109/782]: Loss 0.008387585170567036\n",
      "Training Batch [110/782]: Loss 0.010874570347368717\n",
      "Training Batch [111/782]: Loss 0.0046668932773172855\n",
      "Training Batch [112/782]: Loss 0.011774713173508644\n",
      "Training Batch [113/782]: Loss 0.001999156316742301\n",
      "Training Batch [114/782]: Loss 0.02777944691479206\n",
      "Training Batch [115/782]: Loss 0.011856978759169579\n",
      "Training Batch [116/782]: Loss 0.04378066956996918\n",
      "Training Batch [117/782]: Loss 0.013938326388597488\n",
      "Training Batch [118/782]: Loss 0.06559854745864868\n",
      "Training Batch [119/782]: Loss 0.007232251577079296\n",
      "Training Batch [120/782]: Loss 0.018046725541353226\n",
      "Training Batch [121/782]: Loss 0.01078067347407341\n",
      "Training Batch [122/782]: Loss 0.008217960596084595\n",
      "Training Batch [123/782]: Loss 0.006410183385014534\n",
      "Training Batch [124/782]: Loss 0.028028981760144234\n",
      "Training Batch [125/782]: Loss 0.00920806173235178\n",
      "Training Batch [126/782]: Loss 0.05764331668615341\n",
      "Training Batch [127/782]: Loss 0.007157410029321909\n",
      "Training Batch [128/782]: Loss 0.0006658674101345241\n",
      "Training Batch [129/782]: Loss 0.03281698375940323\n",
      "Training Batch [130/782]: Loss 0.006953987292945385\n",
      "Training Batch [131/782]: Loss 0.0016731347423046827\n",
      "Training Batch [132/782]: Loss 0.0007786589558236301\n",
      "Training Batch [133/782]: Loss 0.016511300578713417\n",
      "Training Batch [134/782]: Loss 0.03335564583539963\n",
      "Training Batch [135/782]: Loss 0.060240089893341064\n",
      "Training Batch [136/782]: Loss 0.0002785116375889629\n",
      "Training Batch [137/782]: Loss 0.02229403704404831\n",
      "Training Batch [138/782]: Loss 0.0006833036313764751\n",
      "Training Batch [139/782]: Loss 0.11094386130571365\n",
      "Training Batch [140/782]: Loss 0.02483334392309189\n",
      "Training Batch [141/782]: Loss 0.009587124921381474\n",
      "Training Batch [142/782]: Loss 0.0008529350161552429\n",
      "Training Batch [143/782]: Loss 0.007499233819544315\n",
      "Training Batch [144/782]: Loss 0.007551930379122496\n",
      "Training Batch [145/782]: Loss 0.01952730119228363\n",
      "Training Batch [146/782]: Loss 0.011386699974536896\n",
      "Training Batch [147/782]: Loss 0.008544954471290112\n",
      "Training Batch [148/782]: Loss 0.009976974688470364\n",
      "Training Batch [149/782]: Loss 0.021602816879749298\n",
      "Training Batch [150/782]: Loss 0.002281439956277609\n",
      "Training Batch [151/782]: Loss 0.001212293398566544\n",
      "Training Batch [152/782]: Loss 0.012481066398322582\n",
      "Training Batch [153/782]: Loss 0.047150030732154846\n",
      "Training Batch [154/782]: Loss 0.04049258306622505\n",
      "Training Batch [155/782]: Loss 0.000872704666107893\n",
      "Training Batch [156/782]: Loss 0.004359217826277018\n",
      "Training Batch [157/782]: Loss 0.01249771285802126\n",
      "Training Batch [158/782]: Loss 0.05930231139063835\n",
      "Training Batch [159/782]: Loss 0.005001916084438562\n",
      "Training Batch [160/782]: Loss 0.0016949456185102463\n",
      "Training Batch [161/782]: Loss 0.0007030171109363437\n",
      "Training Batch [162/782]: Loss 0.009489641524851322\n",
      "Training Batch [163/782]: Loss 0.011246083304286003\n",
      "Training Batch [164/782]: Loss 0.03468824923038483\n",
      "Training Batch [165/782]: Loss 0.0017991075292229652\n",
      "Training Batch [166/782]: Loss 0.04783698171377182\n",
      "Training Batch [167/782]: Loss 0.011215212754905224\n",
      "Training Batch [168/782]: Loss 0.07068108767271042\n",
      "Training Batch [169/782]: Loss 0.05130787193775177\n",
      "Training Batch [170/782]: Loss 0.001493110554292798\n",
      "Training Batch [171/782]: Loss 0.018310721963644028\n",
      "Training Batch [172/782]: Loss 0.0015300947707146406\n",
      "Training Batch [173/782]: Loss 0.03798738867044449\n",
      "Training Batch [174/782]: Loss 0.00329667329788208\n",
      "Training Batch [175/782]: Loss 0.08910179138183594\n",
      "Training Batch [176/782]: Loss 0.027450665831565857\n",
      "Training Batch [177/782]: Loss 0.00904883537441492\n",
      "Training Batch [178/782]: Loss 0.016573116183280945\n",
      "Training Batch [179/782]: Loss 0.003083184128627181\n",
      "Training Batch [180/782]: Loss 0.0059986780397593975\n",
      "Training Batch [181/782]: Loss 0.12561923265457153\n",
      "Training Batch [182/782]: Loss 0.002483404939994216\n",
      "Training Batch [183/782]: Loss 0.053238946944475174\n",
      "Training Batch [184/782]: Loss 0.00895841047167778\n",
      "Training Batch [185/782]: Loss 0.04063565656542778\n",
      "Training Batch [186/782]: Loss 0.008612335659563541\n",
      "Training Batch [187/782]: Loss 0.0033373862970620394\n",
      "Training Batch [188/782]: Loss 0.0022053210996091366\n",
      "Training Batch [189/782]: Loss 0.01071857288479805\n",
      "Training Batch [190/782]: Loss 0.02520420216023922\n",
      "Training Batch [191/782]: Loss 0.08246152102947235\n",
      "Training Batch [192/782]: Loss 0.0025188878644257784\n",
      "Training Batch [193/782]: Loss 0.010086572729051113\n",
      "Training Batch [194/782]: Loss 0.011093205772340298\n",
      "Training Batch [195/782]: Loss 0.008984646759927273\n",
      "Training Batch [196/782]: Loss 0.011387322098016739\n",
      "Training Batch [197/782]: Loss 0.04162080958485603\n",
      "Training Batch [198/782]: Loss 0.003177469363436103\n",
      "Training Batch [199/782]: Loss 0.002657508011907339\n",
      "Training Batch [200/782]: Loss 0.08134325593709946\n",
      "Training Batch [201/782]: Loss 0.015719855204224586\n",
      "Training Batch [202/782]: Loss 0.009408223442733288\n",
      "Training Batch [203/782]: Loss 0.07264930009841919\n",
      "Training Batch [204/782]: Loss 0.003162241540849209\n",
      "Training Batch [205/782]: Loss 0.013423753902316093\n",
      "Training Batch [206/782]: Loss 0.029576893895864487\n",
      "Training Batch [207/782]: Loss 0.00171770213637501\n",
      "Training Batch [208/782]: Loss 0.003926514182239771\n",
      "Training Batch [209/782]: Loss 0.02015499956905842\n",
      "Training Batch [210/782]: Loss 0.053445711731910706\n",
      "Training Batch [211/782]: Loss 0.01397297065705061\n",
      "Training Batch [212/782]: Loss 0.005827758926898241\n",
      "Training Batch [213/782]: Loss 0.0023614356759935617\n",
      "Training Batch [214/782]: Loss 0.04171441122889519\n",
      "Training Batch [215/782]: Loss 0.01655339077115059\n",
      "Training Batch [216/782]: Loss 0.012788593769073486\n",
      "Training Batch [217/782]: Loss 0.024939361959695816\n",
      "Training Batch [218/782]: Loss 0.025787873193621635\n",
      "Training Batch [219/782]: Loss 0.023501340299844742\n",
      "Training Batch [220/782]: Loss 0.0026754147838801146\n",
      "Training Batch [221/782]: Loss 0.023075275123119354\n",
      "Training Batch [222/782]: Loss 0.030045725405216217\n",
      "Training Batch [223/782]: Loss 0.011007644236087799\n",
      "Training Batch [224/782]: Loss 0.07882504910230637\n",
      "Training Batch [225/782]: Loss 0.020381486043334007\n",
      "Training Batch [226/782]: Loss 0.0030413330532610416\n",
      "Training Batch [227/782]: Loss 0.007520649116486311\n",
      "Training Batch [228/782]: Loss 0.002409132895991206\n",
      "Training Batch [229/782]: Loss 0.04895010590553284\n",
      "Training Batch [230/782]: Loss 0.01938644051551819\n",
      "Training Batch [231/782]: Loss 0.00767870806157589\n",
      "Training Batch [232/782]: Loss 0.002926345681771636\n",
      "Training Batch [233/782]: Loss 0.0013391135726124048\n",
      "Training Batch [234/782]: Loss 0.005331102758646011\n",
      "Training Batch [235/782]: Loss 0.023356212303042412\n",
      "Training Batch [236/782]: Loss 0.0014571731444448233\n",
      "Training Batch [237/782]: Loss 0.09627586603164673\n",
      "Training Batch [238/782]: Loss 0.016604602336883545\n",
      "Training Batch [239/782]: Loss 0.010315265506505966\n",
      "Training Batch [240/782]: Loss 0.0010239158291369677\n",
      "Training Batch [241/782]: Loss 0.011403823271393776\n",
      "Training Batch [242/782]: Loss 0.0017135700909420848\n",
      "Training Batch [243/782]: Loss 0.007858810015022755\n",
      "Training Batch [244/782]: Loss 0.055372871458530426\n",
      "Training Batch [245/782]: Loss 0.005060155410319567\n",
      "Training Batch [246/782]: Loss 0.009840176440775394\n",
      "Training Batch [247/782]: Loss 0.0338495634496212\n",
      "Training Batch [248/782]: Loss 0.017277942970395088\n",
      "Training Batch [249/782]: Loss 0.125680610537529\n",
      "Training Batch [250/782]: Loss 0.040835630148649216\n",
      "Training Batch [251/782]: Loss 0.03178277611732483\n",
      "Training Batch [252/782]: Loss 0.0031657726503908634\n",
      "Training Batch [253/782]: Loss 0.03750583156943321\n",
      "Training Batch [254/782]: Loss 0.04041556268930435\n",
      "Training Batch [255/782]: Loss 0.029571574181318283\n",
      "Training Batch [256/782]: Loss 0.0009753642953000963\n",
      "Training Batch [257/782]: Loss 0.008644565008580685\n",
      "Training Batch [258/782]: Loss 0.002247526543214917\n",
      "Training Batch [259/782]: Loss 0.006345483940094709\n",
      "Training Batch [260/782]: Loss 0.0035510228481143713\n",
      "Training Batch [261/782]: Loss 0.05120276287198067\n",
      "Training Batch [262/782]: Loss 0.030387429520487785\n",
      "Training Batch [263/782]: Loss 0.01221725344657898\n",
      "Training Batch [264/782]: Loss 0.01517044473439455\n",
      "Training Batch [265/782]: Loss 0.0015157051384449005\n",
      "Training Batch [266/782]: Loss 0.0022237685043364763\n",
      "Training Batch [267/782]: Loss 0.006792398635298014\n",
      "Training Batch [268/782]: Loss 0.0023776954039931297\n",
      "Training Batch [269/782]: Loss 0.0024944753386080265\n",
      "Training Batch [270/782]: Loss 0.006440161727368832\n",
      "Training Batch [271/782]: Loss 0.01129807997494936\n",
      "Training Batch [272/782]: Loss 0.05244901031255722\n",
      "Training Batch [273/782]: Loss 0.06851673126220703\n",
      "Training Batch [274/782]: Loss 0.006539602298289537\n",
      "Training Batch [275/782]: Loss 0.0024368418380618095\n",
      "Training Batch [276/782]: Loss 0.0007375446730293334\n",
      "Training Batch [277/782]: Loss 0.0010687338653951883\n",
      "Training Batch [278/782]: Loss 0.02146591804921627\n",
      "Training Batch [279/782]: Loss 0.007116276770830154\n",
      "Training Batch [280/782]: Loss 0.12569662928581238\n",
      "Training Batch [281/782]: Loss 0.017322078347206116\n",
      "Training Batch [282/782]: Loss 0.01103935856372118\n",
      "Training Batch [283/782]: Loss 0.007000704295933247\n",
      "Training Batch [284/782]: Loss 0.007152406964451075\n",
      "Training Batch [285/782]: Loss 0.024066194891929626\n",
      "Training Batch [286/782]: Loss 0.01494655478745699\n",
      "Training Batch [287/782]: Loss 0.015818070620298386\n",
      "Training Batch [288/782]: Loss 0.004973645322024822\n",
      "Training Batch [289/782]: Loss 0.017925018444657326\n",
      "Training Batch [290/782]: Loss 0.021297816187143326\n",
      "Training Batch [291/782]: Loss 0.004141276702284813\n",
      "Training Batch [292/782]: Loss 0.0019172301981598139\n",
      "Training Batch [293/782]: Loss 0.025915641337633133\n",
      "Training Batch [294/782]: Loss 0.0025750380009412766\n",
      "Training Batch [295/782]: Loss 0.05014888569712639\n",
      "Training Batch [296/782]: Loss 0.002551669953390956\n",
      "Training Batch [297/782]: Loss 0.0031947866082191467\n",
      "Training Batch [298/782]: Loss 0.03114444762468338\n",
      "Training Batch [299/782]: Loss 0.00936967134475708\n",
      "Training Batch [300/782]: Loss 0.04322506859898567\n",
      "Training Batch [301/782]: Loss 0.01320855226367712\n",
      "Training Batch [302/782]: Loss 0.006939740385860205\n",
      "Training Batch [303/782]: Loss 0.005514713004231453\n",
      "Training Batch [304/782]: Loss 0.0017611882649362087\n",
      "Training Batch [305/782]: Loss 0.021847177296876907\n",
      "Training Batch [306/782]: Loss 0.024816710501909256\n",
      "Training Batch [307/782]: Loss 0.006031273398548365\n",
      "Training Batch [308/782]: Loss 0.006687893532216549\n",
      "Training Batch [309/782]: Loss 0.012911440804600716\n",
      "Training Batch [310/782]: Loss 0.0023602826986461878\n",
      "Training Batch [311/782]: Loss 0.09805212169885635\n",
      "Training Batch [312/782]: Loss 0.01735907420516014\n",
      "Training Batch [313/782]: Loss 0.021159593015909195\n",
      "Training Batch [314/782]: Loss 0.008599862456321716\n",
      "Training Batch [315/782]: Loss 0.022842606529593468\n",
      "Training Batch [316/782]: Loss 0.015864551067352295\n",
      "Training Batch [317/782]: Loss 0.027946263551712036\n",
      "Training Batch [318/782]: Loss 0.009332058019936085\n",
      "Training Batch [319/782]: Loss 0.004991200752556324\n",
      "Training Batch [320/782]: Loss 0.004613735247403383\n",
      "Training Batch [321/782]: Loss 0.008955917321145535\n",
      "Training Batch [322/782]: Loss 0.0016764058964326978\n",
      "Training Batch [323/782]: Loss 0.017420267686247826\n",
      "Training Batch [324/782]: Loss 0.0012502759927883744\n",
      "Training Batch [325/782]: Loss 0.001839170465245843\n",
      "Training Batch [326/782]: Loss 0.007581835612654686\n",
      "Training Batch [327/782]: Loss 0.005245897453278303\n",
      "Training Batch [328/782]: Loss 0.0011564133455976844\n",
      "Training Batch [329/782]: Loss 0.019329769536852837\n",
      "Training Batch [330/782]: Loss 0.008361522108316422\n",
      "Training Batch [331/782]: Loss 0.019594579935073853\n",
      "Training Batch [332/782]: Loss 0.004994496703147888\n",
      "Training Batch [333/782]: Loss 0.004438342526555061\n",
      "Training Batch [334/782]: Loss 0.006262064911425114\n",
      "Training Batch [335/782]: Loss 0.0020293055567890406\n",
      "Training Batch [336/782]: Loss 0.005876739975064993\n",
      "Training Batch [337/782]: Loss 0.053403060883283615\n",
      "Training Batch [338/782]: Loss 0.0015832727076485753\n",
      "Training Batch [339/782]: Loss 0.00313172466121614\n",
      "Training Batch [340/782]: Loss 0.024094784632325172\n",
      "Training Batch [341/782]: Loss 0.05897039920091629\n",
      "Training Batch [342/782]: Loss 0.0008859577355906367\n",
      "Training Batch [343/782]: Loss 0.0017218543216586113\n",
      "Training Batch [344/782]: Loss 0.00435794098302722\n",
      "Training Batch [345/782]: Loss 0.012986571528017521\n",
      "Training Batch [346/782]: Loss 0.002437191316857934\n",
      "Training Batch [347/782]: Loss 0.07290582358837128\n",
      "Training Batch [348/782]: Loss 0.003238383214920759\n",
      "Training Batch [349/782]: Loss 0.014041935093700886\n",
      "Training Batch [350/782]: Loss 0.0013547850539907813\n",
      "Training Batch [351/782]: Loss 0.01286665815860033\n",
      "Training Batch [352/782]: Loss 0.007636296562850475\n",
      "Training Batch [353/782]: Loss 0.07116813212633133\n",
      "Training Batch [354/782]: Loss 0.0057999081909656525\n",
      "Training Batch [355/782]: Loss 0.0023406895343214273\n",
      "Training Batch [356/782]: Loss 0.0037630500737577677\n",
      "Training Batch [357/782]: Loss 0.03263038769364357\n",
      "Training Batch [358/782]: Loss 0.0017804726958274841\n",
      "Training Batch [359/782]: Loss 0.017662018537521362\n",
      "Training Batch [360/782]: Loss 0.005691434256732464\n",
      "Training Batch [361/782]: Loss 0.0313563346862793\n",
      "Training Batch [362/782]: Loss 0.007279231678694487\n",
      "Training Batch [363/782]: Loss 0.021352747455239296\n",
      "Training Batch [364/782]: Loss 0.00023498543305322528\n",
      "Training Batch [365/782]: Loss 0.09837985783815384\n",
      "Training Batch [366/782]: Loss 0.009499085135757923\n",
      "Training Batch [367/782]: Loss 0.0036679550539702177\n",
      "Training Batch [368/782]: Loss 0.004989912733435631\n",
      "Training Batch [369/782]: Loss 0.003671801183372736\n",
      "Training Batch [370/782]: Loss 0.0037415463011711836\n",
      "Training Batch [371/782]: Loss 0.008571263402700424\n",
      "Training Batch [372/782]: Loss 0.0060905572026968\n",
      "Training Batch [373/782]: Loss 0.019530877470970154\n",
      "Training Batch [374/782]: Loss 0.017113128677010536\n",
      "Training Batch [375/782]: Loss 0.04811146482825279\n",
      "Training Batch [376/782]: Loss 0.02024158276617527\n",
      "Training Batch [377/782]: Loss 0.0023555124644190073\n",
      "Training Batch [378/782]: Loss 0.06528905034065247\n",
      "Training Batch [379/782]: Loss 0.003641403978690505\n",
      "Training Batch [380/782]: Loss 0.06594693660736084\n",
      "Training Batch [381/782]: Loss 0.0014550916384905577\n",
      "Training Batch [382/782]: Loss 0.000521756533998996\n",
      "Training Batch [383/782]: Loss 0.027766888961195946\n",
      "Training Batch [384/782]: Loss 0.04892099276185036\n",
      "Training Batch [385/782]: Loss 0.006035846192389727\n",
      "Training Batch [386/782]: Loss 0.03350656107068062\n",
      "Training Batch [387/782]: Loss 0.0338301919400692\n",
      "Training Batch [388/782]: Loss 0.02812470681965351\n",
      "Training Batch [389/782]: Loss 0.038844041526317596\n",
      "Training Batch [390/782]: Loss 0.0006963988416828215\n",
      "Training Batch [391/782]: Loss 0.0293826125562191\n",
      "Training Batch [392/782]: Loss 0.009930901229381561\n",
      "Training Batch [393/782]: Loss 0.05694334954023361\n",
      "Training Batch [394/782]: Loss 0.005505472421646118\n",
      "Training Batch [395/782]: Loss 0.0002699042670428753\n",
      "Training Batch [396/782]: Loss 0.04360739886760712\n",
      "Training Batch [397/782]: Loss 0.0011524962028488517\n",
      "Training Batch [398/782]: Loss 0.01811378449201584\n",
      "Training Batch [399/782]: Loss 0.010037383995950222\n",
      "Training Batch [400/782]: Loss 0.0057862987741827965\n",
      "Training Batch [401/782]: Loss 0.003616660600528121\n",
      "Training Batch [402/782]: Loss 0.005208642687648535\n",
      "Training Batch [403/782]: Loss 0.003922353498637676\n",
      "Training Batch [404/782]: Loss 0.005901962053030729\n",
      "Training Batch [405/782]: Loss 0.01369363535195589\n",
      "Training Batch [406/782]: Loss 0.004279841668903828\n",
      "Training Batch [407/782]: Loss 0.06524550169706345\n",
      "Training Batch [408/782]: Loss 0.0028907745145261288\n",
      "Training Batch [409/782]: Loss 0.0019302753498777747\n",
      "Training Batch [410/782]: Loss 0.01938960701227188\n",
      "Training Batch [411/782]: Loss 0.002288704039528966\n",
      "Training Batch [412/782]: Loss 0.020384956151247025\n",
      "Training Batch [413/782]: Loss 0.03983272984623909\n",
      "Training Batch [414/782]: Loss 0.006399708334356546\n",
      "Training Batch [415/782]: Loss 0.005691722966730595\n",
      "Training Batch [416/782]: Loss 0.05333936586976051\n",
      "Training Batch [417/782]: Loss 0.005996271967887878\n",
      "Training Batch [418/782]: Loss 0.012509401887655258\n",
      "Training Batch [419/782]: Loss 0.005758382845669985\n",
      "Training Batch [420/782]: Loss 0.025446048006415367\n",
      "Training Batch [421/782]: Loss 0.013770036399364471\n",
      "Training Batch [422/782]: Loss 0.011522559449076653\n",
      "Training Batch [423/782]: Loss 0.035816825926303864\n",
      "Training Batch [424/782]: Loss 0.0015166702214628458\n",
      "Training Batch [425/782]: Loss 0.004870987497270107\n",
      "Training Batch [426/782]: Loss 0.000557157676666975\n",
      "Training Batch [427/782]: Loss 0.07358431071043015\n",
      "Training Batch [428/782]: Loss 0.0024449354968965054\n",
      "Training Batch [429/782]: Loss 0.026141054928302765\n",
      "Training Batch [430/782]: Loss 0.0012762128608301282\n",
      "Training Batch [431/782]: Loss 0.033970125019550323\n",
      "Training Batch [432/782]: Loss 0.029953595250844955\n",
      "Training Batch [433/782]: Loss 0.005462689325213432\n",
      "Training Batch [434/782]: Loss 0.05692015960812569\n",
      "Training Batch [435/782]: Loss 0.0058576310984790325\n",
      "Training Batch [436/782]: Loss 0.029104700312018394\n",
      "Training Batch [437/782]: Loss 0.07259660959243774\n",
      "Training Batch [438/782]: Loss 0.011214623227715492\n",
      "Training Batch [439/782]: Loss 0.016237879171967506\n",
      "Training Batch [440/782]: Loss 0.007134858053177595\n",
      "Training Batch [441/782]: Loss 0.004369691014289856\n",
      "Training Batch [442/782]: Loss 0.0011155937099829316\n",
      "Training Batch [443/782]: Loss 0.027307067066431046\n",
      "Training Batch [444/782]: Loss 0.008098706603050232\n",
      "Training Batch [445/782]: Loss 0.010357335209846497\n",
      "Training Batch [446/782]: Loss 0.012742545455694199\n",
      "Training Batch [447/782]: Loss 0.02690248191356659\n",
      "Training Batch [448/782]: Loss 0.07082799077033997\n",
      "Training Batch [449/782]: Loss 0.00045335147297009826\n",
      "Training Batch [450/782]: Loss 0.07075874507427216\n",
      "Training Batch [451/782]: Loss 0.007981463335454464\n",
      "Training Batch [452/782]: Loss 0.0726047232747078\n",
      "Training Batch [453/782]: Loss 0.02634439431130886\n",
      "Training Batch [454/782]: Loss 0.005168061703443527\n",
      "Training Batch [455/782]: Loss 0.00376233272254467\n",
      "Training Batch [456/782]: Loss 0.05921364575624466\n",
      "Training Batch [457/782]: Loss 0.02080521360039711\n",
      "Training Batch [458/782]: Loss 0.007410767022520304\n",
      "Training Batch [459/782]: Loss 0.002313012257218361\n",
      "Training Batch [460/782]: Loss 0.0022141544613987207\n",
      "Training Batch [461/782]: Loss 0.026586376130580902\n",
      "Training Batch [462/782]: Loss 0.02647164650261402\n",
      "Training Batch [463/782]: Loss 0.0029272085521370173\n",
      "Training Batch [464/782]: Loss 0.03910131752490997\n",
      "Training Batch [465/782]: Loss 0.003985976334661245\n",
      "Training Batch [466/782]: Loss 0.013003796339035034\n",
      "Training Batch [467/782]: Loss 0.0013203287962824106\n",
      "Training Batch [468/782]: Loss 0.034239716827869415\n",
      "Training Batch [469/782]: Loss 0.0006452086963690817\n",
      "Training Batch [470/782]: Loss 0.0015622425125911832\n",
      "Training Batch [471/782]: Loss 0.09013035893440247\n",
      "Training Batch [472/782]: Loss 0.005850984714925289\n",
      "Training Batch [473/782]: Loss 0.0007946126279421151\n",
      "Training Batch [474/782]: Loss 0.006961464881896973\n",
      "Training Batch [475/782]: Loss 0.009567304514348507\n",
      "Training Batch [476/782]: Loss 0.005367170087993145\n",
      "Training Batch [477/782]: Loss 0.0012407050235196948\n",
      "Training Batch [478/782]: Loss 0.013782992959022522\n",
      "Training Batch [479/782]: Loss 0.04931815713644028\n",
      "Training Batch [480/782]: Loss 0.0015721417730674148\n",
      "Training Batch [481/782]: Loss 0.006344915367662907\n",
      "Training Batch [482/782]: Loss 0.0013155399356037378\n",
      "Training Batch [483/782]: Loss 0.0021756847854703665\n",
      "Training Batch [484/782]: Loss 0.02382047288119793\n",
      "Training Batch [485/782]: Loss 0.002967924578115344\n",
      "Training Batch [486/782]: Loss 0.06677331775426865\n",
      "Training Batch [487/782]: Loss 0.056016627699136734\n",
      "Training Batch [488/782]: Loss 0.0044017876498401165\n",
      "Training Batch [489/782]: Loss 0.0024461818393319845\n",
      "Training Batch [490/782]: Loss 0.014061344787478447\n",
      "Training Batch [491/782]: Loss 0.0014204862527549267\n",
      "Training Batch [492/782]: Loss 0.010147354565560818\n",
      "Training Batch [493/782]: Loss 0.04824275150895119\n",
      "Training Batch [494/782]: Loss 0.02072536200284958\n",
      "Training Batch [495/782]: Loss 0.027401555329561234\n",
      "Training Batch [496/782]: Loss 0.03897649422287941\n",
      "Training Batch [497/782]: Loss 0.004288916941732168\n",
      "Training Batch [498/782]: Loss 0.004993676673620939\n",
      "Training Batch [499/782]: Loss 0.006379197351634502\n",
      "Training Batch [500/782]: Loss 0.015438773669302464\n",
      "Training Batch [501/782]: Loss 0.023022674024105072\n",
      "Training Batch [502/782]: Loss 0.006159174256026745\n",
      "Training Batch [503/782]: Loss 0.0022121574729681015\n",
      "Training Batch [504/782]: Loss 0.00045820846571587026\n",
      "Training Batch [505/782]: Loss 0.1257915496826172\n",
      "Training Batch [506/782]: Loss 0.006284972652792931\n",
      "Training Batch [507/782]: Loss 0.04810936003923416\n",
      "Training Batch [508/782]: Loss 0.003455252153798938\n",
      "Training Batch [509/782]: Loss 0.061640042811632156\n",
      "Training Batch [510/782]: Loss 0.00463112024590373\n",
      "Training Batch [511/782]: Loss 0.004313324578106403\n",
      "Training Batch [512/782]: Loss 0.001356450724415481\n",
      "Training Batch [513/782]: Loss 0.012709278613328934\n",
      "Training Batch [514/782]: Loss 0.05650533735752106\n",
      "Training Batch [515/782]: Loss 0.0037151556462049484\n",
      "Training Batch [516/782]: Loss 0.03413531556725502\n",
      "Training Batch [517/782]: Loss 0.043667182326316833\n",
      "Training Batch [518/782]: Loss 0.027184301987290382\n",
      "Training Batch [519/782]: Loss 0.0042707971297204494\n",
      "Training Batch [520/782]: Loss 0.10883589088916779\n",
      "Training Batch [521/782]: Loss 0.025197982788085938\n",
      "Training Batch [522/782]: Loss 0.00534076290205121\n",
      "Training Batch [523/782]: Loss 0.0008216840215027332\n",
      "Training Batch [524/782]: Loss 0.018491171300411224\n",
      "Training Batch [525/782]: Loss 0.012054776772856712\n",
      "Training Batch [526/782]: Loss 0.02399219572544098\n",
      "Training Batch [527/782]: Loss 0.04304255172610283\n",
      "Training Batch [528/782]: Loss 0.011354855261743069\n",
      "Training Batch [529/782]: Loss 0.009824307635426521\n",
      "Training Batch [530/782]: Loss 0.011490676552057266\n",
      "Training Batch [531/782]: Loss 0.0077758789993822575\n",
      "Training Batch [532/782]: Loss 0.04636574536561966\n",
      "Training Batch [533/782]: Loss 0.023989800363779068\n",
      "Training Batch [534/782]: Loss 0.04293319582939148\n",
      "Training Batch [535/782]: Loss 0.013276972807943821\n",
      "Training Batch [536/782]: Loss 0.0206767451018095\n",
      "Training Batch [537/782]: Loss 0.03632611781358719\n",
      "Training Batch [538/782]: Loss 0.04047418385744095\n",
      "Training Batch [539/782]: Loss 0.001296033151447773\n",
      "Training Batch [540/782]: Loss 0.024115098640322685\n",
      "Training Batch [541/782]: Loss 0.018250055611133575\n",
      "Training Batch [542/782]: Loss 0.024338943883776665\n",
      "Training Batch [543/782]: Loss 0.007708499673753977\n",
      "Training Batch [544/782]: Loss 0.0023273425176739693\n",
      "Training Batch [545/782]: Loss 0.003209158079698682\n",
      "Training Batch [546/782]: Loss 0.0024884361773729324\n",
      "Training Batch [547/782]: Loss 0.0013446821831166744\n",
      "Training Batch [548/782]: Loss 0.0021622416097670794\n",
      "Training Batch [549/782]: Loss 0.007101255934685469\n",
      "Training Batch [550/782]: Loss 0.005600620526820421\n",
      "Training Batch [551/782]: Loss 0.02579350955784321\n",
      "Training Batch [552/782]: Loss 0.0031887320801615715\n",
      "Training Batch [553/782]: Loss 0.007922012358903885\n",
      "Training Batch [554/782]: Loss 0.05548996850848198\n",
      "Training Batch [555/782]: Loss 0.07937130331993103\n",
      "Training Batch [556/782]: Loss 0.00382994650863111\n",
      "Training Batch [557/782]: Loss 0.03418761491775513\n",
      "Training Batch [558/782]: Loss 0.0067302086390554905\n",
      "Training Batch [559/782]: Loss 0.010849576443433762\n",
      "Training Batch [560/782]: Loss 0.024101346731185913\n",
      "Training Batch [561/782]: Loss 0.006905966438353062\n",
      "Training Batch [562/782]: Loss 0.005956763401627541\n",
      "Training Batch [563/782]: Loss 0.05927064269781113\n",
      "Training Batch [564/782]: Loss 0.030561337247490883\n",
      "Training Batch [565/782]: Loss 0.01343502290546894\n",
      "Training Batch [566/782]: Loss 0.0007109610014595091\n",
      "Training Batch [567/782]: Loss 0.02957998216152191\n",
      "Training Batch [568/782]: Loss 0.00756086315959692\n",
      "Training Batch [569/782]: Loss 0.01108398288488388\n",
      "Training Batch [570/782]: Loss 0.0005188851500861347\n",
      "Training Batch [571/782]: Loss 0.011177566833794117\n",
      "Training Batch [572/782]: Loss 0.002055306686088443\n",
      "Training Batch [573/782]: Loss 0.004820596426725388\n",
      "Training Batch [574/782]: Loss 0.05129086598753929\n",
      "Training Batch [575/782]: Loss 0.0025878283195197582\n",
      "Training Batch [576/782]: Loss 0.01645892858505249\n",
      "Training Batch [577/782]: Loss 0.002661914797499776\n",
      "Training Batch [578/782]: Loss 0.0027686909306794405\n",
      "Training Batch [579/782]: Loss 0.035049378871917725\n",
      "Training Batch [580/782]: Loss 0.011507611721754074\n",
      "Training Batch [581/782]: Loss 0.07542279362678528\n",
      "Training Batch [582/782]: Loss 0.057306818664073944\n",
      "Training Batch [583/782]: Loss 0.000968091597314924\n",
      "Training Batch [584/782]: Loss 0.01583956368267536\n",
      "Training Batch [585/782]: Loss 0.0038927001878619194\n",
      "Training Batch [586/782]: Loss 0.061452388763427734\n",
      "Training Batch [587/782]: Loss 0.002601820044219494\n",
      "Training Batch [588/782]: Loss 0.003701745066791773\n",
      "Training Batch [589/782]: Loss 0.06335368752479553\n",
      "Training Batch [590/782]: Loss 0.0013929496053606272\n",
      "Training Batch [591/782]: Loss 0.04362806677818298\n",
      "Training Batch [592/782]: Loss 0.12137239426374435\n",
      "Training Batch [593/782]: Loss 0.04634912684559822\n",
      "Training Batch [594/782]: Loss 0.08554242551326752\n",
      "Training Batch [595/782]: Loss 0.0010187964653596282\n",
      "Training Batch [596/782]: Loss 0.023278582841157913\n",
      "Training Batch [597/782]: Loss 0.025932788848876953\n",
      "Training Batch [598/782]: Loss 0.02250145934522152\n",
      "Training Batch [599/782]: Loss 0.0073722838424146175\n",
      "Training Batch [600/782]: Loss 0.0103995893150568\n",
      "Training Batch [601/782]: Loss 0.004351736977696419\n",
      "Training Batch [602/782]: Loss 0.009412298910319805\n",
      "Training Batch [603/782]: Loss 0.01079491525888443\n",
      "Training Batch [604/782]: Loss 0.0012834884691983461\n",
      "Training Batch [605/782]: Loss 0.05171273276209831\n",
      "Training Batch [606/782]: Loss 0.011647557839751244\n",
      "Training Batch [607/782]: Loss 0.007358154281973839\n",
      "Training Batch [608/782]: Loss 0.0031868047080934048\n",
      "Training Batch [609/782]: Loss 0.010255434550344944\n",
      "Training Batch [610/782]: Loss 0.045094698667526245\n",
      "Training Batch [611/782]: Loss 0.08907788246870041\n",
      "Training Batch [612/782]: Loss 0.021416107192635536\n",
      "Training Batch [613/782]: Loss 0.008853965438902378\n",
      "Training Batch [614/782]: Loss 0.14099650084972382\n",
      "Training Batch [615/782]: Loss 0.0024552622344344854\n",
      "Training Batch [616/782]: Loss 0.007577537093311548\n",
      "Training Batch [617/782]: Loss 0.001657623564824462\n",
      "Training Batch [618/782]: Loss 0.09604524821043015\n",
      "Training Batch [619/782]: Loss 0.0009222700027748942\n",
      "Training Batch [620/782]: Loss 0.02892822027206421\n",
      "Training Batch [621/782]: Loss 0.08828683197498322\n",
      "Training Batch [622/782]: Loss 0.00841309130191803\n",
      "Training Batch [623/782]: Loss 0.005191569682210684\n",
      "Training Batch [624/782]: Loss 0.04630564525723457\n",
      "Training Batch [625/782]: Loss 0.004237884655594826\n",
      "Training Batch [626/782]: Loss 0.0014714059652760625\n",
      "Training Batch [627/782]: Loss 0.01834736578166485\n",
      "Training Batch [628/782]: Loss 0.000983398873358965\n",
      "Training Batch [629/782]: Loss 0.07118957489728928\n",
      "Training Batch [630/782]: Loss 0.008115171454846859\n",
      "Training Batch [631/782]: Loss 0.04577483981847763\n",
      "Training Batch [632/782]: Loss 0.009690406732261181\n",
      "Training Batch [633/782]: Loss 0.003759126178920269\n",
      "Training Batch [634/782]: Loss 0.09887946397066116\n",
      "Training Batch [635/782]: Loss 0.03059455193579197\n",
      "Training Batch [636/782]: Loss 0.06569201499223709\n",
      "Training Batch [637/782]: Loss 0.010404006578028202\n",
      "Training Batch [638/782]: Loss 0.017992407083511353\n",
      "Training Batch [639/782]: Loss 0.0843479260802269\n",
      "Training Batch [640/782]: Loss 0.012811142951250076\n",
      "Training Batch [641/782]: Loss 0.039625026285648346\n",
      "Training Batch [642/782]: Loss 0.004094278439879417\n",
      "Training Batch [643/782]: Loss 0.0024415715597569942\n",
      "Training Batch [644/782]: Loss 0.0017823094967752695\n",
      "Training Batch [645/782]: Loss 0.0015625065425410867\n",
      "Training Batch [646/782]: Loss 0.06450878083705902\n",
      "Training Batch [647/782]: Loss 0.02595531940460205\n",
      "Training Batch [648/782]: Loss 0.0008346486720256507\n",
      "Training Batch [649/782]: Loss 0.00581276323646307\n",
      "Training Batch [650/782]: Loss 0.01179387979209423\n",
      "Training Batch [651/782]: Loss 0.001173382275737822\n",
      "Training Batch [652/782]: Loss 0.056422214955091476\n",
      "Training Batch [653/782]: Loss 0.040960293263196945\n",
      "Training Batch [654/782]: Loss 0.006254319101572037\n",
      "Training Batch [655/782]: Loss 0.09762546420097351\n",
      "Training Batch [656/782]: Loss 0.008568797260522842\n",
      "Training Batch [657/782]: Loss 0.059304989874362946\n",
      "Training Batch [658/782]: Loss 0.06371888518333435\n",
      "Training Batch [659/782]: Loss 0.003950480837374926\n",
      "Training Batch [660/782]: Loss 0.01156480610370636\n",
      "Training Batch [661/782]: Loss 0.005205119960010052\n",
      "Training Batch [662/782]: Loss 0.0013166101416572928\n",
      "Training Batch [663/782]: Loss 0.03722209855914116\n",
      "Training Batch [664/782]: Loss 0.031270503997802734\n",
      "Training Batch [665/782]: Loss 0.0726059079170227\n",
      "Training Batch [666/782]: Loss 0.11699499934911728\n",
      "Training Batch [667/782]: Loss 0.0005322857177816331\n",
      "Training Batch [668/782]: Loss 0.009556475095450878\n",
      "Training Batch [669/782]: Loss 0.006799215916544199\n",
      "Training Batch [670/782]: Loss 0.0062971459701657295\n",
      "Training Batch [671/782]: Loss 0.011730636470019817\n",
      "Training Batch [672/782]: Loss 0.02853754162788391\n",
      "Training Batch [673/782]: Loss 0.043407805263996124\n",
      "Training Batch [674/782]: Loss 0.006275339052081108\n",
      "Training Batch [675/782]: Loss 0.001509130815975368\n",
      "Training Batch [676/782]: Loss 0.04212579131126404\n",
      "Training Batch [677/782]: Loss 0.020441163331270218\n",
      "Training Batch [678/782]: Loss 0.005608853884041309\n",
      "Training Batch [679/782]: Loss 0.0857747346162796\n",
      "Training Batch [680/782]: Loss 0.05048954486846924\n",
      "Training Batch [681/782]: Loss 0.013153517618775368\n",
      "Training Batch [682/782]: Loss 0.022916769608855247\n",
      "Training Batch [683/782]: Loss 0.006846417672932148\n",
      "Training Batch [684/782]: Loss 0.004939133767038584\n",
      "Training Batch [685/782]: Loss 0.0036212503910064697\n",
      "Training Batch [686/782]: Loss 0.1272725611925125\n",
      "Training Batch [687/782]: Loss 0.0030112469103187323\n",
      "Training Batch [688/782]: Loss 0.03471391648054123\n",
      "Training Batch [689/782]: Loss 0.005912790074944496\n",
      "Training Batch [690/782]: Loss 0.008702555671334267\n",
      "Training Batch [691/782]: Loss 0.119022898375988\n",
      "Training Batch [692/782]: Loss 0.011859229765832424\n",
      "Training Batch [693/782]: Loss 0.02302771806716919\n",
      "Training Batch [694/782]: Loss 0.011955231428146362\n",
      "Training Batch [695/782]: Loss 0.0028650802560150623\n",
      "Training Batch [696/782]: Loss 0.0021170994732528925\n",
      "Training Batch [697/782]: Loss 0.004284853581339121\n",
      "Training Batch [698/782]: Loss 0.015454383566975594\n",
      "Training Batch [699/782]: Loss 0.0011088006431236863\n",
      "Training Batch [700/782]: Loss 0.012559464201331139\n",
      "Training Batch [701/782]: Loss 0.015603995881974697\n",
      "Training Batch [702/782]: Loss 0.02106156013906002\n",
      "Training Batch [703/782]: Loss 0.02623669058084488\n",
      "Training Batch [704/782]: Loss 0.011635931208729744\n",
      "Training Batch [705/782]: Loss 0.019459130242466927\n",
      "Training Batch [706/782]: Loss 0.018749112263321877\n",
      "Training Batch [707/782]: Loss 0.0035980313550680876\n",
      "Training Batch [708/782]: Loss 0.02247435227036476\n",
      "Training Batch [709/782]: Loss 0.007170436903834343\n",
      "Training Batch [710/782]: Loss 0.0056081474758684635\n",
      "Training Batch [711/782]: Loss 0.010719521902501583\n",
      "Training Batch [712/782]: Loss 0.03555993363261223\n",
      "Training Batch [713/782]: Loss 0.00862916186451912\n",
      "Training Batch [714/782]: Loss 0.006009935867041349\n",
      "Training Batch [715/782]: Loss 0.02296338975429535\n",
      "Training Batch [716/782]: Loss 0.011115452274680138\n",
      "Training Batch [717/782]: Loss 0.0019114540191367269\n",
      "Training Batch [718/782]: Loss 0.0013541696825996041\n",
      "Training Batch [719/782]: Loss 0.0016769167268648744\n",
      "Training Batch [720/782]: Loss 0.0008377754129469395\n",
      "Training Batch [721/782]: Loss 0.004364726133644581\n",
      "Training Batch [722/782]: Loss 0.0008554898668080568\n",
      "Training Batch [723/782]: Loss 0.003938198089599609\n",
      "Training Batch [724/782]: Loss 0.1082044392824173\n",
      "Training Batch [725/782]: Loss 0.04358312115073204\n",
      "Training Batch [726/782]: Loss 0.007651668041944504\n",
      "Training Batch [727/782]: Loss 0.020699918270111084\n",
      "Training Batch [728/782]: Loss 0.07759425044059753\n",
      "Training Batch [729/782]: Loss 0.03540064021945\n",
      "Training Batch [730/782]: Loss 0.016061479225754738\n",
      "Training Batch [731/782]: Loss 0.016530899330973625\n",
      "Training Batch [732/782]: Loss 0.013563747517764568\n",
      "Training Batch [733/782]: Loss 0.0003245517145842314\n",
      "Training Batch [734/782]: Loss 0.0033501482103019953\n",
      "Training Batch [735/782]: Loss 0.008205536752939224\n",
      "Training Batch [736/782]: Loss 0.021290980279445648\n",
      "Training Batch [737/782]: Loss 0.004815947730094194\n",
      "Training Batch [738/782]: Loss 0.11590325832366943\n",
      "Training Batch [739/782]: Loss 0.002886311849579215\n",
      "Training Batch [740/782]: Loss 0.006792879663407803\n",
      "Training Batch [741/782]: Loss 0.007617365103214979\n",
      "Training Batch [742/782]: Loss 0.003409751458093524\n",
      "Training Batch [743/782]: Loss 0.032694749534130096\n",
      "Training Batch [744/782]: Loss 0.017430908977985382\n",
      "Training Batch [745/782]: Loss 0.011997929774224758\n",
      "Training Batch [746/782]: Loss 0.014078445732593536\n",
      "Training Batch [747/782]: Loss 0.0009256944758817554\n",
      "Training Batch [748/782]: Loss 0.005040348507463932\n",
      "Training Batch [749/782]: Loss 0.039377983659505844\n",
      "Training Batch [750/782]: Loss 0.00553147355094552\n",
      "Training Batch [751/782]: Loss 0.02062545157968998\n",
      "Training Batch [752/782]: Loss 0.064273402094841\n",
      "Training Batch [753/782]: Loss 0.0005207491922192276\n",
      "Training Batch [754/782]: Loss 0.030242951586842537\n",
      "Training Batch [755/782]: Loss 0.041619688272476196\n",
      "Training Batch [756/782]: Loss 0.032752227038145065\n",
      "Training Batch [757/782]: Loss 0.015274224802851677\n",
      "Training Batch [758/782]: Loss 0.007722980808466673\n",
      "Training Batch [759/782]: Loss 0.012903770431876183\n",
      "Training Batch [760/782]: Loss 0.01964660733938217\n",
      "Training Batch [761/782]: Loss 0.018517902120947838\n",
      "Training Batch [762/782]: Loss 0.019922057166695595\n",
      "Training Batch [763/782]: Loss 0.0015598463360220194\n",
      "Training Batch [764/782]: Loss 0.014885669574141502\n",
      "Training Batch [765/782]: Loss 0.003357588779181242\n",
      "Training Batch [766/782]: Loss 0.0027518633287400007\n",
      "Training Batch [767/782]: Loss 0.08688189089298248\n",
      "Training Batch [768/782]: Loss 0.0016989350551739335\n",
      "Training Batch [769/782]: Loss 0.004480395466089249\n",
      "Training Batch [770/782]: Loss 0.1295136958360672\n",
      "Training Batch [771/782]: Loss 0.001187680521979928\n",
      "Training Batch [772/782]: Loss 0.06638350337743759\n",
      "Training Batch [773/782]: Loss 0.0011926045408472419\n",
      "Training Batch [774/782]: Loss 0.019589319825172424\n",
      "Training Batch [775/782]: Loss 0.023475561290979385\n",
      "Training Batch [776/782]: Loss 0.009269475936889648\n",
      "Training Batch [777/782]: Loss 0.00758375646546483\n",
      "Training Batch [778/782]: Loss 0.0035323360934853554\n",
      "Training Batch [779/782]: Loss 0.015233834274113178\n",
      "Training Batch [780/782]: Loss 0.0022328130435198545\n",
      "Training Batch [781/782]: Loss 0.03833784535527229\n",
      "Training Batch [782/782]: Loss 0.09675930440425873\n",
      "Epoch 26 - Train Loss: 0.0222\n",
      "*********  Epoch 27/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.009050401858985424\n",
      "Training Batch [2/782]: Loss 0.004281720612198114\n",
      "Training Batch [3/782]: Loss 0.06998588889837265\n",
      "Training Batch [4/782]: Loss 0.0016909007681533694\n",
      "Training Batch [5/782]: Loss 0.00966891460120678\n",
      "Training Batch [6/782]: Loss 0.02533368207514286\n",
      "Training Batch [7/782]: Loss 0.003070921404287219\n",
      "Training Batch [8/782]: Loss 0.0029401862993836403\n",
      "Training Batch [9/782]: Loss 0.018476204946637154\n",
      "Training Batch [10/782]: Loss 0.033717986196279526\n",
      "Training Batch [11/782]: Loss 0.048357367515563965\n",
      "Training Batch [12/782]: Loss 0.05992680788040161\n",
      "Training Batch [13/782]: Loss 0.028163345530629158\n",
      "Training Batch [14/782]: Loss 0.004584773909300566\n",
      "Training Batch [15/782]: Loss 0.039503153413534164\n",
      "Training Batch [16/782]: Loss 0.008015348576009274\n",
      "Training Batch [17/782]: Loss 0.006289009936153889\n",
      "Training Batch [18/782]: Loss 0.037035826593637466\n",
      "Training Batch [19/782]: Loss 0.0020770507398992777\n",
      "Training Batch [20/782]: Loss 0.06298800557851791\n",
      "Training Batch [21/782]: Loss 0.05383211374282837\n",
      "Training Batch [22/782]: Loss 0.021166455000638962\n",
      "Training Batch [23/782]: Loss 0.008579917252063751\n",
      "Training Batch [24/782]: Loss 0.017614157870411873\n",
      "Training Batch [25/782]: Loss 0.014325563795864582\n",
      "Training Batch [26/782]: Loss 0.007091608364135027\n",
      "Training Batch [27/782]: Loss 0.1715213507413864\n",
      "Training Batch [28/782]: Loss 0.0360473096370697\n",
      "Training Batch [29/782]: Loss 0.03914962708950043\n",
      "Training Batch [30/782]: Loss 0.01958426833152771\n",
      "Training Batch [31/782]: Loss 0.04355878382921219\n",
      "Training Batch [32/782]: Loss 0.01905122399330139\n",
      "Training Batch [33/782]: Loss 0.009299018420279026\n",
      "Training Batch [34/782]: Loss 0.010208912193775177\n",
      "Training Batch [35/782]: Loss 0.04849201440811157\n",
      "Training Batch [36/782]: Loss 0.12449914216995239\n",
      "Training Batch [37/782]: Loss 0.0007143131806515157\n",
      "Training Batch [38/782]: Loss 0.03293958306312561\n",
      "Training Batch [39/782]: Loss 0.007350503467023373\n",
      "Training Batch [40/782]: Loss 0.004542114213109016\n",
      "Training Batch [41/782]: Loss 0.0013995603658258915\n",
      "Training Batch [42/782]: Loss 0.032767023891210556\n",
      "Training Batch [43/782]: Loss 0.07909778505563736\n",
      "Training Batch [44/782]: Loss 0.0224081352353096\n",
      "Training Batch [45/782]: Loss 0.07412538677453995\n",
      "Training Batch [46/782]: Loss 0.01519631128758192\n",
      "Training Batch [47/782]: Loss 0.003987238276749849\n",
      "Training Batch [48/782]: Loss 0.036981165409088135\n",
      "Training Batch [49/782]: Loss 0.01170850358903408\n",
      "Training Batch [50/782]: Loss 0.020298877730965614\n",
      "Training Batch [51/782]: Loss 0.03862199932336807\n",
      "Training Batch [52/782]: Loss 0.035183753818273544\n",
      "Training Batch [53/782]: Loss 0.013591740280389786\n",
      "Training Batch [54/782]: Loss 0.01685764081776142\n",
      "Training Batch [55/782]: Loss 0.0005682751652784646\n",
      "Training Batch [56/782]: Loss 0.0034451172687113285\n",
      "Training Batch [57/782]: Loss 0.004508316982537508\n",
      "Training Batch [58/782]: Loss 0.014783269725739956\n",
      "Training Batch [59/782]: Loss 0.022436007857322693\n",
      "Training Batch [60/782]: Loss 0.010803302749991417\n",
      "Training Batch [61/782]: Loss 0.016910254955291748\n",
      "Training Batch [62/782]: Loss 0.0006310677854344249\n",
      "Training Batch [63/782]: Loss 0.072487972676754\n",
      "Training Batch [64/782]: Loss 0.0019603432156145573\n",
      "Training Batch [65/782]: Loss 0.000423855526605621\n",
      "Training Batch [66/782]: Loss 0.00488848565146327\n",
      "Training Batch [67/782]: Loss 0.0012655084719881415\n",
      "Training Batch [68/782]: Loss 0.011624927632510662\n",
      "Training Batch [69/782]: Loss 0.005641145631670952\n",
      "Training Batch [70/782]: Loss 0.016153542324900627\n",
      "Training Batch [71/782]: Loss 0.01963683031499386\n",
      "Training Batch [72/782]: Loss 0.0042875478975474834\n",
      "Training Batch [73/782]: Loss 0.006169208325445652\n",
      "Training Batch [74/782]: Loss 0.0003918085712939501\n",
      "Training Batch [75/782]: Loss 0.0399639792740345\n",
      "Training Batch [76/782]: Loss 0.03293808177113533\n",
      "Training Batch [77/782]: Loss 0.0010806951904669404\n",
      "Training Batch [78/782]: Loss 0.0213199220597744\n",
      "Training Batch [79/782]: Loss 0.024221716448664665\n",
      "Training Batch [80/782]: Loss 0.006441181525588036\n",
      "Training Batch [81/782]: Loss 0.00564697477966547\n",
      "Training Batch [82/782]: Loss 0.008788157254457474\n",
      "Training Batch [83/782]: Loss 0.002718835836276412\n",
      "Training Batch [84/782]: Loss 0.0003268092987127602\n",
      "Training Batch [85/782]: Loss 0.0037167074624449015\n",
      "Training Batch [86/782]: Loss 0.004076697863638401\n",
      "Training Batch [87/782]: Loss 0.01293924544006586\n",
      "Training Batch [88/782]: Loss 0.015704842284321785\n",
      "Training Batch [89/782]: Loss 0.008012879639863968\n",
      "Training Batch [90/782]: Loss 0.008801417425274849\n",
      "Training Batch [91/782]: Loss 0.035855311900377274\n",
      "Training Batch [92/782]: Loss 0.02283216081559658\n",
      "Training Batch [93/782]: Loss 0.0023596221581101418\n",
      "Training Batch [94/782]: Loss 0.04430775344371796\n",
      "Training Batch [95/782]: Loss 0.0004363060579635203\n",
      "Training Batch [96/782]: Loss 0.005126082804054022\n",
      "Training Batch [97/782]: Loss 0.00520394928753376\n",
      "Training Batch [98/782]: Loss 0.0014865940902382135\n",
      "Training Batch [99/782]: Loss 0.007714631501585245\n",
      "Training Batch [100/782]: Loss 0.14652995765209198\n",
      "Training Batch [101/782]: Loss 0.0071724168956279755\n",
      "Training Batch [102/782]: Loss 0.004391748923808336\n",
      "Training Batch [103/782]: Loss 0.003118244232609868\n",
      "Training Batch [104/782]: Loss 0.0024861989077180624\n",
      "Training Batch [105/782]: Loss 0.000698429998010397\n",
      "Training Batch [106/782]: Loss 0.01814897172152996\n",
      "Training Batch [107/782]: Loss 0.018184326589107513\n",
      "Training Batch [108/782]: Loss 0.004151220433413982\n",
      "Training Batch [109/782]: Loss 0.003212544834241271\n",
      "Training Batch [110/782]: Loss 0.004548812750726938\n",
      "Training Batch [111/782]: Loss 0.03024183213710785\n",
      "Training Batch [112/782]: Loss 0.02248385362327099\n",
      "Training Batch [113/782]: Loss 0.002955617383122444\n",
      "Training Batch [114/782]: Loss 0.02705092541873455\n",
      "Training Batch [115/782]: Loss 0.0028259786777198315\n",
      "Training Batch [116/782]: Loss 0.0285344198346138\n",
      "Training Batch [117/782]: Loss 0.03495443984866142\n",
      "Training Batch [118/782]: Loss 0.00069659692235291\n",
      "Training Batch [119/782]: Loss 0.010602228343486786\n",
      "Training Batch [120/782]: Loss 0.01096409559249878\n",
      "Training Batch [121/782]: Loss 0.0009578187600709498\n",
      "Training Batch [122/782]: Loss 0.010785034857690334\n",
      "Training Batch [123/782]: Loss 0.00293312082067132\n",
      "Training Batch [124/782]: Loss 0.006199667230248451\n",
      "Training Batch [125/782]: Loss 0.00599336251616478\n",
      "Training Batch [126/782]: Loss 0.004021866712719202\n",
      "Training Batch [127/782]: Loss 0.005210399627685547\n",
      "Training Batch [128/782]: Loss 0.004396555479615927\n",
      "Training Batch [129/782]: Loss 0.029091497883200645\n",
      "Training Batch [130/782]: Loss 0.0008327001705765724\n",
      "Training Batch [131/782]: Loss 0.017827315255999565\n",
      "Training Batch [132/782]: Loss 0.0007490915595553815\n",
      "Training Batch [133/782]: Loss 0.03578527644276619\n",
      "Training Batch [134/782]: Loss 0.002784834010526538\n",
      "Training Batch [135/782]: Loss 0.07002092897891998\n",
      "Training Batch [136/782]: Loss 0.016408614814281464\n",
      "Training Batch [137/782]: Loss 0.004358293488621712\n",
      "Training Batch [138/782]: Loss 0.010429504327476025\n",
      "Training Batch [139/782]: Loss 0.00803431961685419\n",
      "Training Batch [140/782]: Loss 0.0004503769741859287\n",
      "Training Batch [141/782]: Loss 0.02967085689306259\n",
      "Training Batch [142/782]: Loss 0.009595020674169064\n",
      "Training Batch [143/782]: Loss 0.027380170300602913\n",
      "Training Batch [144/782]: Loss 0.02177763357758522\n",
      "Training Batch [145/782]: Loss 0.0014384995447471738\n",
      "Training Batch [146/782]: Loss 0.00036647365777753294\n",
      "Training Batch [147/782]: Loss 0.022122489288449287\n",
      "Training Batch [148/782]: Loss 0.0029647217597812414\n",
      "Training Batch [149/782]: Loss 0.001254433416761458\n",
      "Training Batch [150/782]: Loss 0.01667371392250061\n",
      "Training Batch [151/782]: Loss 0.0022112177684903145\n",
      "Training Batch [152/782]: Loss 0.010883117094635963\n",
      "Training Batch [153/782]: Loss 0.021510101854801178\n",
      "Training Batch [154/782]: Loss 0.002063254127278924\n",
      "Training Batch [155/782]: Loss 0.005287730600684881\n",
      "Training Batch [156/782]: Loss 0.019541027024388313\n",
      "Training Batch [157/782]: Loss 0.0032729373779147863\n",
      "Training Batch [158/782]: Loss 0.0037165533285588026\n",
      "Training Batch [159/782]: Loss 0.0032224650494754314\n",
      "Training Batch [160/782]: Loss 0.010738293640315533\n",
      "Training Batch [161/782]: Loss 0.0235295332968235\n",
      "Training Batch [162/782]: Loss 0.07426252216100693\n",
      "Training Batch [163/782]: Loss 0.001940599293448031\n",
      "Training Batch [164/782]: Loss 0.014790448360145092\n",
      "Training Batch [165/782]: Loss 0.0026573562063276768\n",
      "Training Batch [166/782]: Loss 0.011530192568898201\n",
      "Training Batch [167/782]: Loss 0.009526311419904232\n",
      "Training Batch [168/782]: Loss 0.0012792217312380672\n",
      "Training Batch [169/782]: Loss 0.06977946311235428\n",
      "Training Batch [170/782]: Loss 0.003420291468501091\n",
      "Training Batch [171/782]: Loss 0.004607880488038063\n",
      "Training Batch [172/782]: Loss 0.04241050407290459\n",
      "Training Batch [173/782]: Loss 0.0053245010785758495\n",
      "Training Batch [174/782]: Loss 0.0004126495332457125\n",
      "Training Batch [175/782]: Loss 0.04317136108875275\n",
      "Training Batch [176/782]: Loss 0.008598072454333305\n",
      "Training Batch [177/782]: Loss 0.02550446428358555\n",
      "Training Batch [178/782]: Loss 0.004184315912425518\n",
      "Training Batch [179/782]: Loss 0.001037407899275422\n",
      "Training Batch [180/782]: Loss 0.009609040804207325\n",
      "Training Batch [181/782]: Loss 0.014392410404980183\n",
      "Training Batch [182/782]: Loss 0.0010808249935507774\n",
      "Training Batch [183/782]: Loss 0.016145098954439163\n",
      "Training Batch [184/782]: Loss 0.01593514159321785\n",
      "Training Batch [185/782]: Loss 0.014380752108991146\n",
      "Training Batch [186/782]: Loss 0.003878757357597351\n",
      "Training Batch [187/782]: Loss 0.0008404389955103397\n",
      "Training Batch [188/782]: Loss 0.016332348808646202\n",
      "Training Batch [189/782]: Loss 0.0050855400040745735\n",
      "Training Batch [190/782]: Loss 0.000896595767699182\n",
      "Training Batch [191/782]: Loss 0.0013323096791282296\n",
      "Training Batch [192/782]: Loss 0.000597831211052835\n",
      "Training Batch [193/782]: Loss 0.003571193665266037\n",
      "Training Batch [194/782]: Loss 0.013105425983667374\n",
      "Training Batch [195/782]: Loss 0.007539513986557722\n",
      "Training Batch [196/782]: Loss 0.011464608833193779\n",
      "Training Batch [197/782]: Loss 0.011611053720116615\n",
      "Training Batch [198/782]: Loss 0.0014408628921955824\n",
      "Training Batch [199/782]: Loss 0.0014175213873386383\n",
      "Training Batch [200/782]: Loss 0.0683799684047699\n",
      "Training Batch [201/782]: Loss 0.009172214195132256\n",
      "Training Batch [202/782]: Loss 0.010686882771551609\n",
      "Training Batch [203/782]: Loss 0.0012316250940784812\n",
      "Training Batch [204/782]: Loss 0.026196027174592018\n",
      "Training Batch [205/782]: Loss 0.005442195106297731\n",
      "Training Batch [206/782]: Loss 0.024534592404961586\n",
      "Training Batch [207/782]: Loss 0.00041765524656511843\n",
      "Training Batch [208/782]: Loss 0.012768554501235485\n",
      "Training Batch [209/782]: Loss 0.0020701605826616287\n",
      "Training Batch [210/782]: Loss 0.0004372125258669257\n",
      "Training Batch [211/782]: Loss 0.01122549083083868\n",
      "Training Batch [212/782]: Loss 0.020628806203603745\n",
      "Training Batch [213/782]: Loss 0.002568016992881894\n",
      "Training Batch [214/782]: Loss 0.0037560591008514166\n",
      "Training Batch [215/782]: Loss 0.0024196256417781115\n",
      "Training Batch [216/782]: Loss 0.005204549990594387\n",
      "Training Batch [217/782]: Loss 0.006303281988948584\n",
      "Training Batch [218/782]: Loss 0.005065490026026964\n",
      "Training Batch [219/782]: Loss 0.003298635594546795\n",
      "Training Batch [220/782]: Loss 0.03296204283833504\n",
      "Training Batch [221/782]: Loss 0.000552753743249923\n",
      "Training Batch [222/782]: Loss 0.0025160948280245066\n",
      "Training Batch [223/782]: Loss 0.003363379742950201\n",
      "Training Batch [224/782]: Loss 0.008670495823025703\n",
      "Training Batch [225/782]: Loss 0.0063706496730446815\n",
      "Training Batch [226/782]: Loss 0.004966114182025194\n",
      "Training Batch [227/782]: Loss 0.017032163217663765\n",
      "Training Batch [228/782]: Loss 0.00034839834552258253\n",
      "Training Batch [229/782]: Loss 0.0007852955022826791\n",
      "Training Batch [230/782]: Loss 0.0029440349899232388\n",
      "Training Batch [231/782]: Loss 0.016254886984825134\n",
      "Training Batch [232/782]: Loss 0.039718031883239746\n",
      "Training Batch [233/782]: Loss 0.01787746697664261\n",
      "Training Batch [234/782]: Loss 0.04421168193221092\n",
      "Training Batch [235/782]: Loss 0.07305903732776642\n",
      "Training Batch [236/782]: Loss 0.03248729184269905\n",
      "Training Batch [237/782]: Loss 0.006678435485810041\n",
      "Training Batch [238/782]: Loss 0.0019725558813661337\n",
      "Training Batch [239/782]: Loss 0.0019684182479977608\n",
      "Training Batch [240/782]: Loss 0.0024640450719743967\n",
      "Training Batch [241/782]: Loss 0.011567017063498497\n",
      "Training Batch [242/782]: Loss 0.00958635937422514\n",
      "Training Batch [243/782]: Loss 0.033461518585681915\n",
      "Training Batch [244/782]: Loss 0.004659704864025116\n",
      "Training Batch [245/782]: Loss 0.0404931865632534\n",
      "Training Batch [246/782]: Loss 0.0009350579930469394\n",
      "Training Batch [247/782]: Loss 0.03332876041531563\n",
      "Training Batch [248/782]: Loss 0.011897414922714233\n",
      "Training Batch [249/782]: Loss 0.015260022133588791\n",
      "Training Batch [250/782]: Loss 0.0007547096465714276\n",
      "Training Batch [251/782]: Loss 0.000892048585228622\n",
      "Training Batch [252/782]: Loss 0.00028733236831612885\n",
      "Training Batch [253/782]: Loss 0.0009476776467636228\n",
      "Training Batch [254/782]: Loss 0.0011255135759711266\n",
      "Training Batch [255/782]: Loss 0.00930538959801197\n",
      "Training Batch [256/782]: Loss 0.0033875186927616596\n",
      "Training Batch [257/782]: Loss 0.006969401612877846\n",
      "Training Batch [258/782]: Loss 0.0007716526743024588\n",
      "Training Batch [259/782]: Loss 0.011564829386770725\n",
      "Training Batch [260/782]: Loss 0.00124048488214612\n",
      "Training Batch [261/782]: Loss 0.0030654012225568295\n",
      "Training Batch [262/782]: Loss 0.026429113000631332\n",
      "Training Batch [263/782]: Loss 0.023644089698791504\n",
      "Training Batch [264/782]: Loss 0.004469078965485096\n",
      "Training Batch [265/782]: Loss 0.004997352138161659\n",
      "Training Batch [266/782]: Loss 0.0013236769009381533\n",
      "Training Batch [267/782]: Loss 0.000939806632231921\n",
      "Training Batch [268/782]: Loss 0.0007445109658874571\n",
      "Training Batch [269/782]: Loss 0.0032108810264617205\n",
      "Training Batch [270/782]: Loss 0.0007875129231251776\n",
      "Training Batch [271/782]: Loss 0.0007456104503944516\n",
      "Training Batch [272/782]: Loss 0.032044146209955215\n",
      "Training Batch [273/782]: Loss 0.0017451192252337933\n",
      "Training Batch [274/782]: Loss 0.00228225439786911\n",
      "Training Batch [275/782]: Loss 0.005007580853998661\n",
      "Training Batch [276/782]: Loss 0.001407197560183704\n",
      "Training Batch [277/782]: Loss 0.005491787567734718\n",
      "Training Batch [278/782]: Loss 0.007881947793066502\n",
      "Training Batch [279/782]: Loss 0.0122547447681427\n",
      "Training Batch [280/782]: Loss 0.0003242004895582795\n",
      "Training Batch [281/782]: Loss 0.014851232059299946\n",
      "Training Batch [282/782]: Loss 0.008588972501456738\n",
      "Training Batch [283/782]: Loss 0.0015806128503754735\n",
      "Training Batch [284/782]: Loss 0.0028360348660498857\n",
      "Training Batch [285/782]: Loss 0.0020042010582983494\n",
      "Training Batch [286/782]: Loss 0.0010794197442010045\n",
      "Training Batch [287/782]: Loss 0.008070500567555428\n",
      "Training Batch [288/782]: Loss 0.0004427751700859517\n",
      "Training Batch [289/782]: Loss 0.001998509978875518\n",
      "Training Batch [290/782]: Loss 0.004399082623422146\n",
      "Training Batch [291/782]: Loss 0.020239755511283875\n",
      "Training Batch [292/782]: Loss 0.0005199767183512449\n",
      "Training Batch [293/782]: Loss 0.0030523762106895447\n",
      "Training Batch [294/782]: Loss 0.011266413144767284\n",
      "Training Batch [295/782]: Loss 0.010161720216274261\n",
      "Training Batch [296/782]: Loss 0.00395268714055419\n",
      "Training Batch [297/782]: Loss 0.0007480731001123786\n",
      "Training Batch [298/782]: Loss 0.009066172875463963\n",
      "Training Batch [299/782]: Loss 0.00037670668098144233\n",
      "Training Batch [300/782]: Loss 0.0019978201016783714\n",
      "Training Batch [301/782]: Loss 0.004934236872941256\n",
      "Training Batch [302/782]: Loss 0.003307197941467166\n",
      "Training Batch [303/782]: Loss 0.0007340143783949316\n",
      "Training Batch [304/782]: Loss 0.0027756819035857916\n",
      "Training Batch [305/782]: Loss 0.035885199904441833\n",
      "Training Batch [306/782]: Loss 0.002518794732168317\n",
      "Training Batch [307/782]: Loss 0.0012726214481517673\n",
      "Training Batch [308/782]: Loss 0.0028578832279890776\n",
      "Training Batch [309/782]: Loss 0.0007221101550385356\n",
      "Training Batch [310/782]: Loss 0.0014719825703650713\n",
      "Training Batch [311/782]: Loss 0.003649987978860736\n",
      "Training Batch [312/782]: Loss 0.00654015364125371\n",
      "Training Batch [313/782]: Loss 0.004021725617349148\n",
      "Training Batch [314/782]: Loss 0.0016891146078705788\n",
      "Training Batch [315/782]: Loss 0.0021463341545313597\n",
      "Training Batch [316/782]: Loss 0.0007550785085186362\n",
      "Training Batch [317/782]: Loss 0.005301400553435087\n",
      "Training Batch [318/782]: Loss 0.0012970524840056896\n",
      "Training Batch [319/782]: Loss 0.004550791345536709\n",
      "Training Batch [320/782]: Loss 0.007523273583501577\n",
      "Training Batch [321/782]: Loss 0.0015154340071603656\n",
      "Training Batch [322/782]: Loss 0.0008235007408075035\n",
      "Training Batch [323/782]: Loss 0.0006312871701084077\n",
      "Training Batch [324/782]: Loss 0.026575567200779915\n",
      "Training Batch [325/782]: Loss 0.01069597341120243\n",
      "Training Batch [326/782]: Loss 0.0022602586541324854\n",
      "Training Batch [327/782]: Loss 0.001742953434586525\n",
      "Training Batch [328/782]: Loss 0.0056599960662424564\n",
      "Training Batch [329/782]: Loss 0.0002634833217598498\n",
      "Training Batch [330/782]: Loss 0.00018800608813762665\n",
      "Training Batch [331/782]: Loss 0.0015163038624450564\n",
      "Training Batch [332/782]: Loss 0.0020213904790580273\n",
      "Training Batch [333/782]: Loss 0.0003693969047162682\n",
      "Training Batch [334/782]: Loss 0.0006261991220526397\n",
      "Training Batch [335/782]: Loss 0.0011959580006077886\n",
      "Training Batch [336/782]: Loss 0.0005771180731244385\n",
      "Training Batch [337/782]: Loss 0.0015460781287401915\n",
      "Training Batch [338/782]: Loss 0.001626976067200303\n",
      "Training Batch [339/782]: Loss 0.0005268228705972433\n",
      "Training Batch [340/782]: Loss 0.002613167744129896\n",
      "Training Batch [341/782]: Loss 0.0015842223074287176\n",
      "Training Batch [342/782]: Loss 0.00045511891948990524\n",
      "Training Batch [343/782]: Loss 0.0018989820964634418\n",
      "Training Batch [344/782]: Loss 0.0028418744914233685\n",
      "Training Batch [345/782]: Loss 0.0019108362030237913\n",
      "Training Batch [346/782]: Loss 0.009929691441357136\n",
      "Training Batch [347/782]: Loss 0.0006956428987905383\n",
      "Training Batch [348/782]: Loss 0.0004607382870744914\n",
      "Training Batch [349/782]: Loss 0.0007813894189894199\n",
      "Training Batch [350/782]: Loss 0.0013328714994713664\n",
      "Training Batch [351/782]: Loss 0.003332447726279497\n",
      "Training Batch [352/782]: Loss 0.0023113777860999107\n",
      "Training Batch [353/782]: Loss 0.007191401906311512\n",
      "Training Batch [354/782]: Loss 0.001720481552183628\n",
      "Training Batch [355/782]: Loss 0.011226234957575798\n",
      "Training Batch [356/782]: Loss 0.01335812732577324\n",
      "Training Batch [357/782]: Loss 0.0017947620945051312\n",
      "Training Batch [358/782]: Loss 0.017910372465848923\n",
      "Training Batch [359/782]: Loss 0.0018112905090674758\n",
      "Training Batch [360/782]: Loss 0.001547866384498775\n",
      "Training Batch [361/782]: Loss 0.0009047758067026734\n",
      "Training Batch [362/782]: Loss 0.0018260431243106723\n",
      "Training Batch [363/782]: Loss 0.009744974784553051\n",
      "Training Batch [364/782]: Loss 0.0025188648141920567\n",
      "Training Batch [365/782]: Loss 0.000929223548155278\n",
      "Training Batch [366/782]: Loss 0.003217200981453061\n",
      "Training Batch [367/782]: Loss 0.004041383042931557\n",
      "Training Batch [368/782]: Loss 0.009303173050284386\n",
      "Training Batch [369/782]: Loss 0.018232643604278564\n",
      "Training Batch [370/782]: Loss 0.0011829795548692346\n",
      "Training Batch [371/782]: Loss 0.015426895581185818\n",
      "Training Batch [372/782]: Loss 4.4412554416339844e-05\n",
      "Training Batch [373/782]: Loss 0.001615188317373395\n",
      "Training Batch [374/782]: Loss 0.007734167855232954\n",
      "Training Batch [375/782]: Loss 0.00019688651082105935\n",
      "Training Batch [376/782]: Loss 0.0004993434413336217\n",
      "Training Batch [377/782]: Loss 0.002456665737554431\n",
      "Training Batch [378/782]: Loss 0.0005975696258246899\n",
      "Training Batch [379/782]: Loss 0.000696424744091928\n",
      "Training Batch [380/782]: Loss 0.004132892470806837\n",
      "Training Batch [381/782]: Loss 0.0072181266732513905\n",
      "Training Batch [382/782]: Loss 0.0006247350247576833\n",
      "Training Batch [383/782]: Loss 0.0010842684423550963\n",
      "Training Batch [384/782]: Loss 0.000268742092885077\n",
      "Training Batch [385/782]: Loss 0.010146109387278557\n",
      "Training Batch [386/782]: Loss 0.0038543459959328175\n",
      "Training Batch [387/782]: Loss 0.0009550068061798811\n",
      "Training Batch [388/782]: Loss 0.013522729277610779\n",
      "Training Batch [389/782]: Loss 0.005178755149245262\n",
      "Training Batch [390/782]: Loss 0.002476039109751582\n",
      "Training Batch [391/782]: Loss 0.0004404103965498507\n",
      "Training Batch [392/782]: Loss 0.0012035120744258165\n",
      "Training Batch [393/782]: Loss 0.000673297792673111\n",
      "Training Batch [394/782]: Loss 0.0003254033799748868\n",
      "Training Batch [395/782]: Loss 0.0012558606686070561\n",
      "Training Batch [396/782]: Loss 0.0002690689289011061\n",
      "Training Batch [397/782]: Loss 0.03944125771522522\n",
      "Training Batch [398/782]: Loss 0.03236835449934006\n",
      "Training Batch [399/782]: Loss 0.0014465771382674575\n",
      "Training Batch [400/782]: Loss 0.012942411005496979\n",
      "Training Batch [401/782]: Loss 0.001170427887700498\n",
      "Training Batch [402/782]: Loss 0.0038336908910423517\n",
      "Training Batch [403/782]: Loss 0.0016644445713609457\n",
      "Training Batch [404/782]: Loss 0.0202310960739851\n",
      "Training Batch [405/782]: Loss 0.0026353788562119007\n",
      "Training Batch [406/782]: Loss 0.0006152435089461505\n",
      "Training Batch [407/782]: Loss 0.0029676032718271017\n",
      "Training Batch [408/782]: Loss 0.013923672959208488\n",
      "Training Batch [409/782]: Loss 0.001474145334213972\n",
      "Training Batch [410/782]: Loss 0.0033478932455182076\n",
      "Training Batch [411/782]: Loss 0.0011318670585751534\n",
      "Training Batch [412/782]: Loss 0.00018565201025921851\n",
      "Training Batch [413/782]: Loss 0.0004845826479140669\n",
      "Training Batch [414/782]: Loss 0.001201400998979807\n",
      "Training Batch [415/782]: Loss 0.0029001873917877674\n",
      "Training Batch [416/782]: Loss 0.0016394469421356916\n",
      "Training Batch [417/782]: Loss 0.00015741065726615489\n",
      "Training Batch [418/782]: Loss 0.0023783682845532894\n",
      "Training Batch [419/782]: Loss 0.001816422212868929\n",
      "Training Batch [420/782]: Loss 0.0024609749671071768\n",
      "Training Batch [421/782]: Loss 0.0016939835622906685\n",
      "Training Batch [422/782]: Loss 0.07944950461387634\n",
      "Training Batch [423/782]: Loss 0.0063978987745940685\n",
      "Training Batch [424/782]: Loss 0.0006320957909338176\n",
      "Training Batch [425/782]: Loss 0.009925558231770992\n",
      "Training Batch [426/782]: Loss 0.00016562182281631976\n",
      "Training Batch [427/782]: Loss 0.006813190411776304\n",
      "Training Batch [428/782]: Loss 0.0035388621035963297\n",
      "Training Batch [429/782]: Loss 0.01296838466078043\n",
      "Training Batch [430/782]: Loss 0.0037049937527626753\n",
      "Training Batch [431/782]: Loss 0.0016364562325179577\n",
      "Training Batch [432/782]: Loss 0.0008637583232484758\n",
      "Training Batch [433/782]: Loss 0.003455235157161951\n",
      "Training Batch [434/782]: Loss 0.0007797862053848803\n",
      "Training Batch [435/782]: Loss 0.038291435688734055\n",
      "Training Batch [436/782]: Loss 0.0031992094591259956\n",
      "Training Batch [437/782]: Loss 0.007533459458500147\n",
      "Training Batch [438/782]: Loss 0.0007509369170293212\n",
      "Training Batch [439/782]: Loss 0.0012535720597952604\n",
      "Training Batch [440/782]: Loss 0.027776610106229782\n",
      "Training Batch [441/782]: Loss 0.002870068419724703\n",
      "Training Batch [442/782]: Loss 0.0045011453330516815\n",
      "Training Batch [443/782]: Loss 0.002771723549813032\n",
      "Training Batch [444/782]: Loss 0.0029826578684151173\n",
      "Training Batch [445/782]: Loss 0.0019920323975384235\n",
      "Training Batch [446/782]: Loss 0.000549866002984345\n",
      "Training Batch [447/782]: Loss 0.003919264301657677\n",
      "Training Batch [448/782]: Loss 0.0030350328888744116\n",
      "Training Batch [449/782]: Loss 0.004379148129373789\n",
      "Training Batch [450/782]: Loss 0.015254995785653591\n",
      "Training Batch [451/782]: Loss 0.007529983296990395\n",
      "Training Batch [452/782]: Loss 0.0007517646299675107\n",
      "Training Batch [453/782]: Loss 0.0012088066432625055\n",
      "Training Batch [454/782]: Loss 0.00320909870788455\n",
      "Training Batch [455/782]: Loss 0.019051283597946167\n",
      "Training Batch [456/782]: Loss 0.0021437257528305054\n",
      "Training Batch [457/782]: Loss 0.0006003816961310804\n",
      "Training Batch [458/782]: Loss 0.013377794064581394\n",
      "Training Batch [459/782]: Loss 0.0050249844789505005\n",
      "Training Batch [460/782]: Loss 0.0027027439791709185\n",
      "Training Batch [461/782]: Loss 0.0001776018034433946\n",
      "Training Batch [462/782]: Loss 0.0009430096833966672\n",
      "Training Batch [463/782]: Loss 0.001364897470921278\n",
      "Training Batch [464/782]: Loss 0.005499056540429592\n",
      "Training Batch [465/782]: Loss 0.005088448990136385\n",
      "Training Batch [466/782]: Loss 0.0014282942283898592\n",
      "Training Batch [467/782]: Loss 0.0034561804495751858\n",
      "Training Batch [468/782]: Loss 0.05225590988993645\n",
      "Training Batch [469/782]: Loss 0.0024368935264647007\n",
      "Training Batch [470/782]: Loss 0.01425779890269041\n",
      "Training Batch [471/782]: Loss 0.002793877152726054\n",
      "Training Batch [472/782]: Loss 0.0009459747234359384\n",
      "Training Batch [473/782]: Loss 0.03955306485295296\n",
      "Training Batch [474/782]: Loss 0.0017110714688897133\n",
      "Training Batch [475/782]: Loss 0.0036557826679199934\n",
      "Training Batch [476/782]: Loss 0.0005954798543825746\n",
      "Training Batch [477/782]: Loss 0.002643861109390855\n",
      "Training Batch [478/782]: Loss 0.0016904189251363277\n",
      "Training Batch [479/782]: Loss 0.0036491972859948874\n",
      "Training Batch [480/782]: Loss 0.0017490385798737407\n",
      "Training Batch [481/782]: Loss 0.002804212272167206\n",
      "Training Batch [482/782]: Loss 0.1987355649471283\n",
      "Training Batch [483/782]: Loss 0.0007885665399953723\n",
      "Training Batch [484/782]: Loss 0.017705777660012245\n",
      "Training Batch [485/782]: Loss 0.0030919380951672792\n",
      "Training Batch [486/782]: Loss 0.002591736614704132\n",
      "Training Batch [487/782]: Loss 0.004777013324201107\n",
      "Training Batch [488/782]: Loss 0.0005136181716807187\n",
      "Training Batch [489/782]: Loss 0.002856337698176503\n",
      "Training Batch [490/782]: Loss 0.017348632216453552\n",
      "Training Batch [491/782]: Loss 0.07867812365293503\n",
      "Training Batch [492/782]: Loss 0.05347314476966858\n",
      "Training Batch [493/782]: Loss 0.003364186966791749\n",
      "Training Batch [494/782]: Loss 0.0009188972180709243\n",
      "Training Batch [495/782]: Loss 0.010798192583024502\n",
      "Training Batch [496/782]: Loss 0.00180782750248909\n",
      "Training Batch [497/782]: Loss 0.016132643446326256\n",
      "Training Batch [498/782]: Loss 0.005457865074276924\n",
      "Training Batch [499/782]: Loss 0.022731447592377663\n",
      "Training Batch [500/782]: Loss 0.0035299486480653286\n",
      "Training Batch [501/782]: Loss 0.04634682461619377\n",
      "Training Batch [502/782]: Loss 0.09800602495670319\n",
      "Training Batch [503/782]: Loss 0.005098687019199133\n",
      "Training Batch [504/782]: Loss 0.0035490139853209257\n",
      "Training Batch [505/782]: Loss 0.0027546321507543325\n",
      "Training Batch [506/782]: Loss 0.0005824322579428554\n",
      "Training Batch [507/782]: Loss 0.006382624618709087\n",
      "Training Batch [508/782]: Loss 0.0667053684592247\n",
      "Training Batch [509/782]: Loss 0.013663091696798801\n",
      "Training Batch [510/782]: Loss 0.00238020159304142\n",
      "Training Batch [511/782]: Loss 0.005048173479735851\n",
      "Training Batch [512/782]: Loss 0.00198996695689857\n",
      "Training Batch [513/782]: Loss 0.03865419700741768\n",
      "Training Batch [514/782]: Loss 0.005712210666388273\n",
      "Training Batch [515/782]: Loss 0.002685714978724718\n",
      "Training Batch [516/782]: Loss 0.018852872774004936\n",
      "Training Batch [517/782]: Loss 0.0031680807005614042\n",
      "Training Batch [518/782]: Loss 0.002159066265448928\n",
      "Training Batch [519/782]: Loss 0.010523740202188492\n",
      "Training Batch [520/782]: Loss 0.00037554200389422476\n",
      "Training Batch [521/782]: Loss 0.012072429992258549\n",
      "Training Batch [522/782]: Loss 0.0031079112086445093\n",
      "Training Batch [523/782]: Loss 0.00230597541667521\n",
      "Training Batch [524/782]: Loss 0.010111133567988873\n",
      "Training Batch [525/782]: Loss 0.0073355138301849365\n",
      "Training Batch [526/782]: Loss 0.001668133307248354\n",
      "Training Batch [527/782]: Loss 0.0031738723628222942\n",
      "Training Batch [528/782]: Loss 0.0049322242848575115\n",
      "Training Batch [529/782]: Loss 0.026284722611308098\n",
      "Training Batch [530/782]: Loss 0.0008570163045078516\n",
      "Training Batch [531/782]: Loss 0.008447125554084778\n",
      "Training Batch [532/782]: Loss 0.019048232585191727\n",
      "Training Batch [533/782]: Loss 0.003670517122372985\n",
      "Training Batch [534/782]: Loss 0.0068853567354381084\n",
      "Training Batch [535/782]: Loss 0.024705568328499794\n",
      "Training Batch [536/782]: Loss 0.004079055972397327\n",
      "Training Batch [537/782]: Loss 0.005196144338697195\n",
      "Training Batch [538/782]: Loss 0.01279039029031992\n",
      "Training Batch [539/782]: Loss 0.005379336886107922\n",
      "Training Batch [540/782]: Loss 0.010927528142929077\n",
      "Training Batch [541/782]: Loss 0.001578273717314005\n",
      "Training Batch [542/782]: Loss 0.00040775156230665743\n",
      "Training Batch [543/782]: Loss 0.005293719936162233\n",
      "Training Batch [544/782]: Loss 0.0027339046355336905\n",
      "Training Batch [545/782]: Loss 0.012063608504831791\n",
      "Training Batch [546/782]: Loss 0.014356561936438084\n",
      "Training Batch [547/782]: Loss 0.0019429627573117614\n",
      "Training Batch [548/782]: Loss 0.0018260299693793058\n",
      "Training Batch [549/782]: Loss 0.008012541569769382\n",
      "Training Batch [550/782]: Loss 0.005207446403801441\n",
      "Training Batch [551/782]: Loss 0.056932516396045685\n",
      "Training Batch [552/782]: Loss 0.007423141971230507\n",
      "Training Batch [553/782]: Loss 0.014896257780492306\n",
      "Training Batch [554/782]: Loss 0.0005923084099777043\n",
      "Training Batch [555/782]: Loss 0.0010678418911993504\n",
      "Training Batch [556/782]: Loss 0.01717778854072094\n",
      "Training Batch [557/782]: Loss 0.011826695874333382\n",
      "Training Batch [558/782]: Loss 0.006769253872334957\n",
      "Training Batch [559/782]: Loss 0.025045672431588173\n",
      "Training Batch [560/782]: Loss 0.0022828008513897657\n",
      "Training Batch [561/782]: Loss 0.06684726476669312\n",
      "Training Batch [562/782]: Loss 0.0014214178081601858\n",
      "Training Batch [563/782]: Loss 0.003415660932660103\n",
      "Training Batch [564/782]: Loss 0.046937376260757446\n",
      "Training Batch [565/782]: Loss 0.045847903937101364\n",
      "Training Batch [566/782]: Loss 0.008399486541748047\n",
      "Training Batch [567/782]: Loss 0.0011644179467111826\n",
      "Training Batch [568/782]: Loss 0.057415857911109924\n",
      "Training Batch [569/782]: Loss 0.0028176798950880766\n",
      "Training Batch [570/782]: Loss 0.01897547021508217\n",
      "Training Batch [571/782]: Loss 0.02108493074774742\n",
      "Training Batch [572/782]: Loss 0.033460695296525955\n",
      "Training Batch [573/782]: Loss 0.021129943430423737\n",
      "Training Batch [574/782]: Loss 0.007794585078954697\n",
      "Training Batch [575/782]: Loss 0.014833958819508553\n",
      "Training Batch [576/782]: Loss 0.0022347692865878344\n",
      "Training Batch [577/782]: Loss 0.0013864028733223677\n",
      "Training Batch [578/782]: Loss 0.00378743140026927\n",
      "Training Batch [579/782]: Loss 0.0019704627338796854\n",
      "Training Batch [580/782]: Loss 0.0015266711125150323\n",
      "Training Batch [581/782]: Loss 0.0018722022650763392\n",
      "Training Batch [582/782]: Loss 0.0009932966204360127\n",
      "Training Batch [583/782]: Loss 0.032571833580732346\n",
      "Training Batch [584/782]: Loss 0.029306817799806595\n",
      "Training Batch [585/782]: Loss 0.004899028688669205\n",
      "Training Batch [586/782]: Loss 0.045787740498781204\n",
      "Training Batch [587/782]: Loss 0.06993401795625687\n",
      "Training Batch [588/782]: Loss 0.04971543326973915\n",
      "Training Batch [589/782]: Loss 0.000615317199844867\n",
      "Training Batch [590/782]: Loss 0.0022390265949070454\n",
      "Training Batch [591/782]: Loss 0.002769462065771222\n",
      "Training Batch [592/782]: Loss 0.03515135124325752\n",
      "Training Batch [593/782]: Loss 0.07553617656230927\n",
      "Training Batch [594/782]: Loss 0.0008876481442712247\n",
      "Training Batch [595/782]: Loss 0.01820862852036953\n",
      "Training Batch [596/782]: Loss 0.11517897993326187\n",
      "Training Batch [597/782]: Loss 0.001989068230614066\n",
      "Training Batch [598/782]: Loss 0.0028294329531490803\n",
      "Training Batch [599/782]: Loss 0.0005351179861463606\n",
      "Training Batch [600/782]: Loss 0.0032844359520822763\n",
      "Training Batch [601/782]: Loss 0.0010405265493318439\n",
      "Training Batch [602/782]: Loss 0.00561637943610549\n",
      "Training Batch [603/782]: Loss 0.09629480540752411\n",
      "Training Batch [604/782]: Loss 0.008225478231906891\n",
      "Training Batch [605/782]: Loss 0.012417560443282127\n",
      "Training Batch [606/782]: Loss 0.0034487692173570395\n",
      "Training Batch [607/782]: Loss 0.008275492116808891\n",
      "Training Batch [608/782]: Loss 0.0006974132847972214\n",
      "Training Batch [609/782]: Loss 0.006016061175614595\n",
      "Training Batch [610/782]: Loss 0.001718555693514645\n",
      "Training Batch [611/782]: Loss 0.0479813888669014\n",
      "Training Batch [612/782]: Loss 0.0030980557203292847\n",
      "Training Batch [613/782]: Loss 0.007707824930548668\n",
      "Training Batch [614/782]: Loss 0.013608315959572792\n",
      "Training Batch [615/782]: Loss 0.0045096613466739655\n",
      "Training Batch [616/782]: Loss 0.007454909384250641\n",
      "Training Batch [617/782]: Loss 0.03319191560149193\n",
      "Training Batch [618/782]: Loss 0.004264431074261665\n",
      "Training Batch [619/782]: Loss 0.011740559712052345\n",
      "Training Batch [620/782]: Loss 0.0023407756816595793\n",
      "Training Batch [621/782]: Loss 0.03630693629384041\n",
      "Training Batch [622/782]: Loss 0.033704303205013275\n",
      "Training Batch [623/782]: Loss 0.029851237311959267\n",
      "Training Batch [624/782]: Loss 0.0025108817499130964\n",
      "Training Batch [625/782]: Loss 0.0010388570372015238\n",
      "Training Batch [626/782]: Loss 0.10667834430932999\n",
      "Training Batch [627/782]: Loss 0.022274181246757507\n",
      "Training Batch [628/782]: Loss 0.008131868205964565\n",
      "Training Batch [629/782]: Loss 0.0019861652981489897\n",
      "Training Batch [630/782]: Loss 0.12134416401386261\n",
      "Training Batch [631/782]: Loss 0.0006203801603987813\n",
      "Training Batch [632/782]: Loss 0.05419705808162689\n",
      "Training Batch [633/782]: Loss 0.04274773225188255\n",
      "Training Batch [634/782]: Loss 0.005241698585450649\n",
      "Training Batch [635/782]: Loss 0.0014292174018919468\n",
      "Training Batch [636/782]: Loss 0.009124294854700565\n",
      "Training Batch [637/782]: Loss 0.0012070187367498875\n",
      "Training Batch [638/782]: Loss 0.006235879380255938\n",
      "Training Batch [639/782]: Loss 0.004651155322790146\n",
      "Training Batch [640/782]: Loss 0.03565171733498573\n",
      "Training Batch [641/782]: Loss 0.02167953923344612\n",
      "Training Batch [642/782]: Loss 0.0015381614211946726\n",
      "Training Batch [643/782]: Loss 0.00222603976726532\n",
      "Training Batch [644/782]: Loss 0.006193064618855715\n",
      "Training Batch [645/782]: Loss 0.0024058795534074306\n",
      "Training Batch [646/782]: Loss 0.022047314792871475\n",
      "Training Batch [647/782]: Loss 0.0022764778696000576\n",
      "Training Batch [648/782]: Loss 0.025414714589715004\n",
      "Training Batch [649/782]: Loss 0.03724227100610733\n",
      "Training Batch [650/782]: Loss 0.014470751397311687\n",
      "Training Batch [651/782]: Loss 0.10429411381483078\n",
      "Training Batch [652/782]: Loss 0.028405889868736267\n",
      "Training Batch [653/782]: Loss 0.0016491764690726995\n",
      "Training Batch [654/782]: Loss 0.009301477111876011\n",
      "Training Batch [655/782]: Loss 0.01787979155778885\n",
      "Training Batch [656/782]: Loss 0.018601110205054283\n",
      "Training Batch [657/782]: Loss 0.002428206615149975\n",
      "Training Batch [658/782]: Loss 0.005236090626567602\n",
      "Training Batch [659/782]: Loss 0.0022864998318254948\n",
      "Training Batch [660/782]: Loss 0.0017651808448135853\n",
      "Training Batch [661/782]: Loss 0.0032086235005408525\n",
      "Training Batch [662/782]: Loss 0.010683473199605942\n",
      "Training Batch [663/782]: Loss 0.032878462225198746\n",
      "Training Batch [664/782]: Loss 0.009876369498670101\n",
      "Training Batch [665/782]: Loss 0.007635856047272682\n",
      "Training Batch [666/782]: Loss 0.005427888594567776\n",
      "Training Batch [667/782]: Loss 0.004263739101588726\n",
      "Training Batch [668/782]: Loss 0.009042149409651756\n",
      "Training Batch [669/782]: Loss 0.05387416481971741\n",
      "Training Batch [670/782]: Loss 0.0021030704956501722\n",
      "Training Batch [671/782]: Loss 0.005625877529382706\n",
      "Training Batch [672/782]: Loss 0.08208154141902924\n",
      "Training Batch [673/782]: Loss 0.012437311001121998\n",
      "Training Batch [674/782]: Loss 0.012376527301967144\n",
      "Training Batch [675/782]: Loss 0.007102763745933771\n",
      "Training Batch [676/782]: Loss 0.0032542061526328325\n",
      "Training Batch [677/782]: Loss 0.0010709295747801661\n",
      "Training Batch [678/782]: Loss 0.0007399858441203833\n",
      "Training Batch [679/782]: Loss 0.010611798614263535\n",
      "Training Batch [680/782]: Loss 0.028081541880965233\n",
      "Training Batch [681/782]: Loss 0.060396961867809296\n",
      "Training Batch [682/782]: Loss 0.0025676998775452375\n",
      "Training Batch [683/782]: Loss 0.0034923022612929344\n",
      "Training Batch [684/782]: Loss 0.004411538131535053\n",
      "Training Batch [685/782]: Loss 0.0006034317775629461\n",
      "Training Batch [686/782]: Loss 0.006324794143438339\n",
      "Training Batch [687/782]: Loss 0.0008711282280273736\n",
      "Training Batch [688/782]: Loss 0.00014076985826250166\n",
      "Training Batch [689/782]: Loss 0.030759304761886597\n",
      "Training Batch [690/782]: Loss 0.011457402259111404\n",
      "Training Batch [691/782]: Loss 0.004029023926705122\n",
      "Training Batch [692/782]: Loss 0.028547203168272972\n",
      "Training Batch [693/782]: Loss 0.017840366810560226\n",
      "Training Batch [694/782]: Loss 0.0019380021840333939\n",
      "Training Batch [695/782]: Loss 0.006963892839848995\n",
      "Training Batch [696/782]: Loss 0.0005715677980333567\n",
      "Training Batch [697/782]: Loss 0.017209477722644806\n",
      "Training Batch [698/782]: Loss 0.005977856460958719\n",
      "Training Batch [699/782]: Loss 0.004075620789080858\n",
      "Training Batch [700/782]: Loss 0.01013859175145626\n",
      "Training Batch [701/782]: Loss 0.006543334573507309\n",
      "Training Batch [702/782]: Loss 0.006933268159627914\n",
      "Training Batch [703/782]: Loss 0.0206803847104311\n",
      "Training Batch [704/782]: Loss 0.0008062837878242135\n",
      "Training Batch [705/782]: Loss 0.010581660084426403\n",
      "Training Batch [706/782]: Loss 0.0014773494331166148\n",
      "Training Batch [707/782]: Loss 0.026603469625115395\n",
      "Training Batch [708/782]: Loss 0.003792006755247712\n",
      "Training Batch [709/782]: Loss 0.0002391519519733265\n",
      "Training Batch [710/782]: Loss 0.007845320738852024\n",
      "Training Batch [711/782]: Loss 0.0015452697407454252\n",
      "Training Batch [712/782]: Loss 0.005236014723777771\n",
      "Training Batch [713/782]: Loss 0.009837794117629528\n",
      "Training Batch [714/782]: Loss 0.0007094742613844573\n",
      "Training Batch [715/782]: Loss 0.002580885309726\n",
      "Training Batch [716/782]: Loss 0.025336014106869698\n",
      "Training Batch [717/782]: Loss 0.015922004356980324\n",
      "Training Batch [718/782]: Loss 0.017647508531808853\n",
      "Training Batch [719/782]: Loss 0.0003286795108579099\n",
      "Training Batch [720/782]: Loss 0.01417485810816288\n",
      "Training Batch [721/782]: Loss 0.0008102493593469262\n",
      "Training Batch [722/782]: Loss 0.0036999082658439875\n",
      "Training Batch [723/782]: Loss 0.015671247616410255\n",
      "Training Batch [724/782]: Loss 0.057380352169275284\n",
      "Training Batch [725/782]: Loss 0.018387148156762123\n",
      "Training Batch [726/782]: Loss 0.05114976316690445\n",
      "Training Batch [727/782]: Loss 0.050112515687942505\n",
      "Training Batch [728/782]: Loss 0.0019377836724743247\n",
      "Training Batch [729/782]: Loss 0.00046248961007222533\n",
      "Training Batch [730/782]: Loss 0.0004939668579027057\n",
      "Training Batch [731/782]: Loss 0.004963003098964691\n",
      "Training Batch [732/782]: Loss 0.024293413385748863\n",
      "Training Batch [733/782]: Loss 0.0058457255363464355\n",
      "Training Batch [734/782]: Loss 0.018500426784157753\n",
      "Training Batch [735/782]: Loss 0.0011686088982969522\n",
      "Training Batch [736/782]: Loss 0.002993173198774457\n",
      "Training Batch [737/782]: Loss 0.006930178496986628\n",
      "Training Batch [738/782]: Loss 0.04663152992725372\n",
      "Training Batch [739/782]: Loss 0.008262651041150093\n",
      "Training Batch [740/782]: Loss 0.005193687044084072\n",
      "Training Batch [741/782]: Loss 0.005712463520467281\n",
      "Training Batch [742/782]: Loss 0.0005188008071854711\n",
      "Training Batch [743/782]: Loss 0.029092863202095032\n",
      "Training Batch [744/782]: Loss 0.002907093847170472\n",
      "Training Batch [745/782]: Loss 0.004376429598778486\n",
      "Training Batch [746/782]: Loss 0.0005076302331872284\n",
      "Training Batch [747/782]: Loss 0.003216927871108055\n",
      "Training Batch [748/782]: Loss 0.029170162975788116\n",
      "Training Batch [749/782]: Loss 0.038584671914577484\n",
      "Training Batch [750/782]: Loss 0.08846686780452728\n",
      "Training Batch [751/782]: Loss 0.0035134470090270042\n",
      "Training Batch [752/782]: Loss 0.0006403203005902469\n",
      "Training Batch [753/782]: Loss 0.0052394624799489975\n",
      "Training Batch [754/782]: Loss 0.004642675165086985\n",
      "Training Batch [755/782]: Loss 0.023833978921175003\n",
      "Training Batch [756/782]: Loss 0.008582805283367634\n",
      "Training Batch [757/782]: Loss 0.004584872163832188\n",
      "Training Batch [758/782]: Loss 0.003988056443631649\n",
      "Training Batch [759/782]: Loss 0.006003670860081911\n",
      "Training Batch [760/782]: Loss 0.0015025237808004022\n",
      "Training Batch [761/782]: Loss 0.01270306296646595\n",
      "Training Batch [762/782]: Loss 0.008075868710875511\n",
      "Training Batch [763/782]: Loss 0.0009792520431801677\n",
      "Training Batch [764/782]: Loss 0.0001875142625067383\n",
      "Training Batch [765/782]: Loss 0.014549098908901215\n",
      "Training Batch [766/782]: Loss 0.01631394401192665\n",
      "Training Batch [767/782]: Loss 0.004751121625304222\n",
      "Training Batch [768/782]: Loss 0.0005530624184757471\n",
      "Training Batch [769/782]: Loss 0.000703068682923913\n",
      "Training Batch [770/782]: Loss 0.003350618528202176\n",
      "Training Batch [771/782]: Loss 0.0041122534312307835\n",
      "Training Batch [772/782]: Loss 0.002098487690091133\n",
      "Training Batch [773/782]: Loss 0.002459890441969037\n",
      "Training Batch [774/782]: Loss 0.010144386440515518\n",
      "Training Batch [775/782]: Loss 0.0032240047585219145\n",
      "Training Batch [776/782]: Loss 0.0028030266985297203\n",
      "Training Batch [777/782]: Loss 0.00046620299690403044\n",
      "Training Batch [778/782]: Loss 0.002680219244211912\n",
      "Training Batch [779/782]: Loss 0.0009058394934982061\n",
      "Training Batch [780/782]: Loss 0.008052037097513676\n",
      "Training Batch [781/782]: Loss 0.041285790503025055\n",
      "Training Batch [782/782]: Loss 0.03537755459547043\n",
      "Epoch 27 - Train Loss: 0.0128\n",
      "*********  Epoch 28/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.0010653398931026459\n",
      "Training Batch [2/782]: Loss 0.01104792207479477\n",
      "Training Batch [3/782]: Loss 0.014087853953242302\n",
      "Training Batch [4/782]: Loss 0.0010555455228313804\n",
      "Training Batch [5/782]: Loss 0.0011020861566066742\n",
      "Training Batch [6/782]: Loss 0.003167595248669386\n",
      "Training Batch [7/782]: Loss 0.005397083703428507\n",
      "Training Batch [8/782]: Loss 0.0010352787794545293\n",
      "Training Batch [9/782]: Loss 0.02311541698873043\n",
      "Training Batch [10/782]: Loss 0.0026215054094791412\n",
      "Training Batch [11/782]: Loss 0.00017391357687301934\n",
      "Training Batch [12/782]: Loss 0.015419566072523594\n",
      "Training Batch [13/782]: Loss 0.03770488500595093\n",
      "Training Batch [14/782]: Loss 0.0024008103646337986\n",
      "Training Batch [15/782]: Loss 0.011180720292031765\n",
      "Training Batch [16/782]: Loss 0.014141771011054516\n",
      "Training Batch [17/782]: Loss 0.005595381837338209\n",
      "Training Batch [18/782]: Loss 0.0022553603630512953\n",
      "Training Batch [19/782]: Loss 0.057417213916778564\n",
      "Training Batch [20/782]: Loss 0.012053720653057098\n",
      "Training Batch [21/782]: Loss 0.006688839755952358\n",
      "Training Batch [22/782]: Loss 0.003625029930844903\n",
      "Training Batch [23/782]: Loss 0.04157797992229462\n",
      "Training Batch [24/782]: Loss 0.0015822084387764335\n",
      "Training Batch [25/782]: Loss 0.0038412469439208508\n",
      "Training Batch [26/782]: Loss 0.0377020426094532\n",
      "Training Batch [27/782]: Loss 0.001488006324507296\n",
      "Training Batch [28/782]: Loss 0.008758356794714928\n",
      "Training Batch [29/782]: Loss 0.0012799832038581371\n",
      "Training Batch [30/782]: Loss 0.05231868475675583\n",
      "Training Batch [31/782]: Loss 0.0003855515387840569\n",
      "Training Batch [32/782]: Loss 0.0008317975443787873\n",
      "Training Batch [33/782]: Loss 0.0019465712830424309\n",
      "Training Batch [34/782]: Loss 0.0025596371851861477\n",
      "Training Batch [35/782]: Loss 0.0025808876380324364\n",
      "Training Batch [36/782]: Loss 0.0021529272198677063\n",
      "Training Batch [37/782]: Loss 0.0021244981326162815\n",
      "Training Batch [38/782]: Loss 0.0009228764683939517\n",
      "Training Batch [39/782]: Loss 0.003209358314052224\n",
      "Training Batch [40/782]: Loss 0.09863578528165817\n",
      "Training Batch [41/782]: Loss 0.0006126099033281207\n",
      "Training Batch [42/782]: Loss 0.0017517822561785579\n",
      "Training Batch [43/782]: Loss 0.03291773796081543\n",
      "Training Batch [44/782]: Loss 0.001233875984326005\n",
      "Training Batch [45/782]: Loss 0.023053012788295746\n",
      "Training Batch [46/782]: Loss 0.002507735276594758\n",
      "Training Batch [47/782]: Loss 0.0014763770159333944\n",
      "Training Batch [48/782]: Loss 0.0013133405009284616\n",
      "Training Batch [49/782]: Loss 0.0003996989980805665\n",
      "Training Batch [50/782]: Loss 0.013590088114142418\n",
      "Training Batch [51/782]: Loss 0.009906218387186527\n",
      "Training Batch [52/782]: Loss 0.002444248413667083\n",
      "Training Batch [53/782]: Loss 0.0005179889849387109\n",
      "Training Batch [54/782]: Loss 0.0050609963946044445\n",
      "Training Batch [55/782]: Loss 0.0008099489496089518\n",
      "Training Batch [56/782]: Loss 0.004065786022692919\n",
      "Training Batch [57/782]: Loss 0.0008955182274803519\n",
      "Training Batch [58/782]: Loss 0.0006979985628277063\n",
      "Training Batch [59/782]: Loss 0.0028328325133770704\n",
      "Training Batch [60/782]: Loss 0.0003421794390305877\n",
      "Training Batch [61/782]: Loss 0.000678209587931633\n",
      "Training Batch [62/782]: Loss 0.0028732430655509233\n",
      "Training Batch [63/782]: Loss 0.0005982636357657611\n",
      "Training Batch [64/782]: Loss 0.0016008025268092752\n",
      "Training Batch [65/782]: Loss 0.01583343930542469\n",
      "Training Batch [66/782]: Loss 0.00026038126088678837\n",
      "Training Batch [67/782]: Loss 0.0004887874238193035\n",
      "Training Batch [68/782]: Loss 0.006020175758749247\n",
      "Training Batch [69/782]: Loss 0.0030430760234594345\n",
      "Training Batch [70/782]: Loss 0.002797541907057166\n",
      "Training Batch [71/782]: Loss 0.001849957974627614\n",
      "Training Batch [72/782]: Loss 0.00243000197224319\n",
      "Training Batch [73/782]: Loss 3.145279333693907e-05\n",
      "Training Batch [74/782]: Loss 0.01992439292371273\n",
      "Training Batch [75/782]: Loss 0.0034635637421160936\n",
      "Training Batch [76/782]: Loss 0.001444910652935505\n",
      "Training Batch [77/782]: Loss 0.012654787860810757\n",
      "Training Batch [78/782]: Loss 0.0025442675687372684\n",
      "Training Batch [79/782]: Loss 0.003552221693098545\n",
      "Training Batch [80/782]: Loss 0.004949421156197786\n",
      "Training Batch [81/782]: Loss 0.0007235566154122353\n",
      "Training Batch [82/782]: Loss 0.0006120637408457696\n",
      "Training Batch [83/782]: Loss 0.0005518177058547735\n",
      "Training Batch [84/782]: Loss 0.0006922940956428647\n",
      "Training Batch [85/782]: Loss 0.018249401822686195\n",
      "Training Batch [86/782]: Loss 0.000582763459533453\n",
      "Training Batch [87/782]: Loss 0.028693828731775284\n",
      "Training Batch [88/782]: Loss 0.0003104796924162656\n",
      "Training Batch [89/782]: Loss 0.007463423535227776\n",
      "Training Batch [90/782]: Loss 0.0027639460749924183\n",
      "Training Batch [91/782]: Loss 0.0016490309499204159\n",
      "Training Batch [92/782]: Loss 0.004586430732160807\n",
      "Training Batch [93/782]: Loss 0.0020811946596950293\n",
      "Training Batch [94/782]: Loss 0.008341669104993343\n",
      "Training Batch [95/782]: Loss 0.011642000637948513\n",
      "Training Batch [96/782]: Loss 0.004237563349306583\n",
      "Training Batch [97/782]: Loss 0.001161468680948019\n",
      "Training Batch [98/782]: Loss 0.0007519657956436276\n",
      "Training Batch [99/782]: Loss 0.0039396509528160095\n",
      "Training Batch [100/782]: Loss 0.00455824751406908\n",
      "Training Batch [101/782]: Loss 0.03048604354262352\n",
      "Training Batch [102/782]: Loss 0.0007973471074365079\n",
      "Training Batch [103/782]: Loss 0.0006191577413119376\n",
      "Training Batch [104/782]: Loss 0.0006729344604536891\n",
      "Training Batch [105/782]: Loss 0.00615452928468585\n",
      "Training Batch [106/782]: Loss 0.002644785912707448\n",
      "Training Batch [107/782]: Loss 0.0007154342602007091\n",
      "Training Batch [108/782]: Loss 0.00026768576935864985\n",
      "Training Batch [109/782]: Loss 0.0005199554725550115\n",
      "Training Batch [110/782]: Loss 0.004194935783743858\n",
      "Training Batch [111/782]: Loss 0.0009841606952250004\n",
      "Training Batch [112/782]: Loss 0.0003129960969090462\n",
      "Training Batch [113/782]: Loss 0.0019640778191387653\n",
      "Training Batch [114/782]: Loss 0.004407621454447508\n",
      "Training Batch [115/782]: Loss 0.00048511047498323023\n",
      "Training Batch [116/782]: Loss 0.025397248566150665\n",
      "Training Batch [117/782]: Loss 0.00866968184709549\n",
      "Training Batch [118/782]: Loss 0.0016334501560777426\n",
      "Training Batch [119/782]: Loss 0.0010349230142310262\n",
      "Training Batch [120/782]: Loss 0.02143881842494011\n",
      "Training Batch [121/782]: Loss 0.010034551844000816\n",
      "Training Batch [122/782]: Loss 0.028635798022150993\n",
      "Training Batch [123/782]: Loss 0.009202178567647934\n",
      "Training Batch [124/782]: Loss 0.0025587817654013634\n",
      "Training Batch [125/782]: Loss 0.004862850997596979\n",
      "Training Batch [126/782]: Loss 0.0014242836041375995\n",
      "Training Batch [127/782]: Loss 0.0015923874452710152\n",
      "Training Batch [128/782]: Loss 0.003525399137288332\n",
      "Training Batch [129/782]: Loss 0.00530100055038929\n",
      "Training Batch [130/782]: Loss 0.0035116486251354218\n",
      "Training Batch [131/782]: Loss 0.0005017062067054212\n",
      "Training Batch [132/782]: Loss 0.012564168311655521\n",
      "Training Batch [133/782]: Loss 0.009002603590488434\n",
      "Training Batch [134/782]: Loss 0.010722227394580841\n",
      "Training Batch [135/782]: Loss 0.0008685613283887506\n",
      "Training Batch [136/782]: Loss 0.003545760875567794\n",
      "Training Batch [137/782]: Loss 0.00309233320876956\n",
      "Training Batch [138/782]: Loss 0.000252785423072055\n",
      "Training Batch [139/782]: Loss 0.019183751195669174\n",
      "Training Batch [140/782]: Loss 0.012469474226236343\n",
      "Training Batch [141/782]: Loss 0.008049377240240574\n",
      "Training Batch [142/782]: Loss 0.0013254068326205015\n",
      "Training Batch [143/782]: Loss 0.0014604857424274087\n",
      "Training Batch [144/782]: Loss 0.00025696540251374245\n",
      "Training Batch [145/782]: Loss 0.0013162947725504637\n",
      "Training Batch [146/782]: Loss 0.0035027042031288147\n",
      "Training Batch [147/782]: Loss 0.00016217102529481053\n",
      "Training Batch [148/782]: Loss 0.001466744695790112\n",
      "Training Batch [149/782]: Loss 0.0004032300494145602\n",
      "Training Batch [150/782]: Loss 0.0010633404599502683\n",
      "Training Batch [151/782]: Loss 0.0013760800939053297\n",
      "Training Batch [152/782]: Loss 0.00942275207489729\n",
      "Training Batch [153/782]: Loss 0.001020609401166439\n",
      "Training Batch [154/782]: Loss 0.0002837707579601556\n",
      "Training Batch [155/782]: Loss 0.012410322204232216\n",
      "Training Batch [156/782]: Loss 0.0030572768300771713\n",
      "Training Batch [157/782]: Loss 0.0012948318617418408\n",
      "Training Batch [158/782]: Loss 0.002677398733794689\n",
      "Training Batch [159/782]: Loss 0.0001217374810948968\n",
      "Training Batch [160/782]: Loss 0.00037300438270904124\n",
      "Training Batch [161/782]: Loss 0.00033300233189947903\n",
      "Training Batch [162/782]: Loss 0.0002778299676720053\n",
      "Training Batch [163/782]: Loss 0.0010062613291665912\n",
      "Training Batch [164/782]: Loss 0.0078093428164720535\n",
      "Training Batch [165/782]: Loss 0.0009871722431853414\n",
      "Training Batch [166/782]: Loss 0.0009588415850885212\n",
      "Training Batch [167/782]: Loss 0.003827192122116685\n",
      "Training Batch [168/782]: Loss 0.007270087953656912\n",
      "Training Batch [169/782]: Loss 0.0002921386039815843\n",
      "Training Batch [170/782]: Loss 0.0014918474480509758\n",
      "Training Batch [171/782]: Loss 0.0030414776410907507\n",
      "Training Batch [172/782]: Loss 0.0010115770855918527\n",
      "Training Batch [173/782]: Loss 0.00018306175479665399\n",
      "Training Batch [174/782]: Loss 0.0040099481120705605\n",
      "Training Batch [175/782]: Loss 0.0010355152189731598\n",
      "Training Batch [176/782]: Loss 0.00020728909294120967\n",
      "Training Batch [177/782]: Loss 0.0009305328130722046\n",
      "Training Batch [178/782]: Loss 0.004571196157485247\n",
      "Training Batch [179/782]: Loss 0.0036120214499533176\n",
      "Training Batch [180/782]: Loss 0.005897901952266693\n",
      "Training Batch [181/782]: Loss 0.0004753924149554223\n",
      "Training Batch [182/782]: Loss 0.0008424851694144309\n",
      "Training Batch [183/782]: Loss 0.0006652438314631581\n",
      "Training Batch [184/782]: Loss 0.011178188025951385\n",
      "Training Batch [185/782]: Loss 0.0007420411566272378\n",
      "Training Batch [186/782]: Loss 0.0036272816359996796\n",
      "Training Batch [187/782]: Loss 0.002741199918091297\n",
      "Training Batch [188/782]: Loss 0.0004805733042303473\n",
      "Training Batch [189/782]: Loss 0.0013588162837550044\n",
      "Training Batch [190/782]: Loss 0.00038578128442168236\n",
      "Training Batch [191/782]: Loss 0.0011582229053601623\n",
      "Training Batch [192/782]: Loss 0.0012321879621595144\n",
      "Training Batch [193/782]: Loss 0.00032625457970425487\n",
      "Training Batch [194/782]: Loss 0.001150523661635816\n",
      "Training Batch [195/782]: Loss 0.0018568939995020628\n",
      "Training Batch [196/782]: Loss 0.0003613197186496109\n",
      "Training Batch [197/782]: Loss 0.0006667035631835461\n",
      "Training Batch [198/782]: Loss 0.00477966945618391\n",
      "Training Batch [199/782]: Loss 3.9642669435124844e-05\n",
      "Training Batch [200/782]: Loss 0.00024657792528159916\n",
      "Training Batch [201/782]: Loss 0.00026256529963575304\n",
      "Training Batch [202/782]: Loss 0.002617162186652422\n",
      "Training Batch [203/782]: Loss 0.00042450104956515133\n",
      "Training Batch [204/782]: Loss 0.0037955858279019594\n",
      "Training Batch [205/782]: Loss 0.00634378707036376\n",
      "Training Batch [206/782]: Loss 0.0010444691870361567\n",
      "Training Batch [207/782]: Loss 0.00024362145632039756\n",
      "Training Batch [208/782]: Loss 0.0021519153378903866\n",
      "Training Batch [209/782]: Loss 0.00044795035501010716\n",
      "Training Batch [210/782]: Loss 0.0030429589096456766\n",
      "Training Batch [211/782]: Loss 0.0003853798261843622\n",
      "Training Batch [212/782]: Loss 0.0005232972325757146\n",
      "Training Batch [213/782]: Loss 0.002485196804627776\n",
      "Training Batch [214/782]: Loss 0.0009918574942275882\n",
      "Training Batch [215/782]: Loss 0.0013095048489049077\n",
      "Training Batch [216/782]: Loss 9.70016626524739e-05\n",
      "Training Batch [217/782]: Loss 0.01948556676506996\n",
      "Training Batch [218/782]: Loss 0.0028345673345029354\n",
      "Training Batch [219/782]: Loss 0.00036727741826325655\n",
      "Training Batch [220/782]: Loss 0.0017991476925089955\n",
      "Training Batch [221/782]: Loss 0.00025769456988200545\n",
      "Training Batch [222/782]: Loss 0.009110121056437492\n",
      "Training Batch [223/782]: Loss 0.030054505914449692\n",
      "Training Batch [224/782]: Loss 0.0003396426618564874\n",
      "Training Batch [225/782]: Loss 0.002867752918973565\n",
      "Training Batch [226/782]: Loss 0.0005124153685756028\n",
      "Training Batch [227/782]: Loss 0.0005962851573713124\n",
      "Training Batch [228/782]: Loss 0.0008007001015357673\n",
      "Training Batch [229/782]: Loss 0.0003628192062024027\n",
      "Training Batch [230/782]: Loss 0.0019452638225629926\n",
      "Training Batch [231/782]: Loss 0.005170801654458046\n",
      "Training Batch [232/782]: Loss 0.001114240032620728\n",
      "Training Batch [233/782]: Loss 0.0044104368425905704\n",
      "Training Batch [234/782]: Loss 0.0011230310192331672\n",
      "Training Batch [235/782]: Loss 0.003539047669619322\n",
      "Training Batch [236/782]: Loss 0.001659612637013197\n",
      "Training Batch [237/782]: Loss 0.0012568087549880147\n",
      "Training Batch [238/782]: Loss 0.00048626825446262956\n",
      "Training Batch [239/782]: Loss 0.025227561593055725\n",
      "Training Batch [240/782]: Loss 0.0006181497592478991\n",
      "Training Batch [241/782]: Loss 0.0005084301810711622\n",
      "Training Batch [242/782]: Loss 0.0014435704797506332\n",
      "Training Batch [243/782]: Loss 0.00015476478438358754\n",
      "Training Batch [244/782]: Loss 0.0011712891282513738\n",
      "Training Batch [245/782]: Loss 0.0001106694689951837\n",
      "Training Batch [246/782]: Loss 0.0010695894015952945\n",
      "Training Batch [247/782]: Loss 0.0004626719164662063\n",
      "Training Batch [248/782]: Loss 0.02872552163898945\n",
      "Training Batch [249/782]: Loss 0.0002448372251819819\n",
      "Training Batch [250/782]: Loss 0.00012161619088146836\n",
      "Training Batch [251/782]: Loss 0.012026638723909855\n",
      "Training Batch [252/782]: Loss 0.0006677312776446342\n",
      "Training Batch [253/782]: Loss 0.0004028024268336594\n",
      "Training Batch [254/782]: Loss 0.003177132923156023\n",
      "Training Batch [255/782]: Loss 0.012428243644535542\n",
      "Training Batch [256/782]: Loss 0.00018813835049513727\n",
      "Training Batch [257/782]: Loss 0.0011781193315982819\n",
      "Training Batch [258/782]: Loss 0.0010758418356999755\n",
      "Training Batch [259/782]: Loss 0.0010398869635537267\n",
      "Training Batch [260/782]: Loss 0.006014347542077303\n",
      "Training Batch [261/782]: Loss 0.003887590952217579\n",
      "Training Batch [262/782]: Loss 0.00034117233008146286\n",
      "Training Batch [263/782]: Loss 0.0010425818618386984\n",
      "Training Batch [264/782]: Loss 0.0015310755698010325\n",
      "Training Batch [265/782]: Loss 0.0017882166430354118\n",
      "Training Batch [266/782]: Loss 0.0020713817793875933\n",
      "Training Batch [267/782]: Loss 0.0016098496271297336\n",
      "Training Batch [268/782]: Loss 0.00032052636379376054\n",
      "Training Batch [269/782]: Loss 0.001219861675053835\n",
      "Training Batch [270/782]: Loss 0.0004281004366930574\n",
      "Training Batch [271/782]: Loss 0.0001545475679449737\n",
      "Training Batch [272/782]: Loss 0.0005456566577777267\n",
      "Training Batch [273/782]: Loss 0.006792588625103235\n",
      "Training Batch [274/782]: Loss 0.0038980620447546244\n",
      "Training Batch [275/782]: Loss 0.0005871059256605804\n",
      "Training Batch [276/782]: Loss 0.0005152871599420905\n",
      "Training Batch [277/782]: Loss 0.003243436338379979\n",
      "Training Batch [278/782]: Loss 0.0006305444403551519\n",
      "Training Batch [279/782]: Loss 0.0058414931409060955\n",
      "Training Batch [280/782]: Loss 0.0025950013659894466\n",
      "Training Batch [281/782]: Loss 0.0015392652712762356\n",
      "Training Batch [282/782]: Loss 0.00014856006600894034\n",
      "Training Batch [283/782]: Loss 0.0015770606696605682\n",
      "Training Batch [284/782]: Loss 0.02648858167231083\n",
      "Training Batch [285/782]: Loss 0.0020354618318378925\n",
      "Training Batch [286/782]: Loss 0.0008962947176769376\n",
      "Training Batch [287/782]: Loss 0.0003967777010984719\n",
      "Training Batch [288/782]: Loss 0.002178689930588007\n",
      "Training Batch [289/782]: Loss 0.000735450885258615\n",
      "Training Batch [290/782]: Loss 0.0006369097391143441\n",
      "Training Batch [291/782]: Loss 0.0015248815761879086\n",
      "Training Batch [292/782]: Loss 0.00022097716282587498\n",
      "Training Batch [293/782]: Loss 0.0026810066774487495\n",
      "Training Batch [294/782]: Loss 0.00036339342477731407\n",
      "Training Batch [295/782]: Loss 0.00032326957443729043\n",
      "Training Batch [296/782]: Loss 0.00048402074025943875\n",
      "Training Batch [297/782]: Loss 0.0002192498359363526\n",
      "Training Batch [298/782]: Loss 0.020602086558938026\n",
      "Training Batch [299/782]: Loss 0.009998584166169167\n",
      "Training Batch [300/782]: Loss 0.0020397561602294445\n",
      "Training Batch [301/782]: Loss 0.015746496617794037\n",
      "Training Batch [302/782]: Loss 0.00015091706882230937\n",
      "Training Batch [303/782]: Loss 0.0013845369685441256\n",
      "Training Batch [304/782]: Loss 0.009819735772907734\n",
      "Training Batch [305/782]: Loss 0.0012043253518640995\n",
      "Training Batch [306/782]: Loss 0.007484359201043844\n",
      "Training Batch [307/782]: Loss 0.0001285147445742041\n",
      "Training Batch [308/782]: Loss 0.002627517329528928\n",
      "Training Batch [309/782]: Loss 0.0016524550737813115\n",
      "Training Batch [310/782]: Loss 0.0014048210578039289\n",
      "Training Batch [311/782]: Loss 0.012293254025280476\n",
      "Training Batch [312/782]: Loss 0.0015429984778165817\n",
      "Training Batch [313/782]: Loss 0.02048344537615776\n",
      "Training Batch [314/782]: Loss 0.0016299330163747072\n",
      "Training Batch [315/782]: Loss 0.0006429018685594201\n",
      "Training Batch [316/782]: Loss 0.0007665478042326868\n",
      "Training Batch [317/782]: Loss 0.0005361292278394103\n",
      "Training Batch [318/782]: Loss 0.00026966072618961334\n",
      "Training Batch [319/782]: Loss 0.0002857455110643059\n",
      "Training Batch [320/782]: Loss 0.006843558046966791\n",
      "Training Batch [321/782]: Loss 0.0006722182151861489\n",
      "Training Batch [322/782]: Loss 0.01546439714729786\n",
      "Training Batch [323/782]: Loss 0.00017059955280274153\n",
      "Training Batch [324/782]: Loss 0.0008573980303481221\n",
      "Training Batch [325/782]: Loss 0.0014025685377418995\n",
      "Training Batch [326/782]: Loss 0.0032046795822679996\n",
      "Training Batch [327/782]: Loss 0.01392468623816967\n",
      "Training Batch [328/782]: Loss 0.0007825844804756343\n",
      "Training Batch [329/782]: Loss 0.00019775905821006745\n",
      "Training Batch [330/782]: Loss 0.0023816642351448536\n",
      "Training Batch [331/782]: Loss 0.001173467026092112\n",
      "Training Batch [332/782]: Loss 0.0033452522475272417\n",
      "Training Batch [333/782]: Loss 0.022545667365193367\n",
      "Training Batch [334/782]: Loss 0.0005163667956367135\n",
      "Training Batch [335/782]: Loss 0.0016535629983991385\n",
      "Training Batch [336/782]: Loss 0.001274702837690711\n",
      "Training Batch [337/782]: Loss 0.0002812746970448643\n",
      "Training Batch [338/782]: Loss 0.02992148883640766\n",
      "Training Batch [339/782]: Loss 0.013832570984959602\n",
      "Training Batch [340/782]: Loss 0.0011951862834393978\n",
      "Training Batch [341/782]: Loss 0.04188944399356842\n",
      "Training Batch [342/782]: Loss 0.0033460205886512995\n",
      "Training Batch [343/782]: Loss 0.0048132725059986115\n",
      "Training Batch [344/782]: Loss 0.00974411889910698\n",
      "Training Batch [345/782]: Loss 0.0027893218211829662\n",
      "Training Batch [346/782]: Loss 0.0036014392971992493\n",
      "Training Batch [347/782]: Loss 0.01059664785861969\n",
      "Training Batch [348/782]: Loss 0.0002013509365497157\n",
      "Training Batch [349/782]: Loss 0.02069302648305893\n",
      "Training Batch [350/782]: Loss 0.00046393985394388437\n",
      "Training Batch [351/782]: Loss 0.008482350036501884\n",
      "Training Batch [352/782]: Loss 0.008160416968166828\n",
      "Training Batch [353/782]: Loss 0.017128093168139458\n",
      "Training Batch [354/782]: Loss 0.0032005372922867537\n",
      "Training Batch [355/782]: Loss 0.0014339153422042727\n",
      "Training Batch [356/782]: Loss 0.0010165955172851682\n",
      "Training Batch [357/782]: Loss 0.0010111998999491334\n",
      "Training Batch [358/782]: Loss 0.0006050026277080178\n",
      "Training Batch [359/782]: Loss 0.001518477569334209\n",
      "Training Batch [360/782]: Loss 0.007566964253783226\n",
      "Training Batch [361/782]: Loss 0.01983453705906868\n",
      "Training Batch [362/782]: Loss 0.009357829578220844\n",
      "Training Batch [363/782]: Loss 0.0015734027838334441\n",
      "Training Batch [364/782]: Loss 0.12800441682338715\n",
      "Training Batch [365/782]: Loss 0.003548301989212632\n",
      "Training Batch [366/782]: Loss 0.008545956574380398\n",
      "Training Batch [367/782]: Loss 0.00015574954159092158\n",
      "Training Batch [368/782]: Loss 0.019590068608522415\n",
      "Training Batch [369/782]: Loss 0.005984879098832607\n",
      "Training Batch [370/782]: Loss 0.004778959788382053\n",
      "Training Batch [371/782]: Loss 0.002671406604349613\n",
      "Training Batch [372/782]: Loss 0.012710990384221077\n",
      "Training Batch [373/782]: Loss 0.0026921394746750593\n",
      "Training Batch [374/782]: Loss 0.0003745559661183506\n",
      "Training Batch [375/782]: Loss 0.0035972842015326023\n",
      "Training Batch [376/782]: Loss 0.0076106032356619835\n",
      "Training Batch [377/782]: Loss 0.11428939551115036\n",
      "Training Batch [378/782]: Loss 0.022469796240329742\n",
      "Training Batch [379/782]: Loss 0.07098734378814697\n",
      "Training Batch [380/782]: Loss 0.00039691163692623377\n",
      "Training Batch [381/782]: Loss 0.00024496077094227076\n",
      "Training Batch [382/782]: Loss 0.007577067241072655\n",
      "Training Batch [383/782]: Loss 0.0016443931963294744\n",
      "Training Batch [384/782]: Loss 0.0001695331884548068\n",
      "Training Batch [385/782]: Loss 0.0025333997327834368\n",
      "Training Batch [386/782]: Loss 0.0010401637991890311\n",
      "Training Batch [387/782]: Loss 0.0009428439661860466\n",
      "Training Batch [388/782]: Loss 0.0004947032430209219\n",
      "Training Batch [389/782]: Loss 0.013938561081886292\n",
      "Training Batch [390/782]: Loss 0.0008584036841057241\n",
      "Training Batch [391/782]: Loss 0.0002123373415088281\n",
      "Training Batch [392/782]: Loss 0.023917777463793755\n",
      "Training Batch [393/782]: Loss 0.000334994459990412\n",
      "Training Batch [394/782]: Loss 0.027220522984862328\n",
      "Training Batch [395/782]: Loss 0.001409181160852313\n",
      "Training Batch [396/782]: Loss 0.02078079804778099\n",
      "Training Batch [397/782]: Loss 0.0007061819196678698\n",
      "Training Batch [398/782]: Loss 0.0011000470258295536\n",
      "Training Batch [399/782]: Loss 0.006261730566620827\n",
      "Training Batch [400/782]: Loss 0.001604879042133689\n",
      "Training Batch [401/782]: Loss 0.014101757667958736\n",
      "Training Batch [402/782]: Loss 0.0002519850095268339\n",
      "Training Batch [403/782]: Loss 0.0037530178669840097\n",
      "Training Batch [404/782]: Loss 0.0029604295268654823\n",
      "Training Batch [405/782]: Loss 0.0102688604965806\n",
      "Training Batch [406/782]: Loss 0.0013096656184643507\n",
      "Training Batch [407/782]: Loss 0.0005190709489397705\n",
      "Training Batch [408/782]: Loss 0.00041834512376226485\n",
      "Training Batch [409/782]: Loss 0.0104761878028512\n",
      "Training Batch [410/782]: Loss 0.0010333055397495627\n",
      "Training Batch [411/782]: Loss 0.0009494246332906187\n",
      "Training Batch [412/782]: Loss 0.051649369299411774\n",
      "Training Batch [413/782]: Loss 0.0369454100728035\n",
      "Training Batch [414/782]: Loss 0.006880110129714012\n",
      "Training Batch [415/782]: Loss 0.0026174511294811964\n",
      "Training Batch [416/782]: Loss 0.003845752216875553\n",
      "Training Batch [417/782]: Loss 0.0008567192708142102\n",
      "Training Batch [418/782]: Loss 0.0014666200149804354\n",
      "Training Batch [419/782]: Loss 0.0009064854821190238\n",
      "Training Batch [420/782]: Loss 0.03395523875951767\n",
      "Training Batch [421/782]: Loss 0.03378636762499809\n",
      "Training Batch [422/782]: Loss 0.0009448524797335267\n",
      "Training Batch [423/782]: Loss 0.02638217806816101\n",
      "Training Batch [424/782]: Loss 0.017156464979052544\n",
      "Training Batch [425/782]: Loss 0.008029461838304996\n",
      "Training Batch [426/782]: Loss 0.029087817296385765\n",
      "Training Batch [427/782]: Loss 0.0016038737958297133\n",
      "Training Batch [428/782]: Loss 0.0038033481687307358\n",
      "Training Batch [429/782]: Loss 0.021480783820152283\n",
      "Training Batch [430/782]: Loss 0.005747963208705187\n",
      "Training Batch [431/782]: Loss 0.0005705587100237608\n",
      "Training Batch [432/782]: Loss 0.0022283107973635197\n",
      "Training Batch [433/782]: Loss 0.00010987686255248263\n",
      "Training Batch [434/782]: Loss 0.0033755965996533632\n",
      "Training Batch [435/782]: Loss 0.0061990367248654366\n",
      "Training Batch [436/782]: Loss 0.005778546445071697\n",
      "Training Batch [437/782]: Loss 0.020388038828969002\n",
      "Training Batch [438/782]: Loss 0.0014921316178515553\n",
      "Training Batch [439/782]: Loss 0.0040833731181919575\n",
      "Training Batch [440/782]: Loss 0.004752333741635084\n",
      "Training Batch [441/782]: Loss 0.001725655165500939\n",
      "Training Batch [442/782]: Loss 0.00033655381412245333\n",
      "Training Batch [443/782]: Loss 0.002092266920953989\n",
      "Training Batch [444/782]: Loss 0.00202523497864604\n",
      "Training Batch [445/782]: Loss 0.000482923467643559\n",
      "Training Batch [446/782]: Loss 0.0036256499588489532\n",
      "Training Batch [447/782]: Loss 0.01758798398077488\n",
      "Training Batch [448/782]: Loss 0.008538754656910896\n",
      "Training Batch [449/782]: Loss 0.0008367255213670433\n",
      "Training Batch [450/782]: Loss 0.035034891217947006\n",
      "Training Batch [451/782]: Loss 0.0036191404797136784\n",
      "Training Batch [452/782]: Loss 0.001501160440966487\n",
      "Training Batch [453/782]: Loss 0.00875985436141491\n",
      "Training Batch [454/782]: Loss 0.01756545528769493\n",
      "Training Batch [455/782]: Loss 0.06141502410173416\n",
      "Training Batch [456/782]: Loss 0.003064756514504552\n",
      "Training Batch [457/782]: Loss 0.000594574143178761\n",
      "Training Batch [458/782]: Loss 0.007796388119459152\n",
      "Training Batch [459/782]: Loss 0.0006994369905441999\n",
      "Training Batch [460/782]: Loss 0.010141683742403984\n",
      "Training Batch [461/782]: Loss 0.008738096803426743\n",
      "Training Batch [462/782]: Loss 0.0007985344855114818\n",
      "Training Batch [463/782]: Loss 0.09607324004173279\n",
      "Training Batch [464/782]: Loss 0.05678325146436691\n",
      "Training Batch [465/782]: Loss 0.0017690405948087573\n",
      "Training Batch [466/782]: Loss 0.02092173881828785\n",
      "Training Batch [467/782]: Loss 0.0019172641914337873\n",
      "Training Batch [468/782]: Loss 0.0051990835927426815\n",
      "Training Batch [469/782]: Loss 0.08810146152973175\n",
      "Training Batch [470/782]: Loss 0.04528861865401268\n",
      "Training Batch [471/782]: Loss 0.006920223124325275\n",
      "Training Batch [472/782]: Loss 0.016322815790772438\n",
      "Training Batch [473/782]: Loss 0.02988019771873951\n",
      "Training Batch [474/782]: Loss 0.004898694343864918\n",
      "Training Batch [475/782]: Loss 0.00039453941280953586\n",
      "Training Batch [476/782]: Loss 0.0005277462769299746\n",
      "Training Batch [477/782]: Loss 0.0010708061745390296\n",
      "Training Batch [478/782]: Loss 0.0006067170179449022\n",
      "Training Batch [479/782]: Loss 0.00036078220000490546\n",
      "Training Batch [480/782]: Loss 0.0005887933657504618\n",
      "Training Batch [481/782]: Loss 0.0006036160630173981\n",
      "Training Batch [482/782]: Loss 0.001125516602769494\n",
      "Training Batch [483/782]: Loss 0.007192488759756088\n",
      "Training Batch [484/782]: Loss 0.002840481698513031\n",
      "Training Batch [485/782]: Loss 0.002725786529481411\n",
      "Training Batch [486/782]: Loss 0.002627079840749502\n",
      "Training Batch [487/782]: Loss 0.012901303358376026\n",
      "Training Batch [488/782]: Loss 0.0015266030095517635\n",
      "Training Batch [489/782]: Loss 0.000584188848733902\n",
      "Training Batch [490/782]: Loss 0.004029996693134308\n",
      "Training Batch [491/782]: Loss 0.0016249069012701511\n",
      "Training Batch [492/782]: Loss 0.0037243382539600134\n",
      "Training Batch [493/782]: Loss 0.0003558597818482667\n",
      "Training Batch [494/782]: Loss 0.01050054281949997\n",
      "Training Batch [495/782]: Loss 0.0022094082087278366\n",
      "Training Batch [496/782]: Loss 0.0009647931437939405\n",
      "Training Batch [497/782]: Loss 0.014736657030880451\n",
      "Training Batch [498/782]: Loss 0.0019138548523187637\n",
      "Training Batch [499/782]: Loss 0.0037336365785449743\n",
      "Training Batch [500/782]: Loss 0.0057365563698112965\n",
      "Training Batch [501/782]: Loss 0.004655748605728149\n",
      "Training Batch [502/782]: Loss 0.0013296704273670912\n",
      "Training Batch [503/782]: Loss 0.001211143797263503\n",
      "Training Batch [504/782]: Loss 0.0018127012299373746\n",
      "Training Batch [505/782]: Loss 0.033937446773052216\n",
      "Training Batch [506/782]: Loss 0.002378396922722459\n",
      "Training Batch [507/782]: Loss 0.012044180184602737\n",
      "Training Batch [508/782]: Loss 0.0005990864592604339\n",
      "Training Batch [509/782]: Loss 0.005738840904086828\n",
      "Training Batch [510/782]: Loss 0.006368611473590136\n",
      "Training Batch [511/782]: Loss 0.00021170650143176317\n",
      "Training Batch [512/782]: Loss 0.0019014765275642276\n",
      "Training Batch [513/782]: Loss 0.005136414896696806\n",
      "Training Batch [514/782]: Loss 0.018967028707265854\n",
      "Training Batch [515/782]: Loss 0.028501875698566437\n",
      "Training Batch [516/782]: Loss 0.034231312572956085\n",
      "Training Batch [517/782]: Loss 0.0008887997246347368\n",
      "Training Batch [518/782]: Loss 0.010893948376178741\n",
      "Training Batch [519/782]: Loss 0.02358092926442623\n",
      "Training Batch [520/782]: Loss 0.0027941286098212004\n",
      "Training Batch [521/782]: Loss 0.08013647049665451\n",
      "Training Batch [522/782]: Loss 0.0019632186740636826\n",
      "Training Batch [523/782]: Loss 0.0047186450101435184\n",
      "Training Batch [524/782]: Loss 0.01239592395722866\n",
      "Training Batch [525/782]: Loss 0.008544769138097763\n",
      "Training Batch [526/782]: Loss 0.015042646788060665\n",
      "Training Batch [527/782]: Loss 0.008601688779890537\n",
      "Training Batch [528/782]: Loss 9.694686741568148e-05\n",
      "Training Batch [529/782]: Loss 0.0007014505099505186\n",
      "Training Batch [530/782]: Loss 0.0005866624996997416\n",
      "Training Batch [531/782]: Loss 0.0030378245282918215\n",
      "Training Batch [532/782]: Loss 0.0009303902625106275\n",
      "Training Batch [533/782]: Loss 0.003054536646232009\n",
      "Training Batch [534/782]: Loss 0.013400670140981674\n",
      "Training Batch [535/782]: Loss 0.0006360546685755253\n",
      "Training Batch [536/782]: Loss 0.007800612132996321\n",
      "Training Batch [537/782]: Loss 0.01910191960632801\n",
      "Training Batch [538/782]: Loss 0.00986156053841114\n",
      "Training Batch [539/782]: Loss 0.0025627899449318647\n",
      "Training Batch [540/782]: Loss 0.00044624388101510704\n",
      "Training Batch [541/782]: Loss 0.0035815411247313023\n",
      "Training Batch [542/782]: Loss 0.00044831764535047114\n",
      "Training Batch [543/782]: Loss 0.011770685203373432\n",
      "Training Batch [544/782]: Loss 0.002836956176906824\n",
      "Training Batch [545/782]: Loss 0.007731536868959665\n",
      "Training Batch [546/782]: Loss 0.001467726076953113\n",
      "Training Batch [547/782]: Loss 0.005889588501304388\n",
      "Training Batch [548/782]: Loss 0.0004920818610116839\n",
      "Training Batch [549/782]: Loss 0.01022456306964159\n",
      "Training Batch [550/782]: Loss 0.013178975321352482\n",
      "Training Batch [551/782]: Loss 0.0007270501810126007\n",
      "Training Batch [552/782]: Loss 0.0008823841926641762\n",
      "Training Batch [553/782]: Loss 0.0006000859430059791\n",
      "Training Batch [554/782]: Loss 0.006686961743980646\n",
      "Training Batch [555/782]: Loss 0.0024973133113235235\n",
      "Training Batch [556/782]: Loss 0.0024238519836217165\n",
      "Training Batch [557/782]: Loss 0.0019181249663233757\n",
      "Training Batch [558/782]: Loss 0.0027162323240190744\n",
      "Training Batch [559/782]: Loss 0.008137153461575508\n",
      "Training Batch [560/782]: Loss 0.0160211268812418\n",
      "Training Batch [561/782]: Loss 0.0014072754420340061\n",
      "Training Batch [562/782]: Loss 0.001582274679094553\n",
      "Training Batch [563/782]: Loss 0.0010725780157372355\n",
      "Training Batch [564/782]: Loss 0.007937563583254814\n",
      "Training Batch [565/782]: Loss 0.0006103586056269705\n",
      "Training Batch [566/782]: Loss 0.00019227591110393405\n",
      "Training Batch [567/782]: Loss 0.011853399686515331\n",
      "Training Batch [568/782]: Loss 0.002035655314102769\n",
      "Training Batch [569/782]: Loss 0.045467276126146317\n",
      "Training Batch [570/782]: Loss 0.0026394082233309746\n",
      "Training Batch [571/782]: Loss 0.000790450896602124\n",
      "Training Batch [572/782]: Loss 0.000704364210832864\n",
      "Training Batch [573/782]: Loss 0.03092549741268158\n",
      "Training Batch [574/782]: Loss 0.0007978743524290621\n",
      "Training Batch [575/782]: Loss 0.0004474894667509943\n",
      "Training Batch [576/782]: Loss 0.0005178273422643542\n",
      "Training Batch [577/782]: Loss 0.001442396780475974\n",
      "Training Batch [578/782]: Loss 0.0011932500638067722\n",
      "Training Batch [579/782]: Loss 0.00048138966667465866\n",
      "Training Batch [580/782]: Loss 0.014521394856274128\n",
      "Training Batch [581/782]: Loss 0.0010597737273201346\n",
      "Training Batch [582/782]: Loss 0.0030647621024399996\n",
      "Training Batch [583/782]: Loss 0.0025895352009683847\n",
      "Training Batch [584/782]: Loss 0.0005761784268543124\n",
      "Training Batch [585/782]: Loss 0.002243385184556246\n",
      "Training Batch [586/782]: Loss 0.0004988560103811324\n",
      "Training Batch [587/782]: Loss 0.006113983225077391\n",
      "Training Batch [588/782]: Loss 0.008857370354235172\n",
      "Training Batch [589/782]: Loss 0.0011882936814799905\n",
      "Training Batch [590/782]: Loss 0.0009527374641038477\n",
      "Training Batch [591/782]: Loss 0.0020790903363376856\n",
      "Training Batch [592/782]: Loss 0.017220648005604744\n",
      "Training Batch [593/782]: Loss 0.0014904679264873266\n",
      "Training Batch [594/782]: Loss 0.0019743600860238075\n",
      "Training Batch [595/782]: Loss 0.0011502217967063189\n",
      "Training Batch [596/782]: Loss 0.0004373313277028501\n",
      "Training Batch [597/782]: Loss 0.0005209315568208694\n",
      "Training Batch [598/782]: Loss 0.027178173884749413\n",
      "Training Batch [599/782]: Loss 0.0013378566363826394\n",
      "Training Batch [600/782]: Loss 0.003312184941023588\n",
      "Training Batch [601/782]: Loss 0.0025331107899546623\n",
      "Training Batch [602/782]: Loss 0.007688548415899277\n",
      "Training Batch [603/782]: Loss 0.0009543959749862552\n",
      "Training Batch [604/782]: Loss 0.0004836727457586676\n",
      "Training Batch [605/782]: Loss 0.01705091819167137\n",
      "Training Batch [606/782]: Loss 0.0017576415557414293\n",
      "Training Batch [607/782]: Loss 0.024329109117388725\n",
      "Training Batch [608/782]: Loss 0.000216655942494981\n",
      "Training Batch [609/782]: Loss 0.0014465319691225886\n",
      "Training Batch [610/782]: Loss 0.0008658947772346437\n",
      "Training Batch [611/782]: Loss 0.001743906526826322\n",
      "Training Batch [612/782]: Loss 0.009647026658058167\n",
      "Training Batch [613/782]: Loss 0.0037357837427407503\n",
      "Training Batch [614/782]: Loss 0.002297708299010992\n",
      "Training Batch [615/782]: Loss 0.003089871956035495\n",
      "Training Batch [616/782]: Loss 0.008382735773921013\n",
      "Training Batch [617/782]: Loss 0.00015520279703196138\n",
      "Training Batch [618/782]: Loss 0.0028868401423096657\n",
      "Training Batch [619/782]: Loss 0.0009814348304644227\n",
      "Training Batch [620/782]: Loss 0.0007580467499792576\n",
      "Training Batch [621/782]: Loss 0.00034497943124733865\n",
      "Training Batch [622/782]: Loss 0.000463684496935457\n",
      "Training Batch [623/782]: Loss 0.004213044419884682\n",
      "Training Batch [624/782]: Loss 0.0035121070686727762\n",
      "Training Batch [625/782]: Loss 0.002396005904302001\n",
      "Training Batch [626/782]: Loss 0.0022549782879650593\n",
      "Training Batch [627/782]: Loss 0.0008954887161962688\n",
      "Training Batch [628/782]: Loss 0.000300495041301474\n",
      "Training Batch [629/782]: Loss 0.002369340043514967\n",
      "Training Batch [630/782]: Loss 0.0005362674710340798\n",
      "Training Batch [631/782]: Loss 0.02713802270591259\n",
      "Training Batch [632/782]: Loss 0.002133948029950261\n",
      "Training Batch [633/782]: Loss 0.0014095338992774487\n",
      "Training Batch [634/782]: Loss 0.0029639587737619877\n",
      "Training Batch [635/782]: Loss 0.02237049862742424\n",
      "Training Batch [636/782]: Loss 0.007311899680644274\n",
      "Training Batch [637/782]: Loss 0.017443254590034485\n",
      "Training Batch [638/782]: Loss 0.001349161728285253\n",
      "Training Batch [639/782]: Loss 0.000811008911114186\n",
      "Training Batch [640/782]: Loss 0.0014210480730980635\n",
      "Training Batch [641/782]: Loss 0.001116765197366476\n",
      "Training Batch [642/782]: Loss 0.0016538503114134073\n",
      "Training Batch [643/782]: Loss 0.0026857166085392237\n",
      "Training Batch [644/782]: Loss 0.0027631456032395363\n",
      "Training Batch [645/782]: Loss 0.0010104295797646046\n",
      "Training Batch [646/782]: Loss 0.0715213492512703\n",
      "Training Batch [647/782]: Loss 0.008240935392677784\n",
      "Training Batch [648/782]: Loss 0.0011030626483261585\n",
      "Training Batch [649/782]: Loss 0.0006799359689466655\n",
      "Training Batch [650/782]: Loss 0.0002590288931969553\n",
      "Training Batch [651/782]: Loss 0.010565953329205513\n",
      "Training Batch [652/782]: Loss 0.01905119977891445\n",
      "Training Batch [653/782]: Loss 0.04915160313248634\n",
      "Training Batch [654/782]: Loss 0.0007250587805174291\n",
      "Training Batch [655/782]: Loss 0.00160570302978158\n",
      "Training Batch [656/782]: Loss 0.00021317091886885464\n",
      "Training Batch [657/782]: Loss 0.0011679265880957246\n",
      "Training Batch [658/782]: Loss 0.007321180775761604\n",
      "Training Batch [659/782]: Loss 0.023862706497311592\n",
      "Training Batch [660/782]: Loss 0.002267225179821253\n",
      "Training Batch [661/782]: Loss 0.005298153962939978\n",
      "Training Batch [662/782]: Loss 0.023429298773407936\n",
      "Training Batch [663/782]: Loss 0.022293144837021828\n",
      "Training Batch [664/782]: Loss 0.05859381705522537\n",
      "Training Batch [665/782]: Loss 0.003367920406162739\n",
      "Training Batch [666/782]: Loss 0.05076127126812935\n",
      "Training Batch [667/782]: Loss 0.027788475155830383\n",
      "Training Batch [668/782]: Loss 0.006749078631401062\n",
      "Training Batch [669/782]: Loss 0.001994231715798378\n",
      "Training Batch [670/782]: Loss 0.0011953259818255901\n",
      "Training Batch [671/782]: Loss 0.0017941384576261044\n",
      "Training Batch [672/782]: Loss 0.0026011397130787373\n",
      "Training Batch [673/782]: Loss 0.007085967808961868\n",
      "Training Batch [674/782]: Loss 0.0010852981358766556\n",
      "Training Batch [675/782]: Loss 0.014533251523971558\n",
      "Training Batch [676/782]: Loss 0.0007604965940117836\n",
      "Training Batch [677/782]: Loss 0.0032171939965337515\n",
      "Training Batch [678/782]: Loss 0.002560109132900834\n",
      "Training Batch [679/782]: Loss 0.006349692586809397\n",
      "Training Batch [680/782]: Loss 0.05653606355190277\n",
      "Training Batch [681/782]: Loss 0.0051612211391329765\n",
      "Training Batch [682/782]: Loss 0.02455540932714939\n",
      "Training Batch [683/782]: Loss 0.004344099201261997\n",
      "Training Batch [684/782]: Loss 0.013505993410944939\n",
      "Training Batch [685/782]: Loss 0.0040480294264853\n",
      "Training Batch [686/782]: Loss 0.008377911522984505\n",
      "Training Batch [687/782]: Loss 0.02795642614364624\n",
      "Training Batch [688/782]: Loss 0.0004708793421741575\n",
      "Training Batch [689/782]: Loss 0.0011186160845682025\n",
      "Training Batch [690/782]: Loss 0.003779946593567729\n",
      "Training Batch [691/782]: Loss 0.0018846014281734824\n",
      "Training Batch [692/782]: Loss 0.0012878513662144542\n",
      "Training Batch [693/782]: Loss 0.0016787837957963347\n",
      "Training Batch [694/782]: Loss 0.012891655787825584\n",
      "Training Batch [695/782]: Loss 0.0007037094910629094\n",
      "Training Batch [696/782]: Loss 0.0018711443990468979\n",
      "Training Batch [697/782]: Loss 0.0029097176156938076\n",
      "Training Batch [698/782]: Loss 0.034023359417915344\n",
      "Training Batch [699/782]: Loss 0.038218025118112564\n",
      "Training Batch [700/782]: Loss 0.0020349801052361727\n",
      "Training Batch [701/782]: Loss 0.028132794424891472\n",
      "Training Batch [702/782]: Loss 0.0004516526241786778\n",
      "Training Batch [703/782]: Loss 0.017679773271083832\n",
      "Training Batch [704/782]: Loss 0.009406346827745438\n",
      "Training Batch [705/782]: Loss 0.0010789585066959262\n",
      "Training Batch [706/782]: Loss 0.0013425399083644152\n",
      "Training Batch [707/782]: Loss 0.000378688593627885\n",
      "Training Batch [708/782]: Loss 0.005623278673738241\n",
      "Training Batch [709/782]: Loss 0.002438697963953018\n",
      "Training Batch [710/782]: Loss 0.04706302657723427\n",
      "Training Batch [711/782]: Loss 0.002310338895767927\n",
      "Training Batch [712/782]: Loss 0.00011589600762818009\n",
      "Training Batch [713/782]: Loss 0.0036539502907544374\n",
      "Training Batch [714/782]: Loss 0.0010560175869613886\n",
      "Training Batch [715/782]: Loss 0.010342488065361977\n",
      "Training Batch [716/782]: Loss 0.003089113160967827\n",
      "Training Batch [717/782]: Loss 0.0006338194943964481\n",
      "Training Batch [718/782]: Loss 0.0018470092909410596\n",
      "Training Batch [719/782]: Loss 0.013493027538061142\n",
      "Training Batch [720/782]: Loss 0.004537946544587612\n",
      "Training Batch [721/782]: Loss 0.0007425099611282349\n",
      "Training Batch [722/782]: Loss 0.02394416555762291\n",
      "Training Batch [723/782]: Loss 0.029621554538607597\n",
      "Training Batch [724/782]: Loss 0.030358169227838516\n",
      "Training Batch [725/782]: Loss 0.0035168181639164686\n",
      "Training Batch [726/782]: Loss 0.0023623157758265734\n",
      "Training Batch [727/782]: Loss 0.00847632996737957\n",
      "Training Batch [728/782]: Loss 0.008613783866167068\n",
      "Training Batch [729/782]: Loss 0.0007693219231441617\n",
      "Training Batch [730/782]: Loss 0.0010890052653849125\n",
      "Training Batch [731/782]: Loss 0.0005798700149171054\n",
      "Training Batch [732/782]: Loss 0.018497737124562263\n",
      "Training Batch [733/782]: Loss 0.0047685932368040085\n",
      "Training Batch [734/782]: Loss 0.002309354953467846\n",
      "Training Batch [735/782]: Loss 0.0003544235078152269\n",
      "Training Batch [736/782]: Loss 0.01478610560297966\n",
      "Training Batch [737/782]: Loss 0.011547699570655823\n",
      "Training Batch [738/782]: Loss 0.038982123136520386\n",
      "Training Batch [739/782]: Loss 0.004232710227370262\n",
      "Training Batch [740/782]: Loss 0.0005315878661349416\n",
      "Training Batch [741/782]: Loss 0.009654386900365353\n",
      "Training Batch [742/782]: Loss 0.0003651927108876407\n",
      "Training Batch [743/782]: Loss 0.002353370189666748\n",
      "Training Batch [744/782]: Loss 0.01183411292731762\n",
      "Training Batch [745/782]: Loss 0.0020620187278836966\n",
      "Training Batch [746/782]: Loss 0.0002879639796447009\n",
      "Training Batch [747/782]: Loss 0.0004784957563970238\n",
      "Training Batch [748/782]: Loss 0.005677458364516497\n",
      "Training Batch [749/782]: Loss 0.0004093224124517292\n",
      "Training Batch [750/782]: Loss 0.005490386858582497\n",
      "Training Batch [751/782]: Loss 0.00014373345766216516\n",
      "Training Batch [752/782]: Loss 0.008602455258369446\n",
      "Training Batch [753/782]: Loss 0.0010302631417289376\n",
      "Training Batch [754/782]: Loss 0.0006816796376369894\n",
      "Training Batch [755/782]: Loss 0.004138871096074581\n",
      "Training Batch [756/782]: Loss 0.00023690635862294585\n",
      "Training Batch [757/782]: Loss 0.01825830154120922\n",
      "Training Batch [758/782]: Loss 0.0009703082614578307\n",
      "Training Batch [759/782]: Loss 0.0001292425877181813\n",
      "Training Batch [760/782]: Loss 0.0006771886255592108\n",
      "Training Batch [761/782]: Loss 0.10073460638523102\n",
      "Training Batch [762/782]: Loss 0.000356017000740394\n",
      "Training Batch [763/782]: Loss 0.00018442443979438394\n",
      "Training Batch [764/782]: Loss 0.006152358837425709\n",
      "Training Batch [765/782]: Loss 0.0010102832457050681\n",
      "Training Batch [766/782]: Loss 0.02293677255511284\n",
      "Training Batch [767/782]: Loss 0.03927691653370857\n",
      "Training Batch [768/782]: Loss 0.0023489331360906363\n",
      "Training Batch [769/782]: Loss 0.0002845980634447187\n",
      "Training Batch [770/782]: Loss 0.0014815522590652108\n",
      "Training Batch [771/782]: Loss 0.001556714647449553\n",
      "Training Batch [772/782]: Loss 0.000364190258551389\n",
      "Training Batch [773/782]: Loss 0.0004912717267870903\n",
      "Training Batch [774/782]: Loss 0.0021323710680007935\n",
      "Training Batch [775/782]: Loss 0.006231972016394138\n",
      "Training Batch [776/782]: Loss 0.002756618894636631\n",
      "Training Batch [777/782]: Loss 0.01397698000073433\n",
      "Training Batch [778/782]: Loss 0.0022390158846974373\n",
      "Training Batch [779/782]: Loss 0.0011747755343094468\n",
      "Training Batch [780/782]: Loss 0.00037201514351181686\n",
      "Training Batch [781/782]: Loss 0.01514849066734314\n",
      "Training Batch [782/782]: Loss 6.723316619172692e-05\n",
      "Epoch 28 - Train Loss: 0.0073\n",
      "*********  Epoch 29/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.00028605107218027115\n",
      "Training Batch [2/782]: Loss 0.001671382924541831\n",
      "Training Batch [3/782]: Loss 9.199480700772256e-05\n",
      "Training Batch [4/782]: Loss 0.0030436324886977673\n",
      "Training Batch [5/782]: Loss 0.0005812846357002854\n",
      "Training Batch [6/782]: Loss 0.00011606252519413829\n",
      "Training Batch [7/782]: Loss 0.0027613146230578423\n",
      "Training Batch [8/782]: Loss 0.01201461162418127\n",
      "Training Batch [9/782]: Loss 0.0005097264656797051\n",
      "Training Batch [10/782]: Loss 0.008664306253194809\n",
      "Training Batch [11/782]: Loss 0.0011645782506093383\n",
      "Training Batch [12/782]: Loss 0.0009938833536580205\n",
      "Training Batch [13/782]: Loss 0.006386094726622105\n",
      "Training Batch [14/782]: Loss 0.0003485472407191992\n",
      "Training Batch [15/782]: Loss 0.001765087596140802\n",
      "Training Batch [16/782]: Loss 0.00529557466506958\n",
      "Training Batch [17/782]: Loss 0.01329484861344099\n",
      "Training Batch [18/782]: Loss 0.01658148691058159\n",
      "Training Batch [19/782]: Loss 0.0018607518868520856\n",
      "Training Batch [20/782]: Loss 0.003563431790098548\n",
      "Training Batch [21/782]: Loss 0.001295390771701932\n",
      "Training Batch [22/782]: Loss 0.000588093011174351\n",
      "Training Batch [23/782]: Loss 0.004799503367394209\n",
      "Training Batch [24/782]: Loss 0.0003141248889733106\n",
      "Training Batch [25/782]: Loss 0.0006586281233467162\n",
      "Training Batch [26/782]: Loss 0.0002726821694523096\n",
      "Training Batch [27/782]: Loss 0.00040126050589606166\n",
      "Training Batch [28/782]: Loss 0.06090612709522247\n",
      "Training Batch [29/782]: Loss 0.0012032873928546906\n",
      "Training Batch [30/782]: Loss 0.0012043211609125137\n",
      "Training Batch [31/782]: Loss 0.0013052498688921332\n",
      "Training Batch [32/782]: Loss 0.00029566948069259524\n",
      "Training Batch [33/782]: Loss 0.005367935635149479\n",
      "Training Batch [34/782]: Loss 0.0007104380056262016\n",
      "Training Batch [35/782]: Loss 0.009686894714832306\n",
      "Training Batch [36/782]: Loss 0.034938517957925797\n",
      "Training Batch [37/782]: Loss 0.06956534832715988\n",
      "Training Batch [38/782]: Loss 0.00023361228522844613\n",
      "Training Batch [39/782]: Loss 0.0010218205861747265\n",
      "Training Batch [40/782]: Loss 0.0032344406936317682\n",
      "Training Batch [41/782]: Loss 0.00026041475939564407\n",
      "Training Batch [42/782]: Loss 0.0003229227149859071\n",
      "Training Batch [43/782]: Loss 0.0005485265864990652\n",
      "Training Batch [44/782]: Loss 0.0008606373448856175\n",
      "Training Batch [45/782]: Loss 0.001351966755464673\n",
      "Training Batch [46/782]: Loss 0.002386817242950201\n",
      "Training Batch [47/782]: Loss 0.0005947473109699786\n",
      "Training Batch [48/782]: Loss 0.008178490214049816\n",
      "Training Batch [49/782]: Loss 0.00411258963868022\n",
      "Training Batch [50/782]: Loss 0.0500435046851635\n",
      "Training Batch [51/782]: Loss 0.014475609175860882\n",
      "Training Batch [52/782]: Loss 0.004943927749991417\n",
      "Training Batch [53/782]: Loss 0.004020853899419308\n",
      "Training Batch [54/782]: Loss 0.005584143102169037\n",
      "Training Batch [55/782]: Loss 0.00045618010335601866\n",
      "Training Batch [56/782]: Loss 0.0004570888413581997\n",
      "Training Batch [57/782]: Loss 0.0019675481598824263\n",
      "Training Batch [58/782]: Loss 0.006293949205428362\n",
      "Training Batch [59/782]: Loss 0.011118140071630478\n",
      "Training Batch [60/782]: Loss 0.0006590200355276465\n",
      "Training Batch [61/782]: Loss 0.0013847735244780779\n",
      "Training Batch [62/782]: Loss 0.005196768324822187\n",
      "Training Batch [63/782]: Loss 0.04105036333203316\n",
      "Training Batch [64/782]: Loss 0.0010715123498812318\n",
      "Training Batch [65/782]: Loss 0.015506903640925884\n",
      "Training Batch [66/782]: Loss 0.005617326591163874\n",
      "Training Batch [67/782]: Loss 0.0006758156232535839\n",
      "Training Batch [68/782]: Loss 0.00367152551189065\n",
      "Training Batch [69/782]: Loss 0.0037003885954618454\n",
      "Training Batch [70/782]: Loss 0.00243851775303483\n",
      "Training Batch [71/782]: Loss 0.00019896921003237367\n",
      "Training Batch [72/782]: Loss 0.0006390467169694602\n",
      "Training Batch [73/782]: Loss 0.001164086745120585\n",
      "Training Batch [74/782]: Loss 0.0035920164082199335\n",
      "Training Batch [75/782]: Loss 0.0033585159108042717\n",
      "Training Batch [76/782]: Loss 0.004702645353972912\n",
      "Training Batch [77/782]: Loss 0.0017195132095366716\n",
      "Training Batch [78/782]: Loss 0.0008459024829789996\n",
      "Training Batch [79/782]: Loss 0.00275348755531013\n",
      "Training Batch [80/782]: Loss 0.004308151081204414\n",
      "Training Batch [81/782]: Loss 0.00037468658410944045\n",
      "Training Batch [82/782]: Loss 0.0004399017198011279\n",
      "Training Batch [83/782]: Loss 0.005700289271771908\n",
      "Training Batch [84/782]: Loss 0.002699184464290738\n",
      "Training Batch [85/782]: Loss 0.0016385851195082068\n",
      "Training Batch [86/782]: Loss 0.0003135359729640186\n",
      "Training Batch [87/782]: Loss 0.0007173699559643865\n",
      "Training Batch [88/782]: Loss 0.002909907139837742\n",
      "Training Batch [89/782]: Loss 0.05124800279736519\n",
      "Training Batch [90/782]: Loss 0.00032767755328677595\n",
      "Training Batch [91/782]: Loss 0.0007582883117720485\n",
      "Training Batch [92/782]: Loss 0.03035472147166729\n",
      "Training Batch [93/782]: Loss 0.0004627938033081591\n",
      "Training Batch [94/782]: Loss 0.00020657027198467404\n",
      "Training Batch [95/782]: Loss 0.001133129931986332\n",
      "Training Batch [96/782]: Loss 0.00024620944168418646\n",
      "Training Batch [97/782]: Loss 0.007581847254186869\n",
      "Training Batch [98/782]: Loss 0.005516805220395327\n",
      "Training Batch [99/782]: Loss 0.06300043314695358\n",
      "Training Batch [100/782]: Loss 0.0008520261035300791\n",
      "Training Batch [101/782]: Loss 0.000991865643300116\n",
      "Training Batch [102/782]: Loss 0.002901108469814062\n",
      "Training Batch [103/782]: Loss 0.0003785121371038258\n",
      "Training Batch [104/782]: Loss 6.286147254286334e-05\n",
      "Training Batch [105/782]: Loss 0.0025728049222379923\n",
      "Training Batch [106/782]: Loss 0.03705085813999176\n",
      "Training Batch [107/782]: Loss 0.002325595822185278\n",
      "Training Batch [108/782]: Loss 0.00015203341899905354\n",
      "Training Batch [109/782]: Loss 0.0031786789186298847\n",
      "Training Batch [110/782]: Loss 0.0005277696764096618\n",
      "Training Batch [111/782]: Loss 0.0014913837658241391\n",
      "Training Batch [112/782]: Loss 0.0009435017127543688\n",
      "Training Batch [113/782]: Loss 0.001782909850589931\n",
      "Training Batch [114/782]: Loss 0.007805062923580408\n",
      "Training Batch [115/782]: Loss 0.0026242337189614773\n",
      "Training Batch [116/782]: Loss 0.003498444566503167\n",
      "Training Batch [117/782]: Loss 0.005898613948374987\n",
      "Training Batch [118/782]: Loss 0.09238941222429276\n",
      "Training Batch [119/782]: Loss 0.10455667972564697\n",
      "Training Batch [120/782]: Loss 0.002116404240950942\n",
      "Training Batch [121/782]: Loss 0.004251929000020027\n",
      "Training Batch [122/782]: Loss 0.00032804429065436125\n",
      "Training Batch [123/782]: Loss 0.038253627717494965\n",
      "Training Batch [124/782]: Loss 0.001987713621929288\n",
      "Training Batch [125/782]: Loss 0.0002300603809999302\n",
      "Training Batch [126/782]: Loss 0.00523360725492239\n",
      "Training Batch [127/782]: Loss 0.00030791512108407915\n",
      "Training Batch [128/782]: Loss 0.0003602667711675167\n",
      "Training Batch [129/782]: Loss 0.000451207859441638\n",
      "Training Batch [130/782]: Loss 0.0018299680668860674\n",
      "Training Batch [131/782]: Loss 0.014990529045462608\n",
      "Training Batch [132/782]: Loss 0.0005264445790089667\n",
      "Training Batch [133/782]: Loss 0.0013663131976500154\n",
      "Training Batch [134/782]: Loss 0.08122972398996353\n",
      "Training Batch [135/782]: Loss 0.00038561641122214496\n",
      "Training Batch [136/782]: Loss 0.0011075431248173118\n",
      "Training Batch [137/782]: Loss 0.00042978161945939064\n",
      "Training Batch [138/782]: Loss 0.0009384700679220259\n",
      "Training Batch [139/782]: Loss 0.0014725039945915341\n",
      "Training Batch [140/782]: Loss 0.0006118635646998882\n",
      "Training Batch [141/782]: Loss 0.0016022276831790805\n",
      "Training Batch [142/782]: Loss 0.0015183145878836513\n",
      "Training Batch [143/782]: Loss 0.00131345191039145\n",
      "Training Batch [144/782]: Loss 0.001917235553264618\n",
      "Training Batch [145/782]: Loss 0.0011744053335860372\n",
      "Training Batch [146/782]: Loss 0.01219544280320406\n",
      "Training Batch [147/782]: Loss 0.0018480818253010511\n",
      "Training Batch [148/782]: Loss 0.004240448586642742\n",
      "Training Batch [149/782]: Loss 0.010411032475531101\n",
      "Training Batch [150/782]: Loss 0.007237222511321306\n",
      "Training Batch [151/782]: Loss 0.00635130237787962\n",
      "Training Batch [152/782]: Loss 0.0022543673403561115\n",
      "Training Batch [153/782]: Loss 0.0007623013225384057\n",
      "Training Batch [154/782]: Loss 0.002885931869968772\n",
      "Training Batch [155/782]: Loss 0.002302672481164336\n",
      "Training Batch [156/782]: Loss 0.10144805163145065\n",
      "Training Batch [157/782]: Loss 0.002806546399369836\n",
      "Training Batch [158/782]: Loss 0.008017688989639282\n",
      "Training Batch [159/782]: Loss 0.0035955265630036592\n",
      "Training Batch [160/782]: Loss 0.018529260531067848\n",
      "Training Batch [161/782]: Loss 0.0021141001489013433\n",
      "Training Batch [162/782]: Loss 0.0011417984496802092\n",
      "Training Batch [163/782]: Loss 0.01618482545018196\n",
      "Training Batch [164/782]: Loss 0.01715887151658535\n",
      "Training Batch [165/782]: Loss 0.0007393660489469767\n",
      "Training Batch [166/782]: Loss 0.03061784617602825\n",
      "Training Batch [167/782]: Loss 0.007998229004442692\n",
      "Training Batch [168/782]: Loss 0.0017424217658117414\n",
      "Training Batch [169/782]: Loss 0.021647455170750618\n",
      "Training Batch [170/782]: Loss 0.005556438118219376\n",
      "Training Batch [171/782]: Loss 0.0017341416096314788\n",
      "Training Batch [172/782]: Loss 0.0031645020935684443\n",
      "Training Batch [173/782]: Loss 0.0007300149882212281\n",
      "Training Batch [174/782]: Loss 0.035041000694036484\n",
      "Training Batch [175/782]: Loss 0.0032226182520389557\n",
      "Training Batch [176/782]: Loss 0.00027733686147257686\n",
      "Training Batch [177/782]: Loss 0.000574192323256284\n",
      "Training Batch [178/782]: Loss 0.005097480490803719\n",
      "Training Batch [179/782]: Loss 0.04008059576153755\n",
      "Training Batch [180/782]: Loss 0.0006492155371233821\n",
      "Training Batch [181/782]: Loss 0.021407131105661392\n",
      "Training Batch [182/782]: Loss 0.019964883103966713\n",
      "Training Batch [183/782]: Loss 0.001494762720540166\n",
      "Training Batch [184/782]: Loss 0.010326521471142769\n",
      "Training Batch [185/782]: Loss 0.0015062113525345922\n",
      "Training Batch [186/782]: Loss 0.0001916028995765373\n",
      "Training Batch [187/782]: Loss 0.021154971793293953\n",
      "Training Batch [188/782]: Loss 0.026645608246326447\n",
      "Training Batch [189/782]: Loss 0.061030495911836624\n",
      "Training Batch [190/782]: Loss 0.0012351017212495208\n",
      "Training Batch [191/782]: Loss 0.030321773141622543\n",
      "Training Batch [192/782]: Loss 0.0003809313348028809\n",
      "Training Batch [193/782]: Loss 0.0025056705344468355\n",
      "Training Batch [194/782]: Loss 0.00027719984063878655\n",
      "Training Batch [195/782]: Loss 0.0005608907667919993\n",
      "Training Batch [196/782]: Loss 0.002129306085407734\n",
      "Training Batch [197/782]: Loss 0.0009573830757290125\n",
      "Training Batch [198/782]: Loss 0.00034858856815844774\n",
      "Training Batch [199/782]: Loss 0.0006243634270504117\n",
      "Training Batch [200/782]: Loss 0.0038953216280788183\n",
      "Training Batch [201/782]: Loss 0.0012942091561853886\n",
      "Training Batch [202/782]: Loss 0.0005058092647232115\n",
      "Training Batch [203/782]: Loss 0.02057712897658348\n",
      "Training Batch [204/782]: Loss 0.027934405952692032\n",
      "Training Batch [205/782]: Loss 0.10154372453689575\n",
      "Training Batch [206/782]: Loss 0.0009236604673787951\n",
      "Training Batch [207/782]: Loss 0.0016272469656541944\n",
      "Training Batch [208/782]: Loss 0.0018959254957735538\n",
      "Training Batch [209/782]: Loss 0.001039033173583448\n",
      "Training Batch [210/782]: Loss 0.0301089808344841\n",
      "Training Batch [211/782]: Loss 0.007920646108686924\n",
      "Training Batch [212/782]: Loss 0.0012484723702073097\n",
      "Training Batch [213/782]: Loss 0.0006023294990882277\n",
      "Training Batch [214/782]: Loss 0.005453974008560181\n",
      "Training Batch [215/782]: Loss 0.0039258794859051704\n",
      "Training Batch [216/782]: Loss 0.00510379346087575\n",
      "Training Batch [217/782]: Loss 0.0009136605658568442\n",
      "Training Batch [218/782]: Loss 0.0012121842009946704\n",
      "Training Batch [219/782]: Loss 0.0013669347390532494\n",
      "Training Batch [220/782]: Loss 0.015117381699383259\n",
      "Training Batch [221/782]: Loss 0.0030977355781942606\n",
      "Training Batch [222/782]: Loss 0.013705686666071415\n",
      "Training Batch [223/782]: Loss 0.0009458159911446273\n",
      "Training Batch [224/782]: Loss 0.0036105127073824406\n",
      "Training Batch [225/782]: Loss 0.00400770828127861\n",
      "Training Batch [226/782]: Loss 0.007596742361783981\n",
      "Training Batch [227/782]: Loss 0.003526026615872979\n",
      "Training Batch [228/782]: Loss 0.00796903483569622\n",
      "Training Batch [229/782]: Loss 0.004719202406704426\n",
      "Training Batch [230/782]: Loss 0.032973337918519974\n",
      "Training Batch [231/782]: Loss 0.0031145592220127583\n",
      "Training Batch [232/782]: Loss 0.0036460612900555134\n",
      "Training Batch [233/782]: Loss 0.02330111898481846\n",
      "Training Batch [234/782]: Loss 0.0009845391614362597\n",
      "Training Batch [235/782]: Loss 0.06938319653272629\n",
      "Training Batch [236/782]: Loss 0.00962047465145588\n",
      "Training Batch [237/782]: Loss 0.0036544890608638525\n",
      "Training Batch [238/782]: Loss 0.008336735889315605\n",
      "Training Batch [239/782]: Loss 0.01016250066459179\n",
      "Training Batch [240/782]: Loss 0.002357043791562319\n",
      "Training Batch [241/782]: Loss 0.02477065846323967\n",
      "Training Batch [242/782]: Loss 0.03291849046945572\n",
      "Training Batch [243/782]: Loss 0.0006038820720277727\n",
      "Training Batch [244/782]: Loss 0.00021182272757869214\n",
      "Training Batch [245/782]: Loss 0.00650066090747714\n",
      "Training Batch [246/782]: Loss 0.0022538816556334496\n",
      "Training Batch [247/782]: Loss 0.003268480533733964\n",
      "Training Batch [248/782]: Loss 0.0015926342457532883\n",
      "Training Batch [249/782]: Loss 0.011081566102802753\n",
      "Training Batch [250/782]: Loss 0.009275879710912704\n",
      "Training Batch [251/782]: Loss 0.004654211923480034\n",
      "Training Batch [252/782]: Loss 0.0006973440176807344\n",
      "Training Batch [253/782]: Loss 0.0006583475042134523\n",
      "Training Batch [254/782]: Loss 0.007783957291394472\n",
      "Training Batch [255/782]: Loss 0.010434534400701523\n",
      "Training Batch [256/782]: Loss 0.01740848459303379\n",
      "Training Batch [257/782]: Loss 0.00016830387176014483\n",
      "Training Batch [258/782]: Loss 0.001651325379498303\n",
      "Training Batch [259/782]: Loss 0.005490154959261417\n",
      "Training Batch [260/782]: Loss 0.00551194092258811\n",
      "Training Batch [261/782]: Loss 0.0012987398076802492\n",
      "Training Batch [262/782]: Loss 0.0018376914085820317\n",
      "Training Batch [263/782]: Loss 0.02527989074587822\n",
      "Training Batch [264/782]: Loss 0.000123094956506975\n",
      "Training Batch [265/782]: Loss 0.006559253670275211\n",
      "Training Batch [266/782]: Loss 0.0007309016073122621\n",
      "Training Batch [267/782]: Loss 0.0006239403737708926\n",
      "Training Batch [268/782]: Loss 0.03270414099097252\n",
      "Training Batch [269/782]: Loss 0.007959396578371525\n",
      "Training Batch [270/782]: Loss 0.0031296638771891594\n",
      "Training Batch [271/782]: Loss 0.0035858629271388054\n",
      "Training Batch [272/782]: Loss 0.002604961395263672\n",
      "Training Batch [273/782]: Loss 0.0011529350886121392\n",
      "Training Batch [274/782]: Loss 0.001606596983037889\n",
      "Training Batch [275/782]: Loss 0.00541240768507123\n",
      "Training Batch [276/782]: Loss 0.0012561302864924073\n",
      "Training Batch [277/782]: Loss 0.000364643638022244\n",
      "Training Batch [278/782]: Loss 0.05433211103081703\n",
      "Training Batch [279/782]: Loss 0.0006650307332165539\n",
      "Training Batch [280/782]: Loss 0.000605034816544503\n",
      "Training Batch [281/782]: Loss 0.002409677952528\n",
      "Training Batch [282/782]: Loss 0.00027695944299921393\n",
      "Training Batch [283/782]: Loss 0.05144506320357323\n",
      "Training Batch [284/782]: Loss 0.012545627541840076\n",
      "Training Batch [285/782]: Loss 0.0006665542023256421\n",
      "Training Batch [286/782]: Loss 0.00893485825508833\n",
      "Training Batch [287/782]: Loss 0.013242458924651146\n",
      "Training Batch [288/782]: Loss 0.0004193777858745307\n",
      "Training Batch [289/782]: Loss 0.0011349361157044768\n",
      "Training Batch [290/782]: Loss 0.009590663947165012\n",
      "Training Batch [291/782]: Loss 0.0016127650160342455\n",
      "Training Batch [292/782]: Loss 0.050784144550561905\n",
      "Training Batch [293/782]: Loss 0.0012829334009438753\n",
      "Training Batch [294/782]: Loss 0.019786449149250984\n",
      "Training Batch [295/782]: Loss 0.022258352488279343\n",
      "Training Batch [296/782]: Loss 0.005734734702855349\n",
      "Training Batch [297/782]: Loss 0.003705035662278533\n",
      "Training Batch [298/782]: Loss 0.0001618717360543087\n",
      "Training Batch [299/782]: Loss 0.03807825595140457\n",
      "Training Batch [300/782]: Loss 8.213992259697989e-05\n",
      "Training Batch [301/782]: Loss 0.0067682513035833836\n",
      "Training Batch [302/782]: Loss 0.0004614701902028173\n",
      "Training Batch [303/782]: Loss 0.0015790484612807631\n",
      "Training Batch [304/782]: Loss 0.041381895542144775\n",
      "Training Batch [305/782]: Loss 0.06287889182567596\n",
      "Training Batch [306/782]: Loss 0.0019285054877400398\n",
      "Training Batch [307/782]: Loss 0.00793954636901617\n",
      "Training Batch [308/782]: Loss 0.0016424296190962195\n",
      "Training Batch [309/782]: Loss 0.0018592040287330747\n",
      "Training Batch [310/782]: Loss 0.0025681143160909414\n",
      "Training Batch [311/782]: Loss 0.023686451837420464\n",
      "Training Batch [312/782]: Loss 0.01760794036090374\n",
      "Training Batch [313/782]: Loss 0.0016176050994545221\n",
      "Training Batch [314/782]: Loss 0.001966541400179267\n",
      "Training Batch [315/782]: Loss 0.0009882906451821327\n",
      "Training Batch [316/782]: Loss 0.01139053888618946\n",
      "Training Batch [317/782]: Loss 0.0014513010391965508\n",
      "Training Batch [318/782]: Loss 0.0014276114525273442\n",
      "Training Batch [319/782]: Loss 0.012617886066436768\n",
      "Training Batch [320/782]: Loss 0.008045420981943607\n",
      "Training Batch [321/782]: Loss 0.010438043624162674\n",
      "Training Batch [322/782]: Loss 0.0001997374347411096\n",
      "Training Batch [323/782]: Loss 0.000786817108746618\n",
      "Training Batch [324/782]: Loss 0.005862090270966291\n",
      "Training Batch [325/782]: Loss 0.013531710021197796\n",
      "Training Batch [326/782]: Loss 0.001770664588548243\n",
      "Training Batch [327/782]: Loss 0.013040666468441486\n",
      "Training Batch [328/782]: Loss 0.0008975592791102827\n",
      "Training Batch [329/782]: Loss 0.0015665792161598802\n",
      "Training Batch [330/782]: Loss 0.003897711867466569\n",
      "Training Batch [331/782]: Loss 0.0072821625508368015\n",
      "Training Batch [332/782]: Loss 0.0033941701985895634\n",
      "Training Batch [333/782]: Loss 8.458388765575364e-05\n",
      "Training Batch [334/782]: Loss 0.08451756834983826\n",
      "Training Batch [335/782]: Loss 0.013068802654743195\n",
      "Training Batch [336/782]: Loss 0.001604963792487979\n",
      "Training Batch [337/782]: Loss 0.0015861954307183623\n",
      "Training Batch [338/782]: Loss 0.0011626699706539512\n",
      "Training Batch [339/782]: Loss 0.0005048195598646998\n",
      "Training Batch [340/782]: Loss 0.00384767958894372\n",
      "Training Batch [341/782]: Loss 0.007126850076019764\n",
      "Training Batch [342/782]: Loss 0.0009467601776123047\n",
      "Training Batch [343/782]: Loss 0.003920550923794508\n",
      "Training Batch [344/782]: Loss 0.005209663882851601\n",
      "Training Batch [345/782]: Loss 0.012486248277127743\n",
      "Training Batch [346/782]: Loss 0.0021326527930796146\n",
      "Training Batch [347/782]: Loss 0.0009130749385803938\n",
      "Training Batch [348/782]: Loss 0.0011265755165368319\n",
      "Training Batch [349/782]: Loss 0.00277891312725842\n",
      "Training Batch [350/782]: Loss 0.013432850129902363\n",
      "Training Batch [351/782]: Loss 0.036795057356357574\n",
      "Training Batch [352/782]: Loss 0.0117002809420228\n",
      "Training Batch [353/782]: Loss 0.00431285984814167\n",
      "Training Batch [354/782]: Loss 0.0019540220964699984\n",
      "Training Batch [355/782]: Loss 0.017236102372407913\n",
      "Training Batch [356/782]: Loss 0.011369014158844948\n",
      "Training Batch [357/782]: Loss 0.007070201449096203\n",
      "Training Batch [358/782]: Loss 0.0034080236218869686\n",
      "Training Batch [359/782]: Loss 0.00036757945781573653\n",
      "Training Batch [360/782]: Loss 0.010499846190214157\n",
      "Training Batch [361/782]: Loss 0.0027719689533114433\n",
      "Training Batch [362/782]: Loss 0.005559688434004784\n",
      "Training Batch [363/782]: Loss 0.0032354549039155245\n",
      "Training Batch [364/782]: Loss 0.0015058916760608554\n",
      "Training Batch [365/782]: Loss 0.0028429385274648666\n",
      "Training Batch [366/782]: Loss 0.0027302009984850883\n",
      "Training Batch [367/782]: Loss 0.07659976184368134\n",
      "Training Batch [368/782]: Loss 0.0009052773239091039\n",
      "Training Batch [369/782]: Loss 0.0011940397089347243\n",
      "Training Batch [370/782]: Loss 0.0026128620374947786\n",
      "Training Batch [371/782]: Loss 0.00040105031803250313\n",
      "Training Batch [372/782]: Loss 0.002502423245459795\n",
      "Training Batch [373/782]: Loss 0.016327546909451485\n",
      "Training Batch [374/782]: Loss 0.010320842266082764\n",
      "Training Batch [375/782]: Loss 0.00028928410029038787\n",
      "Training Batch [376/782]: Loss 0.0013533340534195304\n",
      "Training Batch [377/782]: Loss 0.0015018726699054241\n",
      "Training Batch [378/782]: Loss 0.0009332806221209466\n",
      "Training Batch [379/782]: Loss 0.0007598568336106837\n",
      "Training Batch [380/782]: Loss 0.0130454758182168\n",
      "Training Batch [381/782]: Loss 0.0014447126304730773\n",
      "Training Batch [382/782]: Loss 0.024034393951296806\n",
      "Training Batch [383/782]: Loss 0.0004859562322963029\n",
      "Training Batch [384/782]: Loss 9.169969416689128e-05\n",
      "Training Batch [385/782]: Loss 0.010805928148329258\n",
      "Training Batch [386/782]: Loss 0.15838523209095\n",
      "Training Batch [387/782]: Loss 0.002464468125253916\n",
      "Training Batch [388/782]: Loss 0.009943983517587185\n",
      "Training Batch [389/782]: Loss 0.0050612459890544415\n",
      "Training Batch [390/782]: Loss 0.01248856633901596\n",
      "Training Batch [391/782]: Loss 0.0022325366735458374\n",
      "Training Batch [392/782]: Loss 0.0005389229045249522\n",
      "Training Batch [393/782]: Loss 0.013876481913030148\n",
      "Training Batch [394/782]: Loss 0.0052496627904474735\n",
      "Training Batch [395/782]: Loss 0.0006871555233374238\n",
      "Training Batch [396/782]: Loss 0.0002764530072454363\n",
      "Training Batch [397/782]: Loss 0.0009373119683004916\n",
      "Training Batch [398/782]: Loss 0.002982701174914837\n",
      "Training Batch [399/782]: Loss 0.0058891549706459045\n",
      "Training Batch [400/782]: Loss 0.0020052967593073845\n",
      "Training Batch [401/782]: Loss 0.002336434554308653\n",
      "Training Batch [402/782]: Loss 0.02712104842066765\n",
      "Training Batch [403/782]: Loss 0.0012125637149438262\n",
      "Training Batch [404/782]: Loss 0.0003190681745763868\n",
      "Training Batch [405/782]: Loss 0.0012148629175499082\n",
      "Training Batch [406/782]: Loss 0.04089614376425743\n",
      "Training Batch [407/782]: Loss 0.0025096035096794367\n",
      "Training Batch [408/782]: Loss 0.0033606570214033127\n",
      "Training Batch [409/782]: Loss 0.006340456195175648\n",
      "Training Batch [410/782]: Loss 0.0010229324689134955\n",
      "Training Batch [411/782]: Loss 0.0006635891040787101\n",
      "Training Batch [412/782]: Loss 0.0028805690817534924\n",
      "Training Batch [413/782]: Loss 0.007554642856121063\n",
      "Training Batch [414/782]: Loss 0.0005679430905729532\n",
      "Training Batch [415/782]: Loss 0.00255332188680768\n",
      "Training Batch [416/782]: Loss 0.0002964676823467016\n",
      "Training Batch [417/782]: Loss 0.0009152837446890771\n",
      "Training Batch [418/782]: Loss 0.00439161853864789\n",
      "Training Batch [419/782]: Loss 0.0004033539444208145\n",
      "Training Batch [420/782]: Loss 0.0003607753315009177\n",
      "Training Batch [421/782]: Loss 0.00038983236299827695\n",
      "Training Batch [422/782]: Loss 0.10033278167247772\n",
      "Training Batch [423/782]: Loss 0.00452805170789361\n",
      "Training Batch [424/782]: Loss 0.000821460213046521\n",
      "Training Batch [425/782]: Loss 0.09905276447534561\n",
      "Training Batch [426/782]: Loss 0.001610543462447822\n",
      "Training Batch [427/782]: Loss 0.005410999990999699\n",
      "Training Batch [428/782]: Loss 0.0023905839771032333\n",
      "Training Batch [429/782]: Loss 0.06982215493917465\n",
      "Training Batch [430/782]: Loss 0.0064306193962693214\n",
      "Training Batch [431/782]: Loss 0.0004498498747125268\n",
      "Training Batch [432/782]: Loss 0.0029422608204185963\n",
      "Training Batch [433/782]: Loss 0.0012274879263713956\n",
      "Training Batch [434/782]: Loss 0.005624808836728334\n",
      "Training Batch [435/782]: Loss 0.00944246631115675\n",
      "Training Batch [436/782]: Loss 0.06356696784496307\n",
      "Training Batch [437/782]: Loss 0.009250523522496223\n",
      "Training Batch [438/782]: Loss 0.002044845372438431\n",
      "Training Batch [439/782]: Loss 0.0009186536772176623\n",
      "Training Batch [440/782]: Loss 0.005080904811620712\n",
      "Training Batch [441/782]: Loss 0.021022237837314606\n",
      "Training Batch [442/782]: Loss 0.00026999320834875107\n",
      "Training Batch [443/782]: Loss 0.004252871498465538\n",
      "Training Batch [444/782]: Loss 0.06391405314207077\n",
      "Training Batch [445/782]: Loss 0.0016965322429314256\n",
      "Training Batch [446/782]: Loss 0.0007678549736738205\n",
      "Training Batch [447/782]: Loss 0.002093954710289836\n",
      "Training Batch [448/782]: Loss 0.00017856691556517035\n",
      "Training Batch [449/782]: Loss 0.000868419127073139\n",
      "Training Batch [450/782]: Loss 0.007629524450749159\n",
      "Training Batch [451/782]: Loss 0.0053385584615170956\n",
      "Training Batch [452/782]: Loss 0.014162581413984299\n",
      "Training Batch [453/782]: Loss 0.0044371383264660835\n",
      "Training Batch [454/782]: Loss 0.045186761766672134\n",
      "Training Batch [455/782]: Loss 0.0004649744078051299\n",
      "Training Batch [456/782]: Loss 0.0007425961084663868\n",
      "Training Batch [457/782]: Loss 0.0023969949688762426\n",
      "Training Batch [458/782]: Loss 0.039909325540065765\n",
      "Training Batch [459/782]: Loss 0.005748639814555645\n",
      "Training Batch [460/782]: Loss 0.008487183600664139\n",
      "Training Batch [461/782]: Loss 0.0005127960466779768\n",
      "Training Batch [462/782]: Loss 0.004692412912845612\n",
      "Training Batch [463/782]: Loss 0.000426730839535594\n",
      "Training Batch [464/782]: Loss 0.013075553812086582\n",
      "Training Batch [465/782]: Loss 0.04439421370625496\n",
      "Training Batch [466/782]: Loss 0.004366726614534855\n",
      "Training Batch [467/782]: Loss 0.0044548469595611095\n",
      "Training Batch [468/782]: Loss 0.009370507672429085\n",
      "Training Batch [469/782]: Loss 0.028312353417277336\n",
      "Training Batch [470/782]: Loss 0.0003782788116950542\n",
      "Training Batch [471/782]: Loss 0.0026740769390016794\n",
      "Training Batch [472/782]: Loss 0.01149251963943243\n",
      "Training Batch [473/782]: Loss 0.008516456931829453\n",
      "Training Batch [474/782]: Loss 0.001300780801102519\n",
      "Training Batch [475/782]: Loss 0.0028322990983724594\n",
      "Training Batch [476/782]: Loss 0.006274817511439323\n",
      "Training Batch [477/782]: Loss 0.003305210964754224\n",
      "Training Batch [478/782]: Loss 0.03120235912501812\n",
      "Training Batch [479/782]: Loss 0.035235386341810226\n",
      "Training Batch [480/782]: Loss 0.005050056613981724\n",
      "Training Batch [481/782]: Loss 0.005036821588873863\n",
      "Training Batch [482/782]: Loss 0.0012222285149618983\n",
      "Training Batch [483/782]: Loss 0.0006215636967681348\n",
      "Training Batch [484/782]: Loss 0.00554846553131938\n",
      "Training Batch [485/782]: Loss 0.017666086554527283\n",
      "Training Batch [486/782]: Loss 0.007661002688109875\n",
      "Training Batch [487/782]: Loss 0.003786807181313634\n",
      "Training Batch [488/782]: Loss 0.07338198274374008\n",
      "Training Batch [489/782]: Loss 0.012807243503630161\n",
      "Training Batch [490/782]: Loss 0.0005123960436321795\n",
      "Training Batch [491/782]: Loss 0.005443783011287451\n",
      "Training Batch [492/782]: Loss 0.028968559578061104\n",
      "Training Batch [493/782]: Loss 0.001349619822576642\n",
      "Training Batch [494/782]: Loss 0.0002936365199275315\n",
      "Training Batch [495/782]: Loss 0.08659520745277405\n",
      "Training Batch [496/782]: Loss 0.0005857909563928843\n",
      "Training Batch [497/782]: Loss 0.08463820070028305\n",
      "Training Batch [498/782]: Loss 0.12756840884685516\n",
      "Training Batch [499/782]: Loss 0.0006362652638927102\n",
      "Training Batch [500/782]: Loss 0.0018215903546661139\n",
      "Training Batch [501/782]: Loss 0.000714859867002815\n",
      "Training Batch [502/782]: Loss 0.0010815162677317858\n",
      "Training Batch [503/782]: Loss 0.01678180694580078\n",
      "Training Batch [504/782]: Loss 0.013286903500556946\n",
      "Training Batch [505/782]: Loss 0.0009498081053607166\n",
      "Training Batch [506/782]: Loss 0.004633249714970589\n",
      "Training Batch [507/782]: Loss 0.0024458318948745728\n",
      "Training Batch [508/782]: Loss 0.006730357650667429\n",
      "Training Batch [509/782]: Loss 0.00479871965944767\n",
      "Training Batch [510/782]: Loss 0.001663199975155294\n",
      "Training Batch [511/782]: Loss 0.00704299658536911\n",
      "Training Batch [512/782]: Loss 0.003419854212552309\n",
      "Training Batch [513/782]: Loss 0.028909938409924507\n",
      "Training Batch [514/782]: Loss 0.002585862297564745\n",
      "Training Batch [515/782]: Loss 0.003751385025680065\n",
      "Training Batch [516/782]: Loss 0.00400652876123786\n",
      "Training Batch [517/782]: Loss 0.012839031405746937\n",
      "Training Batch [518/782]: Loss 0.0013631251640617847\n",
      "Training Batch [519/782]: Loss 0.007468922063708305\n",
      "Training Batch [520/782]: Loss 0.003474451368674636\n",
      "Training Batch [521/782]: Loss 0.0003594073641579598\n",
      "Training Batch [522/782]: Loss 0.014668025076389313\n",
      "Training Batch [523/782]: Loss 0.002642538631334901\n",
      "Training Batch [524/782]: Loss 0.002455631736665964\n",
      "Training Batch [525/782]: Loss 0.0022998556960374117\n",
      "Training Batch [526/782]: Loss 0.06116048991680145\n",
      "Training Batch [527/782]: Loss 0.012897364795207977\n",
      "Training Batch [528/782]: Loss 0.017830679193139076\n",
      "Training Batch [529/782]: Loss 0.006107300519943237\n",
      "Training Batch [530/782]: Loss 0.0013240481493994594\n",
      "Training Batch [531/782]: Loss 0.005093757063150406\n",
      "Training Batch [532/782]: Loss 0.0019070704001933336\n",
      "Training Batch [533/782]: Loss 0.0002621409948915243\n",
      "Training Batch [534/782]: Loss 0.0015121918404474854\n",
      "Training Batch [535/782]: Loss 0.005673352628946304\n",
      "Training Batch [536/782]: Loss 0.00036547821946442127\n",
      "Training Batch [537/782]: Loss 0.07253865152597427\n",
      "Training Batch [538/782]: Loss 0.00021798555098939687\n",
      "Training Batch [539/782]: Loss 0.000891486241016537\n",
      "Training Batch [540/782]: Loss 0.0007114724721759558\n",
      "Training Batch [541/782]: Loss 0.0029710063245147467\n",
      "Training Batch [542/782]: Loss 0.0009489182848483324\n",
      "Training Batch [543/782]: Loss 0.0013117657508701086\n",
      "Training Batch [544/782]: Loss 0.00048308284021914005\n",
      "Training Batch [545/782]: Loss 0.0006685833795927465\n",
      "Training Batch [546/782]: Loss 0.0010292436927556992\n",
      "Training Batch [547/782]: Loss 0.003185149049386382\n",
      "Training Batch [548/782]: Loss 0.0008941420237533748\n",
      "Training Batch [549/782]: Loss 0.0002904228458646685\n",
      "Training Batch [550/782]: Loss 0.0016690901247784495\n",
      "Training Batch [551/782]: Loss 0.0013798743020743132\n",
      "Training Batch [552/782]: Loss 0.021165745332837105\n",
      "Training Batch [553/782]: Loss 0.0002813228056766093\n",
      "Training Batch [554/782]: Loss 0.0010926187969744205\n",
      "Training Batch [555/782]: Loss 0.0048595005646348\n",
      "Training Batch [556/782]: Loss 0.005413802806288004\n",
      "Training Batch [557/782]: Loss 0.0006071337265893817\n",
      "Training Batch [558/782]: Loss 0.004320317413657904\n",
      "Training Batch [559/782]: Loss 0.04112505167722702\n",
      "Training Batch [560/782]: Loss 9.33827250264585e-05\n",
      "Training Batch [561/782]: Loss 0.010618452914059162\n",
      "Training Batch [562/782]: Loss 0.005392301827669144\n",
      "Training Batch [563/782]: Loss 0.00169308518525213\n",
      "Training Batch [564/782]: Loss 0.0033165186177939177\n",
      "Training Batch [565/782]: Loss 0.0032992816995829344\n",
      "Training Batch [566/782]: Loss 0.001635831082239747\n",
      "Training Batch [567/782]: Loss 0.004106620792299509\n",
      "Training Batch [568/782]: Loss 0.018506880849599838\n",
      "Training Batch [569/782]: Loss 0.0013300543650984764\n",
      "Training Batch [570/782]: Loss 0.0009658278431743383\n",
      "Training Batch [571/782]: Loss 0.0009620037162676454\n",
      "Training Batch [572/782]: Loss 0.00466088205575943\n",
      "Training Batch [573/782]: Loss 0.0003078238805755973\n",
      "Training Batch [574/782]: Loss 0.0005930576007813215\n",
      "Training Batch [575/782]: Loss 0.022903038188815117\n",
      "Training Batch [576/782]: Loss 0.010007389821112156\n",
      "Training Batch [577/782]: Loss 0.00431177020072937\n",
      "Training Batch [578/782]: Loss 0.06304425746202469\n",
      "Training Batch [579/782]: Loss 0.0002476231020409614\n",
      "Training Batch [580/782]: Loss 0.010006992146372795\n",
      "Training Batch [581/782]: Loss 0.003676345804706216\n",
      "Training Batch [582/782]: Loss 0.015670549124479294\n",
      "Training Batch [583/782]: Loss 0.023678243160247803\n",
      "Training Batch [584/782]: Loss 0.009391662664711475\n",
      "Training Batch [585/782]: Loss 0.0039688642136752605\n",
      "Training Batch [586/782]: Loss 0.0004669015179388225\n",
      "Training Batch [587/782]: Loss 0.007063474506139755\n",
      "Training Batch [588/782]: Loss 0.0009399130358360708\n",
      "Training Batch [589/782]: Loss 0.005055026151239872\n",
      "Training Batch [590/782]: Loss 0.004513207823038101\n",
      "Training Batch [591/782]: Loss 0.000342891231412068\n",
      "Training Batch [592/782]: Loss 0.003734528087079525\n",
      "Training Batch [593/782]: Loss 9.588558168616146e-05\n",
      "Training Batch [594/782]: Loss 0.000814378319773823\n",
      "Training Batch [595/782]: Loss 0.0014615101972594857\n",
      "Training Batch [596/782]: Loss 0.0068032266572117805\n",
      "Training Batch [597/782]: Loss 0.0008102626889012754\n",
      "Training Batch [598/782]: Loss 0.06637627631425858\n",
      "Training Batch [599/782]: Loss 0.018975792452692986\n",
      "Training Batch [600/782]: Loss 0.024459054693579674\n",
      "Training Batch [601/782]: Loss 0.0007534520118497312\n",
      "Training Batch [602/782]: Loss 0.0030384808778762817\n",
      "Training Batch [603/782]: Loss 0.008150759153068066\n",
      "Training Batch [604/782]: Loss 0.000875231227837503\n",
      "Training Batch [605/782]: Loss 0.008386317640542984\n",
      "Training Batch [606/782]: Loss 0.00029741827165707946\n",
      "Training Batch [607/782]: Loss 0.0010698604164645076\n",
      "Training Batch [608/782]: Loss 0.0036633301060646772\n",
      "Training Batch [609/782]: Loss 0.0010935436002910137\n",
      "Training Batch [610/782]: Loss 0.030629975721240044\n",
      "Training Batch [611/782]: Loss 0.0046899267472326756\n",
      "Training Batch [612/782]: Loss 0.0482628233730793\n",
      "Training Batch [613/782]: Loss 0.022681953385472298\n",
      "Training Batch [614/782]: Loss 0.000895005592610687\n",
      "Training Batch [615/782]: Loss 0.0012251734733581543\n",
      "Training Batch [616/782]: Loss 0.003076678141951561\n",
      "Training Batch [617/782]: Loss 0.01890265941619873\n",
      "Training Batch [618/782]: Loss 0.0007521184161305428\n",
      "Training Batch [619/782]: Loss 0.0014308530371636152\n",
      "Training Batch [620/782]: Loss 0.000879861181601882\n",
      "Training Batch [621/782]: Loss 0.0003344828146509826\n",
      "Training Batch [622/782]: Loss 0.0002328456612303853\n",
      "Training Batch [623/782]: Loss 0.0004538797657005489\n",
      "Training Batch [624/782]: Loss 4.243325747665949e-05\n",
      "Training Batch [625/782]: Loss 0.0006151240668259561\n",
      "Training Batch [626/782]: Loss 0.0035560689866542816\n",
      "Training Batch [627/782]: Loss 0.015737518668174744\n",
      "Training Batch [628/782]: Loss 0.0012173407012596726\n",
      "Training Batch [629/782]: Loss 0.05206790938973427\n",
      "Training Batch [630/782]: Loss 0.0004093055904377252\n",
      "Training Batch [631/782]: Loss 0.0007490250864066184\n",
      "Training Batch [632/782]: Loss 0.0009238033671863377\n",
      "Training Batch [633/782]: Loss 0.0007829666719771922\n",
      "Training Batch [634/782]: Loss 0.01196015439927578\n",
      "Training Batch [635/782]: Loss 0.03237926587462425\n",
      "Training Batch [636/782]: Loss 0.0029425565153360367\n",
      "Training Batch [637/782]: Loss 0.0004782767500728369\n",
      "Training Batch [638/782]: Loss 0.0013666828162968159\n",
      "Training Batch [639/782]: Loss 0.02945542149245739\n",
      "Training Batch [640/782]: Loss 0.0020688045769929886\n",
      "Training Batch [641/782]: Loss 0.02962876297533512\n",
      "Training Batch [642/782]: Loss 0.0003837350814137608\n",
      "Training Batch [643/782]: Loss 0.0024861861020326614\n",
      "Training Batch [644/782]: Loss 0.004467684775590897\n",
      "Training Batch [645/782]: Loss 0.014925971627235413\n",
      "Training Batch [646/782]: Loss 0.011129481717944145\n",
      "Training Batch [647/782]: Loss 0.02132606692612171\n",
      "Training Batch [648/782]: Loss 0.012959923595190048\n",
      "Training Batch [649/782]: Loss 0.0048061152920126915\n",
      "Training Batch [650/782]: Loss 0.004416961222887039\n",
      "Training Batch [651/782]: Loss 0.0035100516397506\n",
      "Training Batch [652/782]: Loss 0.05295758321881294\n",
      "Training Batch [653/782]: Loss 0.0005866624996997416\n",
      "Training Batch [654/782]: Loss 0.00026665435871109366\n",
      "Training Batch [655/782]: Loss 0.027963906526565552\n",
      "Training Batch [656/782]: Loss 0.002353377640247345\n",
      "Training Batch [657/782]: Loss 0.00055572425480932\n",
      "Training Batch [658/782]: Loss 0.0005580725846812129\n",
      "Training Batch [659/782]: Loss 0.001437463564798236\n",
      "Training Batch [660/782]: Loss 0.003591903019696474\n",
      "Training Batch [661/782]: Loss 0.02589731104671955\n",
      "Training Batch [662/782]: Loss 0.05228837579488754\n",
      "Training Batch [663/782]: Loss 0.011925017461180687\n",
      "Training Batch [664/782]: Loss 0.006723135244101286\n",
      "Training Batch [665/782]: Loss 0.00865917094051838\n",
      "Training Batch [666/782]: Loss 0.00460922671481967\n",
      "Training Batch [667/782]: Loss 0.0035467110574245453\n",
      "Training Batch [668/782]: Loss 0.008414540439844131\n",
      "Training Batch [669/782]: Loss 0.013964743353426456\n",
      "Training Batch [670/782]: Loss 0.0006159127806313336\n",
      "Training Batch [671/782]: Loss 0.0018164652865380049\n",
      "Training Batch [672/782]: Loss 0.00031225779093801975\n",
      "Training Batch [673/782]: Loss 0.001609637401998043\n",
      "Training Batch [674/782]: Loss 0.012479327619075775\n",
      "Training Batch [675/782]: Loss 0.00190390192437917\n",
      "Training Batch [676/782]: Loss 0.0015837016981095076\n",
      "Training Batch [677/782]: Loss 0.019390935078263283\n",
      "Training Batch [678/782]: Loss 0.0011410750448703766\n",
      "Training Batch [679/782]: Loss 0.026760434731841087\n",
      "Training Batch [680/782]: Loss 0.00030740012880414724\n",
      "Training Batch [681/782]: Loss 0.005080546252429485\n",
      "Training Batch [682/782]: Loss 0.0008378649945370853\n",
      "Training Batch [683/782]: Loss 0.04148018732666969\n",
      "Training Batch [684/782]: Loss 0.0009337468654848635\n",
      "Training Batch [685/782]: Loss 0.0008717525633983314\n",
      "Training Batch [686/782]: Loss 0.015679981559515\n",
      "Training Batch [687/782]: Loss 0.001963598420843482\n",
      "Training Batch [688/782]: Loss 0.000558178115170449\n",
      "Training Batch [689/782]: Loss 0.0006535015418194234\n",
      "Training Batch [690/782]: Loss 0.00030107409111224115\n",
      "Training Batch [691/782]: Loss 0.003697651671245694\n",
      "Training Batch [692/782]: Loss 0.006223586387932301\n",
      "Training Batch [693/782]: Loss 0.003937229514122009\n",
      "Training Batch [694/782]: Loss 0.0008511021733283997\n",
      "Training Batch [695/782]: Loss 0.0004891441203653812\n",
      "Training Batch [696/782]: Loss 0.0032651613000780344\n",
      "Training Batch [697/782]: Loss 0.03525497764348984\n",
      "Training Batch [698/782]: Loss 0.0005975221865810454\n",
      "Training Batch [699/782]: Loss 0.003532174276188016\n",
      "Training Batch [700/782]: Loss 0.007843321189284325\n",
      "Training Batch [701/782]: Loss 0.04425962641835213\n",
      "Training Batch [702/782]: Loss 0.0004472872242331505\n",
      "Training Batch [703/782]: Loss 0.0056830900721251965\n",
      "Training Batch [704/782]: Loss 8.400910155614838e-05\n",
      "Training Batch [705/782]: Loss 0.003506873268634081\n",
      "Training Batch [706/782]: Loss 0.0014101555570960045\n",
      "Training Batch [707/782]: Loss 0.003559141419827938\n",
      "Training Batch [708/782]: Loss 0.0026318924501538277\n",
      "Training Batch [709/782]: Loss 0.0013463929062709212\n",
      "Training Batch [710/782]: Loss 0.0005298597970977426\n",
      "Training Batch [711/782]: Loss 0.0024762155953794718\n",
      "Training Batch [712/782]: Loss 0.0007387843797914684\n",
      "Training Batch [713/782]: Loss 0.0213962160050869\n",
      "Training Batch [714/782]: Loss 0.003039136528968811\n",
      "Training Batch [715/782]: Loss 0.0001383956550853327\n",
      "Training Batch [716/782]: Loss 0.0003388065961189568\n",
      "Training Batch [717/782]: Loss 0.00015312875621020794\n",
      "Training Batch [718/782]: Loss 0.04041251167654991\n",
      "Training Batch [719/782]: Loss 0.0030430168844759464\n",
      "Training Batch [720/782]: Loss 0.020191863179206848\n",
      "Training Batch [721/782]: Loss 0.0008357680053450167\n",
      "Training Batch [722/782]: Loss 0.0020140251144766808\n",
      "Training Batch [723/782]: Loss 0.017168324440717697\n",
      "Training Batch [724/782]: Loss 0.0034128245897591114\n",
      "Training Batch [725/782]: Loss 0.002924020867794752\n",
      "Training Batch [726/782]: Loss 0.08689170330762863\n",
      "Training Batch [727/782]: Loss 0.0634068101644516\n",
      "Training Batch [728/782]: Loss 0.006292909849435091\n",
      "Training Batch [729/782]: Loss 0.016050094738602638\n",
      "Training Batch [730/782]: Loss 0.008974257856607437\n",
      "Training Batch [731/782]: Loss 0.001359528978355229\n",
      "Training Batch [732/782]: Loss 0.004608325660228729\n",
      "Training Batch [733/782]: Loss 0.004479017574340105\n",
      "Training Batch [734/782]: Loss 0.0027924631722271442\n",
      "Training Batch [735/782]: Loss 0.011058506555855274\n",
      "Training Batch [736/782]: Loss 0.007589920423924923\n",
      "Training Batch [737/782]: Loss 0.00595587445423007\n",
      "Training Batch [738/782]: Loss 0.0021579540334641933\n",
      "Training Batch [739/782]: Loss 0.004030212759971619\n",
      "Training Batch [740/782]: Loss 0.0016084177186712623\n",
      "Training Batch [741/782]: Loss 0.013572419993579388\n",
      "Training Batch [742/782]: Loss 0.024602754041552544\n",
      "Training Batch [743/782]: Loss 0.002962221624329686\n",
      "Training Batch [744/782]: Loss 0.00171315623447299\n",
      "Training Batch [745/782]: Loss 0.05110716074705124\n",
      "Training Batch [746/782]: Loss 0.00819111056625843\n",
      "Training Batch [747/782]: Loss 0.023816993460059166\n",
      "Training Batch [748/782]: Loss 0.00031303343712352216\n",
      "Training Batch [749/782]: Loss 0.0013979170471429825\n",
      "Training Batch [750/782]: Loss 0.010969886556267738\n",
      "Training Batch [751/782]: Loss 0.00727083208039403\n",
      "Training Batch [752/782]: Loss 0.06097313389182091\n",
      "Training Batch [753/782]: Loss 0.02599787339568138\n",
      "Training Batch [754/782]: Loss 0.0007204126450233161\n",
      "Training Batch [755/782]: Loss 0.033612556755542755\n",
      "Training Batch [756/782]: Loss 0.02714209444820881\n",
      "Training Batch [757/782]: Loss 0.003566654399037361\n",
      "Training Batch [758/782]: Loss 0.024401389062404633\n",
      "Training Batch [759/782]: Loss 0.001034335931763053\n",
      "Training Batch [760/782]: Loss 0.0014129469636827707\n",
      "Training Batch [761/782]: Loss 0.0005924617871642113\n",
      "Training Batch [762/782]: Loss 0.004153140354901552\n",
      "Training Batch [763/782]: Loss 0.0007783781038597226\n",
      "Training Batch [764/782]: Loss 0.002019014675170183\n",
      "Training Batch [765/782]: Loss 0.027198249474167824\n",
      "Training Batch [766/782]: Loss 9.64008504524827e-05\n",
      "Training Batch [767/782]: Loss 0.0021282806992530823\n",
      "Training Batch [768/782]: Loss 0.09054297208786011\n",
      "Training Batch [769/782]: Loss 0.0003602811775635928\n",
      "Training Batch [770/782]: Loss 0.003782125422731042\n",
      "Training Batch [771/782]: Loss 0.007885767146945\n",
      "Training Batch [772/782]: Loss 0.004671483300626278\n",
      "Training Batch [773/782]: Loss 0.0032862157095223665\n",
      "Training Batch [774/782]: Loss 0.002653946867212653\n",
      "Training Batch [775/782]: Loss 0.0006687689456157386\n",
      "Training Batch [776/782]: Loss 0.00217495602555573\n",
      "Training Batch [777/782]: Loss 0.02287118509411812\n",
      "Training Batch [778/782]: Loss 0.00032560774707235396\n",
      "Training Batch [779/782]: Loss 0.0020193462260067463\n",
      "Training Batch [780/782]: Loss 0.0021561537869274616\n",
      "Training Batch [781/782]: Loss 0.020974045619368553\n",
      "Training Batch [782/782]: Loss 0.00019357381097506732\n",
      "Epoch 29 - Train Loss: 0.0098\n",
      "*********  Epoch 30/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.00039006126462481916\n",
      "Training Batch [2/782]: Loss 0.005476823076605797\n",
      "Training Batch [3/782]: Loss 0.000591890187934041\n",
      "Training Batch [4/782]: Loss 0.003741757245734334\n",
      "Training Batch [5/782]: Loss 0.011143328621983528\n",
      "Training Batch [6/782]: Loss 0.014513170346617699\n",
      "Training Batch [7/782]: Loss 0.0013662552228197455\n",
      "Training Batch [8/782]: Loss 0.00157981738448143\n",
      "Training Batch [9/782]: Loss 0.0003905541088897735\n",
      "Training Batch [10/782]: Loss 0.00040832889499142766\n",
      "Training Batch [11/782]: Loss 0.0013560280203819275\n",
      "Training Batch [12/782]: Loss 0.0010290774516761303\n",
      "Training Batch [13/782]: Loss 0.008003600873053074\n",
      "Training Batch [14/782]: Loss 0.005540838930755854\n",
      "Training Batch [15/782]: Loss 0.012806903570890427\n",
      "Training Batch [16/782]: Loss 0.00014310273400042206\n",
      "Training Batch [17/782]: Loss 0.0032672835513949394\n",
      "Training Batch [18/782]: Loss 0.0003531882830429822\n",
      "Training Batch [19/782]: Loss 0.0006190675776451826\n",
      "Training Batch [20/782]: Loss 0.007623110432177782\n",
      "Training Batch [21/782]: Loss 0.0006189041305333376\n",
      "Training Batch [22/782]: Loss 0.007551310118287802\n",
      "Training Batch [23/782]: Loss 0.0042227935045957565\n",
      "Training Batch [24/782]: Loss 0.003132287645712495\n",
      "Training Batch [25/782]: Loss 0.005073810927569866\n",
      "Training Batch [26/782]: Loss 0.0032155532389879227\n",
      "Training Batch [27/782]: Loss 0.00032479697256349027\n",
      "Training Batch [28/782]: Loss 0.03373026102781296\n",
      "Training Batch [29/782]: Loss 0.015165265649557114\n",
      "Training Batch [30/782]: Loss 0.0005537417018786073\n",
      "Training Batch [31/782]: Loss 0.0021514007821679115\n",
      "Training Batch [32/782]: Loss 0.00276757194660604\n",
      "Training Batch [33/782]: Loss 0.0003732283948920667\n",
      "Training Batch [34/782]: Loss 0.0015038970159366727\n",
      "Training Batch [35/782]: Loss 0.0790630355477333\n",
      "Training Batch [36/782]: Loss 0.0008294901344925165\n",
      "Training Batch [37/782]: Loss 0.0005785168032161891\n",
      "Training Batch [38/782]: Loss 0.0032535050995647907\n",
      "Training Batch [39/782]: Loss 0.0035702986642718315\n",
      "Training Batch [40/782]: Loss 0.0009946620557457209\n",
      "Training Batch [41/782]: Loss 0.0006725963903591037\n",
      "Training Batch [42/782]: Loss 0.0018641093047335744\n",
      "Training Batch [43/782]: Loss 0.0005991937359794974\n",
      "Training Batch [44/782]: Loss 0.0015973391709849238\n",
      "Training Batch [45/782]: Loss 0.00020045076962560415\n",
      "Training Batch [46/782]: Loss 0.00260246847756207\n",
      "Training Batch [47/782]: Loss 0.0766945332288742\n",
      "Training Batch [48/782]: Loss 0.0025214089546352625\n",
      "Training Batch [49/782]: Loss 0.001097245840355754\n",
      "Training Batch [50/782]: Loss 0.0008892350597307086\n",
      "Training Batch [51/782]: Loss 0.00010019000910688192\n",
      "Training Batch [52/782]: Loss 0.0003349449543748051\n",
      "Training Batch [53/782]: Loss 0.0031218347139656544\n",
      "Training Batch [54/782]: Loss 0.03749466314911842\n",
      "Training Batch [55/782]: Loss 0.08957072347402573\n",
      "Training Batch [56/782]: Loss 0.00042440620018169284\n",
      "Training Batch [57/782]: Loss 0.0019417747389525175\n",
      "Training Batch [58/782]: Loss 0.0019049843540415168\n",
      "Training Batch [59/782]: Loss 0.0032604807056486607\n",
      "Training Batch [60/782]: Loss 0.002895423909649253\n",
      "Training Batch [61/782]: Loss 0.007132235914468765\n",
      "Training Batch [62/782]: Loss 0.001021666219457984\n",
      "Training Batch [63/782]: Loss 0.005429565440863371\n",
      "Training Batch [64/782]: Loss 0.037054240703582764\n",
      "Training Batch [65/782]: Loss 0.01210792362689972\n",
      "Training Batch [66/782]: Loss 0.0031249478925019503\n",
      "Training Batch [67/782]: Loss 0.004396033473312855\n",
      "Training Batch [68/782]: Loss 0.012071826495230198\n",
      "Training Batch [69/782]: Loss 0.16327430307865143\n",
      "Training Batch [70/782]: Loss 0.0016295231180265546\n",
      "Training Batch [71/782]: Loss 0.006115713156759739\n",
      "Training Batch [72/782]: Loss 0.007471340708434582\n",
      "Training Batch [73/782]: Loss 0.01258096657693386\n",
      "Training Batch [74/782]: Loss 0.0006393025396391749\n",
      "Training Batch [75/782]: Loss 0.010419204831123352\n",
      "Training Batch [76/782]: Loss 0.00040350438212044537\n",
      "Training Batch [77/782]: Loss 0.008019453845918179\n",
      "Training Batch [78/782]: Loss 0.009124287404119968\n",
      "Training Batch [79/782]: Loss 0.00447431206703186\n",
      "Training Batch [80/782]: Loss 0.01953721232712269\n",
      "Training Batch [81/782]: Loss 0.0021576040890067816\n",
      "Training Batch [82/782]: Loss 0.0009556439472362399\n",
      "Training Batch [83/782]: Loss 0.01062499824911356\n",
      "Training Batch [84/782]: Loss 0.0020668725483119488\n",
      "Training Batch [85/782]: Loss 0.012991310097277164\n",
      "Training Batch [86/782]: Loss 0.0011760775232687593\n",
      "Training Batch [87/782]: Loss 0.0002778119232971221\n",
      "Training Batch [88/782]: Loss 0.0008607039926573634\n",
      "Training Batch [89/782]: Loss 0.018350031226873398\n",
      "Training Batch [90/782]: Loss 0.000783315219450742\n",
      "Training Batch [91/782]: Loss 0.002158030867576599\n",
      "Training Batch [92/782]: Loss 0.0013797490391880274\n",
      "Training Batch [93/782]: Loss 0.0031168146524578333\n",
      "Training Batch [94/782]: Loss 0.0008647387148812413\n",
      "Training Batch [95/782]: Loss 0.003524660598486662\n",
      "Training Batch [96/782]: Loss 0.00072619563434273\n",
      "Training Batch [97/782]: Loss 0.016890455037355423\n",
      "Training Batch [98/782]: Loss 0.0013124964898452163\n",
      "Training Batch [99/782]: Loss 0.0004594850179273635\n",
      "Training Batch [100/782]: Loss 0.00029784056823700666\n",
      "Training Batch [101/782]: Loss 0.0007744336617179215\n",
      "Training Batch [102/782]: Loss 0.0007460364722646773\n",
      "Training Batch [103/782]: Loss 0.0028260003309696913\n",
      "Training Batch [104/782]: Loss 0.002486300189048052\n",
      "Training Batch [105/782]: Loss 0.00021170520631130785\n",
      "Training Batch [106/782]: Loss 0.0037684962153434753\n",
      "Training Batch [107/782]: Loss 0.002131293062120676\n",
      "Training Batch [108/782]: Loss 0.00012316049833316356\n",
      "Training Batch [109/782]: Loss 0.012359431944787502\n",
      "Training Batch [110/782]: Loss 0.0009307317668572068\n",
      "Training Batch [111/782]: Loss 0.00035398142063058913\n",
      "Training Batch [112/782]: Loss 0.0011733649298548698\n",
      "Training Batch [113/782]: Loss 0.01060198899358511\n",
      "Training Batch [114/782]: Loss 0.010398691520094872\n",
      "Training Batch [115/782]: Loss 0.00012424690066836774\n",
      "Training Batch [116/782]: Loss 0.00031623331597074866\n",
      "Training Batch [117/782]: Loss 0.0003046959172934294\n",
      "Training Batch [118/782]: Loss 0.0008959496626630425\n",
      "Training Batch [119/782]: Loss 0.0016945648239925504\n",
      "Training Batch [120/782]: Loss 0.0034646166022866964\n",
      "Training Batch [121/782]: Loss 0.0022992421872913837\n",
      "Training Batch [122/782]: Loss 0.0349426232278347\n",
      "Training Batch [123/782]: Loss 0.00017181815928779542\n",
      "Training Batch [124/782]: Loss 0.00835754256695509\n",
      "Training Batch [125/782]: Loss 0.0006386939203366637\n",
      "Training Batch [126/782]: Loss 0.004009486176073551\n",
      "Training Batch [127/782]: Loss 0.0007502338266931474\n",
      "Training Batch [128/782]: Loss 0.0005164495087228715\n",
      "Training Batch [129/782]: Loss 0.003441101871430874\n",
      "Training Batch [130/782]: Loss 0.0010581210954114795\n",
      "Training Batch [131/782]: Loss 0.0014336263993754983\n",
      "Training Batch [132/782]: Loss 0.0010248443577438593\n",
      "Training Batch [133/782]: Loss 0.0004032202123198658\n",
      "Training Batch [134/782]: Loss 0.00047896854812279344\n",
      "Training Batch [135/782]: Loss 0.0055360374972224236\n",
      "Training Batch [136/782]: Loss 0.0005899423267692327\n",
      "Training Batch [137/782]: Loss 0.0010674588847905397\n",
      "Training Batch [138/782]: Loss 0.0008903215057216585\n",
      "Training Batch [139/782]: Loss 6.575192674063146e-05\n",
      "Training Batch [140/782]: Loss 0.00030296071781776845\n",
      "Training Batch [141/782]: Loss 0.0008072775090113282\n",
      "Training Batch [142/782]: Loss 0.002552452962845564\n",
      "Training Batch [143/782]: Loss 0.006618277635425329\n",
      "Training Batch [144/782]: Loss 0.0007682443829253316\n",
      "Training Batch [145/782]: Loss 0.0008013942278921604\n",
      "Training Batch [146/782]: Loss 0.004375824239104986\n",
      "Training Batch [147/782]: Loss 0.0010557741625234485\n",
      "Training Batch [148/782]: Loss 0.0006240059155970812\n",
      "Training Batch [149/782]: Loss 0.0011462497059255838\n",
      "Training Batch [150/782]: Loss 0.0021844999864697456\n",
      "Training Batch [151/782]: Loss 0.0006110795075073838\n",
      "Training Batch [152/782]: Loss 0.00020044513803441077\n",
      "Training Batch [153/782]: Loss 0.0007062447257339954\n",
      "Training Batch [154/782]: Loss 0.0014595401007682085\n",
      "Training Batch [155/782]: Loss 0.008569041267037392\n",
      "Training Batch [156/782]: Loss 0.0012223338708281517\n",
      "Training Batch [157/782]: Loss 0.0005668060039170086\n",
      "Training Batch [158/782]: Loss 0.00788203626871109\n",
      "Training Batch [159/782]: Loss 0.0016184543492272496\n",
      "Training Batch [160/782]: Loss 0.00020021726959384978\n",
      "Training Batch [161/782]: Loss 0.0020176281686872244\n",
      "Training Batch [162/782]: Loss 0.000243017275352031\n",
      "Training Batch [163/782]: Loss 0.0004897398175671697\n",
      "Training Batch [164/782]: Loss 0.004655139520764351\n",
      "Training Batch [165/782]: Loss 0.0010623211273923516\n",
      "Training Batch [166/782]: Loss 0.0008242894546128809\n",
      "Training Batch [167/782]: Loss 0.008538397029042244\n",
      "Training Batch [168/782]: Loss 0.0016224001301452518\n",
      "Training Batch [169/782]: Loss 0.006100526079535484\n",
      "Training Batch [170/782]: Loss 0.026982227340340614\n",
      "Training Batch [171/782]: Loss 0.0008361547370441258\n",
      "Training Batch [172/782]: Loss 0.00037137779872864485\n",
      "Training Batch [173/782]: Loss 0.00018123863264918327\n",
      "Training Batch [174/782]: Loss 0.012300938367843628\n",
      "Training Batch [175/782]: Loss 5.2444949687924236e-05\n",
      "Training Batch [176/782]: Loss 0.0007808391819708049\n",
      "Training Batch [177/782]: Loss 0.001259436598047614\n",
      "Training Batch [178/782]: Loss 0.0002248483942821622\n",
      "Training Batch [179/782]: Loss 0.00018566024664323777\n",
      "Training Batch [180/782]: Loss 0.002414679853245616\n",
      "Training Batch [181/782]: Loss 0.0011120627168565989\n",
      "Training Batch [182/782]: Loss 0.005582363344728947\n",
      "Training Batch [183/782]: Loss 0.0036487895995378494\n",
      "Training Batch [184/782]: Loss 0.0002637931611388922\n",
      "Training Batch [185/782]: Loss 0.0006539445603266358\n",
      "Training Batch [186/782]: Loss 0.003437785431742668\n",
      "Training Batch [187/782]: Loss 0.007428324315696955\n",
      "Training Batch [188/782]: Loss 0.0005500693223439157\n",
      "Training Batch [189/782]: Loss 0.0004014676087535918\n",
      "Training Batch [190/782]: Loss 0.00012187316315248609\n",
      "Training Batch [191/782]: Loss 7.503648521378636e-05\n",
      "Training Batch [192/782]: Loss 0.0014430321753025055\n",
      "Training Batch [193/782]: Loss 0.0005880005192011595\n",
      "Training Batch [194/782]: Loss 0.00223934487439692\n",
      "Training Batch [195/782]: Loss 0.0016237698728218675\n",
      "Training Batch [196/782]: Loss 0.0024394853971898556\n",
      "Training Batch [197/782]: Loss 0.00015855481615290046\n",
      "Training Batch [198/782]: Loss 0.0002830881276167929\n",
      "Training Batch [199/782]: Loss 0.00018968505901284516\n",
      "Training Batch [200/782]: Loss 0.004907885566353798\n",
      "Training Batch [201/782]: Loss 0.009869230911135674\n",
      "Training Batch [202/782]: Loss 0.0011503291316330433\n",
      "Training Batch [203/782]: Loss 0.00028218288207426667\n",
      "Training Batch [204/782]: Loss 0.00012597194290719926\n",
      "Training Batch [205/782]: Loss 0.0014023754047229886\n",
      "Training Batch [206/782]: Loss 0.0049438923597335815\n",
      "Training Batch [207/782]: Loss 0.001677177264355123\n",
      "Training Batch [208/782]: Loss 0.0009494840633124113\n",
      "Training Batch [209/782]: Loss 0.009591027162969112\n",
      "Training Batch [210/782]: Loss 0.00048087278264574707\n",
      "Training Batch [211/782]: Loss 0.0019664308056235313\n",
      "Training Batch [212/782]: Loss 0.0014689482050016522\n",
      "Training Batch [213/782]: Loss 0.0007658724789507687\n",
      "Training Batch [214/782]: Loss 0.000674545532092452\n",
      "Training Batch [215/782]: Loss 0.0010046589886769652\n",
      "Training Batch [216/782]: Loss 0.0004452634311746806\n",
      "Training Batch [217/782]: Loss 0.00038380175828933716\n",
      "Training Batch [218/782]: Loss 0.045870836824178696\n",
      "Training Batch [219/782]: Loss 0.011221264488995075\n",
      "Training Batch [220/782]: Loss 0.0003195741446688771\n",
      "Training Batch [221/782]: Loss 0.0005086350138299167\n",
      "Training Batch [222/782]: Loss 0.0002286049711983651\n",
      "Training Batch [223/782]: Loss 0.0017413606401532888\n",
      "Training Batch [224/782]: Loss 0.0008373925229534507\n",
      "Training Batch [225/782]: Loss 0.001113409292884171\n",
      "Training Batch [226/782]: Loss 0.0013133678585290909\n",
      "Training Batch [227/782]: Loss 0.00018546328647062182\n",
      "Training Batch [228/782]: Loss 0.004379564430564642\n",
      "Training Batch [229/782]: Loss 0.0019436618313193321\n",
      "Training Batch [230/782]: Loss 0.0007797488942742348\n",
      "Training Batch [231/782]: Loss 0.00028105301316827536\n",
      "Training Batch [232/782]: Loss 0.005127494223415852\n",
      "Training Batch [233/782]: Loss 0.0015163036296144128\n",
      "Training Batch [234/782]: Loss 0.0003721129905898124\n",
      "Training Batch [235/782]: Loss 0.00022594205802306533\n",
      "Training Batch [236/782]: Loss 0.00010294593084836379\n",
      "Training Batch [237/782]: Loss 0.0011452436447143555\n",
      "Training Batch [238/782]: Loss 0.0001511423906777054\n",
      "Training Batch [239/782]: Loss 0.001894290093332529\n",
      "Training Batch [240/782]: Loss 0.0018667797558009624\n",
      "Training Batch [241/782]: Loss 0.00023281080939341336\n",
      "Training Batch [242/782]: Loss 0.011474923230707645\n",
      "Training Batch [243/782]: Loss 0.004756746347993612\n",
      "Training Batch [244/782]: Loss 0.0014091855846345425\n",
      "Training Batch [245/782]: Loss 0.0005821173544973135\n",
      "Training Batch [246/782]: Loss 0.00019398661970626563\n",
      "Training Batch [247/782]: Loss 0.0012878048000857234\n",
      "Training Batch [248/782]: Loss 2.1398036551545374e-05\n",
      "Training Batch [249/782]: Loss 0.009544727392494678\n",
      "Training Batch [250/782]: Loss 0.04110696166753769\n",
      "Training Batch [251/782]: Loss 0.0022185014095157385\n",
      "Training Batch [252/782]: Loss 5.4157986596692353e-05\n",
      "Training Batch [253/782]: Loss 0.00017282132466789335\n",
      "Training Batch [254/782]: Loss 0.0045845345593988895\n",
      "Training Batch [255/782]: Loss 0.00030609118402935565\n",
      "Training Batch [256/782]: Loss 0.0003060171438846737\n",
      "Training Batch [257/782]: Loss 0.00035252870293334126\n",
      "Training Batch [258/782]: Loss 0.00012450692884158343\n",
      "Training Batch [259/782]: Loss 0.005829220172017813\n",
      "Training Batch [260/782]: Loss 0.00010052609286503866\n",
      "Training Batch [261/782]: Loss 6.956954166525975e-05\n",
      "Training Batch [262/782]: Loss 0.0182515736669302\n",
      "Training Batch [263/782]: Loss 0.0017890477320179343\n",
      "Training Batch [264/782]: Loss 0.00744561105966568\n",
      "Training Batch [265/782]: Loss 0.01585465483367443\n",
      "Training Batch [266/782]: Loss 0.00031525877420790493\n",
      "Training Batch [267/782]: Loss 7.877073221607134e-05\n",
      "Training Batch [268/782]: Loss 0.00023725305800326169\n",
      "Training Batch [269/782]: Loss 0.00020349967235233635\n",
      "Training Batch [270/782]: Loss 0.003721250221133232\n",
      "Training Batch [271/782]: Loss 0.001206269720569253\n",
      "Training Batch [272/782]: Loss 0.0015583414351567626\n",
      "Training Batch [273/782]: Loss 0.0010883404174819589\n",
      "Training Batch [274/782]: Loss 0.0003531101974658668\n",
      "Training Batch [275/782]: Loss 0.015083592385053635\n",
      "Training Batch [276/782]: Loss 0.002898253733292222\n",
      "Training Batch [277/782]: Loss 0.007641172502189875\n",
      "Training Batch [278/782]: Loss 0.012584022246301174\n",
      "Training Batch [279/782]: Loss 0.003347742836922407\n",
      "Training Batch [280/782]: Loss 0.004668137524276972\n",
      "Training Batch [281/782]: Loss 0.00580939743667841\n",
      "Training Batch [282/782]: Loss 0.00017905194545164704\n",
      "Training Batch [283/782]: Loss 0.00011819244537036866\n",
      "Training Batch [284/782]: Loss 0.017845910042524338\n",
      "Training Batch [285/782]: Loss 8.751672430662438e-05\n",
      "Training Batch [286/782]: Loss 0.0028991526924073696\n",
      "Training Batch [287/782]: Loss 0.00218038447201252\n",
      "Training Batch [288/782]: Loss 0.00048373540630564094\n",
      "Training Batch [289/782]: Loss 0.00017372224829159677\n",
      "Training Batch [290/782]: Loss 0.0030535936821252108\n",
      "Training Batch [291/782]: Loss 0.00024241373466793448\n",
      "Training Batch [292/782]: Loss 0.00013770529767498374\n",
      "Training Batch [293/782]: Loss 0.0006102899787947536\n",
      "Training Batch [294/782]: Loss 0.0005573707167059183\n",
      "Training Batch [295/782]: Loss 0.0005475983489304781\n",
      "Training Batch [296/782]: Loss 0.0019755472894757986\n",
      "Training Batch [297/782]: Loss 0.004348371177911758\n",
      "Training Batch [298/782]: Loss 0.0006775493966415524\n",
      "Training Batch [299/782]: Loss 0.015141375362873077\n",
      "Training Batch [300/782]: Loss 0.0013292456278577447\n",
      "Training Batch [301/782]: Loss 0.003980521112680435\n",
      "Training Batch [302/782]: Loss 0.00013183499686419964\n",
      "Training Batch [303/782]: Loss 0.0005163506721146405\n",
      "Training Batch [304/782]: Loss 0.007276847492903471\n",
      "Training Batch [305/782]: Loss 0.0009877976262941957\n",
      "Training Batch [306/782]: Loss 0.02994370460510254\n",
      "Training Batch [307/782]: Loss 0.000328498164890334\n",
      "Training Batch [308/782]: Loss 0.0005263598286546767\n",
      "Training Batch [309/782]: Loss 3.139580076094717e-05\n",
      "Training Batch [310/782]: Loss 0.0017140655545517802\n",
      "Training Batch [311/782]: Loss 0.001871146378107369\n",
      "Training Batch [312/782]: Loss 0.001956193009391427\n",
      "Training Batch [313/782]: Loss 0.0004691426001954824\n",
      "Training Batch [314/782]: Loss 9.013660746859387e-05\n",
      "Training Batch [315/782]: Loss 0.0002030831965385005\n",
      "Training Batch [316/782]: Loss 0.0011986999306827784\n",
      "Training Batch [317/782]: Loss 0.00011492152407299727\n",
      "Training Batch [318/782]: Loss 0.004956924822181463\n",
      "Training Batch [319/782]: Loss 0.000249243137659505\n",
      "Training Batch [320/782]: Loss 6.946605572011322e-05\n",
      "Training Batch [321/782]: Loss 0.0059049092233181\n",
      "Training Batch [322/782]: Loss 0.0002143731398973614\n",
      "Training Batch [323/782]: Loss 9.541592589812353e-05\n",
      "Training Batch [324/782]: Loss 0.000260085187619552\n",
      "Training Batch [325/782]: Loss 0.010504437610507011\n",
      "Training Batch [326/782]: Loss 0.001698288251645863\n",
      "Training Batch [327/782]: Loss 5.6649216276127845e-05\n",
      "Training Batch [328/782]: Loss 0.0009271031012758613\n",
      "Training Batch [329/782]: Loss 0.01010537426918745\n",
      "Training Batch [330/782]: Loss 0.0008688625530339777\n",
      "Training Batch [331/782]: Loss 0.0028780552092939615\n",
      "Training Batch [332/782]: Loss 4.784881093655713e-05\n",
      "Training Batch [333/782]: Loss 0.00012168123066658154\n",
      "Training Batch [334/782]: Loss 0.00012970779789611697\n",
      "Training Batch [335/782]: Loss 0.0002745466190390289\n",
      "Training Batch [336/782]: Loss 0.0007971410523168743\n",
      "Training Batch [337/782]: Loss 0.0013302387669682503\n",
      "Training Batch [338/782]: Loss 0.002216434571892023\n",
      "Training Batch [339/782]: Loss 0.0017265701899304986\n",
      "Training Batch [340/782]: Loss 0.0009077736176550388\n",
      "Training Batch [341/782]: Loss 0.01585378497838974\n",
      "Training Batch [342/782]: Loss 0.0016416081925854087\n",
      "Training Batch [343/782]: Loss 0.00022568383428733796\n",
      "Training Batch [344/782]: Loss 0.0015813790960237384\n",
      "Training Batch [345/782]: Loss 5.1257797167636454e-05\n",
      "Training Batch [346/782]: Loss 0.0001495293981861323\n",
      "Training Batch [347/782]: Loss 0.00033444142900407314\n",
      "Training Batch [348/782]: Loss 0.003963035997003317\n",
      "Training Batch [349/782]: Loss 0.008021258749067783\n",
      "Training Batch [350/782]: Loss 0.006151717156171799\n",
      "Training Batch [351/782]: Loss 0.01937444880604744\n",
      "Training Batch [352/782]: Loss 7.954522152431309e-05\n",
      "Training Batch [353/782]: Loss 0.0010723571758717299\n",
      "Training Batch [354/782]: Loss 0.001260190736502409\n",
      "Training Batch [355/782]: Loss 7.317135896300897e-05\n",
      "Training Batch [356/782]: Loss 0.0024036599788814783\n",
      "Training Batch [357/782]: Loss 0.004404002800583839\n",
      "Training Batch [358/782]: Loss 0.00689867464825511\n",
      "Training Batch [359/782]: Loss 0.00046210215077735484\n",
      "Training Batch [360/782]: Loss 0.0005754559533670545\n",
      "Training Batch [361/782]: Loss 0.0013116735499352217\n",
      "Training Batch [362/782]: Loss 0.001277224044315517\n",
      "Training Batch [363/782]: Loss 0.0001829192042350769\n",
      "Training Batch [364/782]: Loss 0.0001923575036926195\n",
      "Training Batch [365/782]: Loss 0.0005347238620743155\n",
      "Training Batch [366/782]: Loss 0.00023803708609193563\n",
      "Training Batch [367/782]: Loss 0.0003065470664296299\n",
      "Training Batch [368/782]: Loss 0.0022483495995402336\n",
      "Training Batch [369/782]: Loss 0.00037711122422479093\n",
      "Training Batch [370/782]: Loss 0.0002344634267501533\n",
      "Training Batch [371/782]: Loss 0.0048773158341646194\n",
      "Training Batch [372/782]: Loss 7.588747394038364e-05\n",
      "Training Batch [373/782]: Loss 3.5532320907805115e-05\n",
      "Training Batch [374/782]: Loss 0.0021838995162397623\n",
      "Training Batch [375/782]: Loss 0.007703815586864948\n",
      "Training Batch [376/782]: Loss 0.031061941757798195\n",
      "Training Batch [377/782]: Loss 0.0007654287037439644\n",
      "Training Batch [378/782]: Loss 0.000503713614307344\n",
      "Training Batch [379/782]: Loss 0.0008827454294078052\n",
      "Training Batch [380/782]: Loss 0.00018344068666920066\n",
      "Training Batch [381/782]: Loss 0.0001956810156116262\n",
      "Training Batch [382/782]: Loss 0.012304681353271008\n",
      "Training Batch [383/782]: Loss 0.0008854158222675323\n",
      "Training Batch [384/782]: Loss 0.0009363266872242093\n",
      "Training Batch [385/782]: Loss 0.001341755734756589\n",
      "Training Batch [386/782]: Loss 0.0003393067163415253\n",
      "Training Batch [387/782]: Loss 6.837165710749105e-05\n",
      "Training Batch [388/782]: Loss 0.009941251017153263\n",
      "Training Batch [389/782]: Loss 0.00016685668379068375\n",
      "Training Batch [390/782]: Loss 0.0008912403136491776\n",
      "Training Batch [391/782]: Loss 0.004617201630026102\n",
      "Training Batch [392/782]: Loss 0.002488345606252551\n",
      "Training Batch [393/782]: Loss 0.008525407873094082\n",
      "Training Batch [394/782]: Loss 0.0005103781004436314\n",
      "Training Batch [395/782]: Loss 0.00016894108557607979\n",
      "Training Batch [396/782]: Loss 0.0015001497231423855\n",
      "Training Batch [397/782]: Loss 0.026077525690197945\n",
      "Training Batch [398/782]: Loss 0.0008986830362118781\n",
      "Training Batch [399/782]: Loss 0.00016795785631984472\n",
      "Training Batch [400/782]: Loss 0.0008578550186939538\n",
      "Training Batch [401/782]: Loss 0.006322066765278578\n",
      "Training Batch [402/782]: Loss 1.779626472853124e-05\n",
      "Training Batch [403/782]: Loss 0.00019369299116078764\n",
      "Training Batch [404/782]: Loss 0.00032252780511043966\n",
      "Training Batch [405/782]: Loss 0.0016763977473601699\n",
      "Training Batch [406/782]: Loss 0.00012174360017525032\n",
      "Training Batch [407/782]: Loss 0.0001827783416956663\n",
      "Training Batch [408/782]: Loss 7.821058534318581e-05\n",
      "Training Batch [409/782]: Loss 0.0016237274976447225\n",
      "Training Batch [410/782]: Loss 0.0012423261068761349\n",
      "Training Batch [411/782]: Loss 0.0001341501047136262\n",
      "Training Batch [412/782]: Loss 0.0005545816966332495\n",
      "Training Batch [413/782]: Loss 0.00010136020136997104\n",
      "Training Batch [414/782]: Loss 0.00046098686289042234\n",
      "Training Batch [415/782]: Loss 9.696940833237022e-05\n",
      "Training Batch [416/782]: Loss 0.00023708048684056848\n",
      "Training Batch [417/782]: Loss 0.0013489832635968924\n",
      "Training Batch [418/782]: Loss 0.0007300229626707733\n",
      "Training Batch [419/782]: Loss 0.0004617025551851839\n",
      "Training Batch [420/782]: Loss 0.00027641747146844864\n",
      "Training Batch [421/782]: Loss 0.00014446220302488655\n",
      "Training Batch [422/782]: Loss 0.001612361753359437\n",
      "Training Batch [423/782]: Loss 0.0001241100690094754\n",
      "Training Batch [424/782]: Loss 0.0033185575157403946\n",
      "Training Batch [425/782]: Loss 0.00038813138962723315\n",
      "Training Batch [426/782]: Loss 0.00019724912999663502\n",
      "Training Batch [427/782]: Loss 0.03347842022776604\n",
      "Training Batch [428/782]: Loss 0.00012641165812965482\n",
      "Training Batch [429/782]: Loss 0.000833209662232548\n",
      "Training Batch [430/782]: Loss 0.0007948215934447944\n",
      "Training Batch [431/782]: Loss 0.005003301426768303\n",
      "Training Batch [432/782]: Loss 0.0005373152671381831\n",
      "Training Batch [433/782]: Loss 0.0009617640753276646\n",
      "Training Batch [434/782]: Loss 0.00047228994662873447\n",
      "Training Batch [435/782]: Loss 0.004688161890953779\n",
      "Training Batch [436/782]: Loss 0.031716473400592804\n",
      "Training Batch [437/782]: Loss 0.0004113582253921777\n",
      "Training Batch [438/782]: Loss 0.003393928986042738\n",
      "Training Batch [439/782]: Loss 0.0046731033362448215\n",
      "Training Batch [440/782]: Loss 0.0005145777831785381\n",
      "Training Batch [441/782]: Loss 0.0030742455273866653\n",
      "Training Batch [442/782]: Loss 0.0019182901596650481\n",
      "Training Batch [443/782]: Loss 0.03990546241402626\n",
      "Training Batch [444/782]: Loss 0.001794707728549838\n",
      "Training Batch [445/782]: Loss 0.0001060804643202573\n",
      "Training Batch [446/782]: Loss 0.0016684446018189192\n",
      "Training Batch [447/782]: Loss 0.000842107692733407\n",
      "Training Batch [448/782]: Loss 0.002893495839089155\n",
      "Training Batch [449/782]: Loss 0.0036225346848368645\n",
      "Training Batch [450/782]: Loss 0.036274254322052\n",
      "Training Batch [451/782]: Loss 0.003254530718550086\n",
      "Training Batch [452/782]: Loss 0.004209729842841625\n",
      "Training Batch [453/782]: Loss 0.0025391429662704468\n",
      "Training Batch [454/782]: Loss 0.0008813426247797906\n",
      "Training Batch [455/782]: Loss 0.010213282890617847\n",
      "Training Batch [456/782]: Loss 0.013833262957632542\n",
      "Training Batch [457/782]: Loss 0.02185332216322422\n",
      "Training Batch [458/782]: Loss 0.0035306140780448914\n",
      "Training Batch [459/782]: Loss 0.00706640537828207\n",
      "Training Batch [460/782]: Loss 0.0005074354121461511\n",
      "Training Batch [461/782]: Loss 0.001178805367089808\n",
      "Training Batch [462/782]: Loss 0.0013842553598806262\n",
      "Training Batch [463/782]: Loss 0.0001870925771072507\n",
      "Training Batch [464/782]: Loss 0.03672512248158455\n",
      "Training Batch [465/782]: Loss 0.0017709118546918035\n",
      "Training Batch [466/782]: Loss 0.00019061111379414797\n",
      "Training Batch [467/782]: Loss 0.0003475485136732459\n",
      "Training Batch [468/782]: Loss 0.00533688347786665\n",
      "Training Batch [469/782]: Loss 0.0003099326277151704\n",
      "Training Batch [470/782]: Loss 0.00046701659448444843\n",
      "Training Batch [471/782]: Loss 0.002133239759132266\n",
      "Training Batch [472/782]: Loss 0.00142016657628119\n",
      "Training Batch [473/782]: Loss 0.015854021534323692\n",
      "Training Batch [474/782]: Loss 0.0011788465781137347\n",
      "Training Batch [475/782]: Loss 0.01056011114269495\n",
      "Training Batch [476/782]: Loss 0.0033695667516440153\n",
      "Training Batch [477/782]: Loss 0.12358269095420837\n",
      "Training Batch [478/782]: Loss 0.0006539274472743273\n",
      "Training Batch [479/782]: Loss 0.0021177087910473347\n",
      "Training Batch [480/782]: Loss 0.002104160375893116\n",
      "Training Batch [481/782]: Loss 0.0026038032956421375\n",
      "Training Batch [482/782]: Loss 0.002947724424302578\n",
      "Training Batch [483/782]: Loss 0.0014993968652561307\n",
      "Training Batch [484/782]: Loss 0.0006399834528565407\n",
      "Training Batch [485/782]: Loss 0.0007429436664097011\n",
      "Training Batch [486/782]: Loss 0.0008270323160104454\n",
      "Training Batch [487/782]: Loss 0.0053780898451805115\n",
      "Training Batch [488/782]: Loss 0.0024783138651400805\n",
      "Training Batch [489/782]: Loss 0.0007872782880440354\n",
      "Training Batch [490/782]: Loss 0.00013448332902044058\n",
      "Training Batch [491/782]: Loss 0.001662677270360291\n",
      "Training Batch [492/782]: Loss 0.0010766416089609265\n",
      "Training Batch [493/782]: Loss 0.0022163516841828823\n",
      "Training Batch [494/782]: Loss 0.0016086375107988715\n",
      "Training Batch [495/782]: Loss 0.07388471066951752\n",
      "Training Batch [496/782]: Loss 0.0005511303315870464\n",
      "Training Batch [497/782]: Loss 0.0003610735002439469\n",
      "Training Batch [498/782]: Loss 0.002480793744325638\n",
      "Training Batch [499/782]: Loss 0.0017981058917939663\n",
      "Training Batch [500/782]: Loss 0.000371696864021942\n",
      "Training Batch [501/782]: Loss 0.00860260333865881\n",
      "Training Batch [502/782]: Loss 0.0009781140834093094\n",
      "Training Batch [503/782]: Loss 0.007541436702013016\n",
      "Training Batch [504/782]: Loss 0.0019216254586353898\n",
      "Training Batch [505/782]: Loss 0.000343674881150946\n",
      "Training Batch [506/782]: Loss 0.0006148365209810436\n",
      "Training Batch [507/782]: Loss 0.12956686317920685\n",
      "Training Batch [508/782]: Loss 0.0019024937646463513\n",
      "Training Batch [509/782]: Loss 0.00036022847052663565\n",
      "Training Batch [510/782]: Loss 0.001383050112053752\n",
      "Training Batch [511/782]: Loss 0.011240087449550629\n",
      "Training Batch [512/782]: Loss 0.000787084165494889\n",
      "Training Batch [513/782]: Loss 0.0025364658795297146\n",
      "Training Batch [514/782]: Loss 0.14211229979991913\n",
      "Training Batch [515/782]: Loss 0.0037544909864664078\n",
      "Training Batch [516/782]: Loss 0.00303657166659832\n",
      "Training Batch [517/782]: Loss 0.0007397235604003072\n",
      "Training Batch [518/782]: Loss 0.00023507630976382643\n",
      "Training Batch [519/782]: Loss 0.009918991476297379\n",
      "Training Batch [520/782]: Loss 0.0017247149953618646\n",
      "Training Batch [521/782]: Loss 0.0007777155842632055\n",
      "Training Batch [522/782]: Loss 0.0008088501635938883\n",
      "Training Batch [523/782]: Loss 0.01691216230392456\n",
      "Training Batch [524/782]: Loss 0.0028842741157859564\n",
      "Training Batch [525/782]: Loss 0.0016017901943996549\n",
      "Training Batch [526/782]: Loss 0.03972762078046799\n",
      "Training Batch [527/782]: Loss 0.0010180252138525248\n",
      "Training Batch [528/782]: Loss 0.0007034086738713086\n",
      "Training Batch [529/782]: Loss 0.05655389279127121\n",
      "Training Batch [530/782]: Loss 0.07411263138055801\n",
      "Training Batch [531/782]: Loss 0.003356153843924403\n",
      "Training Batch [532/782]: Loss 0.0016305478056892753\n",
      "Training Batch [533/782]: Loss 0.002353699877858162\n",
      "Training Batch [534/782]: Loss 0.0755462571978569\n",
      "Training Batch [535/782]: Loss 0.003113004146143794\n",
      "Training Batch [536/782]: Loss 0.00021443283185362816\n",
      "Training Batch [537/782]: Loss 0.002790984697639942\n",
      "Training Batch [538/782]: Loss 0.000499934540130198\n",
      "Training Batch [539/782]: Loss 0.007466950919479132\n",
      "Training Batch [540/782]: Loss 0.0006049094372428954\n",
      "Training Batch [541/782]: Loss 0.0003335780056659132\n",
      "Training Batch [542/782]: Loss 0.0007915047463029623\n",
      "Training Batch [543/782]: Loss 0.03183239698410034\n",
      "Training Batch [544/782]: Loss 0.0008140329155139625\n",
      "Training Batch [545/782]: Loss 0.01144975796341896\n",
      "Training Batch [546/782]: Loss 0.00034293674980290234\n",
      "Training Batch [547/782]: Loss 0.0333562046289444\n",
      "Training Batch [548/782]: Loss 0.004160451702773571\n",
      "Training Batch [549/782]: Loss 0.0032039922662079334\n",
      "Training Batch [550/782]: Loss 0.015432612970471382\n",
      "Training Batch [551/782]: Loss 0.010515923611819744\n",
      "Training Batch [552/782]: Loss 0.006656387355178595\n",
      "Training Batch [553/782]: Loss 0.0009816905949264765\n",
      "Training Batch [554/782]: Loss 0.04551820829510689\n",
      "Training Batch [555/782]: Loss 0.052637748420238495\n",
      "Training Batch [556/782]: Loss 0.0011270832037553191\n",
      "Training Batch [557/782]: Loss 0.005054758861660957\n",
      "Training Batch [558/782]: Loss 0.0007799360901117325\n",
      "Training Batch [559/782]: Loss 0.0005995372193865478\n",
      "Training Batch [560/782]: Loss 0.0018478815909475088\n",
      "Training Batch [561/782]: Loss 0.00039924983866512775\n",
      "Training Batch [562/782]: Loss 0.008220578543841839\n",
      "Training Batch [563/782]: Loss 0.018754009157419205\n",
      "Training Batch [564/782]: Loss 0.022942814975976944\n",
      "Training Batch [565/782]: Loss 0.00464783888310194\n",
      "Training Batch [566/782]: Loss 0.0009614123264327645\n",
      "Training Batch [567/782]: Loss 0.003861902514472604\n",
      "Training Batch [568/782]: Loss 0.04352419078350067\n",
      "Training Batch [569/782]: Loss 0.00046285943244583905\n",
      "Training Batch [570/782]: Loss 0.0007453992730006576\n",
      "Training Batch [571/782]: Loss 0.0015749179292470217\n",
      "Training Batch [572/782]: Loss 0.0008515146910212934\n",
      "Training Batch [573/782]: Loss 0.0004913323791697621\n",
      "Training Batch [574/782]: Loss 0.003417905420064926\n",
      "Training Batch [575/782]: Loss 0.0009594213333912194\n",
      "Training Batch [576/782]: Loss 0.05501411110162735\n",
      "Training Batch [577/782]: Loss 0.01221944484859705\n",
      "Training Batch [578/782]: Loss 0.00986270047724247\n",
      "Training Batch [579/782]: Loss 0.001648863311856985\n",
      "Training Batch [580/782]: Loss 0.004245697055011988\n",
      "Training Batch [581/782]: Loss 0.015263686887919903\n",
      "Training Batch [582/782]: Loss 0.00045627905637957156\n",
      "Training Batch [583/782]: Loss 0.0031086765229701996\n",
      "Training Batch [584/782]: Loss 0.0074353753589093685\n",
      "Training Batch [585/782]: Loss 0.037094518542289734\n",
      "Training Batch [586/782]: Loss 0.07613525539636612\n",
      "Training Batch [587/782]: Loss 0.030329478904604912\n",
      "Training Batch [588/782]: Loss 0.003432749304920435\n",
      "Training Batch [589/782]: Loss 0.0063416422344744205\n",
      "Training Batch [590/782]: Loss 0.0015452969819307327\n",
      "Training Batch [591/782]: Loss 0.0002419501543045044\n",
      "Training Batch [592/782]: Loss 0.00290284538641572\n",
      "Training Batch [593/782]: Loss 0.00047935725888237357\n",
      "Training Batch [594/782]: Loss 0.03436008840799332\n",
      "Training Batch [595/782]: Loss 0.008248607628047466\n",
      "Training Batch [596/782]: Loss 0.0022786108311265707\n",
      "Training Batch [597/782]: Loss 0.0009252237505279481\n",
      "Training Batch [598/782]: Loss 0.00121766934171319\n",
      "Training Batch [599/782]: Loss 0.0012023474555462599\n",
      "Training Batch [600/782]: Loss 0.0009082760661840439\n",
      "Training Batch [601/782]: Loss 0.00042988010682165623\n",
      "Training Batch [602/782]: Loss 0.014609375968575478\n",
      "Training Batch [603/782]: Loss 0.0006778302486054599\n",
      "Training Batch [604/782]: Loss 0.0018934319959953427\n",
      "Training Batch [605/782]: Loss 0.0014564369339495897\n",
      "Training Batch [606/782]: Loss 0.019577166065573692\n",
      "Training Batch [607/782]: Loss 0.015943752601742744\n",
      "Training Batch [608/782]: Loss 0.0003997175954282284\n",
      "Training Batch [609/782]: Loss 0.0010259969858452678\n",
      "Training Batch [610/782]: Loss 0.0015729466686025262\n",
      "Training Batch [611/782]: Loss 0.0013202939881011844\n",
      "Training Batch [612/782]: Loss 0.0007966113043949008\n",
      "Training Batch [613/782]: Loss 0.00130518211517483\n",
      "Training Batch [614/782]: Loss 0.003931061364710331\n",
      "Training Batch [615/782]: Loss 0.0010335197439417243\n",
      "Training Batch [616/782]: Loss 0.0008072216878645122\n",
      "Training Batch [617/782]: Loss 0.005499601364135742\n",
      "Training Batch [618/782]: Loss 0.003192852484062314\n",
      "Training Batch [619/782]: Loss 0.0014505728613585234\n",
      "Training Batch [620/782]: Loss 0.0005561486468650401\n",
      "Training Batch [621/782]: Loss 0.004680931102484465\n",
      "Training Batch [622/782]: Loss 0.004367676097899675\n",
      "Training Batch [623/782]: Loss 0.002321918960660696\n",
      "Training Batch [624/782]: Loss 0.06928396970033646\n",
      "Training Batch [625/782]: Loss 0.010878141038119793\n",
      "Training Batch [626/782]: Loss 0.000802064489107579\n",
      "Training Batch [627/782]: Loss 0.005834769923239946\n",
      "Training Batch [628/782]: Loss 0.005422299727797508\n",
      "Training Batch [629/782]: Loss 0.008057273924350739\n",
      "Training Batch [630/782]: Loss 0.000687055813614279\n",
      "Training Batch [631/782]: Loss 0.002925295615568757\n",
      "Training Batch [632/782]: Loss 0.010847542434930801\n",
      "Training Batch [633/782]: Loss 0.0010208658641204238\n",
      "Training Batch [634/782]: Loss 0.003352945437654853\n",
      "Training Batch [635/782]: Loss 0.0005038257222622633\n",
      "Training Batch [636/782]: Loss 0.0013622839469462633\n",
      "Training Batch [637/782]: Loss 0.0017570314230397344\n",
      "Training Batch [638/782]: Loss 0.0007454255246557295\n",
      "Training Batch [639/782]: Loss 0.019227784126996994\n",
      "Training Batch [640/782]: Loss 0.004246939439326525\n",
      "Training Batch [641/782]: Loss 0.0045580314472317696\n",
      "Training Batch [642/782]: Loss 0.0009719700901769102\n",
      "Training Batch [643/782]: Loss 0.0015850769123062491\n",
      "Training Batch [644/782]: Loss 0.0015833877259865403\n",
      "Training Batch [645/782]: Loss 0.003612011903896928\n",
      "Training Batch [646/782]: Loss 0.00026752264238893986\n",
      "Training Batch [647/782]: Loss 0.0067115649580955505\n",
      "Training Batch [648/782]: Loss 0.002707485342398286\n",
      "Training Batch [649/782]: Loss 0.001717161270789802\n",
      "Training Batch [650/782]: Loss 6.196232425281778e-05\n",
      "Training Batch [651/782]: Loss 0.0003627818950917572\n",
      "Training Batch [652/782]: Loss 0.002260928973555565\n",
      "Training Batch [653/782]: Loss 0.0026001669466495514\n",
      "Training Batch [654/782]: Loss 0.0020946457516402006\n",
      "Training Batch [655/782]: Loss 0.0007678639376536012\n",
      "Training Batch [656/782]: Loss 0.00010421937622595578\n",
      "Training Batch [657/782]: Loss 0.09399106353521347\n",
      "Training Batch [658/782]: Loss 0.007266962435096502\n",
      "Training Batch [659/782]: Loss 0.0008117310935631394\n",
      "Training Batch [660/782]: Loss 0.00037768876063637435\n",
      "Training Batch [661/782]: Loss 0.005876700393855572\n",
      "Training Batch [662/782]: Loss 0.0018483848543837667\n",
      "Training Batch [663/782]: Loss 0.017923645675182343\n",
      "Training Batch [664/782]: Loss 0.00024165160721167922\n",
      "Training Batch [665/782]: Loss 0.0009366463054902852\n",
      "Training Batch [666/782]: Loss 0.001663213362917304\n",
      "Training Batch [667/782]: Loss 0.0005492652417160571\n",
      "Training Batch [668/782]: Loss 0.004977852571755648\n",
      "Training Batch [669/782]: Loss 0.0013758420245721936\n",
      "Training Batch [670/782]: Loss 0.00013030559057369828\n",
      "Training Batch [671/782]: Loss 0.00034979224437847733\n",
      "Training Batch [672/782]: Loss 0.01507458183914423\n",
      "Training Batch [673/782]: Loss 0.0009813141077756882\n",
      "Training Batch [674/782]: Loss 0.010570570826530457\n",
      "Training Batch [675/782]: Loss 0.0007695018430240452\n",
      "Training Batch [676/782]: Loss 0.016145607456564903\n",
      "Training Batch [677/782]: Loss 0.00016145365952979773\n",
      "Training Batch [678/782]: Loss 0.0027294980827718973\n",
      "Training Batch [679/782]: Loss 0.005229891277849674\n",
      "Training Batch [680/782]: Loss 0.03902720659971237\n",
      "Training Batch [681/782]: Loss 0.0002834250626619905\n",
      "Training Batch [682/782]: Loss 0.03490040451288223\n",
      "Training Batch [683/782]: Loss 0.0001779510930646211\n",
      "Training Batch [684/782]: Loss 0.00048475302173756063\n",
      "Training Batch [685/782]: Loss 9.29455563891679e-05\n",
      "Training Batch [686/782]: Loss 0.0006213177694007754\n",
      "Training Batch [687/782]: Loss 0.00037282536504790187\n",
      "Training Batch [688/782]: Loss 7.591291068820283e-05\n",
      "Training Batch [689/782]: Loss 0.003156553488224745\n",
      "Training Batch [690/782]: Loss 0.03047945164144039\n",
      "Training Batch [691/782]: Loss 0.05545080080628395\n",
      "Training Batch [692/782]: Loss 0.001064393436536193\n",
      "Training Batch [693/782]: Loss 0.0003082611074205488\n",
      "Training Batch [694/782]: Loss 0.016209565103054047\n",
      "Training Batch [695/782]: Loss 0.004191771149635315\n",
      "Training Batch [696/782]: Loss 0.000570846488699317\n",
      "Training Batch [697/782]: Loss 0.023094529286026955\n",
      "Training Batch [698/782]: Loss 0.00015836766397114843\n",
      "Training Batch [699/782]: Loss 0.023210307583212852\n",
      "Training Batch [700/782]: Loss 0.029803458601236343\n",
      "Training Batch [701/782]: Loss 0.0006583582144230604\n",
      "Training Batch [702/782]: Loss 0.00031672880868427455\n",
      "Training Batch [703/782]: Loss 0.000646763073746115\n",
      "Training Batch [704/782]: Loss 0.005200670100748539\n",
      "Training Batch [705/782]: Loss 0.0009541573817841709\n",
      "Training Batch [706/782]: Loss 0.00041933622560463846\n",
      "Training Batch [707/782]: Loss 0.002288807649165392\n",
      "Training Batch [708/782]: Loss 0.00286415359005332\n",
      "Training Batch [709/782]: Loss 0.10909680277109146\n",
      "Training Batch [710/782]: Loss 0.0004602088883984834\n",
      "Training Batch [711/782]: Loss 0.004799020476639271\n",
      "Training Batch [712/782]: Loss 0.0003572139539755881\n",
      "Training Batch [713/782]: Loss 0.03643641993403435\n",
      "Training Batch [714/782]: Loss 0.0001636656525079161\n",
      "Training Batch [715/782]: Loss 0.0008194846450351179\n",
      "Training Batch [716/782]: Loss 0.001600368763320148\n",
      "Training Batch [717/782]: Loss 0.0038873576559126377\n",
      "Training Batch [718/782]: Loss 0.004552727099508047\n",
      "Training Batch [719/782]: Loss 0.0006837538676336408\n",
      "Training Batch [720/782]: Loss 0.02548295445740223\n",
      "Training Batch [721/782]: Loss 0.014868427067995071\n",
      "Training Batch [722/782]: Loss 0.00021061433653812855\n",
      "Training Batch [723/782]: Loss 0.0010268532205373049\n",
      "Training Batch [724/782]: Loss 0.000554430007468909\n",
      "Training Batch [725/782]: Loss 0.00047169317258521914\n",
      "Training Batch [726/782]: Loss 0.011030708439648151\n",
      "Training Batch [727/782]: Loss 0.002594882622361183\n",
      "Training Batch [728/782]: Loss 0.00916866771876812\n",
      "Training Batch [729/782]: Loss 0.0031701065599918365\n",
      "Training Batch [730/782]: Loss 0.0029916800558567047\n",
      "Training Batch [731/782]: Loss 0.0027398932725191116\n",
      "Training Batch [732/782]: Loss 0.004173281602561474\n",
      "Training Batch [733/782]: Loss 0.0005847010761499405\n",
      "Training Batch [734/782]: Loss 0.00025821448070928454\n",
      "Training Batch [735/782]: Loss 0.00665438175201416\n",
      "Training Batch [736/782]: Loss 0.015461422502994537\n",
      "Training Batch [737/782]: Loss 0.004594558849930763\n",
      "Training Batch [738/782]: Loss 0.014469119720160961\n",
      "Training Batch [739/782]: Loss 0.0012703967513516545\n",
      "Training Batch [740/782]: Loss 0.002404020866379142\n",
      "Training Batch [741/782]: Loss 0.013806776143610477\n",
      "Training Batch [742/782]: Loss 0.0014392167795449495\n",
      "Training Batch [743/782]: Loss 0.0018909939099103212\n",
      "Training Batch [744/782]: Loss 0.0015891862567514181\n",
      "Training Batch [745/782]: Loss 0.005724679213017225\n",
      "Training Batch [746/782]: Loss 0.0004957889323122799\n",
      "Training Batch [747/782]: Loss 0.08062224090099335\n",
      "Training Batch [748/782]: Loss 0.031111840158700943\n",
      "Training Batch [749/782]: Loss 0.0020054341293871403\n",
      "Training Batch [750/782]: Loss 0.005219406448304653\n",
      "Training Batch [751/782]: Loss 0.0008239542949013412\n",
      "Training Batch [752/782]: Loss 0.0037030293606221676\n",
      "Training Batch [753/782]: Loss 0.004874909296631813\n",
      "Training Batch [754/782]: Loss 0.001473124953918159\n",
      "Training Batch [755/782]: Loss 0.0002196818240918219\n",
      "Training Batch [756/782]: Loss 0.0006397117394953966\n",
      "Training Batch [757/782]: Loss 0.05347482115030289\n",
      "Training Batch [758/782]: Loss 0.002091781934723258\n",
      "Training Batch [759/782]: Loss 0.0036287428811192513\n",
      "Training Batch [760/782]: Loss 0.028495607897639275\n",
      "Training Batch [761/782]: Loss 0.0025933668948709965\n",
      "Training Batch [762/782]: Loss 0.012623689137399197\n",
      "Training Batch [763/782]: Loss 0.0008821523515507579\n",
      "Training Batch [764/782]: Loss 0.009744510985910892\n",
      "Training Batch [765/782]: Loss 0.018473098054528236\n",
      "Training Batch [766/782]: Loss 0.012698285281658173\n",
      "Training Batch [767/782]: Loss 0.006092031951993704\n",
      "Training Batch [768/782]: Loss 0.0009292573668062687\n",
      "Training Batch [769/782]: Loss 0.003419889137148857\n",
      "Training Batch [770/782]: Loss 0.005823682527989149\n",
      "Training Batch [771/782]: Loss 0.007758874911814928\n",
      "Training Batch [772/782]: Loss 0.026349691674113274\n",
      "Training Batch [773/782]: Loss 0.0018405003938823938\n",
      "Training Batch [774/782]: Loss 0.01617496833205223\n",
      "Training Batch [775/782]: Loss 0.07954130321741104\n",
      "Training Batch [776/782]: Loss 0.003299835603684187\n",
      "Training Batch [777/782]: Loss 0.012437804602086544\n",
      "Training Batch [778/782]: Loss 0.003150079632177949\n",
      "Training Batch [779/782]: Loss 0.000498624867759645\n",
      "Training Batch [780/782]: Loss 0.02578827552497387\n",
      "Training Batch [781/782]: Loss 0.008921201340854168\n",
      "Training Batch [782/782]: Loss 0.12857885658740997\n",
      "Epoch 30 - Train Loss: 0.0068\n",
      "*********  Epoch 31/50  *********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Batch [1/782]: Loss 0.0019755023531615734\n",
      "Training Batch [2/782]: Loss 0.001226123538799584\n",
      "Training Batch [3/782]: Loss 0.0023182264994829893\n",
      "Training Batch [4/782]: Loss 0.003525844542309642\n",
      "Training Batch [5/782]: Loss 0.00010720214049797505\n",
      "Training Batch [6/782]: Loss 0.0005947884055785835\n",
      "Training Batch [7/782]: Loss 0.0010722620645537972\n",
      "Training Batch [8/782]: Loss 0.0021413967479020357\n",
      "Training Batch [9/782]: Loss 0.05060046166181564\n",
      "Training Batch [10/782]: Loss 0.010898370295763016\n",
      "Training Batch [11/782]: Loss 0.16629824042320251\n",
      "Training Batch [12/782]: Loss 0.0005315057351253927\n",
      "Training Batch [13/782]: Loss 0.04592514410614967\n",
      "Training Batch [14/782]: Loss 0.04312952235341072\n",
      "Training Batch [15/782]: Loss 0.03498021513223648\n",
      "Training Batch [16/782]: Loss 0.04168299585580826\n",
      "Training Batch [17/782]: Loss 0.0098885428160429\n",
      "Training Batch [18/782]: Loss 0.019686169922351837\n",
      "Training Batch [19/782]: Loss 0.0033759267535060644\n",
      "Training Batch [20/782]: Loss 0.06426408886909485\n",
      "Training Batch [21/782]: Loss 0.003337639383971691\n",
      "Training Batch [22/782]: Loss 0.004650220740586519\n",
      "Training Batch [23/782]: Loss 0.001696751220151782\n",
      "Training Batch [24/782]: Loss 0.017916087061166763\n",
      "Training Batch [25/782]: Loss 0.003628611098974943\n",
      "Training Batch [26/782]: Loss 0.004155452828854322\n",
      "Training Batch [27/782]: Loss 0.002127458341419697\n",
      "Training Batch [28/782]: Loss 0.07206742465496063\n",
      "Training Batch [29/782]: Loss 0.004516595043241978\n",
      "Training Batch [30/782]: Loss 0.008960681967437267\n",
      "Training Batch [31/782]: Loss 0.02234853245317936\n",
      "Training Batch [32/782]: Loss 0.009784670546650887\n",
      "Training Batch [33/782]: Loss 0.0012872908264398575\n",
      "Training Batch [34/782]: Loss 0.005247882101684809\n",
      "Training Batch [35/782]: Loss 0.005336513742804527\n",
      "Training Batch [36/782]: Loss 0.04465402662754059\n",
      "Training Batch [37/782]: Loss 0.0600459985435009\n",
      "Training Batch [38/782]: Loss 0.018317846581339836\n",
      "Training Batch [39/782]: Loss 0.000608147878665477\n",
      "Training Batch [40/782]: Loss 0.0002823624527081847\n",
      "Training Batch [41/782]: Loss 0.001966869691386819\n",
      "Training Batch [42/782]: Loss 0.001866521080955863\n",
      "Training Batch [43/782]: Loss 0.07128550857305527\n",
      "Training Batch [44/782]: Loss 0.0018458853010088205\n",
      "Training Batch [45/782]: Loss 0.05721677094697952\n",
      "Training Batch [46/782]: Loss 0.006413604598492384\n",
      "Training Batch [47/782]: Loss 0.009203747846186161\n",
      "Training Batch [48/782]: Loss 0.0014632862294092774\n",
      "Training Batch [49/782]: Loss 0.03009451925754547\n",
      "Training Batch [50/782]: Loss 0.009324069134891033\n",
      "Training Batch [51/782]: Loss 0.00021572504192590714\n",
      "Training Batch [52/782]: Loss 0.0014392365701496601\n",
      "Training Batch [53/782]: Loss 0.006793372333049774\n",
      "Training Batch [54/782]: Loss 8.328138937940821e-05\n",
      "Training Batch [55/782]: Loss 0.003612870816141367\n",
      "Training Batch [56/782]: Loss 0.002008122391998768\n",
      "Training Batch [57/782]: Loss 0.028486574068665504\n",
      "Training Batch [58/782]: Loss 0.006690835114568472\n",
      "Training Batch [59/782]: Loss 0.009184468537569046\n",
      "Training Batch [60/782]: Loss 0.04379519820213318\n",
      "Training Batch [61/782]: Loss 0.0029046458657830954\n",
      "Training Batch [62/782]: Loss 0.007210082374513149\n",
      "Training Batch [63/782]: Loss 0.005368928425014019\n",
      "Training Batch [64/782]: Loss 0.0015770611353218555\n",
      "Training Batch [65/782]: Loss 0.004290214739739895\n",
      "Training Batch [66/782]: Loss 0.0017709170933812857\n",
      "Training Batch [67/782]: Loss 0.014658274129033089\n",
      "Training Batch [68/782]: Loss 0.010423317551612854\n",
      "Training Batch [69/782]: Loss 0.0011211656965315342\n",
      "Training Batch [70/782]: Loss 0.008656496182084084\n",
      "Training Batch [71/782]: Loss 0.0012164550134912133\n",
      "Training Batch [72/782]: Loss 0.0021859623957425356\n",
      "Training Batch [73/782]: Loss 0.0021195339504629374\n",
      "Training Batch [74/782]: Loss 0.00024431818746961653\n",
      "Training Batch [75/782]: Loss 0.02677694708108902\n",
      "Training Batch [76/782]: Loss 0.002120695076882839\n",
      "Training Batch [77/782]: Loss 0.0004277484549675137\n",
      "Training Batch [78/782]: Loss 0.04007316008210182\n",
      "Training Batch [79/782]: Loss 0.04294043779373169\n",
      "Training Batch [80/782]: Loss 0.002253601560369134\n",
      "Training Batch [81/782]: Loss 0.003687833435833454\n",
      "Training Batch [82/782]: Loss 0.0018334267660975456\n",
      "Training Batch [83/782]: Loss 0.0010213563218712807\n",
      "Training Batch [84/782]: Loss 0.0008463800186291337\n",
      "Training Batch [85/782]: Loss 0.023261306807398796\n",
      "Training Batch [86/782]: Loss 0.001801416277885437\n",
      "Training Batch [87/782]: Loss 0.0031279604882001877\n",
      "Training Batch [88/782]: Loss 0.0005136586260050535\n",
      "Training Batch [89/782]: Loss 0.010359629988670349\n",
      "Training Batch [90/782]: Loss 0.007868624292314053\n",
      "Training Batch [91/782]: Loss 0.004875234793871641\n",
      "Training Batch [92/782]: Loss 0.0032587656751275063\n",
      "Training Batch [93/782]: Loss 0.09539741277694702\n",
      "Training Batch [94/782]: Loss 0.0028656814247369766\n",
      "Training Batch [95/782]: Loss 0.007094844710081816\n",
      "Training Batch [96/782]: Loss 0.0026308356318622828\n",
      "Training Batch [97/782]: Loss 0.0012278214562684298\n",
      "Training Batch [98/782]: Loss 0.0004420233890414238\n",
      "Training Batch [99/782]: Loss 0.008154251612722874\n",
      "Training Batch [100/782]: Loss 0.00295870378613472\n",
      "Training Batch [101/782]: Loss 0.0017554770456627011\n",
      "Training Batch [102/782]: Loss 0.0013378376606851816\n",
      "Training Batch [103/782]: Loss 0.0010178639786317945\n",
      "Training Batch [104/782]: Loss 0.0003858814015984535\n",
      "Training Batch [105/782]: Loss 0.0031671160832047462\n",
      "Training Batch [106/782]: Loss 0.0021386300213634968\n",
      "Training Batch [107/782]: Loss 0.00047826714580878615\n",
      "Training Batch [108/782]: Loss 0.008259644731879234\n",
      "Training Batch [109/782]: Loss 0.009470563381910324\n",
      "Training Batch [110/782]: Loss 0.028989359736442566\n",
      "Training Batch [111/782]: Loss 0.003580389078706503\n",
      "Training Batch [112/782]: Loss 0.003109054174274206\n",
      "Training Batch [113/782]: Loss 0.003355228342115879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*********  Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  *********\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Batch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]: Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"*********  Epoch {epoch + 1}/{num_epochs}  *********\")\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}\")\n",
    "    torch.save(model.state_dict(), \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a191c2e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#CIFAR labels to human readable labels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m CIFAR10_CLASSES \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplane\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbird\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mship\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruck\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m ]\n\u001b[0;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_loader\u001b[49m))\n\u001b[1;32m      6\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Check the shape of the images and labels\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "#CIFAR labels to human readable labels\n",
    "CIFAR10_CLASSES = [\n",
    "    'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "data = next(iter(test_loader))\n",
    "images, labels = data\n",
    "\n",
    "# Check the shape of the images and labels\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(2, 2))  # keep this small to avoid blur\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')  # no interpolation\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "idx = 0\n",
    "imshow(images[idx])\n",
    "\n",
    "# images = images.to(device)\n",
    "# y = model(images) # B, num_classes\n",
    "# print(f\"logits: {y[idx]}\")\n",
    "# pred = torch.nn.functional.softmax(y, dim=1) # B, num_classes\n",
    "# print(f\"probabilities: {pred[idx]}\")\n",
    "# pred = torch.argmax(pred, dim=1) # B\n",
    "# print(f\"predicted class: {pred[idx]}\")\n",
    "\n",
    "# print(f\"label: {CIFAR10_CLASSES[labels[idx].item()]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9f953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
