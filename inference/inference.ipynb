{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "!pip install matplotlib\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import FlanT5Tokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "import config, dataset, tokenizer\n",
    "importlib.reload(config)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(tokenizer)\n",
    "\n",
    "from config import Blip2Config\n",
    "from dataset import Blip2Dataset\n",
    "from tokenizer import FlanT5Tokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: image\n",
      "Skipping non-image file: image\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "t5_model_name = \"google/flan-t5-small\"\n",
    "bert_autotokenizer =  AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "t5_autotokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "config = Blip2Config()\n",
    "\n",
    "flan_t5_tokenizer = FlanT5Tokenizer(config, t5_autotokenizer)\n",
    "bert_tokenizer = BertTokenizer(config, bert_autotokenizer)\n",
    "config.bert_vocab_size = bert_tokenizer.n_vocab\n",
    "config.t5_vocab_size = flan_t5_tokenizer.n_vocab\n",
    "\n",
    "stage1_train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=bert_tokenizer.tokenize_text, type=\"bert\")\n",
    "stage1_train_dataloader = DataLoader(stage1_train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "stage2_train_dataset = Blip2Dataset(config, split=\"train\", tokenizer=flan_t5_tokenizer.tokenize_text, type=\"flan_t5\")\n",
    "stage2_train_dataloader = DataLoader(stage2_train_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import timm\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class BertMLPBlock(nn.Module):\n",
    "    def __init__(self, intermediate, output):\n",
    "        super().__init__()\n",
    "        self.intermediate = intermediate\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_output = self.intermediate(x)\n",
    "        return self.output(intermediate_output, x)\n",
    "    \n",
    "\n",
    "class BertEncoderBlock(nn.Module):\n",
    "    def __init__(self, bert_layer, bert_config, is_cross_attn=False):\n",
    "        super().__init__()\n",
    "        self.bert_config = bert_config\n",
    "        self.is_cross_attn = is_cross_attn\n",
    "\n",
    "        d = bert_config.hidden_size\n",
    "        h = bert_config.num_attention_heads\n",
    "        self.self_attn = nn.MultiheadAttention(d, h, batch_first=True)\n",
    "        self.self_ln = nn.LayerNorm(d)\n",
    "\n",
    "        self.mlp_img_transformer = BertMLPBlock(bert_layer.intermediate, bert_layer.output)\n",
    "        self.mlp_text_transformer = BertMLPBlock(\n",
    "                    copy.deepcopy(bert_layer.intermediate), \n",
    "                    copy.deepcopy(bert_layer.output)\n",
    "                    )\n",
    "        if is_cross_attn:\n",
    "            self.cross_attn = nn.MultiheadAttention(embed_dim=self.bert_config.hidden_size, \n",
    "                                                    num_heads=self.bert_config.num_attention_heads, \n",
    "                                                    batch_first=True)\n",
    "            self.cross_layer_norm = nn.LayerNorm(self.bert_config.hidden_size)\n",
    "        \n",
    "    def forward(self, query_embds, img_embds, text_embds, attn_mask):\n",
    "        _, Qs, _ = query_embds.shape\n",
    "        _, Ts, _ = text_embds.shape\n",
    "\n",
    "        combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
    "\n",
    "        attn_out, _ = self.self_attn(\n",
    "            combined_embds, combined_embds, combined_embds,\n",
    "            attn_mask=attn_mask,         # (L, L), broadcast over batch & heads\n",
    "            key_padding_mask=None     # (B, L) bool, True=mask (optional)\n",
    "        )\n",
    "        combined_embds = self.self_ln(combined_embds+ attn_out)\n",
    "\n",
    "        query_embds = combined_embds[:, :Qs]\n",
    "        text_embds= combined_embds[:, Qs:]\n",
    "        \n",
    "        if self.is_cross_attn:\n",
    "            hidden_states = self.cross_attn(query_embds, img_embds, img_embds)[0]\n",
    "            query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
    "\n",
    "        query_embds = self.mlp_img_transformer(query_embds)\n",
    "        text_embds = self.mlp_text_transformer(text_embds)\n",
    "        return query_embds, text_embds\n",
    "\n",
    "\n",
    "class QTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert_cfg  = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\", config = self.bert_cfg)\n",
    "        print(\"num of bert layers \",self.bert_model.encoder.layer)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for i, bert_layer in enumerate(self.bert_model.encoder.layer):\n",
    "            if(i>6):\n",
    "                break\n",
    "            self.encoder.append(BertEncoderBlock(bert_layer, self.bert_cfg, i % 2 == 0))\n",
    "        \n",
    "        qs = config.num_queries\n",
    "        ts = config.context_length\n",
    "        combined_seq_len = qs + ts\n",
    "\n",
    "        ####  STAGE 1: ITC, ITM, ITG Loss Masks ####\n",
    "        # ITC Loss Mask\n",
    "        itc_attn_mask = torch.zeros((combined_seq_len, combined_seq_len))\n",
    "        itc_attn_mask[:qs, :qs] = 1\n",
    "        itc_attn_mask[qs:, qs:] = 1\n",
    "        \n",
    "\n",
    "        # ITM Loss Mask\n",
    "        itm_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "\n",
    "        # ITG Loss Mask\n",
    "        itg_attn_mask = torch.ones((combined_seq_len, combined_seq_len))\n",
    "        itg_attn_mask[:qs, qs:] = 0\n",
    "        itg_attn_mask[qs:, qs:] = torch.tril(itg_attn_mask[qs:, qs:], diagonal=0)\n",
    "\n",
    "\n",
    "        self.register_buffer(\"itc_attn_mask\", itc_attn_mask)\n",
    "        self.register_buffer(\"itm_attn_mask\", itm_attn_mask)\n",
    "        self.register_buffer(\"itg_attn_mask\", itg_attn_mask)\n",
    "\n",
    "        ####  STAGE 2: ####\n",
    "        # ITC Loss Mask will be same as stage 1 and reused for stage 2\n",
    "\n",
    "    def forward(self, query_embds, img_embds, cls_text_embds, dec_text_embds, stage):\n",
    "\n",
    "        itc_query_embds = query_embds.clone()\n",
    "        itm_query_embds = query_embds.clone()\n",
    "        itg_query_embds = query_embds.clone()\n",
    "\n",
    "        itc_text_embds = cls_text_embds.clone()\n",
    "        itm_text_embds = cls_text_embds.clone()\n",
    "        itg_text_embds = dec_text_embds.clone()\n",
    "\n",
    "        device = query_embds.device\n",
    "        dtype  = query_embds.dtype\n",
    "\n",
    "        # Convert base masks to additive for this device/dtype\n",
    "        itc_add = to_additive_mask(self.itc_attn_mask, device=device, dtype=dtype)\n",
    "        itm_add = to_additive_mask(self.itm_attn_mask, device=device, dtype=dtype)\n",
    "        itg_add = to_additive_mask(self.itg_attn_mask, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "        for encoder in self.encoder:\n",
    "            itc_query_embds, itc_text_embds = encoder(itc_query_embds, img_embds, itc_text_embds, itc_add)\n",
    "            if stage == 1:\n",
    "                itm_query_embds, itm_text_embds = encoder(itm_query_embds, img_embds, itm_text_embds, itm_add)\n",
    "                itg_query_embds, itg_text_embds = encoder(itg_query_embds, img_embds, itg_text_embds, itg_add)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds\n",
    "    \n",
    "\n",
    "class QFormer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q_transformer = QTransformer(config)\n",
    "        self.learned_query = nn.Parameter(torch.randn(config.num_queries, config.embedding_dim))\n",
    "        self.output_embedding  = nn.Embedding(config.bert_vocab_size, config.embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(config.context_length, config.embedding_dim)\n",
    "\n",
    "        position_ids = torch.arange(self.config.context_length).unsqueeze(0)\n",
    "        self.register_buffer(\"position_ids\", position_ids)\n",
    "\n",
    "    def forward(self, image_embedding: torch.tensor, cls_tokens: torch.tensor, dec_tokens: torch.tensor, stage:int):\n",
    "        B, S, E = image_embedding.shape\n",
    "        learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "        cls_text_embeddings = self.output_embedding(cls_tokens) #(S,768)\n",
    "        cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "        dec_text_embeddings = self.output_embedding(dec_tokens) #(S,768)\n",
    "        dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_query_embds, itg_text_embds = self.q_transformer(\n",
    "            learned_query, image_embedding, cls_text_embeddings, dec_text_embeddings, stage)\n",
    "\n",
    "        if itg_text_embds is not None:\n",
    "            itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
    "        else:\n",
    "            itg_logits = None\n",
    "\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "\n",
    "\n",
    "class FlanT5Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlanT5Model, self).__init__()\n",
    "        self.lm_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "        for param in self.lm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, query_embedding, input_token, label, enc_mask):\n",
    "        #query_embd : (B,32,512)\n",
    "        # input_token : (B,L)\n",
    "        B, Q, d = query_embedding.shape\n",
    "        device = query_embedding.device\n",
    "        with torch.no_grad():\n",
    "            input_embd = self.lm_model.encoder.embed_tokens(input_token)  #(B,L,512)\n",
    "\n",
    "        encoder_input = torch.concat((query_embedding, input_embd) , dim = 1).contiguous()\n",
    "\n",
    "        prefix_mask = torch.ones((B, Q ), dtype= enc_mask.dtype, device=device)\n",
    "        attention_mask = torch.concat((prefix_mask, enc_mask) , dim=1).contiguous()  # [B, 32+L]\n",
    "        label = label.contiguous()  # [B, L]\n",
    "        out = self.lm_model(inputs_embeds=encoder_input,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=label,\n",
    "                                return_dict=True)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def predict(self, query_embedding, input_token, enc_mask):\n",
    "        B, Q, d = query_embedding.shape\n",
    "        device = query_embedding.device\n",
    "        with torch.no_grad():\n",
    "            input_embd = self.lm_model.encoder.embed_tokens(input_token)  #(B,L,512)\n",
    "\n",
    "        encoder_input = torch.concat((query_embedding, input_embd) , dim = 1)\n",
    "\n",
    "        prefix_mask = torch.ones((B, Q ), dtype= enc_mask.dtype, device=device)\n",
    "        attention_mask = torch.concat((prefix_mask, enc_mask) , dim=1)  # [B, 32+L]\n",
    "        \n",
    "        enc_out = self.lm_model.encoder(\n",
    "            inputs_embeds=encoder_input,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "            )\n",
    "\n",
    "        gen_ids = self.lm_model.generate(\n",
    "            encoder_outputs=enc_out,\n",
    "            max_new_tokens=30,\n",
    "            decoder_start_token_id=self.lm_model.config.decoder_start_token_id,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        return gen_ids\n",
    "\n",
    "\n",
    "class Blip2Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.image_encoder = timm.create_model('vit_tiny_patch16_224', pretrained=True)\n",
    "        self.image_encoder.reset_classifier(0)\n",
    "\n",
    "        for param in self.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.image_proj = nn.Linear(config.img_embd_dim, config.embedding_dim)\n",
    "\n",
    "        self.q_former = QFormer(config)\n",
    "        self.z_proj = nn.Linear(config.embedding_dim, config.lm_embedding_dim)\n",
    "\n",
    "        self.lm_model = FlanT5Model()\n",
    "    \n",
    "    \n",
    "    def stage1(self, image:torch.tensor, cls_caption:torch.tensor, dec_caption:torch.tensor):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        print(image_embedding.shape)\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, cls_caption, dec_caption, 1)\n",
    "        return itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits\n",
    "    \n",
    "    \n",
    "    def stage2(self, image, input_token, label, enc_mask, dummy_input_size):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "        \n",
    "        cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
    "        dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, \n",
    "                                                            cls_caption_dummy, dec_caption_dummy, 2)\n",
    "        \n",
    "        z = self.z_proj(itc_query_embds)  # [B, Qs, D]\n",
    "\n",
    "        out = self.lm_model(z, input_token, label, enc_mask)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def forward(self, image, input_token, enc_mask, dummy_input_size):\n",
    "        image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
    "        image_embedding = self.image_proj(image_embedding)\n",
    "\n",
    "        cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
    "        dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
    "\n",
    "        itc_query_embds, itc_text_embds, itm_query_embds, itm_text_embds, itg_logits = self.q_former(image_embedding, \n",
    "                                                            cls_caption_dummy, dec_caption_dummy, 2)\n",
    "        \n",
    "        z = self.z_proj(itc_query_embds)\n",
    "\n",
    "        gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
    "        return gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_additive_mask(mask01: torch.Tensor, *, device, dtype):\n",
    "    \"\"\"\n",
    "    Convert 0/1 'allow mask' (1=allow, 0=block) of shape (L,L)\n",
    "    into additive (L,L) with 0.0 for allow, -inf for block.\n",
    "    \"\"\"\n",
    "    m = mask01.to(device=device, dtype=dtype)\n",
    "    # where allow(1) -> 0.0, block(0) -> -inf\n",
    "    return torch.where(m > 0, torch.zeros_like(m), torch.full_like(m, float(\"-inf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of bert layers  ModuleList(\n",
      "  (0-11): 12 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSdpaSelfAttention(\n",
      "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Blip2Model(config)\n",
    "device = torch.device('cuda')\n",
    "model.load_state_dict(torch.load(\"blip2.pt\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.359362..4.309112].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.1266 seconds\n",
      "A man is laying on a bench in a park.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(npimg, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "img, question_placeholder,cls_caption, dec_caption  = stage2_train_dataloader.dataset[5]\n",
    "img = torch.randn(3,224,224)\n",
    "img = img.unsqueeze(0)\n",
    "print(img.shape)\n",
    "img = img.to(device)\n",
    "\n",
    "imshow(img[0].cpu().permute(1,2,0).numpy())\n",
    "\n",
    "input_caption = \"Question: describe the image. Answer: \"\n",
    "input_tokens, input_mask = flan_t5_tokenizer.tokenize_text(input_caption)\n",
    "\n",
    "input_tokens = input_tokens.to(device)\n",
    "input_mask = input_mask.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "gen_ids = model.forward(img[:1], input_tokens, input_mask, (input_tokens.shape[0], input_tokens.shape[1]))\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Inference time: {duration:.4f} seconds\")\n",
    "\n",
    "decoded_output = flan_t5_tokenizer.decode(gen_ids[0])\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pycuda\n",
      "  Downloading pycuda-2025.1.2.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytools>=2011.2 (from pycuda)\n",
      "  Downloading pytools-2025.2.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pycuda) (4.3.8)\n",
      "Collecting mako (from pycuda)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
      "  Downloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from pytools>=2011.2->pycuda) (4.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from mako->pycuda) (3.0.2)\n",
      "Downloading pytools-2025.2.4-py3-none-any.whl (99 kB)\n",
      "Downloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (103 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycuda: filename=pycuda-2025.1.2-cp312-cp312-linux_x86_64.whl size=664724 sha256=54fb09a5b438bb3a858c9ea5d721db790e803550ade926766d6904698381434d\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/d5/36/f3/ac5f09d768cad3fa15d5a3449bdfe65c3de58e69d036c73228\n",
      "Successfully built pycuda\n",
      "Installing collected packages: siphash24, mako, pytools, pycuda\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4/4\u001b[0m [pycuda]2m3/4\u001b[0m [pycuda]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mako-1.3.10 pycuda-2025.1.2 pytools-2025.2.4 siphash24-1.8\n"
     ]
    }
   ],
   "source": [
    "# Install TensorRT\n",
    "# !pip install nvidia-pyindex\n",
    "# !pip install nvidia-tensorrt\n",
    "# !apt-get update && apt-get install -y python3-pip libboost-all-dev libpython3-dev && rm -rf /var/lib/apt/lists/*\n",
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT version: 10.13.2.6\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTensorRT version:\u001b[39m\u001b[33m\"\u001b[39m, trt.__version__)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Example: Run a TensorRT executable (assuming you have a .trt engine file)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpycuda\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdriver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpycuda\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautoinit\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load TensorRT engine\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pycuda'"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "\n",
    "# Verify TensorRT installation\n",
    "print(\"TensorRT version:\", trt.__version__)\n",
    "\n",
    "# Example: Run a TensorRT executable (assuming you have a .trt engine file)\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "# Load TensorRT engine\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "engine_file_path = \"model.trt\"  # Replace with your TensorRT engine file path\n",
    "\n",
    "with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "# Create execution context\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# Allocate memory for inputs and outputs\n",
    "input_shape = (1, 3, 224, 224)  # Replace with your model's input shape\n",
    "output_shape = (1, 1000)  # Replace with your model's output shape\n",
    "input_data = np.random.random(input_shape).astype(np.float32)\n",
    "output_data = np.empty(output_shape, dtype=np.float32)\n",
    "\n",
    "# Allocate device memory\n",
    "d_input = cuda.mem_alloc(input_data.nbytes)\n",
    "d_output = cuda.mem_alloc(output_data.nbytes)\n",
    "\n",
    "# Copy input data to device\n",
    "cuda.memcpy_htod(d_input, input_data)\n",
    "\n",
    "# Run inference\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "context.execute_v2(bindings)\n",
    "\n",
    "# Copy output data back to host\n",
    "cuda.memcpy_dtoh(output_data, d_output)\n",
    "\n",
    "print(\"Inference output:\", output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cuda-python in /usr/local/lib/python3.12/dist-packages (13.0.0)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.18.0)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: cuda-bindings~=13.0.0 in /usr/local/lib/python3.12/dist-packages (from cuda-python) (13.0.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.0 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings~=13.0.0->cuda-python) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (6.31.1)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.14.1)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4/4\u001b[0m [onnxruntime]\u001b[0m [onnxruntime]\n",
      "\u001b[1A\u001b[2KSuccessfully installed coloredlogs-15.0.1 flatbuffers-25.2.10 humanfriendly-10.0 onnxruntime-1.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir cuda-python onnx onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnxruntime in /home/ubuntu/.local/lib/python3.12/site-packages (1.22.1)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/.local/lib/python3.12/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.local/lib/python3.12/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (6.31.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/.local/lib/python3.12/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "ORT version: 1.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade onnxruntime\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/.local/lib/python3.12/site-packages\")\n",
    "import onnxruntime as ort\n",
    "print(\"ORT version:\", ort.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Blip2Model([...]` with `torch.export.export(..., strict=False)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 13:14:34.902000 1989 torch/fx/experimental/symbolic_shapes.py:7903] Unable to find user code corresponding to {u1}\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[1, 1, 192]\", arg1_1: \"f32[1, 197, 192]\", arg2_1: \"f32[192, 3, 16, 16]\", arg3_1: \"f32[192]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192]\", arg6_1: \"f32[576, 192]\", arg7_1: \"f32[576]\", arg8_1: \"f32[192, 192]\", arg9_1: \"f32[192]\", arg10_1: \"f32[192]\", arg11_1: \"f32[192]\", arg12_1: \"f32[768, 192]\", arg13_1: \"f32[768]\", arg14_1: \"f32[192, 768]\", arg15_1: \"f32[192]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192]\", arg18_1: \"f32[576, 192]\", arg19_1: \"f32[576]\", arg20_1: \"f32[192, 192]\", arg21_1: \"f32[192]\", arg22_1: \"f32[192]\", arg23_1: \"f32[192]\", arg24_1: \"f32[768, 192]\", arg25_1: \"f32[768]\", arg26_1: \"f32[192, 768]\", arg27_1: \"f32[192]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192]\", arg30_1: \"f32[576, 192]\", arg31_1: \"f32[576]\", arg32_1: \"f32[192, 192]\", arg33_1: \"f32[192]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192]\", arg36_1: \"f32[768, 192]\", arg37_1: \"f32[768]\", arg38_1: \"f32[192, 768]\", arg39_1: \"f32[192]\", arg40_1: \"f32[192]\", arg41_1: \"f32[192]\", arg42_1: \"f32[576, 192]\", arg43_1: \"f32[576]\", arg44_1: \"f32[192, 192]\", arg45_1: \"f32[192]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192]\", arg48_1: \"f32[768, 192]\", arg49_1: \"f32[768]\", arg50_1: \"f32[192, 768]\", arg51_1: \"f32[192]\", arg52_1: \"f32[192]\", arg53_1: \"f32[192]\", arg54_1: \"f32[576, 192]\", arg55_1: \"f32[576]\", arg56_1: \"f32[192, 192]\", arg57_1: \"f32[192]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192]\", arg60_1: \"f32[768, 192]\", arg61_1: \"f32[768]\", arg62_1: \"f32[192, 768]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[576, 192]\", arg67_1: \"f32[576]\", arg68_1: \"f32[192, 192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[768, 192]\", arg73_1: \"f32[768]\", arg74_1: \"f32[192, 768]\", arg75_1: \"f32[192]\", arg76_1: \"f32[192]\", arg77_1: \"f32[192]\", arg78_1: \"f32[576, 192]\", arg79_1: \"f32[576]\", arg80_1: \"f32[192, 192]\", arg81_1: \"f32[192]\", arg82_1: \"f32[192]\", arg83_1: \"f32[192]\", arg84_1: \"f32[768, 192]\", arg85_1: \"f32[768]\", arg86_1: \"f32[192, 768]\", arg87_1: \"f32[192]\", arg88_1: \"f32[192]\", arg89_1: \"f32[192]\", arg90_1: \"f32[576, 192]\", arg91_1: \"f32[576]\", arg92_1: \"f32[192, 192]\", arg93_1: \"f32[192]\", arg94_1: \"f32[192]\", arg95_1: \"f32[192]\", arg96_1: \"f32[768, 192]\", arg97_1: \"f32[768]\", arg98_1: \"f32[192, 768]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[576, 192]\", arg103_1: \"f32[576]\", arg104_1: \"f32[192, 192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[768, 192]\", arg109_1: \"f32[768]\", arg110_1: \"f32[192, 768]\", arg111_1: \"f32[192]\", arg112_1: \"f32[192]\", arg113_1: \"f32[192]\", arg114_1: \"f32[576, 192]\", arg115_1: \"f32[576]\", arg116_1: \"f32[192, 192]\", arg117_1: \"f32[192]\", arg118_1: \"f32[192]\", arg119_1: \"f32[192]\", arg120_1: \"f32[768, 192]\", arg121_1: \"f32[768]\", arg122_1: \"f32[192, 768]\", arg123_1: \"f32[192]\", arg124_1: \"f32[192]\", arg125_1: \"f32[192]\", arg126_1: \"f32[576, 192]\", arg127_1: \"f32[576]\", arg128_1: \"f32[192, 192]\", arg129_1: \"f32[192]\", arg130_1: \"f32[192]\", arg131_1: \"f32[192]\", arg132_1: \"f32[768, 192]\", arg133_1: \"f32[768]\", arg134_1: \"f32[192, 768]\", arg135_1: \"f32[192]\", arg136_1: \"f32[192]\", arg137_1: \"f32[192]\", arg138_1: \"f32[576, 192]\", arg139_1: \"f32[576]\", arg140_1: \"f32[192, 192]\", arg141_1: \"f32[192]\", arg142_1: \"f32[192]\", arg143_1: \"f32[192]\", arg144_1: \"f32[768, 192]\", arg145_1: \"f32[768]\", arg146_1: \"f32[192, 768]\", arg147_1: \"f32[192]\", arg148_1: \"f32[192]\", arg149_1: \"f32[192]\", arg150_1: \"f32[768, 192]\", arg151_1: \"f32[768]\", arg152_1: \"f32[32, 768]\", arg153_1: \"f32[30522, 768]\", arg154_1: \"f32[512, 768]\", arg155_1: \"f32[2, 768]\", arg156_1: \"f32[768]\", arg157_1: \"f32[768]\", arg158_1: \"f32[768, 768]\", arg159_1: \"f32[768]\", arg160_1: \"f32[768, 768]\", arg161_1: \"f32[768]\", arg162_1: \"f32[768, 768]\", arg163_1: \"f32[768]\", arg164_1: \"f32[768, 768]\", arg165_1: \"f32[768]\", arg166_1: \"f32[768]\", arg167_1: \"f32[768]\", arg168_1: \"f32[3072, 768]\", arg169_1: \"f32[3072]\", arg170_1: \"f32[768, 3072]\", arg171_1: \"f32[768]\", arg172_1: \"f32[768]\", arg173_1: \"f32[768]\", arg174_1: \"f32[768, 768]\", arg175_1: \"f32[768]\", arg176_1: \"f32[768, 768]\", arg177_1: \"f32[768]\", arg178_1: \"f32[768, 768]\", arg179_1: \"f32[768]\", arg180_1: \"f32[768, 768]\", arg181_1: \"f32[768]\", arg182_1: \"f32[768]\", arg183_1: \"f32[768]\", arg184_1: \"f32[3072, 768]\", arg185_1: \"f32[3072]\", arg186_1: \"f32[768, 3072]\", arg187_1: \"f32[768]\", arg188_1: \"f32[768]\", arg189_1: \"f32[768]\", arg190_1: \"f32[768, 768]\", arg191_1: \"f32[768]\", arg192_1: \"f32[768, 768]\", arg193_1: \"f32[768]\", arg194_1: \"f32[768, 768]\", arg195_1: \"f32[768]\", arg196_1: \"f32[768, 768]\", arg197_1: \"f32[768]\", arg198_1: \"f32[768]\", arg199_1: \"f32[768]\", arg200_1: \"f32[3072, 768]\", arg201_1: \"f32[3072]\", arg202_1: \"f32[768, 3072]\", arg203_1: \"f32[768]\", arg204_1: \"f32[768]\", arg205_1: \"f32[768]\", arg206_1: \"f32[768, 768]\", arg207_1: \"f32[768]\", arg208_1: \"f32[768, 768]\", arg209_1: \"f32[768]\", arg210_1: \"f32[768, 768]\", arg211_1: \"f32[768]\", arg212_1: \"f32[768, 768]\", arg213_1: \"f32[768]\", arg214_1: \"f32[768]\", arg215_1: \"f32[768]\", arg216_1: \"f32[3072, 768]\", arg217_1: \"f32[3072]\", arg218_1: \"f32[768, 3072]\", arg219_1: \"f32[768]\", arg220_1: \"f32[768]\", arg221_1: \"f32[768]\", arg222_1: \"f32[768, 768]\", arg223_1: \"f32[768]\", arg224_1: \"f32[768, 768]\", arg225_1: \"f32[768]\", arg226_1: \"f32[768, 768]\", arg227_1: \"f32[768]\", arg228_1: \"f32[768, 768]\", arg229_1: \"f32[768]\", arg230_1: \"f32[768]\", arg231_1: \"f32[768]\", arg232_1: \"f32[3072, 768]\", arg233_1: \"f32[3072]\", arg234_1: \"f32[768, 3072]\", arg235_1: \"f32[768]\", arg236_1: \"f32[768]\", arg237_1: \"f32[768]\", arg238_1: \"f32[768, 768]\", arg239_1: \"f32[768]\", arg240_1: \"f32[768, 768]\", arg241_1: \"f32[768]\", arg242_1: \"f32[768, 768]\", arg243_1: \"f32[768]\", arg244_1: \"f32[768, 768]\", arg245_1: \"f32[768]\", arg246_1: \"f32[768]\", arg247_1: \"f32[768]\", arg248_1: \"f32[3072, 768]\", arg249_1: \"f32[3072]\", arg250_1: \"f32[768, 3072]\", arg251_1: \"f32[768]\", arg252_1: \"f32[768]\", arg253_1: \"f32[768]\", arg254_1: \"f32[768, 768]\", arg255_1: \"f32[768]\", arg256_1: \"f32[768, 768]\", arg257_1: \"f32[768]\", arg258_1: \"f32[768, 768]\", arg259_1: \"f32[768]\", arg260_1: \"f32[768, 768]\", arg261_1: \"f32[768]\", arg262_1: \"f32[768]\", arg263_1: \"f32[768]\", arg264_1: \"f32[3072, 768]\", arg265_1: \"f32[3072]\", arg266_1: \"f32[768, 3072]\", arg267_1: \"f32[768]\", arg268_1: \"f32[768]\", arg269_1: \"f32[768]\", arg270_1: \"f32[768, 768]\", arg271_1: \"f32[768]\", arg272_1: \"f32[768, 768]\", arg273_1: \"f32[768]\", arg274_1: \"f32[768, 768]\", arg275_1: \"f32[768]\", arg276_1: \"f32[768, 768]\", arg277_1: \"f32[768]\", arg278_1: \"f32[768]\", arg279_1: \"f32[768]\", arg280_1: \"f32[3072, 768]\", arg281_1: \"f32[3072]\", arg282_1: \"f32[768, 3072]\", arg283_1: \"f32[768]\", arg284_1: \"f32[768]\", arg285_1: \"f32[768]\", arg286_1: \"f32[768, 768]\", arg287_1: \"f32[768]\", arg288_1: \"f32[768, 768]\", arg289_1: \"f32[768]\", arg290_1: \"f32[768, 768]\", arg291_1: \"f32[768]\", arg292_1: \"f32[768, 768]\", arg293_1: \"f32[768]\", arg294_1: \"f32[768]\", arg295_1: \"f32[768]\", arg296_1: \"f32[3072, 768]\", arg297_1: \"f32[3072]\", arg298_1: \"f32[768, 3072]\", arg299_1: \"f32[768]\", arg300_1: \"f32[768]\", arg301_1: \"f32[768]\", arg302_1: \"f32[768, 768]\", arg303_1: \"f32[768]\", arg304_1: \"f32[768, 768]\", arg305_1: \"f32[768]\", arg306_1: \"f32[768, 768]\", arg307_1: \"f32[768]\", arg308_1: \"f32[768, 768]\", arg309_1: \"f32[768]\", arg310_1: \"f32[768]\", arg311_1: \"f32[768]\", arg312_1: \"f32[3072, 768]\", arg313_1: \"f32[3072]\", arg314_1: \"f32[768, 3072]\", arg315_1: \"f32[768]\", arg316_1: \"f32[768]\", arg317_1: \"f32[768]\", arg318_1: \"f32[768, 768]\", arg319_1: \"f32[768]\", arg320_1: \"f32[768, 768]\", arg321_1: \"f32[768]\", arg322_1: \"f32[768, 768]\", arg323_1: \"f32[768]\", arg324_1: \"f32[768, 768]\", arg325_1: \"f32[768]\", arg326_1: \"f32[768]\", arg327_1: \"f32[768]\", arg328_1: \"f32[3072, 768]\", arg329_1: \"f32[3072]\", arg330_1: \"f32[768, 3072]\", arg331_1: \"f32[768]\", arg332_1: \"f32[768]\", arg333_1: \"f32[768]\", arg334_1: \"f32[768, 768]\", arg335_1: \"f32[768]\", arg336_1: \"f32[768, 768]\", arg337_1: \"f32[768]\", arg338_1: \"f32[768, 768]\", arg339_1: \"f32[768]\", arg340_1: \"f32[768, 768]\", arg341_1: \"f32[768]\", arg342_1: \"f32[768]\", arg343_1: \"f32[768]\", arg344_1: \"f32[3072, 768]\", arg345_1: \"f32[3072]\", arg346_1: \"f32[768, 3072]\", arg347_1: \"f32[768]\", arg348_1: \"f32[768]\", arg349_1: \"f32[768]\", arg350_1: \"f32[768, 768]\", arg351_1: \"f32[768]\", arg352_1: \"f32[2304, 768]\", arg353_1: \"f32[2304]\", arg354_1: \"f32[768, 768]\", arg355_1: \"f32[768]\", arg356_1: \"f32[768]\", arg357_1: \"f32[768]\", arg358_1: \"f32[3072, 768]\", arg359_1: \"f32[3072]\", arg360_1: \"f32[768, 3072]\", arg361_1: \"f32[768]\", arg362_1: \"f32[768]\", arg363_1: \"f32[768]\", arg364_1: \"f32[3072, 768]\", arg365_1: \"f32[3072]\", arg366_1: \"f32[768, 3072]\", arg367_1: \"f32[768]\", arg368_1: \"f32[768]\", arg369_1: \"f32[768]\", arg370_1: \"f32[2304, 768]\", arg371_1: \"f32[2304]\", arg372_1: \"f32[768, 768]\", arg373_1: \"f32[768]\", arg374_1: \"f32[768]\", arg375_1: \"f32[768]\", arg376_1: \"f32[2304, 768]\", arg377_1: \"f32[2304]\", arg378_1: \"f32[768, 768]\", arg379_1: \"f32[768]\", arg380_1: \"f32[768]\", arg381_1: \"f32[768]\", arg382_1: \"f32[3072, 768]\", arg383_1: \"f32[3072]\", arg384_1: \"f32[768, 3072]\", arg385_1: \"f32[768]\", arg386_1: \"f32[768]\", arg387_1: \"f32[768]\", arg388_1: \"f32[3072, 768]\", arg389_1: \"f32[3072]\", arg390_1: \"f32[768, 3072]\", arg391_1: \"f32[768]\", arg392_1: \"f32[768]\", arg393_1: \"f32[768]\", arg394_1: \"f32[2304, 768]\", arg395_1: \"f32[2304]\", arg396_1: \"f32[768, 768]\", arg397_1: \"f32[768]\", arg398_1: \"f32[768]\", arg399_1: \"f32[768]\", arg400_1: \"f32[3072, 768]\", arg401_1: \"f32[3072]\", arg402_1: \"f32[768, 3072]\", arg403_1: \"f32[768]\", arg404_1: \"f32[768]\", arg405_1: \"f32[768]\", arg406_1: \"f32[3072, 768]\", arg407_1: \"f32[3072]\", arg408_1: \"f32[768, 3072]\", arg409_1: \"f32[768]\", arg410_1: \"f32[768]\", arg411_1: \"f32[768]\", arg412_1: \"f32[2304, 768]\", arg413_1: \"f32[2304]\", arg414_1: \"f32[768, 768]\", arg415_1: \"f32[768]\", arg416_1: \"f32[768]\", arg417_1: \"f32[768]\", arg418_1: \"f32[2304, 768]\", arg419_1: \"f32[2304]\", arg420_1: \"f32[768, 768]\", arg421_1: \"f32[768]\", arg422_1: \"f32[768]\", arg423_1: \"f32[768]\", arg424_1: \"f32[3072, 768]\", arg425_1: \"f32[3072]\", arg426_1: \"f32[768, 3072]\", arg427_1: \"f32[768]\", arg428_1: \"f32[768]\", arg429_1: \"f32[768]\", arg430_1: \"f32[3072, 768]\", arg431_1: \"f32[3072]\", arg432_1: \"f32[768, 3072]\", arg433_1: \"f32[768]\", arg434_1: \"f32[768]\", arg435_1: \"f32[768]\", arg436_1: \"f32[2304, 768]\", arg437_1: \"f32[2304]\", arg438_1: \"f32[768, 768]\", arg439_1: \"f32[768]\", arg440_1: \"f32[768]\", arg441_1: \"f32[768]\", arg442_1: \"f32[3072, 768]\", arg443_1: \"f32[3072]\", arg444_1: \"f32[768, 3072]\", arg445_1: \"f32[768]\", arg446_1: \"f32[768]\", arg447_1: \"f32[768]\", arg448_1: \"f32[3072, 768]\", arg449_1: \"f32[3072]\", arg450_1: \"f32[768, 3072]\", arg451_1: \"f32[768]\", arg452_1: \"f32[768]\", arg453_1: \"f32[768]\", arg454_1: \"f32[2304, 768]\", arg455_1: \"f32[2304]\", arg456_1: \"f32[768, 768]\", arg457_1: \"f32[768]\", arg458_1: \"f32[768]\", arg459_1: \"f32[768]\", arg460_1: \"f32[2304, 768]\", arg461_1: \"f32[2304]\", arg462_1: \"f32[768, 768]\", arg463_1: \"f32[768]\", arg464_1: \"f32[768]\", arg465_1: \"f32[768]\", arg466_1: \"f32[3072, 768]\", arg467_1: \"f32[3072]\", arg468_1: \"f32[768, 3072]\", arg469_1: \"f32[768]\", arg470_1: \"f32[768]\", arg471_1: \"f32[768]\", arg472_1: \"f32[3072, 768]\", arg473_1: \"f32[3072]\", arg474_1: \"f32[768, 3072]\", arg475_1: \"f32[768]\", arg476_1: \"f32[768]\", arg477_1: \"f32[768]\", arg478_1: \"f32[2304, 768]\", arg479_1: \"f32[2304]\", arg480_1: \"f32[768, 768]\", arg481_1: \"f32[768]\", arg482_1: \"f32[768]\", arg483_1: \"f32[768]\", arg484_1: \"f32[3072, 768]\", arg485_1: \"f32[3072]\", arg486_1: \"f32[768, 3072]\", arg487_1: \"f32[768]\", arg488_1: \"f32[768]\", arg489_1: \"f32[768]\", arg490_1: \"f32[3072, 768]\", arg491_1: \"f32[3072]\", arg492_1: \"f32[768, 3072]\", arg493_1: \"f32[768]\", arg494_1: \"f32[768]\", arg495_1: \"f32[768]\", arg496_1: \"f32[2304, 768]\", arg497_1: \"f32[2304]\", arg498_1: \"f32[768, 768]\", arg499_1: \"f32[768]\", arg500_1: \"f32[768]\", arg501_1: \"f32[768]\", arg502_1: \"f32[30522, 768]\", arg503_1: \"f32[77, 768]\", arg504_1: \"f32[512, 768]\", arg505_1: \"f32[512]\", arg506_1: \"f32[32128, 512]\", arg507_1: \"f32[32128, 512]\", arg508_1: \"f32[384, 512]\", arg509_1: \"f32[384, 512]\", arg510_1: \"f32[384, 512]\", arg511_1: \"f32[512, 384]\", arg512_1: \"f32[32, 6]\", arg513_1: \"f32[512]\", arg514_1: \"f32[1024, 512]\", arg515_1: \"f32[1024, 512]\", arg516_1: \"f32[512, 1024]\", arg517_1: \"f32[512]\", arg518_1: \"f32[384, 512]\", arg519_1: \"f32[384, 512]\", arg520_1: \"f32[384, 512]\", arg521_1: \"f32[512, 384]\", arg522_1: \"f32[512]\", arg523_1: \"f32[1024, 512]\", arg524_1: \"f32[1024, 512]\", arg525_1: \"f32[512, 1024]\", arg526_1: \"f32[512]\", arg527_1: \"f32[384, 512]\", arg528_1: \"f32[384, 512]\", arg529_1: \"f32[384, 512]\", arg530_1: \"f32[512, 384]\", arg531_1: \"f32[512]\", arg532_1: \"f32[1024, 512]\", arg533_1: \"f32[1024, 512]\", arg534_1: \"f32[512, 1024]\", arg535_1: \"f32[512]\", arg536_1: \"f32[384, 512]\", arg537_1: \"f32[384, 512]\", arg538_1: \"f32[384, 512]\", arg539_1: \"f32[512, 384]\", arg540_1: \"f32[512]\", arg541_1: \"f32[1024, 512]\", arg542_1: \"f32[1024, 512]\", arg543_1: \"f32[512, 1024]\", arg544_1: \"f32[512]\", arg545_1: \"f32[384, 512]\", arg546_1: \"f32[384, 512]\", arg547_1: \"f32[384, 512]\", arg548_1: \"f32[512, 384]\", arg549_1: \"f32[512]\", arg550_1: \"f32[1024, 512]\", arg551_1: \"f32[1024, 512]\", arg552_1: \"f32[512, 1024]\", arg553_1: \"f32[512]\", arg554_1: \"f32[384, 512]\", arg555_1: \"f32[384, 512]\", arg556_1: \"f32[384, 512]\", arg557_1: \"f32[512, 384]\", arg558_1: \"f32[512]\", arg559_1: \"f32[1024, 512]\", arg560_1: \"f32[1024, 512]\", arg561_1: \"f32[512, 1024]\", arg562_1: \"f32[512]\", arg563_1: \"f32[384, 512]\", arg564_1: \"f32[384, 512]\", arg565_1: \"f32[384, 512]\", arg566_1: \"f32[512, 384]\", arg567_1: \"f32[512]\", arg568_1: \"f32[1024, 512]\", arg569_1: \"f32[1024, 512]\", arg570_1: \"f32[512, 1024]\", arg571_1: \"f32[512]\", arg572_1: \"f32[384, 512]\", arg573_1: \"f32[384, 512]\", arg574_1: \"f32[384, 512]\", arg575_1: \"f32[512, 384]\", arg576_1: \"f32[512]\", arg577_1: \"f32[1024, 512]\", arg578_1: \"f32[1024, 512]\", arg579_1: \"f32[512, 1024]\", arg580_1: \"f32[512]\", arg581_1: \"f32[512]\", arg582_1: \"f32[32128, 512]\", arg583_1: \"f32[384, 512]\", arg584_1: \"f32[384, 512]\", arg585_1: \"f32[384, 512]\", arg586_1: \"f32[512, 384]\", arg587_1: \"f32[32, 6]\", arg588_1: \"f32[512]\", arg589_1: \"f32[384, 512]\", arg590_1: \"f32[384, 512]\", arg591_1: \"f32[384, 512]\", arg592_1: \"f32[512, 384]\", arg593_1: \"f32[512]\", arg594_1: \"f32[1024, 512]\", arg595_1: \"f32[1024, 512]\", arg596_1: \"f32[512, 1024]\", arg597_1: \"f32[512]\", arg598_1: \"f32[384, 512]\", arg599_1: \"f32[384, 512]\", arg600_1: \"f32[384, 512]\", arg601_1: \"f32[512, 384]\", arg602_1: \"f32[512]\", arg603_1: \"f32[384, 512]\", arg604_1: \"f32[384, 512]\", arg605_1: \"f32[384, 512]\", arg606_1: \"f32[512, 384]\", arg607_1: \"f32[512]\", arg608_1: \"f32[1024, 512]\", arg609_1: \"f32[1024, 512]\", arg610_1: \"f32[512, 1024]\", arg611_1: \"f32[512]\", arg612_1: \"f32[384, 512]\", arg613_1: \"f32[384, 512]\", arg614_1: \"f32[384, 512]\", arg615_1: \"f32[512, 384]\", arg616_1: \"f32[512]\", arg617_1: \"f32[384, 512]\", arg618_1: \"f32[384, 512]\", arg619_1: \"f32[384, 512]\", arg620_1: \"f32[512, 384]\", arg621_1: \"f32[512]\", arg622_1: \"f32[1024, 512]\", arg623_1: \"f32[1024, 512]\", arg624_1: \"f32[512, 1024]\", arg625_1: \"f32[512]\", arg626_1: \"f32[384, 512]\", arg627_1: \"f32[384, 512]\", arg628_1: \"f32[384, 512]\", arg629_1: \"f32[512, 384]\", arg630_1: \"f32[512]\", arg631_1: \"f32[384, 512]\", arg632_1: \"f32[384, 512]\", arg633_1: \"f32[384, 512]\", arg634_1: \"f32[512, 384]\", arg635_1: \"f32[512]\", arg636_1: \"f32[1024, 512]\", arg637_1: \"f32[1024, 512]\", arg638_1: \"f32[512, 1024]\", arg639_1: \"f32[512]\", arg640_1: \"f32[384, 512]\", arg641_1: \"f32[384, 512]\", arg642_1: \"f32[384, 512]\", arg643_1: \"f32[512, 384]\", arg644_1: \"f32[512]\", arg645_1: \"f32[384, 512]\", arg646_1: \"f32[384, 512]\", arg647_1: \"f32[384, 512]\", arg648_1: \"f32[512, 384]\", arg649_1: \"f32[512]\", arg650_1: \"f32[1024, 512]\", arg651_1: \"f32[1024, 512]\", arg652_1: \"f32[512, 1024]\", arg653_1: \"f32[512]\", arg654_1: \"f32[384, 512]\", arg655_1: \"f32[384, 512]\", arg656_1: \"f32[384, 512]\", arg657_1: \"f32[512, 384]\", arg658_1: \"f32[512]\", arg659_1: \"f32[384, 512]\", arg660_1: \"f32[384, 512]\", arg661_1: \"f32[384, 512]\", arg662_1: \"f32[512, 384]\", arg663_1: \"f32[512]\", arg664_1: \"f32[1024, 512]\", arg665_1: \"f32[1024, 512]\", arg666_1: \"f32[512, 1024]\", arg667_1: \"f32[512]\", arg668_1: \"f32[384, 512]\", arg669_1: \"f32[384, 512]\", arg670_1: \"f32[384, 512]\", arg671_1: \"f32[512, 384]\", arg672_1: \"f32[512]\", arg673_1: \"f32[384, 512]\", arg674_1: \"f32[384, 512]\", arg675_1: \"f32[384, 512]\", arg676_1: \"f32[512, 384]\", arg677_1: \"f32[512]\", arg678_1: \"f32[1024, 512]\", arg679_1: \"f32[1024, 512]\", arg680_1: \"f32[512, 1024]\", arg681_1: \"f32[512]\", arg682_1: \"f32[384, 512]\", arg683_1: \"f32[384, 512]\", arg684_1: \"f32[384, 512]\", arg685_1: \"f32[512, 384]\", arg686_1: \"f32[512]\", arg687_1: \"f32[384, 512]\", arg688_1: \"f32[384, 512]\", arg689_1: \"f32[384, 512]\", arg690_1: \"f32[512, 384]\", arg691_1: \"f32[512]\", arg692_1: \"f32[1024, 512]\", arg693_1: \"f32[1024, 512]\", arg694_1: \"f32[512, 1024]\", arg695_1: \"f32[512]\", arg696_1: \"f32[512]\", arg697_1: \"f32[32128, 512]\", arg698_1: \"i64[1, 77]\", arg699_1: \"f32[109, 109]\", arg700_1: \"f32[109, 109]\", arg701_1: \"f32[109, 109]\", arg702_1: \"i64[1, 512]\", arg703_1: \"i64[1, 512]\", arg704_1: \"f32[1, 3, 224, 224]\", arg705_1: \"i64[1, 77]\", arg706_1: \"i64[1, 77]\", arg707_1: \"i64[]\"):\n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:554 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d: \"f32[1, 192, 14, 14]\" = torch.ops.aten.conv2d.default(arg704_1, arg2_1, arg3_1, [16, 16]);  arg704_1 = arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/patch_embed.py:133 in forward, code: x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
      "    flatten: \"f32[1, 192, 196]\" = torch.ops.aten.flatten.using_ints(conv2d, 2);  conv2d = None\n",
      "    transpose: \"f32[1, 196, 192]\" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:260 in forward, code: image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
      "    expand: \"f32[1, 1, 192]\" = torch.ops.aten.expand.default(arg0_1, [1, -1, -1]);  arg0_1 = None\n",
      "    cat: \"f32[1, 197, 192]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
      "    add: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(cat, arg1_1);  cat = arg1_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(add, 0.0, False);  add = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(dropout, [192], arg4_1, arg5_1, 1e-06);  arg4_1 = arg5_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm, arg6_1, arg7_1);  layer_norm = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear, [1, 197, 3, 3, 64]);  linear = None\n",
      "    permute: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape, [2, 0, 3, 1, 4]);  reshape = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind = torch.ops.aten.unbind.int(permute);  permute = None\n",
      "    getitem: \"f32[1, 3, 197, 64]\" = unbind[0]\n",
      "    getitem_1: \"f32[1, 3, 197, 64]\" = unbind[1]\n",
      "    getitem_2: \"f32[1, 3, 197, 64]\" = unbind[2];  unbind = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem, getitem_1, getitem_2);  getitem = getitem_1 = getitem_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_1: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
      "    reshape_1: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_1, [1, 197, 192]);  transpose_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_1: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_1, arg8_1, arg9_1);  reshape_1 = arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_1: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_1, 0.0, False);  linear_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_1: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(dropout, dropout_1);  dropout = dropout_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_1: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_1, [192], arg10_1, arg11_1, 1e-06);  arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_2: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_1, arg12_1, arg13_1);  layer_norm_1 = arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_2: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu, 0.0, False);  gelu = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_3: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_2, arg14_1, arg15_1);  dropout_2 = arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_3: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_2: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_1, dropout_3);  add_1 = dropout_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_2: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_2, [192], arg16_1, arg17_1, 1e-06);  arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_4: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_2, arg18_1, arg19_1);  layer_norm_2 = arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_2: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_4, [1, 197, 3, 3, 64]);  linear_4 = None\n",
      "    permute_1: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_2, [2, 0, 3, 1, 4]);  reshape_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_1 = torch.ops.aten.unbind.int(permute_1);  permute_1 = None\n",
      "    getitem_3: \"f32[1, 3, 197, 64]\" = unbind_1[0]\n",
      "    getitem_4: \"f32[1, 3, 197, 64]\" = unbind_1[1]\n",
      "    getitem_5: \"f32[1, 3, 197, 64]\" = unbind_1[2];  unbind_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_1: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_3, getitem_4, getitem_5);  getitem_3 = getitem_4 = getitem_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_2: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
      "    reshape_3: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_2, [1, 197, 192]);  transpose_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_5: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_3, arg20_1, arg21_1);  reshape_3 = arg20_1 = arg21_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_4: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_3: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_2, dropout_4);  add_2 = dropout_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_3: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_3, [192], arg22_1, arg23_1, 1e-06);  arg22_1 = arg23_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_6: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_3, arg24_1, arg25_1);  layer_norm_3 = arg24_1 = arg25_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_1: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_5: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_1, 0.0, False);  gelu_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_7: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_5, arg26_1, arg27_1);  dropout_5 = arg26_1 = arg27_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_6: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_7, 0.0, False);  linear_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_4: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_3, dropout_6);  add_3 = dropout_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_4: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_4, [192], arg28_1, arg29_1, 1e-06);  arg28_1 = arg29_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_8: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_4, arg30_1, arg31_1);  layer_norm_4 = arg30_1 = arg31_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_4: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_8, [1, 197, 3, 3, 64]);  linear_8 = None\n",
      "    permute_2: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_4, [2, 0, 3, 1, 4]);  reshape_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_2 = torch.ops.aten.unbind.int(permute_2);  permute_2 = None\n",
      "    getitem_6: \"f32[1, 3, 197, 64]\" = unbind_2[0]\n",
      "    getitem_7: \"f32[1, 3, 197, 64]\" = unbind_2[1]\n",
      "    getitem_8: \"f32[1, 3, 197, 64]\" = unbind_2[2];  unbind_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_2: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_6, getitem_7, getitem_8);  getitem_6 = getitem_7 = getitem_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_3: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
      "    reshape_5: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_3, [1, 197, 192]);  transpose_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_9: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_5, arg32_1, arg33_1);  reshape_5 = arg32_1 = arg33_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_7: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_9, 0.0, False);  linear_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_5: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_4, dropout_7);  add_4 = dropout_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_5: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_5, [192], arg34_1, arg35_1, 1e-06);  arg34_1 = arg35_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_10: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_5, arg36_1, arg37_1);  layer_norm_5 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_2: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_8: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_2, 0.0, False);  gelu_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_11: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_8, arg38_1, arg39_1);  dropout_8 = arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_9: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_6: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_5, dropout_9);  add_5 = dropout_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_6: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_6, [192], arg40_1, arg41_1, 1e-06);  arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_12: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_6, arg42_1, arg43_1);  layer_norm_6 = arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_6: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_12, [1, 197, 3, 3, 64]);  linear_12 = None\n",
      "    permute_3: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_6, [2, 0, 3, 1, 4]);  reshape_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_3 = torch.ops.aten.unbind.int(permute_3);  permute_3 = None\n",
      "    getitem_9: \"f32[1, 3, 197, 64]\" = unbind_3[0]\n",
      "    getitem_10: \"f32[1, 3, 197, 64]\" = unbind_3[1]\n",
      "    getitem_11: \"f32[1, 3, 197, 64]\" = unbind_3[2];  unbind_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_3: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_9, getitem_10, getitem_11);  getitem_9 = getitem_10 = getitem_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_4: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
      "    reshape_7: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_4, [1, 197, 192]);  transpose_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_13: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_7, arg44_1, arg45_1);  reshape_7 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_10: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_13, 0.0, False);  linear_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_7: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_6, dropout_10);  add_6 = dropout_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_7: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_7, [192], arg46_1, arg47_1, 1e-06);  arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_14: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_7, arg48_1, arg49_1);  layer_norm_7 = arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_3: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_11: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_3, 0.0, False);  gelu_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_15: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_11, arg50_1, arg51_1);  dropout_11 = arg50_1 = arg51_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_12: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_15, 0.0, False);  linear_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_8: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_7, dropout_12);  add_7 = dropout_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_8: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_8, [192], arg52_1, arg53_1, 1e-06);  arg52_1 = arg53_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_16: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_8, arg54_1, arg55_1);  layer_norm_8 = arg54_1 = arg55_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_8: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_16, [1, 197, 3, 3, 64]);  linear_16 = None\n",
      "    permute_4: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_8, [2, 0, 3, 1, 4]);  reshape_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_4 = torch.ops.aten.unbind.int(permute_4);  permute_4 = None\n",
      "    getitem_12: \"f32[1, 3, 197, 64]\" = unbind_4[0]\n",
      "    getitem_13: \"f32[1, 3, 197, 64]\" = unbind_4[1]\n",
      "    getitem_14: \"f32[1, 3, 197, 64]\" = unbind_4[2];  unbind_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_4: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_12, getitem_13, getitem_14);  getitem_12 = getitem_13 = getitem_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_5: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
      "    reshape_9: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_5, [1, 197, 192]);  transpose_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_17: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_9, arg56_1, arg57_1);  reshape_9 = arg56_1 = arg57_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_13: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_9: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_8, dropout_13);  add_8 = dropout_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_9: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_9, [192], arg58_1, arg59_1, 1e-06);  arg58_1 = arg59_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_18: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_9, arg60_1, arg61_1);  layer_norm_9 = arg60_1 = arg61_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_4: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_14: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_4, 0.0, False);  gelu_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_19: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_14, arg62_1, arg63_1);  dropout_14 = arg62_1 = arg63_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_15: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_19, 0.0, False);  linear_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_10: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_9, dropout_15);  add_9 = dropout_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_10: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_10, [192], arg64_1, arg65_1, 1e-06);  arg64_1 = arg65_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_20: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_10, arg66_1, arg67_1);  layer_norm_10 = arg66_1 = arg67_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_10: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_20, [1, 197, 3, 3, 64]);  linear_20 = None\n",
      "    permute_5: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_10, [2, 0, 3, 1, 4]);  reshape_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_5 = torch.ops.aten.unbind.int(permute_5);  permute_5 = None\n",
      "    getitem_15: \"f32[1, 3, 197, 64]\" = unbind_5[0]\n",
      "    getitem_16: \"f32[1, 3, 197, 64]\" = unbind_5[1]\n",
      "    getitem_17: \"f32[1, 3, 197, 64]\" = unbind_5[2];  unbind_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_5: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_15, getitem_16, getitem_17);  getitem_15 = getitem_16 = getitem_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_6: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
      "    reshape_11: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_6, [1, 197, 192]);  transpose_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_21: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_11, arg68_1, arg69_1);  reshape_11 = arg68_1 = arg69_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_16: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_21, 0.0, False);  linear_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_11: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_10, dropout_16);  add_10 = dropout_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_11: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_11, [192], arg70_1, arg71_1, 1e-06);  arg70_1 = arg71_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_22: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_11, arg72_1, arg73_1);  layer_norm_11 = arg72_1 = arg73_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_5: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_17: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_5, 0.0, False);  gelu_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_23: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_17, arg74_1, arg75_1);  dropout_17 = arg74_1 = arg75_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_18: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_12: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_11, dropout_18);  add_11 = dropout_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_12: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_12, [192], arg76_1, arg77_1, 1e-06);  arg76_1 = arg77_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_24: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_12, arg78_1, arg79_1);  layer_norm_12 = arg78_1 = arg79_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_12: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_24, [1, 197, 3, 3, 64]);  linear_24 = None\n",
      "    permute_6: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_12, [2, 0, 3, 1, 4]);  reshape_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_6 = torch.ops.aten.unbind.int(permute_6);  permute_6 = None\n",
      "    getitem_18: \"f32[1, 3, 197, 64]\" = unbind_6[0]\n",
      "    getitem_19: \"f32[1, 3, 197, 64]\" = unbind_6[1]\n",
      "    getitem_20: \"f32[1, 3, 197, 64]\" = unbind_6[2];  unbind_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_6: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_18, getitem_19, getitem_20);  getitem_18 = getitem_19 = getitem_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_7: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
      "    reshape_13: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_7, [1, 197, 192]);  transpose_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_25: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_13, arg80_1, arg81_1);  reshape_13 = arg80_1 = arg81_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_19: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_25, 0.0, False);  linear_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_13: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_12, dropout_19);  add_12 = dropout_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_13: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_13, [192], arg82_1, arg83_1, 1e-06);  arg82_1 = arg83_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_26: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_13, arg84_1, arg85_1);  layer_norm_13 = arg84_1 = arg85_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_6: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_20: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_6, 0.0, False);  gelu_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_27: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_20, arg86_1, arg87_1);  dropout_20 = arg86_1 = arg87_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_21: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_27, 0.0, False);  linear_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_14: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_13, dropout_21);  add_13 = dropout_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_14: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_14, [192], arg88_1, arg89_1, 1e-06);  arg88_1 = arg89_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_28: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_14, arg90_1, arg91_1);  layer_norm_14 = arg90_1 = arg91_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_14: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_28, [1, 197, 3, 3, 64]);  linear_28 = None\n",
      "    permute_7: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_14, [2, 0, 3, 1, 4]);  reshape_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_7 = torch.ops.aten.unbind.int(permute_7);  permute_7 = None\n",
      "    getitem_21: \"f32[1, 3, 197, 64]\" = unbind_7[0]\n",
      "    getitem_22: \"f32[1, 3, 197, 64]\" = unbind_7[1]\n",
      "    getitem_23: \"f32[1, 3, 197, 64]\" = unbind_7[2];  unbind_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_7: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_21, getitem_22, getitem_23);  getitem_21 = getitem_22 = getitem_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_8: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
      "    reshape_15: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_8, [1, 197, 192]);  transpose_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_29: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_15, arg92_1, arg93_1);  reshape_15 = arg92_1 = arg93_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_22: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_29, 0.0, False);  linear_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_15: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_14, dropout_22);  add_14 = dropout_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_15: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_15, [192], arg94_1, arg95_1, 1e-06);  arg94_1 = arg95_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_30: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_15, arg96_1, arg97_1);  layer_norm_15 = arg96_1 = arg97_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_7: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_23: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_31: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_23, arg98_1, arg99_1);  dropout_23 = arg98_1 = arg99_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_24: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_31, 0.0, False);  linear_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_16: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_15, dropout_24);  add_15 = dropout_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_16: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_16, [192], arg100_1, arg101_1, 1e-06);  arg100_1 = arg101_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_32: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_16, arg102_1, arg103_1);  layer_norm_16 = arg102_1 = arg103_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_16: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_32, [1, 197, 3, 3, 64]);  linear_32 = None\n",
      "    permute_8: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_16, [2, 0, 3, 1, 4]);  reshape_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_8 = torch.ops.aten.unbind.int(permute_8);  permute_8 = None\n",
      "    getitem_24: \"f32[1, 3, 197, 64]\" = unbind_8[0]\n",
      "    getitem_25: \"f32[1, 3, 197, 64]\" = unbind_8[1]\n",
      "    getitem_26: \"f32[1, 3, 197, 64]\" = unbind_8[2];  unbind_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_8: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_24, getitem_25, getitem_26);  getitem_24 = getitem_25 = getitem_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_9: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
      "    reshape_17: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_9, [1, 197, 192]);  transpose_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_33: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_17, arg104_1, arg105_1);  reshape_17 = arg104_1 = arg105_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_25: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_33, 0.0, False);  linear_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_17: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_16, dropout_25);  add_16 = dropout_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_17: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_17, [192], arg106_1, arg107_1, 1e-06);  arg106_1 = arg107_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_34: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_17, arg108_1, arg109_1);  layer_norm_17 = arg108_1 = arg109_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_8: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_26: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_8, 0.0, False);  gelu_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_35: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_26, arg110_1, arg111_1);  dropout_26 = arg110_1 = arg111_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_27: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_35, 0.0, False);  linear_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_18: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_17, dropout_27);  add_17 = dropout_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_18: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_18, [192], arg112_1, arg113_1, 1e-06);  arg112_1 = arg113_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_36: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_18, arg114_1, arg115_1);  layer_norm_18 = arg114_1 = arg115_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_18: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_36, [1, 197, 3, 3, 64]);  linear_36 = None\n",
      "    permute_9: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_18, [2, 0, 3, 1, 4]);  reshape_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_9 = torch.ops.aten.unbind.int(permute_9);  permute_9 = None\n",
      "    getitem_27: \"f32[1, 3, 197, 64]\" = unbind_9[0]\n",
      "    getitem_28: \"f32[1, 3, 197, 64]\" = unbind_9[1]\n",
      "    getitem_29: \"f32[1, 3, 197, 64]\" = unbind_9[2];  unbind_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_9: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_27, getitem_28, getitem_29);  getitem_27 = getitem_28 = getitem_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_10: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
      "    reshape_19: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_10, [1, 197, 192]);  transpose_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_37: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_19, arg116_1, arg117_1);  reshape_19 = arg116_1 = arg117_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_28: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_37, 0.0, False);  linear_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_19: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_18, dropout_28);  add_18 = dropout_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_19: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_19, [192], arg118_1, arg119_1, 1e-06);  arg118_1 = arg119_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_38: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_19, arg120_1, arg121_1);  layer_norm_19 = arg120_1 = arg121_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_9: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_29: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_39: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_29, arg122_1, arg123_1);  dropout_29 = arg122_1 = arg123_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_30: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_39, 0.0, False);  linear_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_20: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_19, dropout_30);  add_19 = dropout_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_20: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_20, [192], arg124_1, arg125_1, 1e-06);  arg124_1 = arg125_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_40: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_20, arg126_1, arg127_1);  layer_norm_20 = arg126_1 = arg127_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_20: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_40, [1, 197, 3, 3, 64]);  linear_40 = None\n",
      "    permute_10: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_20, [2, 0, 3, 1, 4]);  reshape_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_10 = torch.ops.aten.unbind.int(permute_10);  permute_10 = None\n",
      "    getitem_30: \"f32[1, 3, 197, 64]\" = unbind_10[0]\n",
      "    getitem_31: \"f32[1, 3, 197, 64]\" = unbind_10[1]\n",
      "    getitem_32: \"f32[1, 3, 197, 64]\" = unbind_10[2];  unbind_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_10: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_30, getitem_31, getitem_32);  getitem_30 = getitem_31 = getitem_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_11: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
      "    reshape_21: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_11, [1, 197, 192]);  transpose_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_41: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_21, arg128_1, arg129_1);  reshape_21 = arg128_1 = arg129_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_31: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_41, 0.0, False);  linear_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_21: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_20, dropout_31);  add_20 = dropout_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_21: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_21, [192], arg130_1, arg131_1, 1e-06);  arg130_1 = arg131_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_42: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_21, arg132_1, arg133_1);  layer_norm_21 = arg132_1 = arg133_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_10: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_32: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_10, 0.0, False);  gelu_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_43: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_32, arg134_1, arg135_1);  dropout_32 = arg134_1 = arg135_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_33: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_43, 0.0, False);  linear_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_22: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_21, dropout_33);  add_21 = dropout_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_22: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_22, [192], arg136_1, arg137_1, 1e-06);  arg136_1 = arg137_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_44: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_22, arg138_1, arg139_1);  layer_norm_22 = arg138_1 = arg139_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_22: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_44, [1, 197, 3, 3, 64]);  linear_44 = None\n",
      "    permute_11: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_22, [2, 0, 3, 1, 4]);  reshape_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_11 = torch.ops.aten.unbind.int(permute_11);  permute_11 = None\n",
      "    getitem_33: \"f32[1, 3, 197, 64]\" = unbind_11[0]\n",
      "    getitem_34: \"f32[1, 3, 197, 64]\" = unbind_11[1]\n",
      "    getitem_35: \"f32[1, 3, 197, 64]\" = unbind_11[2];  unbind_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_11: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_33, getitem_34, getitem_35);  getitem_33 = getitem_34 = getitem_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_12: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
      "    reshape_23: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_12, [1, 197, 192]);  transpose_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_45: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_23, arg140_1, arg141_1);  reshape_23 = arg140_1 = arg141_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_34: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_45, 0.0, False);  linear_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_23: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_22, dropout_34);  add_22 = dropout_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_23: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_23, [192], arg142_1, arg143_1, 1e-06);  arg142_1 = arg143_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_46: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_23, arg144_1, arg145_1);  layer_norm_23 = arg144_1 = arg145_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_11: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_35: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_47: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_35, arg146_1, arg147_1);  dropout_35 = arg146_1 = arg147_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_36: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_47, 0.0, False);  linear_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_24: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_23, dropout_36);  add_23 = dropout_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_24: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_24, [192], arg148_1, arg149_1, 1e-06);  add_24 = arg148_1 = arg149_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_48: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_24, arg150_1, arg151_1);  layer_norm_24 = arg150_1 = arg151_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:263 in forward, code: cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    item: \"Sym(77)\" = torch.ops.aten.item.default(arg707_1)\n",
      "    zeros: \"i64[77]\" = torch.ops.aten.zeros.default([item], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:264 in forward, code: dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    item_1: \"Sym(77)\" = torch.ops.aten.item.default(arg707_1);  arg707_1 = item_1 = None\n",
      "    zeros_1: \"i64[77]\" = torch.ops.aten.zeros.default([item], dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  item = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:144 in forward, code: learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
      "    unsqueeze: \"f32[1, 32, 768]\" = torch.ops.aten.unsqueeze.default(arg152_1, 0);  arg152_1 = None\n",
      "    expand_1: \"f32[1, 32, 768]\" = torch.ops.aten.expand.default(unsqueeze, [1, -1, -1]);  unsqueeze = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros);  zeros = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_2: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1])\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_1: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_2);  expand_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_25: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_2: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros_1);  zeros_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_3: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1]);  arg698_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_3: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_3);  arg503_1 = expand_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_26: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding_2, embedding_3);  embedding_2 = embedding_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:105 in forward, code: itc_query_embds = query_embds.clone()\n",
      "    clone: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:106 in forward, code: itm_query_embds = query_embds.clone()\n",
      "    clone_1: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  clone_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:107 in forward, code: itg_query_embds = query_embds.clone()\n",
      "    clone_2: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  expand_1 = clone_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:109 in forward, code: itc_text_embds = cls_text_embds.clone()\n",
      "    clone_3: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:110 in forward, code: itm_text_embds = cls_text_embds.clone()\n",
      "    clone_4: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25);  add_25 = clone_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:111 in forward, code: itg_text_embds = dec_text_embds.clone()\n",
      "    clone_5: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_26);  add_26 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:117 in forward, code: itc_add = to_additive_mask(self.itc_attn_mask, device=device, dtype=dtype)\n",
      "    to: \"f32[109, 109]\" = torch.ops.aten.to.device(arg699_1, device(type='cpu'), torch.float32);  arg699_1 = None\n",
      "    gt: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to, 0)\n",
      "    zeros_like: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to, pin_memory = False)\n",
      "    full_like: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to, -inf, pin_memory = False);  to = None\n",
      "    where: \"f32[109, 109]\" = torch.ops.aten.where.self(gt, zeros_like, full_like);  gt = zeros_like = full_like = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:118 in forward, code: itm_add = to_additive_mask(self.itm_attn_mask, device=device, dtype=dtype)\n",
      "    to_1: \"f32[109, 109]\" = torch.ops.aten.to.device(arg700_1, device(type='cpu'), torch.float32);  arg700_1 = None\n",
      "    gt_1: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_1, 0)\n",
      "    zeros_like_1: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_1, pin_memory = False)\n",
      "    full_like_1: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_1, -inf, pin_memory = False);  to_1 = None\n",
      "    where_1: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_1, zeros_like_1, full_like_1);  gt_1 = zeros_like_1 = full_like_1 = where_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:119 in forward, code: itg_add = to_additive_mask(self.itg_attn_mask, device=device, dtype=dtype)\n",
      "    to_2: \"f32[109, 109]\" = torch.ops.aten.to.device(arg701_1, device(type='cpu'), torch.float32);  arg701_1 = None\n",
      "    gt_2: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_2, 0)\n",
      "    zeros_like_2: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_2, pin_memory = False)\n",
      "    full_like_2: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_2, -inf, pin_memory = False);  to_2 = None\n",
      "    where_2: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_2, zeros_like_2, full_like_2);  gt_2 = zeros_like_2 = full_like_2 = where_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([clone, clone_3], 1);  clone = clone_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_13: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_49: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_13, arg352_1, arg353_1);  transpose_13 = arg352_1 = arg353_1 = None\n",
      "    unflatten: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_49, -1, [3, 768]);  linear_49 = None\n",
      "    unsqueeze_1: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten, 0);  unflatten = None\n",
      "    transpose_14: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_1, 0, -2);  unsqueeze_1 = None\n",
      "    squeeze: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_14, -2);  transpose_14 = None\n",
      "    contiguous: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze);  squeeze = None\n",
      "    select: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 0)\n",
      "    select_1: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 1)\n",
      "    select_2: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 2);  contiguous = None\n",
      "    unsqueeze_2: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select, [109, 12, 64]);  select = None\n",
      "    transpose_15: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view, 0, 1);  view = None\n",
      "    view_1: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_1, [109, 12, 64]);  select_1 = None\n",
      "    transpose_16: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_1, 0, 1);  view_1 = None\n",
      "    view_2: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_2, [109, 12, 64]);  select_2 = None\n",
      "    transpose_17: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_2, 0, 1);  view_2 = None\n",
      "    mul: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_15, 0.125);  transpose_15 = None\n",
      "    transpose_18: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_16, -2, -1);  transpose_16 = None\n",
      "    baddbmm: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_2, mul, transpose_18);  unsqueeze_2 = mul = transpose_18 = None\n",
      "    softmax: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm, -1);  baddbmm = None\n",
      "    bmm: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax, transpose_17);  transpose_17 = None\n",
      "    transpose_19: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm, 0, 1);  bmm = None\n",
      "    contiguous_1: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_19);  transpose_19 = None\n",
      "    view_3: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_1, [109, 768]);  contiguous_1 = None\n",
      "    linear_50: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_3, arg354_1, arg355_1);  view_3 = arg354_1 = arg355_1 = None\n",
      "    view_4: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_50, [109, 1, 768]);  linear_50 = None\n",
      "    view_5: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax, [1, 12, 109, 109]);  softmax = None\n",
      "    mean: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_5, [1]);  view_5 = mean = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_20: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_4, 1, 0);  view_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_27: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat, transpose_20);  concat = transpose_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_25: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_27, [768], arg356_1, arg357_1);  add_27 = arg356_1 = arg357_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_1: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25)\n",
      "    slice_2: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_1, 1, None, 32);  slice_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_3: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25);  layer_norm_25 = None\n",
      "    slice_4: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_3, 1, 32);  slice_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_21: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_2, 1, 0)\n",
      "    transpose_22: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(arg370_1, [768, 1536]);  arg370_1 = None\n",
      "    getitem_36: \"f32[768, 768]\" = split_with_sizes[0]\n",
      "    getitem_37: \"f32[1536, 768]\" = split_with_sizes[1];  split_with_sizes = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(arg371_1, [768, 1536]);  arg371_1 = None\n",
      "    getitem_38: \"f32[768]\" = split_with_sizes_1[0]\n",
      "    getitem_39: \"f32[1536]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
      "    linear_51: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_21, getitem_36, getitem_38);  transpose_21 = getitem_36 = getitem_38 = None\n",
      "    linear_52: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_22, getitem_37, getitem_39);  transpose_22 = getitem_37 = getitem_39 = None\n",
      "    unflatten_1: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_52, -1, [2, 768]);  linear_52 = None\n",
      "    unsqueeze_3: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_1, 0);  unflatten_1 = None\n",
      "    transpose_23: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_3, 0, -2);  unsqueeze_3 = None\n",
      "    squeeze_1: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_23, -2);  transpose_23 = None\n",
      "    contiguous_2: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_1);  squeeze_1 = None\n",
      "    select_3: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 0)\n",
      "    select_4: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 1);  contiguous_2 = None\n",
      "    view_6: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_51, [32, 12, 64]);  linear_51 = None\n",
      "    transpose_24: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_6, 0, 1);  view_6 = None\n",
      "    view_7: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_3, [197, 12, 64]);  select_3 = None\n",
      "    transpose_25: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_7, 0, 1);  view_7 = None\n",
      "    view_8: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_4, [197, 12, 64]);  select_4 = None\n",
      "    transpose_26: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_8, 0, 1);  view_8 = None\n",
      "    mul_1: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_24, 0.125);  transpose_24 = None\n",
      "    transpose_27: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_25, -2, -1);  transpose_25 = None\n",
      "    bmm_1: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_1, transpose_27);  mul_1 = transpose_27 = None\n",
      "    softmax_1: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_1, -1);  bmm_1 = None\n",
      "    bmm_2: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_1, transpose_26);  transpose_26 = None\n",
      "    transpose_28: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_2, 0, 1);  bmm_2 = None\n",
      "    contiguous_3: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_28);  transpose_28 = None\n",
      "    view_9: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_3, [32, 768]);  contiguous_3 = None\n",
      "    linear_53: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_9, arg372_1, arg373_1);  view_9 = arg372_1 = arg373_1 = None\n",
      "    view_10: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_53, [32, 1, 768]);  linear_53 = None\n",
      "    view_11: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_1, [1, 12, 32, 197]);  softmax_1 = None\n",
      "    mean_1: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_11, [1]);  view_11 = mean_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_29: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_10, 1, 0);  view_10 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_28: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_2, transpose_29);  slice_2 = transpose_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_26: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_28, [768], arg374_1, arg375_1);  add_28 = arg374_1 = arg375_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_54: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_26, arg358_1, arg359_1);  arg358_1 = arg359_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_12: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_54);  linear_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_55: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_12, arg360_1, arg361_1);  gelu_12 = arg360_1 = arg361_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_37: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_55, 0.1, False);  linear_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_29: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_37, layer_norm_26);  dropout_37 = layer_norm_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_27: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_29, [768], arg362_1, arg363_1, 1e-12);  add_29 = arg362_1 = arg363_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_56: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_4, arg364_1, arg365_1);  arg364_1 = arg365_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_13: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_56);  linear_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_57: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_13, arg366_1, arg367_1);  gelu_13 = arg366_1 = arg367_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_38: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_57, 0.1, False);  linear_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_30: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_38, slice_4);  dropout_38 = slice_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_28: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_30, [768], arg368_1, arg369_1, 1e-12);  add_30 = arg368_1 = arg369_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_1: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_27, layer_norm_28], 1);  layer_norm_27 = layer_norm_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_30: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_1, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_58: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_30, arg376_1, arg377_1);  transpose_30 = arg376_1 = arg377_1 = None\n",
      "    unflatten_2: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_58, -1, [3, 768]);  linear_58 = None\n",
      "    unsqueeze_4: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_2, 0);  unflatten_2 = None\n",
      "    transpose_31: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_4, 0, -2);  unsqueeze_4 = None\n",
      "    squeeze_2: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_31, -2);  transpose_31 = None\n",
      "    contiguous_4: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_2);  squeeze_2 = None\n",
      "    select_5: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 0)\n",
      "    select_6: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 1)\n",
      "    select_7: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 2);  contiguous_4 = None\n",
      "    unsqueeze_5: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_12: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_5, [109, 12, 64]);  select_5 = None\n",
      "    transpose_32: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_12, 0, 1);  view_12 = None\n",
      "    view_13: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_6, [109, 12, 64]);  select_6 = None\n",
      "    transpose_33: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_13, 0, 1);  view_13 = None\n",
      "    view_14: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_7, [109, 12, 64]);  select_7 = None\n",
      "    transpose_34: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_14, 0, 1);  view_14 = None\n",
      "    mul_2: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_32, 0.125);  transpose_32 = None\n",
      "    transpose_35: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_33, -2, -1);  transpose_33 = None\n",
      "    baddbmm_1: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_5, mul_2, transpose_35);  unsqueeze_5 = mul_2 = transpose_35 = None\n",
      "    softmax_2: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_1, -1);  baddbmm_1 = None\n",
      "    bmm_3: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_2, transpose_34);  transpose_34 = None\n",
      "    transpose_36: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_3, 0, 1);  bmm_3 = None\n",
      "    contiguous_5: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
      "    view_15: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_5, [109, 768]);  contiguous_5 = None\n",
      "    linear_59: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_15, arg378_1, arg379_1);  view_15 = arg378_1 = arg379_1 = None\n",
      "    view_16: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_59, [109, 1, 768]);  linear_59 = None\n",
      "    view_17: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_2, [1, 12, 109, 109]);  softmax_2 = None\n",
      "    mean_2: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_17, [1]);  view_17 = mean_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_37: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_16, 1, 0);  view_16 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_31: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_1, transpose_37);  concat_1 = transpose_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_29: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_31, [768], arg380_1, arg381_1);  add_31 = arg380_1 = arg381_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_5: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29)\n",
      "    slice_6: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_5, 1, None, 32);  slice_5 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_7: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29);  layer_norm_29 = None\n",
      "    slice_8: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_7, 1, 32);  slice_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_60: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_6, arg382_1, arg383_1);  arg382_1 = arg383_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_14: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_60);  linear_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_61: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_14, arg384_1, arg385_1);  gelu_14 = arg384_1 = arg385_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_39: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_61, 0.1, False);  linear_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_32: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_39, slice_6);  dropout_39 = slice_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_30: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_32, [768], arg386_1, arg387_1, 1e-12);  add_32 = arg386_1 = arg387_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_62: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_8, arg388_1, arg389_1);  arg388_1 = arg389_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_15: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_62);  linear_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_63: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_15, arg390_1, arg391_1);  gelu_15 = arg390_1 = arg391_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_40: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_63, 0.1, False);  linear_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_33: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_40, slice_8);  dropout_40 = slice_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_31: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_33, [768], arg392_1, arg393_1, 1e-12);  add_33 = arg392_1 = arg393_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_2: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_30, layer_norm_31], 1);  layer_norm_30 = layer_norm_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_38: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_2, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_64: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_38, arg394_1, arg395_1);  transpose_38 = arg394_1 = arg395_1 = None\n",
      "    unflatten_3: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_64, -1, [3, 768]);  linear_64 = None\n",
      "    unsqueeze_6: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_3, 0);  unflatten_3 = None\n",
      "    transpose_39: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_6, 0, -2);  unsqueeze_6 = None\n",
      "    squeeze_3: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_39, -2);  transpose_39 = None\n",
      "    contiguous_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_3);  squeeze_3 = None\n",
      "    select_8: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 0)\n",
      "    select_9: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 1)\n",
      "    select_10: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 2);  contiguous_6 = None\n",
      "    unsqueeze_7: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_18: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_8, [109, 12, 64]);  select_8 = None\n",
      "    transpose_40: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_18, 0, 1);  view_18 = None\n",
      "    view_19: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_9, [109, 12, 64]);  select_9 = None\n",
      "    transpose_41: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_19, 0, 1);  view_19 = None\n",
      "    view_20: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_10, [109, 12, 64]);  select_10 = None\n",
      "    transpose_42: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_20, 0, 1);  view_20 = None\n",
      "    mul_3: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_40, 0.125);  transpose_40 = None\n",
      "    transpose_43: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_41, -2, -1);  transpose_41 = None\n",
      "    baddbmm_2: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_7, mul_3, transpose_43);  unsqueeze_7 = mul_3 = transpose_43 = None\n",
      "    softmax_3: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_2, -1);  baddbmm_2 = None\n",
      "    bmm_4: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_3, transpose_42);  transpose_42 = None\n",
      "    transpose_44: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_4, 0, 1);  bmm_4 = None\n",
      "    contiguous_7: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_44);  transpose_44 = None\n",
      "    view_21: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_7, [109, 768]);  contiguous_7 = None\n",
      "    linear_65: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_21, arg396_1, arg397_1);  view_21 = arg396_1 = arg397_1 = None\n",
      "    view_22: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_65, [109, 1, 768]);  linear_65 = None\n",
      "    view_23: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_3, [1, 12, 109, 109]);  softmax_3 = None\n",
      "    mean_3: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_23, [1]);  view_23 = mean_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_45: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_22, 1, 0);  view_22 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_34: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_2, transpose_45);  concat_2 = transpose_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_32: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_34, [768], arg398_1, arg399_1);  add_34 = arg398_1 = arg399_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_9: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32)\n",
      "    slice_10: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_9, 1, None, 32);  slice_9 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_11: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32);  layer_norm_32 = None\n",
      "    slice_12: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_11, 1, 32);  slice_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_46: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_10, 1, 0)\n",
      "    transpose_47: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(arg412_1, [768, 1536]);  arg412_1 = None\n",
      "    getitem_40: \"f32[768, 768]\" = split_with_sizes_2[0]\n",
      "    getitem_41: \"f32[1536, 768]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
      "    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(arg413_1, [768, 1536]);  arg413_1 = None\n",
      "    getitem_42: \"f32[768]\" = split_with_sizes_3[0]\n",
      "    getitem_43: \"f32[1536]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
      "    linear_66: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_46, getitem_40, getitem_42);  transpose_46 = getitem_40 = getitem_42 = None\n",
      "    linear_67: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_47, getitem_41, getitem_43);  transpose_47 = getitem_41 = getitem_43 = None\n",
      "    unflatten_4: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_67, -1, [2, 768]);  linear_67 = None\n",
      "    unsqueeze_8: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_4, 0);  unflatten_4 = None\n",
      "    transpose_48: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_8, 0, -2);  unsqueeze_8 = None\n",
      "    squeeze_4: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_48, -2);  transpose_48 = None\n",
      "    contiguous_8: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_4);  squeeze_4 = None\n",
      "    select_11: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 0)\n",
      "    select_12: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 1);  contiguous_8 = None\n",
      "    view_24: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_66, [32, 12, 64]);  linear_66 = None\n",
      "    transpose_49: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_24, 0, 1);  view_24 = None\n",
      "    view_25: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_11, [197, 12, 64]);  select_11 = None\n",
      "    transpose_50: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_25, 0, 1);  view_25 = None\n",
      "    view_26: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_12, [197, 12, 64]);  select_12 = None\n",
      "    transpose_51: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_26, 0, 1);  view_26 = None\n",
      "    mul_4: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_49, 0.125);  transpose_49 = None\n",
      "    transpose_52: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_50, -2, -1);  transpose_50 = None\n",
      "    bmm_5: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_4, transpose_52);  mul_4 = transpose_52 = None\n",
      "    softmax_4: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_5, -1);  bmm_5 = None\n",
      "    bmm_6: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_4, transpose_51);  transpose_51 = None\n",
      "    transpose_53: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_6, 0, 1);  bmm_6 = None\n",
      "    contiguous_9: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_53);  transpose_53 = None\n",
      "    view_27: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_9, [32, 768]);  contiguous_9 = None\n",
      "    linear_68: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_27, arg414_1, arg415_1);  view_27 = arg414_1 = arg415_1 = None\n",
      "    view_28: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_68, [32, 1, 768]);  linear_68 = None\n",
      "    view_29: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_4, [1, 12, 32, 197]);  softmax_4 = None\n",
      "    mean_4: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_29, [1]);  view_29 = mean_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_54: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_28, 1, 0);  view_28 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_35: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_10, transpose_54);  slice_10 = transpose_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_33: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_35, [768], arg416_1, arg417_1);  add_35 = arg416_1 = arg417_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_69: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_33, arg400_1, arg401_1);  arg400_1 = arg401_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_16: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_69);  linear_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_70: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_16, arg402_1, arg403_1);  gelu_16 = arg402_1 = arg403_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_41: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_70, 0.1, False);  linear_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_36: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_41, layer_norm_33);  dropout_41 = layer_norm_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_34: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_36, [768], arg404_1, arg405_1, 1e-12);  add_36 = arg404_1 = arg405_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_71: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_12, arg406_1, arg407_1);  arg406_1 = arg407_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_17: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_71);  linear_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_72: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_17, arg408_1, arg409_1);  gelu_17 = arg408_1 = arg409_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_42: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_72, 0.1, False);  linear_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_37: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_42, slice_12);  dropout_42 = slice_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_35: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_37, [768], arg410_1, arg411_1, 1e-12);  add_37 = arg410_1 = arg411_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_3: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_34, layer_norm_35], 1);  layer_norm_34 = layer_norm_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_55: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_3, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_73: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_55, arg418_1, arg419_1);  transpose_55 = arg418_1 = arg419_1 = None\n",
      "    unflatten_5: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_73, -1, [3, 768]);  linear_73 = None\n",
      "    unsqueeze_9: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_5, 0);  unflatten_5 = None\n",
      "    transpose_56: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_9, 0, -2);  unsqueeze_9 = None\n",
      "    squeeze_5: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_56, -2);  transpose_56 = None\n",
      "    contiguous_10: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_5);  squeeze_5 = None\n",
      "    select_13: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 0)\n",
      "    select_14: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 1)\n",
      "    select_15: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 2);  contiguous_10 = None\n",
      "    unsqueeze_10: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_30: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_13, [109, 12, 64]);  select_13 = None\n",
      "    transpose_57: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None\n",
      "    view_31: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_14, [109, 12, 64]);  select_14 = None\n",
      "    transpose_58: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_31, 0, 1);  view_31 = None\n",
      "    view_32: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_15, [109, 12, 64]);  select_15 = None\n",
      "    transpose_59: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_32, 0, 1);  view_32 = None\n",
      "    mul_5: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_57, 0.125);  transpose_57 = None\n",
      "    transpose_60: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_58, -2, -1);  transpose_58 = None\n",
      "    baddbmm_3: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_10, mul_5, transpose_60);  unsqueeze_10 = mul_5 = transpose_60 = None\n",
      "    softmax_5: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_3, -1);  baddbmm_3 = None\n",
      "    bmm_7: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_5, transpose_59);  transpose_59 = None\n",
      "    transpose_61: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_7, 0, 1);  bmm_7 = None\n",
      "    contiguous_11: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_61);  transpose_61 = None\n",
      "    view_33: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_11, [109, 768]);  contiguous_11 = None\n",
      "    linear_74: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_33, arg420_1, arg421_1);  view_33 = arg420_1 = arg421_1 = None\n",
      "    view_34: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_74, [109, 1, 768]);  linear_74 = None\n",
      "    view_35: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_5, [1, 12, 109, 109]);  softmax_5 = None\n",
      "    mean_5: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_35, [1]);  view_35 = mean_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_62: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_34, 1, 0);  view_34 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_38: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_3, transpose_62);  concat_3 = transpose_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_36: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_38, [768], arg422_1, arg423_1);  add_38 = arg422_1 = arg423_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_13: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36)\n",
      "    slice_14: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_13, 1, None, 32);  slice_13 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_15: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36);  layer_norm_36 = None\n",
      "    slice_16: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_15, 1, 32);  slice_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_75: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_14, arg424_1, arg425_1);  arg424_1 = arg425_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_18: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_75);  linear_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_76: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_18, arg426_1, arg427_1);  gelu_18 = arg426_1 = arg427_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_43: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_76, 0.1, False);  linear_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_39: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_43, slice_14);  dropout_43 = slice_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_37: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_39, [768], arg428_1, arg429_1, 1e-12);  add_39 = arg428_1 = arg429_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_77: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_16, arg430_1, arg431_1);  arg430_1 = arg431_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_19: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_77);  linear_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_78: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_19, arg432_1, arg433_1);  gelu_19 = arg432_1 = arg433_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_44: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_78, 0.1, False);  linear_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_40: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_44, slice_16);  dropout_44 = slice_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_38: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_40, [768], arg434_1, arg435_1, 1e-12);  add_40 = arg434_1 = arg435_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_4: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_37, layer_norm_38], 1);  layer_norm_37 = layer_norm_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_63: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_4, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_79: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_63, arg436_1, arg437_1);  transpose_63 = arg436_1 = arg437_1 = None\n",
      "    unflatten_6: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_79, -1, [3, 768]);  linear_79 = None\n",
      "    unsqueeze_11: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_6, 0);  unflatten_6 = None\n",
      "    transpose_64: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_11, 0, -2);  unsqueeze_11 = None\n",
      "    squeeze_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_64, -2);  transpose_64 = None\n",
      "    contiguous_12: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_6);  squeeze_6 = None\n",
      "    select_16: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 0)\n",
      "    select_17: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 1)\n",
      "    select_18: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 2);  contiguous_12 = None\n",
      "    unsqueeze_12: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_36: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_16, [109, 12, 64]);  select_16 = None\n",
      "    transpose_65: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_36, 0, 1);  view_36 = None\n",
      "    view_37: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_17, [109, 12, 64]);  select_17 = None\n",
      "    transpose_66: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_37, 0, 1);  view_37 = None\n",
      "    view_38: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_18, [109, 12, 64]);  select_18 = None\n",
      "    transpose_67: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_38, 0, 1);  view_38 = None\n",
      "    mul_6: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_65, 0.125);  transpose_65 = None\n",
      "    transpose_68: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_66, -2, -1);  transpose_66 = None\n",
      "    baddbmm_4: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_12, mul_6, transpose_68);  unsqueeze_12 = mul_6 = transpose_68 = None\n",
      "    softmax_6: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_4, -1);  baddbmm_4 = None\n",
      "    bmm_8: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_6, transpose_67);  transpose_67 = None\n",
      "    transpose_69: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_8, 0, 1);  bmm_8 = None\n",
      "    contiguous_13: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_69);  transpose_69 = None\n",
      "    view_39: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_13, [109, 768]);  contiguous_13 = None\n",
      "    linear_80: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_39, arg438_1, arg439_1);  view_39 = arg438_1 = arg439_1 = None\n",
      "    view_40: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_80, [109, 1, 768]);  linear_80 = None\n",
      "    view_41: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_6, [1, 12, 109, 109]);  softmax_6 = None\n",
      "    mean_6: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_41, [1]);  view_41 = mean_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_70: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_40, 1, 0);  view_40 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_41: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_4, transpose_70);  concat_4 = transpose_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_39: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_41, [768], arg440_1, arg441_1);  add_41 = arg440_1 = arg441_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_17: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39)\n",
      "    slice_18: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_17, 1, None, 32);  slice_17 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_19: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39);  layer_norm_39 = None\n",
      "    slice_20: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_19, 1, 32);  slice_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_71: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_18, 1, 0)\n",
      "    transpose_72: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(arg454_1, [768, 1536]);  arg454_1 = None\n",
      "    getitem_44: \"f32[768, 768]\" = split_with_sizes_4[0]\n",
      "    getitem_45: \"f32[1536, 768]\" = split_with_sizes_4[1];  split_with_sizes_4 = None\n",
      "    split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(arg455_1, [768, 1536]);  arg455_1 = None\n",
      "    getitem_46: \"f32[768]\" = split_with_sizes_5[0]\n",
      "    getitem_47: \"f32[1536]\" = split_with_sizes_5[1];  split_with_sizes_5 = None\n",
      "    linear_81: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_71, getitem_44, getitem_46);  transpose_71 = getitem_44 = getitem_46 = None\n",
      "    linear_82: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_72, getitem_45, getitem_47);  transpose_72 = getitem_45 = getitem_47 = None\n",
      "    unflatten_7: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_82, -1, [2, 768]);  linear_82 = None\n",
      "    unsqueeze_13: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_7, 0);  unflatten_7 = None\n",
      "    transpose_73: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_13, 0, -2);  unsqueeze_13 = None\n",
      "    squeeze_7: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_73, -2);  transpose_73 = None\n",
      "    contiguous_14: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_7);  squeeze_7 = None\n",
      "    select_19: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 0)\n",
      "    select_20: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 1);  contiguous_14 = None\n",
      "    view_42: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_81, [32, 12, 64]);  linear_81 = None\n",
      "    transpose_74: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_42, 0, 1);  view_42 = None\n",
      "    view_43: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_19, [197, 12, 64]);  select_19 = None\n",
      "    transpose_75: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_43, 0, 1);  view_43 = None\n",
      "    view_44: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_20, [197, 12, 64]);  select_20 = None\n",
      "    transpose_76: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_44, 0, 1);  view_44 = None\n",
      "    mul_7: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_74, 0.125);  transpose_74 = None\n",
      "    transpose_77: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_75, -2, -1);  transpose_75 = None\n",
      "    bmm_9: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_7, transpose_77);  mul_7 = transpose_77 = None\n",
      "    softmax_7: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_9, -1);  bmm_9 = None\n",
      "    bmm_10: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_7, transpose_76);  transpose_76 = None\n",
      "    transpose_78: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_10, 0, 1);  bmm_10 = None\n",
      "    contiguous_15: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_78);  transpose_78 = None\n",
      "    view_45: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_15, [32, 768]);  contiguous_15 = None\n",
      "    linear_83: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_45, arg456_1, arg457_1);  view_45 = arg456_1 = arg457_1 = None\n",
      "    view_46: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_83, [32, 1, 768]);  linear_83 = None\n",
      "    view_47: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_7, [1, 12, 32, 197]);  softmax_7 = None\n",
      "    mean_7: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_47, [1]);  view_47 = mean_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_79: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_46, 1, 0);  view_46 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_42: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_18, transpose_79);  slice_18 = transpose_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_40: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_42, [768], arg458_1, arg459_1);  add_42 = arg458_1 = arg459_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_84: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_40, arg442_1, arg443_1);  arg442_1 = arg443_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_20: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_84);  linear_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_85: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_20, arg444_1, arg445_1);  gelu_20 = arg444_1 = arg445_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_45: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_85, 0.1, False);  linear_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_43: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_45, layer_norm_40);  dropout_45 = layer_norm_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_41: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_43, [768], arg446_1, arg447_1, 1e-12);  add_43 = arg446_1 = arg447_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_86: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_20, arg448_1, arg449_1);  arg448_1 = arg449_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_21: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_86);  linear_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_87: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_21, arg450_1, arg451_1);  gelu_21 = arg450_1 = arg451_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_46: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_87, 0.1, False);  linear_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_44: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_46, slice_20);  dropout_46 = slice_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_42: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_44, [768], arg452_1, arg453_1, 1e-12);  add_44 = arg452_1 = arg453_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_5: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_41, layer_norm_42], 1);  layer_norm_41 = layer_norm_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_80: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_5, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_88: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_80, arg460_1, arg461_1);  transpose_80 = arg460_1 = arg461_1 = None\n",
      "    unflatten_8: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_88, -1, [3, 768]);  linear_88 = None\n",
      "    unsqueeze_14: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_8, 0);  unflatten_8 = None\n",
      "    transpose_81: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_14, 0, -2);  unsqueeze_14 = None\n",
      "    squeeze_8: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_81, -2);  transpose_81 = None\n",
      "    contiguous_16: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_8);  squeeze_8 = None\n",
      "    select_21: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 0)\n",
      "    select_22: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 1)\n",
      "    select_23: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 2);  contiguous_16 = None\n",
      "    unsqueeze_15: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_48: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_21, [109, 12, 64]);  select_21 = None\n",
      "    transpose_82: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_48, 0, 1);  view_48 = None\n",
      "    view_49: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_22, [109, 12, 64]);  select_22 = None\n",
      "    transpose_83: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_49, 0, 1);  view_49 = None\n",
      "    view_50: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_23, [109, 12, 64]);  select_23 = None\n",
      "    transpose_84: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_50, 0, 1);  view_50 = None\n",
      "    mul_8: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_82, 0.125);  transpose_82 = None\n",
      "    transpose_85: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_83, -2, -1);  transpose_83 = None\n",
      "    baddbmm_5: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_15, mul_8, transpose_85);  unsqueeze_15 = mul_8 = transpose_85 = None\n",
      "    softmax_8: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_5, -1);  baddbmm_5 = None\n",
      "    bmm_11: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_8, transpose_84);  transpose_84 = None\n",
      "    transpose_86: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_11, 0, 1);  bmm_11 = None\n",
      "    contiguous_17: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_86);  transpose_86 = None\n",
      "    view_51: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_17, [109, 768]);  contiguous_17 = None\n",
      "    linear_89: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_51, arg462_1, arg463_1);  view_51 = arg462_1 = arg463_1 = None\n",
      "    view_52: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_89, [109, 1, 768]);  linear_89 = None\n",
      "    view_53: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_8, [1, 12, 109, 109]);  softmax_8 = None\n",
      "    mean_8: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_53, [1]);  view_53 = mean_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_87: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_52, 1, 0);  view_52 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_45: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_5, transpose_87);  concat_5 = transpose_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_43: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_45, [768], arg464_1, arg465_1);  add_45 = arg464_1 = arg465_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_21: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43)\n",
      "    slice_22: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_21, 1, None, 32);  slice_21 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_23: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43);  layer_norm_43 = None\n",
      "    slice_24: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_23, 1, 32);  slice_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_90: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_22, arg466_1, arg467_1);  arg466_1 = arg467_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_22: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_90);  linear_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_91: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_22, arg468_1, arg469_1);  gelu_22 = arg468_1 = arg469_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_47: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_91, 0.1, False);  linear_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_46: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_47, slice_22);  dropout_47 = slice_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_44: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_46, [768], arg470_1, arg471_1, 1e-12);  add_46 = arg470_1 = arg471_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_92: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_24, arg472_1, arg473_1);  arg472_1 = arg473_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_23: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_92);  linear_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_93: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_23, arg474_1, arg475_1);  gelu_23 = arg474_1 = arg475_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_48: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_93, 0.1, False);  linear_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_47: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_48, slice_24);  dropout_48 = slice_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_45: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_47, [768], arg476_1, arg477_1, 1e-12);  add_47 = arg476_1 = arg477_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_6: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_44, layer_norm_45], 1);  layer_norm_44 = layer_norm_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_88: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_6, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_94: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_88, arg478_1, arg479_1);  transpose_88 = arg478_1 = arg479_1 = None\n",
      "    unflatten_9: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_94, -1, [3, 768]);  linear_94 = None\n",
      "    unsqueeze_16: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_9, 0);  unflatten_9 = None\n",
      "    transpose_89: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_16, 0, -2);  unsqueeze_16 = None\n",
      "    squeeze_9: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_89, -2);  transpose_89 = None\n",
      "    contiguous_18: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_9);  squeeze_9 = None\n",
      "    select_24: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 0)\n",
      "    select_25: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 1)\n",
      "    select_26: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 2);  contiguous_18 = None\n",
      "    unsqueeze_17: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0);  where = None\n",
      "    view_54: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_24, [109, 12, 64]);  select_24 = None\n",
      "    transpose_90: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_54, 0, 1);  view_54 = None\n",
      "    view_55: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_25, [109, 12, 64]);  select_25 = None\n",
      "    transpose_91: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_55, 0, 1);  view_55 = None\n",
      "    view_56: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_26, [109, 12, 64]);  select_26 = None\n",
      "    transpose_92: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_56, 0, 1);  view_56 = None\n",
      "    mul_9: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_90, 0.125);  transpose_90 = None\n",
      "    transpose_93: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_91, -2, -1);  transpose_91 = None\n",
      "    baddbmm_6: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_17, mul_9, transpose_93);  unsqueeze_17 = mul_9 = transpose_93 = None\n",
      "    softmax_9: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_6, -1);  baddbmm_6 = None\n",
      "    bmm_12: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_9, transpose_92);  transpose_92 = None\n",
      "    transpose_94: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_12, 0, 1);  bmm_12 = None\n",
      "    contiguous_19: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_94);  transpose_94 = None\n",
      "    view_57: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_19, [109, 768]);  contiguous_19 = None\n",
      "    linear_95: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_57, arg480_1, arg481_1);  view_57 = arg480_1 = arg481_1 = None\n",
      "    view_58: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_95, [109, 1, 768]);  linear_95 = None\n",
      "    view_59: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_9, [1, 12, 109, 109]);  softmax_9 = None\n",
      "    mean_9: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_59, [1]);  view_59 = mean_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_95: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_58, 1, 0);  view_58 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_48: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_6, transpose_95);  concat_6 = transpose_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_46: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_48, [768], arg482_1, arg483_1);  add_48 = arg482_1 = arg483_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_25: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46)\n",
      "    slice_26: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_25, 1, None, 32);  slice_25 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_27: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46);  layer_norm_46 = None\n",
      "    slice_28: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_27, 1, 32);  slice_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_96: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_26, 1, 0)\n",
      "    transpose_97: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0);  linear_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(arg496_1, [768, 1536]);  arg496_1 = None\n",
      "    getitem_48: \"f32[768, 768]\" = split_with_sizes_6[0]\n",
      "    getitem_49: \"f32[1536, 768]\" = split_with_sizes_6[1];  split_with_sizes_6 = None\n",
      "    split_with_sizes_7 = torch.ops.aten.split_with_sizes.default(arg497_1, [768, 1536]);  arg497_1 = None\n",
      "    getitem_50: \"f32[768]\" = split_with_sizes_7[0]\n",
      "    getitem_51: \"f32[1536]\" = split_with_sizes_7[1];  split_with_sizes_7 = None\n",
      "    linear_96: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_96, getitem_48, getitem_50);  transpose_96 = getitem_48 = getitem_50 = None\n",
      "    linear_97: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_97, getitem_49, getitem_51);  transpose_97 = getitem_49 = getitem_51 = None\n",
      "    unflatten_10: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_97, -1, [2, 768]);  linear_97 = None\n",
      "    unsqueeze_18: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_10, 0);  unflatten_10 = None\n",
      "    transpose_98: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_18, 0, -2);  unsqueeze_18 = None\n",
      "    squeeze_10: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_98, -2);  transpose_98 = None\n",
      "    contiguous_20: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_10);  squeeze_10 = None\n",
      "    select_27: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 0)\n",
      "    select_28: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 1);  contiguous_20 = None\n",
      "    view_60: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_96, [32, 12, 64]);  linear_96 = None\n",
      "    transpose_99: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_60, 0, 1);  view_60 = None\n",
      "    view_61: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_27, [197, 12, 64]);  select_27 = None\n",
      "    transpose_100: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_61, 0, 1);  view_61 = None\n",
      "    view_62: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_28, [197, 12, 64]);  select_28 = None\n",
      "    transpose_101: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_62, 0, 1);  view_62 = None\n",
      "    mul_10: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_99, 0.125);  transpose_99 = None\n",
      "    transpose_102: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_100, -2, -1);  transpose_100 = None\n",
      "    bmm_13: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_10, transpose_102);  mul_10 = transpose_102 = None\n",
      "    softmax_10: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_13, -1);  bmm_13 = None\n",
      "    bmm_14: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_10, transpose_101);  transpose_101 = None\n",
      "    transpose_103: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_14, 0, 1);  bmm_14 = None\n",
      "    contiguous_21: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_103);  transpose_103 = None\n",
      "    view_63: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_21, [32, 768]);  contiguous_21 = None\n",
      "    linear_98: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_63, arg498_1, arg499_1);  view_63 = arg498_1 = arg499_1 = None\n",
      "    view_64: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_98, [32, 1, 768]);  linear_98 = None\n",
      "    view_65: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_10, [1, 12, 32, 197]);  softmax_10 = None\n",
      "    mean_10: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_65, [1]);  view_65 = mean_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_104: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_64, 1, 0);  view_64 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_49: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_26, transpose_104);  slice_26 = transpose_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_47: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_49, [768], arg500_1, arg501_1);  add_49 = arg500_1 = arg501_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_99: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_47, arg484_1, arg485_1);  arg484_1 = arg485_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_24: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_99);  linear_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_100: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_24, arg486_1, arg487_1);  gelu_24 = arg486_1 = arg487_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_49: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_100, 0.1, False);  linear_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_50: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_49, layer_norm_47);  dropout_49 = layer_norm_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_48: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_50, [768], arg488_1, arg489_1, 1e-12);  add_50 = arg488_1 = arg489_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_101: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_28, arg490_1, arg491_1);  arg490_1 = arg491_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_25: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_101);  linear_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_102: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_25, arg492_1, arg493_1);  gelu_25 = arg492_1 = arg493_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_50: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_102, 0.1, False);  linear_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_51: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_50, slice_28);  dropout_50 = slice_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_49: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_51, [768], arg494_1, arg495_1, 1e-12);  add_51 = arg494_1 = arg495_1 = layer_norm_49 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:155 in forward, code: itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
      "    numpy_t: \"f32[768, 30522]\" = torch.ops.aten.numpy_T.default(arg502_1);  arg502_1 = None\n",
      "    matmul: \"f32[1, 77, 30522]\" = torch.ops.aten.matmul.default(clone_5, numpy_t);  clone_5 = numpy_t = matmul = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_103: \"f32[1, 32, 512]\" = torch.ops.aten.linear.default(layer_norm_48, arg504_1, arg505_1);  layer_norm_48 = arg504_1 = arg505_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_4: \"f32[1, 77, 512]\" = torch.ops.aten.embedding.default(arg582_1, arg705_1);  arg705_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_1 = torch._C._set_grad_enabled(False);  _set_grad_enabled_1 = None\n",
      "    concat_7: \"f32[1, 109, 512]\" = torch.ops.aten.concat.default([linear_103, embedding_4], 1);  linear_103 = embedding_4 = None\n",
      "    ones: \"i64[1, 32]\" = torch.ops.aten.ones.default([1, 32], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    concat_8: \"i64[1, 109]\" = torch.ops.aten.concat.default([ones, arg706_1], 1);  ones = arg706_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1025 in forward, code: cache_position = torch.arange(\n",
      "    arange: \"i64[109]\" = torch.ops.aten.arange.start(0, 109, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1045 in forward, code: causal_mask = attention_mask[:, None, None, :]\n",
      "    slice_29: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_19: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_29, 1);  slice_29 = None\n",
      "    unsqueeze_20: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_19, 2);  unsqueeze_19 = None\n",
      "    slice_30: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_20, 3, 0, 9223372036854775807);  unsqueeze_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1046 in forward, code: causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n",
      "    to_3: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_30, torch.float32);  slice_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1047 in forward, code: causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n",
      "    rsub: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_3, 1.0);  to_3 = None\n",
      "    mul_11: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub, -3.4028234663852886e+38);  rsub = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_51: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(concat_7, 0.1, False);  concat_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_4: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(dropout_51, torch.float32);  dropout_51 = None\n",
      "    pow_1: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_4, 2)\n",
      "    mean_11: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n",
      "    add_52: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None\n",
      "    rsqrt: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_52);  add_52 = None\n",
      "    mul_12: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_4, rsqrt);  rsqrt = None\n",
      "    mul_13: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg513_1, mul_12);  arg513_1 = mul_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_104: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg508_1);  arg508_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_66: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_104, [1, -1, 6, 64]);  linear_104 = None\n",
      "    transpose_105: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_66, 1, 2);  view_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_105: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg509_1);  arg509_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_106: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg510_1);  mul_13 = arg510_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_67: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_105, [1, -1, 6, 64]);  linear_105 = None\n",
      "    transpose_106: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_67, 1, 2);  view_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_68: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_106, [1, -1, 6, 64]);  linear_106 = None\n",
      "    transpose_107: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_108: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_106, 3, 2);  transpose_106 = None\n",
      "    matmul_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_105, transpose_108);  transpose_105 = transpose_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_29: \"i64[]\" = torch.ops.aten.select.int(arange, 0, -1)\n",
      "    add_53: \"i64[]\" = torch.ops.aten.add.Tensor(select_29, 1);  select_29 = add_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_31: \"i64[109]\" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807);  arange = None\n",
      "    unsqueeze_21: \"i64[109, 1]\" = torch.ops.aten.unsqueeze.default(slice_31, 1);  slice_31 = None\n",
      "    to_5: \"i64[109, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_21, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_21 = None\n",
      "    arange_1: \"i64[109]\" = torch.ops.aten.arange.default(109, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_22: \"i64[1, 109]\" = torch.ops.aten.unsqueeze.default(arange_1, 0);  arange_1 = None\n",
      "    slice_32: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_22, 1, 0, 9223372036854775807);  unsqueeze_22 = None\n",
      "    sub: \"i64[109, 109]\" = torch.ops.aten.sub.Tensor(slice_32, to_5);  slice_32 = to_5 = None\n",
      "    gt_3: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(sub, 0)\n",
      "    to_6: \"i64[109, 109]\" = torch.ops.aten.to.dtype(gt_3, torch.int64);  gt_3 = None\n",
      "    mul_14: \"i64[109, 109]\" = torch.ops.aten.mul.Tensor(to_6, 16);  to_6 = None\n",
      "    add_54: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(mul_14, 0);  mul_14 = None\n",
      "    abs_1: \"i64[109, 109]\" = torch.ops.aten.abs.default(sub);  sub = None\n",
      "    lt: \"b8[109, 109]\" = torch.ops.aten.lt.Scalar(abs_1, 8)\n",
      "    to_7: \"f32[109, 109]\" = torch.ops.aten.to.dtype(abs_1, torch.float32)\n",
      "    div: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(to_7, 8);  to_7 = None\n",
      "    log: \"f32[109, 109]\" = torch.ops.aten.log.default(div);  div = None\n",
      "    div_1: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(log, 2.772588722239781);  log = None\n",
      "    mul_15: \"f32[109, 109]\" = torch.ops.aten.mul.Tensor(div_1, 8);  div_1 = None\n",
      "    to_8: \"i64[109, 109]\" = torch.ops.aten.to.dtype(mul_15, torch.int64);  mul_15 = None\n",
      "    add_55: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(to_8, 8);  to_8 = None\n",
      "    full_like_3: \"i64[109, 109]\" = torch.ops.aten.full_like.default(add_55, 15, pin_memory = False)\n",
      "    min_1: \"i64[109, 109]\" = torch.ops.aten.min.other(add_55, full_like_3);  add_55 = full_like_3 = None\n",
      "    where_3: \"i64[109, 109]\" = torch.ops.aten.where.self(lt, abs_1, min_1);  lt = abs_1 = min_1 = None\n",
      "    add_: \"i64[109, 109]\" = torch.ops.aten.add_.Tensor(add_54, where_3);  add_54 = where_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_5: \"f32[109, 109, 6]\" = torch.ops.aten.embedding.default(arg512_1, add_);  arg512_1 = add_ = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_12: \"f32[6, 109, 109]\" = torch.ops.aten.permute.default(embedding_5, [2, 0, 1]);  embedding_5 = None\n",
      "    unsqueeze_23: \"f32[1, 6, 109, 109]\" = torch.ops.aten.unsqueeze.default(permute_12, 0);  permute_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_33: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_23);  unsqueeze_23 = None\n",
      "    slice_34: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_33, 1);  slice_33 = None\n",
      "    slice_35: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_34, 2, -109);  slice_34 = None\n",
      "    slice_36: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_35, 3);  slice_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_37: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_11);  mul_11 = None\n",
      "    slice_38: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_37, 1);  slice_37 = None\n",
      "    slice_39: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_38, 2);  slice_38 = None\n",
      "    slice_40: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_39, 3, None, 109);  slice_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add.Tensor(slice_36, slice_40);  slice_36 = slice_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_1, add_56);  matmul_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__1, torch.float32);  add__1 = None\n",
      "    softmax_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_9, -1)\n",
      "    type_as: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_11, to_9);  softmax_11 = to_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_52: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as, 0.1, False);  type_as = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_2: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_52, transpose_107);  dropout_52 = transpose_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_109: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_2, 1, 2);  matmul_2 = None\n",
      "    contiguous_22: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_109);  transpose_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_69: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_22, [1, -1, 384]);  contiguous_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_107: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_69, arg511_1);  view_69 = arg511_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_53: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_107, 0.1, False);  linear_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_57: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_4, dropout_53);  to_4 = dropout_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_10: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_57, torch.float32);  add_57 = None\n",
      "    pow_2: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)\n",
      "    mean_12: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None\n",
      "    add_58: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None\n",
      "    rsqrt_1: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_58);  add_58 = None\n",
      "    mul_16: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_10, rsqrt_1);  rsqrt_1 = None\n",
      "    mul_17: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg517_1, mul_16);  arg517_1 = mul_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_108: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg514_1);  arg514_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_18: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_108, 0.5)\n",
      "    pow_3: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_108, 3.0)\n",
      "    mul_19: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_3, 0.044715);  pow_3 = None\n",
      "    add_59: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_108, mul_19);  linear_108 = mul_19 = None\n",
      "    mul_20: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_59, 0.7978845608028654);  add_59 = None\n",
      "    tanh: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_20);  mul_20 = None\n",
      "    add_60: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh, 1.0);  tanh = None\n",
      "    mul_21: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_18, add_60);  mul_18 = add_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_109: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg515_1);  mul_17 = arg515_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_22: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_21, linear_109);  mul_21 = linear_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_54: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_22, 0.1, False);  mul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_110: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_54, arg516_1);  dropout_54 = arg516_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_55: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_110, 0.1, False);  linear_110 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_61: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_10, dropout_55);  to_10 = dropout_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_11: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_61, torch.float32);  add_61 = None\n",
      "    pow_4: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_11, 2)\n",
      "    mean_13: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None\n",
      "    add_62: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None\n",
      "    rsqrt_2: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_62);  add_62 = None\n",
      "    mul_23: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_11, rsqrt_2);  rsqrt_2 = None\n",
      "    mul_24: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg522_1, mul_23);  arg522_1 = mul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_111: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg518_1);  arg518_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_70: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_111, [1, -1, 6, 64]);  linear_111 = None\n",
      "    transpose_110: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_70, 1, 2);  view_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_112: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg519_1);  arg519_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_113: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg520_1);  mul_24 = arg520_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_71: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_112, [1, -1, 6, 64]);  linear_112 = None\n",
      "    transpose_111: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_72: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_113, [1, -1, 6, 64]);  linear_113 = None\n",
      "    transpose_112: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_113: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_111, 3, 2);  transpose_111 = None\n",
      "    matmul_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_110, transpose_113);  transpose_110 = transpose_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_3, add_56);  matmul_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__2, torch.float32);  add__2 = None\n",
      "    softmax_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_12, -1)\n",
      "    type_as_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_12, to_12);  softmax_12 = to_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_1, 0.1, False);  type_as_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_56, transpose_112);  dropout_56 = transpose_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_114: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_4, 1, 2);  matmul_4 = None\n",
      "    contiguous_23: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_114);  transpose_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_73: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_23, [1, -1, 384]);  contiguous_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_114: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_73, arg521_1);  view_73 = arg521_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_57: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_114, 0.1, False);  linear_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_63: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_11, dropout_57);  to_11 = dropout_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_13: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_63, torch.float32);  add_63 = None\n",
      "    pow_5: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_13, 2)\n",
      "    mean_14: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None\n",
      "    add_64: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None\n",
      "    rsqrt_3: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_64);  add_64 = None\n",
      "    mul_25: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_13, rsqrt_3);  rsqrt_3 = None\n",
      "    mul_26: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg526_1, mul_25);  arg526_1 = mul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_115: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg523_1);  arg523_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_27: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_115, 0.5)\n",
      "    pow_6: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_115, 3.0)\n",
      "    mul_28: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_6, 0.044715);  pow_6 = None\n",
      "    add_65: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_115, mul_28);  linear_115 = mul_28 = None\n",
      "    mul_29: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_65, 0.7978845608028654);  add_65 = None\n",
      "    tanh_1: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_29);  mul_29 = None\n",
      "    add_66: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_1, 1.0);  tanh_1 = None\n",
      "    mul_30: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_27, add_66);  mul_27 = add_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_116: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg524_1);  mul_26 = arg524_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_31: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_30, linear_116);  mul_30 = linear_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_58: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_31, 0.1, False);  mul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_117: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_58, arg525_1);  dropout_58 = arg525_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_59: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_117, 0.1, False);  linear_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_67: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_13, dropout_59);  to_13 = dropout_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_14: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_67, torch.float32);  add_67 = None\n",
      "    pow_7: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)\n",
      "    mean_15: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_7, [-1], True);  pow_7 = None\n",
      "    add_68: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None\n",
      "    rsqrt_4: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_68);  add_68 = None\n",
      "    mul_32: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_14, rsqrt_4);  rsqrt_4 = None\n",
      "    mul_33: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg531_1, mul_32);  arg531_1 = mul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_118: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg527_1);  arg527_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_74: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_118, [1, -1, 6, 64]);  linear_118 = None\n",
      "    transpose_115: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_119: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg528_1);  arg528_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_120: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg529_1);  mul_33 = arg529_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_75: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_119, [1, -1, 6, 64]);  linear_119 = None\n",
      "    transpose_116: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_75, 1, 2);  view_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_76: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_120, [1, -1, 6, 64]);  linear_120 = None\n",
      "    transpose_117: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_76, 1, 2);  view_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_118: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_116, 3, 2);  transpose_116 = None\n",
      "    matmul_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_115, transpose_118);  transpose_115 = transpose_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_5, add_56);  matmul_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__3, torch.float32);  add__3 = None\n",
      "    softmax_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_15, -1)\n",
      "    type_as_2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_13, to_15);  softmax_13 = to_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_60: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_2, 0.1, False);  type_as_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_6: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_60, transpose_117);  dropout_60 = transpose_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_119: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_6, 1, 2);  matmul_6 = None\n",
      "    contiguous_24: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_119);  transpose_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_77: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_24, [1, -1, 384]);  contiguous_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_121: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_77, arg530_1);  view_77 = arg530_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_61: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_121, 0.1, False);  linear_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_69: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_14, dropout_61);  to_14 = dropout_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_16: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_69, torch.float32);  add_69 = None\n",
      "    pow_8: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_16, 2)\n",
      "    mean_16: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_8, [-1], True);  pow_8 = None\n",
      "    add_70: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None\n",
      "    rsqrt_5: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_70);  add_70 = None\n",
      "    mul_34: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_16, rsqrt_5);  rsqrt_5 = None\n",
      "    mul_35: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg535_1, mul_34);  arg535_1 = mul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_122: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg532_1);  arg532_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_36: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_122, 0.5)\n",
      "    pow_9: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_122, 3.0)\n",
      "    mul_37: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_9, 0.044715);  pow_9 = None\n",
      "    add_71: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_122, mul_37);  linear_122 = mul_37 = None\n",
      "    mul_38: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_71, 0.7978845608028654);  add_71 = None\n",
      "    tanh_2: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_38);  mul_38 = None\n",
      "    add_72: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_2, 1.0);  tanh_2 = None\n",
      "    mul_39: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_36, add_72);  mul_36 = add_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_123: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg533_1);  mul_35 = arg533_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_40: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_39, linear_123);  mul_39 = linear_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_62: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_40, 0.1, False);  mul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_124: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_62, arg534_1);  dropout_62 = arg534_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_63: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_124, 0.1, False);  linear_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_73: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_16, dropout_63);  to_16 = dropout_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_17: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_73, torch.float32);  add_73 = None\n",
      "    pow_10: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_17, 2)\n",
      "    mean_17: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_10, [-1], True);  pow_10 = None\n",
      "    add_74: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None\n",
      "    rsqrt_6: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_74);  add_74 = None\n",
      "    mul_41: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_17, rsqrt_6);  rsqrt_6 = None\n",
      "    mul_42: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg540_1, mul_41);  arg540_1 = mul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_125: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg536_1);  arg536_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_78: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_125, [1, -1, 6, 64]);  linear_125 = None\n",
      "    transpose_120: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_126: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg537_1);  arg537_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_127: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg538_1);  mul_42 = arg538_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_79: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_126, [1, -1, 6, 64]);  linear_126 = None\n",
      "    transpose_121: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_80: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_127, [1, -1, 6, 64]);  linear_127 = None\n",
      "    transpose_122: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_123: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_121, 3, 2);  transpose_121 = None\n",
      "    matmul_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_120, transpose_123);  transpose_120 = transpose_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_7, add_56);  matmul_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__4, torch.float32);  add__4 = None\n",
      "    softmax_14: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_18, -1)\n",
      "    type_as_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_14, to_18);  softmax_14 = to_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_64: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_3, 0.1, False);  type_as_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_64, transpose_122);  dropout_64 = transpose_122 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_124: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_8, 1, 2);  matmul_8 = None\n",
      "    contiguous_25: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_124);  transpose_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_81: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_25, [1, -1, 384]);  contiguous_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_128: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_81, arg539_1);  view_81 = arg539_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_65: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_128, 0.1, False);  linear_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_75: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_17, dropout_65);  to_17 = dropout_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_19: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_75, torch.float32);  add_75 = None\n",
      "    pow_11: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_19, 2)\n",
      "    mean_18: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_11, [-1], True);  pow_11 = None\n",
      "    add_76: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None\n",
      "    rsqrt_7: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_76);  add_76 = None\n",
      "    mul_43: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_19, rsqrt_7);  rsqrt_7 = None\n",
      "    mul_44: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg544_1, mul_43);  arg544_1 = mul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_129: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg541_1);  arg541_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_45: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_129, 0.5)\n",
      "    pow_12: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_129, 3.0)\n",
      "    mul_46: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_12, 0.044715);  pow_12 = None\n",
      "    add_77: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_129, mul_46);  linear_129 = mul_46 = None\n",
      "    mul_47: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_77, 0.7978845608028654);  add_77 = None\n",
      "    tanh_3: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_47);  mul_47 = None\n",
      "    add_78: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_3, 1.0);  tanh_3 = None\n",
      "    mul_48: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_45, add_78);  mul_45 = add_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_130: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg542_1);  mul_44 = arg542_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_49: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_48, linear_130);  mul_48 = linear_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_66: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_49, 0.1, False);  mul_49 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_131: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_66, arg543_1);  dropout_66 = arg543_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_67: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_131, 0.1, False);  linear_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_79: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_19, dropout_67);  to_19 = dropout_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_20: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_79, torch.float32);  add_79 = None\n",
      "    pow_13: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_20, 2)\n",
      "    mean_19: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_13, [-1], True);  pow_13 = None\n",
      "    add_80: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None\n",
      "    rsqrt_8: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_80);  add_80 = None\n",
      "    mul_50: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_20, rsqrt_8);  rsqrt_8 = None\n",
      "    mul_51: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg549_1, mul_50);  arg549_1 = mul_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_132: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg545_1);  arg545_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_82: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_132, [1, -1, 6, 64]);  linear_132 = None\n",
      "    transpose_125: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_82, 1, 2);  view_82 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_133: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg546_1);  arg546_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_134: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg547_1);  mul_51 = arg547_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_83: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_133, [1, -1, 6, 64]);  linear_133 = None\n",
      "    transpose_126: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_84: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_134, [1, -1, 6, 64]);  linear_134 = None\n",
      "    transpose_127: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_84, 1, 2);  view_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_128: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_126, 3, 2);  transpose_126 = None\n",
      "    matmul_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_125, transpose_128);  transpose_125 = transpose_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_9, add_56);  matmul_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_21: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__5, torch.float32);  add__5 = None\n",
      "    softmax_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_21, -1)\n",
      "    type_as_4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_15, to_21);  softmax_15 = to_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_68: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_4, 0.1, False);  type_as_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_10: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_68, transpose_127);  dropout_68 = transpose_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_10, 1, 2);  matmul_10 = None\n",
      "    contiguous_26: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_129);  transpose_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_85: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_26, [1, -1, 384]);  contiguous_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_135: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_85, arg548_1);  view_85 = arg548_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_69: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_135, 0.1, False);  linear_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_81: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_20, dropout_69);  to_20 = dropout_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_22: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_81, torch.float32);  add_81 = None\n",
      "    pow_14: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_22, 2)\n",
      "    mean_20: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_14, [-1], True);  pow_14 = None\n",
      "    add_82: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_20, 1e-06);  mean_20 = None\n",
      "    rsqrt_9: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_82);  add_82 = None\n",
      "    mul_52: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_22, rsqrt_9);  rsqrt_9 = None\n",
      "    mul_53: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg553_1, mul_52);  arg553_1 = mul_52 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_136: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg550_1);  arg550_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_54: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_136, 0.5)\n",
      "    pow_15: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_136, 3.0)\n",
      "    mul_55: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_15, 0.044715);  pow_15 = None\n",
      "    add_83: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_136, mul_55);  linear_136 = mul_55 = None\n",
      "    mul_56: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_83, 0.7978845608028654);  add_83 = None\n",
      "    tanh_4: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_56);  mul_56 = None\n",
      "    add_84: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_4, 1.0);  tanh_4 = None\n",
      "    mul_57: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_54, add_84);  mul_54 = add_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_137: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg551_1);  mul_53 = arg551_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_58: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_57, linear_137);  mul_57 = linear_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_70: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_58, 0.1, False);  mul_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_138: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_70, arg552_1);  dropout_70 = arg552_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_71: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_138, 0.1, False);  linear_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_85: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_22, dropout_71);  to_22 = dropout_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_23: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_85, torch.float32);  add_85 = None\n",
      "    pow_16: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_23, 2)\n",
      "    mean_21: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_16, [-1], True);  pow_16 = None\n",
      "    add_86: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_21, 1e-06);  mean_21 = None\n",
      "    rsqrt_10: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_86);  add_86 = None\n",
      "    mul_59: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_23, rsqrt_10);  rsqrt_10 = None\n",
      "    mul_60: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg558_1, mul_59);  arg558_1 = mul_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_139: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg554_1);  arg554_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_86: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_139, [1, -1, 6, 64]);  linear_139 = None\n",
      "    transpose_130: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_86, 1, 2);  view_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_140: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg555_1);  arg555_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_141: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg556_1);  mul_60 = arg556_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_87: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_140, [1, -1, 6, 64]);  linear_140 = None\n",
      "    transpose_131: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_87, 1, 2);  view_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_88: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_141, [1, -1, 6, 64]);  linear_141 = None\n",
      "    transpose_132: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_88, 1, 2);  view_88 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_133: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_131, 3, 2);  transpose_131 = None\n",
      "    matmul_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_130, transpose_133);  transpose_130 = transpose_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_11, add_56);  matmul_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_24: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__6, torch.float32);  add__6 = None\n",
      "    softmax_16: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_24, -1)\n",
      "    type_as_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_16, to_24);  softmax_16 = to_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_72: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_5, 0.1, False);  type_as_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_72, transpose_132);  dropout_72 = transpose_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_134: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_12, 1, 2);  matmul_12 = None\n",
      "    contiguous_27: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_134);  transpose_134 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_89: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_27, [1, -1, 384]);  contiguous_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_142: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_89, arg557_1);  view_89 = arg557_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_73: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_142, 0.1, False);  linear_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_87: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_23, dropout_73);  to_23 = dropout_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_25: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_87, torch.float32);  add_87 = None\n",
      "    pow_17: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_25, 2)\n",
      "    mean_22: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_17, [-1], True);  pow_17 = None\n",
      "    add_88: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_22, 1e-06);  mean_22 = None\n",
      "    rsqrt_11: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_88);  add_88 = None\n",
      "    mul_61: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_25, rsqrt_11);  rsqrt_11 = None\n",
      "    mul_62: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg562_1, mul_61);  arg562_1 = mul_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_143: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg559_1);  arg559_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_63: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_143, 0.5)\n",
      "    pow_18: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_143, 3.0)\n",
      "    mul_64: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_18, 0.044715);  pow_18 = None\n",
      "    add_89: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_143, mul_64);  linear_143 = mul_64 = None\n",
      "    mul_65: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_89, 0.7978845608028654);  add_89 = None\n",
      "    tanh_5: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_65);  mul_65 = None\n",
      "    add_90: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_5, 1.0);  tanh_5 = None\n",
      "    mul_66: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_63, add_90);  mul_63 = add_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_144: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg560_1);  mul_62 = arg560_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_67: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_66, linear_144);  mul_66 = linear_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_74: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_67, 0.1, False);  mul_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_145: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_74, arg561_1);  dropout_74 = arg561_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_75: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_145, 0.1, False);  linear_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_91: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_25, dropout_75);  to_25 = dropout_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_26: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_91, torch.float32);  add_91 = None\n",
      "    pow_19: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_26, 2)\n",
      "    mean_23: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_19, [-1], True);  pow_19 = None\n",
      "    add_92: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_23, 1e-06);  mean_23 = None\n",
      "    rsqrt_12: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_92);  add_92 = None\n",
      "    mul_68: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_26, rsqrt_12);  rsqrt_12 = None\n",
      "    mul_69: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg567_1, mul_68);  arg567_1 = mul_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_146: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg563_1);  arg563_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_90: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_146, [1, -1, 6, 64]);  linear_146 = None\n",
      "    transpose_135: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_90, 1, 2);  view_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_147: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg564_1);  arg564_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_148: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg565_1);  mul_69 = arg565_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_91: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_147, [1, -1, 6, 64]);  linear_147 = None\n",
      "    transpose_136: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_91, 1, 2);  view_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_92: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_148, [1, -1, 6, 64]);  linear_148 = None\n",
      "    transpose_137: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_138: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_136, 3, 2);  transpose_136 = None\n",
      "    matmul_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_135, transpose_138);  transpose_135 = transpose_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_13, add_56);  matmul_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_27: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__7, torch.float32);  add__7 = None\n",
      "    softmax_17: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_27, -1)\n",
      "    type_as_6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_17, to_27);  softmax_17 = to_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_76: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_6, 0.1, False);  type_as_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_14: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_76, transpose_137);  dropout_76 = transpose_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_139: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_14, 1, 2);  matmul_14 = None\n",
      "    contiguous_28: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_139);  transpose_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_93: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_28, [1, -1, 384]);  contiguous_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_149: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_93, arg566_1);  view_93 = arg566_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_77: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_149, 0.1, False);  linear_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_93: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_26, dropout_77);  to_26 = dropout_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_28: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_93, torch.float32);  add_93 = None\n",
      "    pow_20: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_28, 2)\n",
      "    mean_24: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_20, [-1], True);  pow_20 = None\n",
      "    add_94: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_24, 1e-06);  mean_24 = None\n",
      "    rsqrt_13: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_94);  add_94 = None\n",
      "    mul_70: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_28, rsqrt_13);  rsqrt_13 = None\n",
      "    mul_71: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg571_1, mul_70);  arg571_1 = mul_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_150: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg568_1);  arg568_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_72: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_150, 0.5)\n",
      "    pow_21: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_150, 3.0)\n",
      "    mul_73: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_21, 0.044715);  pow_21 = None\n",
      "    add_95: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_150, mul_73);  linear_150 = mul_73 = None\n",
      "    mul_74: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_95, 0.7978845608028654);  add_95 = None\n",
      "    tanh_6: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_74);  mul_74 = None\n",
      "    add_96: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_6, 1.0);  tanh_6 = None\n",
      "    mul_75: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_72, add_96);  mul_72 = add_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_151: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg569_1);  mul_71 = arg569_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_76: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_75, linear_151);  mul_75 = linear_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_78: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_76, 0.1, False);  mul_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_152: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_78, arg570_1);  dropout_78 = arg570_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_79: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_152, 0.1, False);  linear_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_97: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_28, dropout_79);  to_28 = dropout_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_29: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_97, torch.float32);  add_97 = None\n",
      "    pow_22: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_29, 2)\n",
      "    mean_25: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_22, [-1], True);  pow_22 = None\n",
      "    add_98: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_25, 1e-06);  mean_25 = None\n",
      "    rsqrt_14: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_98);  add_98 = None\n",
      "    mul_77: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_29, rsqrt_14);  rsqrt_14 = None\n",
      "    mul_78: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg576_1, mul_77);  arg576_1 = mul_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_153: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg572_1);  arg572_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_94: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_153, [1, -1, 6, 64]);  linear_153 = None\n",
      "    transpose_140: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_94, 1, 2);  view_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_154: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg573_1);  arg573_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_155: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg574_1);  mul_78 = arg574_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_95: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_154, [1, -1, 6, 64]);  linear_154 = None\n",
      "    transpose_141: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_95, 1, 2);  view_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_96: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_155, [1, -1, 6, 64]);  linear_155 = None\n",
      "    transpose_142: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_96, 1, 2);  view_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_143: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_141, 3, 2);  transpose_141 = None\n",
      "    matmul_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_140, transpose_143);  transpose_140 = transpose_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__8: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_15, add_56);  matmul_15 = add_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_30: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__8, torch.float32);  add__8 = None\n",
      "    softmax_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_30, -1)\n",
      "    type_as_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_18, to_30);  softmax_18 = to_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_80: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_7, 0.1, False);  type_as_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_80, transpose_142);  dropout_80 = transpose_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_16, 1, 2);  matmul_16 = None\n",
      "    contiguous_29: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_144);  transpose_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_97: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_29, [1, -1, 384]);  contiguous_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_156: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_97, arg575_1);  view_97 = arg575_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_81: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_156, 0.1, False);  linear_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_99: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_29, dropout_81);  to_29 = dropout_81 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_31: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_99, torch.float32);  add_99 = None\n",
      "    pow_23: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_31, 2)\n",
      "    mean_26: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_23, [-1], True);  pow_23 = None\n",
      "    add_100: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_26, 1e-06);  mean_26 = None\n",
      "    rsqrt_15: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_100);  add_100 = None\n",
      "    mul_79: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_31, rsqrt_15);  rsqrt_15 = None\n",
      "    mul_80: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg580_1, mul_79);  arg580_1 = mul_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_157: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg577_1);  arg577_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_81: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_157, 0.5)\n",
      "    pow_24: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_157, 3.0)\n",
      "    mul_82: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_24, 0.044715);  pow_24 = None\n",
      "    add_101: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_157, mul_82);  linear_157 = mul_82 = None\n",
      "    mul_83: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_101, 0.7978845608028654);  add_101 = None\n",
      "    tanh_7: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_83);  mul_83 = None\n",
      "    add_102: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_7, 1.0);  tanh_7 = None\n",
      "    mul_84: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_81, add_102);  mul_81 = add_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_158: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg578_1);  mul_80 = arg578_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_85: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_84, linear_158);  mul_84 = linear_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_82: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_85, 0.1, False);  mul_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_159: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_82, arg579_1);  dropout_82 = arg579_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_83: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_159, 0.1, False);  linear_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_103: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_31, dropout_83);  to_31 = dropout_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_32: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_103, torch.float32);  add_103 = None\n",
      "    pow_25: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_32, 2)\n",
      "    mean_27: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_25, [-1], True);  pow_25 = None\n",
      "    add_104: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_27, 1e-06);  mean_27 = None\n",
      "    rsqrt_16: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_104);  add_104 = None\n",
      "    mul_86: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_32, rsqrt_16);  to_32 = rsqrt_16 = None\n",
      "    mul_87: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg581_1, mul_86);  arg581_1 = mul_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_84: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(mul_87, 0.1, False);  mul_87 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_2 = torch._C._set_grad_enabled(False);  _set_grad_enabled_2 = None\n",
      "    ones_1: \"i64[1, 109]\" = torch.ops.aten.ones.default([1, 109], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_88: \"i64[1, 109]\" = torch.ops.aten.mul.Tensor(ones_1, -100);  ones_1 = mul_88 = None\n",
      "    _tensor_constant0 = self._tensor_constant0\n",
      "    lift_fresh_copy: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "    detach_: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n",
      "    _tensor_constant1 = self._tensor_constant1\n",
      "    lift_fresh_copy_1: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
      "    detach__1: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None\n",
      "    _tensor_constant2 = self._tensor_constant2\n",
      "    lift_fresh_copy_2: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
      "    detach__2: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_2);  lift_fresh_copy_2 = None\n",
      "    unsqueeze_24: \"i64[1]\" = torch.ops.aten.unsqueeze.default(detach_, 0);  detach_ = None\n",
      "    isin: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(unsqueeze_24, detach__1)\n",
      "    any_1: \"b8[]\" = torch.ops.aten.any.default(isin);  isin = None\n",
      "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(any_1, 0);  any_1 = ne = None\n",
      "    lt_1: \"b8[1]\" = torch.ops.aten.lt.Scalar(unsqueeze_24, 0)\n",
      "    any_2: \"b8[]\" = torch.ops.aten.any.default(lt_1);  lt_1 = None\n",
      "    ne_1: \"b8[]\" = torch.ops.aten.ne.Scalar(any_2, 0);  any_2 = ne_1 = None\n",
      "    ones_2: \"i64[1, 1]\" = torch.ops.aten.ones.default([1, 1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_89: \"i64[1, 1]\" = torch.ops.aten.mul.Tensor(ones_2, detach__2);  ones_2 = detach__2 = None\n",
      "    ones_3: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    ones_4: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    cumsum: \"i64[1]\" = torch.ops.aten.cumsum.default(ones_4, 0);  ones_4 = None\n",
      "    sub_1: \"i64[1]\" = torch.ops.aten.sub.Tensor(cumsum, 1);  cumsum = None\n",
      "    slice_41: \"i64[1]\" = torch.ops.aten.slice.Tensor(sub_1, 0, 0);  sub_1 = None\n",
      "    slice_42: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(mul_89, 0, 0, 9223372036854775807)\n",
      "    index: \"i64[1, 1]\" = torch.ops.aten.index.Tensor(slice_42, [None, slice_41]);  slice_42 = None\n",
      "    clone_6: \"i64[1, 1]\" = torch.ops.aten.clone.default(index, memory_format = torch.contiguous_format);  index = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:985 in forward, code: input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    view_98: \"i64[1, 1]\" = torch.ops.aten.view.default(clone_6, [-1, 1]);  clone_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_6: \"f32[1, 1, 512]\" = torch.ops.aten.embedding.default(arg582_1, view_98);  arg582_1 = view_98 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1035 in forward, code: causal_mask = self._update_causal_mask(\n",
      "    full: \"f32[1, 2]\" = torch.ops.aten.full.default([1, 2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_2: \"i64[2]\" = torch.ops.aten.arange.default(2, device = device(type='cpu'), pin_memory = False)\n",
      "    reshape_24: \"i64[1, 1]\" = torch.ops.aten.reshape.default(slice_41, [-1, 1])\n",
      "    gt_4: \"b8[1, 2]\" = torch.ops.aten.gt.Tensor(arange_2, reshape_24);  arange_2 = reshape_24 = None\n",
      "    mul_: \"f32[1, 2]\" = torch.ops.aten.mul_.Tensor(full, gt_4);  full = gt_4 = None\n",
      "    unsqueeze_25: \"f32[1, 1, 2]\" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None\n",
      "    unsqueeze_26: \"f32[1, 1, 1, 2]\" = torch.ops.aten.unsqueeze.default(unsqueeze_25, 1);  unsqueeze_25 = None\n",
      "    slice_43: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(unsqueeze_26, 2, 0, 9223372036854775807);  unsqueeze_26 = None\n",
      "    slice_44: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_43, 3, 0, 9223372036854775807);  slice_43 = None\n",
      "    expand_4: \"f32[1, 1, 1, 2]\" = torch.ops.aten.expand.default(slice_44, [1, 1, -1, -1]);  slice_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1060 in forward, code: encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
      "    slice_45: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807);  concat_8 = None\n",
      "    unsqueeze_27: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_45, 1);  slice_45 = None\n",
      "    unsqueeze_28: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_27, 2);  unsqueeze_27 = None\n",
      "    slice_46: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_28, 3, 0, 9223372036854775807);  unsqueeze_28 = None\n",
      "    to_33: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_46, torch.float32);  slice_46 = None\n",
      "    rsub_1: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_33, 1.0);  to_33 = None\n",
      "    mul_90: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub_1, -3.4028234663852886e+38);  rsub_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_85: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(embedding_6, 0.1, False);  embedding_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_34: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(dropout_85, torch.float32);  dropout_85 = None\n",
      "    pow_26: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_34, 2)\n",
      "    mean_28: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_26, [-1], True);  pow_26 = None\n",
      "    add_105: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_28, 1e-06);  mean_28 = None\n",
      "    rsqrt_17: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_105);  add_105 = None\n",
      "    mul_91: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_34, rsqrt_17);  rsqrt_17 = None\n",
      "    mul_92: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg588_1, mul_91);  arg588_1 = mul_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_160: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg583_1);  arg583_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_99: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_160, [1, -1, 6, 64]);  linear_160 = None\n",
      "    transpose_145: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_161: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg584_1);  arg584_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_162: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg585_1);  mul_92 = arg585_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_100: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_161, [1, -1, 6, 64]);  linear_161 = None\n",
      "    transpose_146: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_100, 1, 2);  view_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_101: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_162, [1, -1, 6, 64]);  linear_162 = None\n",
      "    transpose_147: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_101, 1, 2);  view_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant3 = self._tensor_constant3\n",
      "    lift_fresh_copy_3: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
      "    detach__3: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_3);  lift_fresh_copy_3 = None\n",
      "    _tensor_constant4 = self._tensor_constant4\n",
      "    lift_fresh_copy_4: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
      "    detach__4: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_4);  lift_fresh_copy_4 = None\n",
      "    cat_1: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__3, transpose_146], -2);  detach__3 = transpose_146 = None\n",
      "    cat_2: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__4, transpose_147], -2);  detach__4 = transpose_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_148: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_1, 3, 2);  cat_1 = None\n",
      "    matmul_17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_145, transpose_148);  transpose_145 = transpose_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_30: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_106: \"i64[]\" = torch.ops.aten.add.Tensor(select_30, 1);  select_30 = add_106 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_47: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_29: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_47, 1);  slice_47 = None\n",
      "    to_35: \"i64[1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_29, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_29 = None\n",
      "    arange_3: \"i64[1]\" = torch.ops.aten.arange.default(1, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_30: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None\n",
      "    slice_48: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_30, 1, 0, 9223372036854775807);  unsqueeze_30 = None\n",
      "    sub_2: \"i64[1, 1]\" = torch.ops.aten.sub.Tensor(slice_48, to_35);  slice_48 = to_35 = None\n",
      "    zeros_like_3: \"i64[1, 1]\" = torch.ops.aten.zeros_like.default(sub_2, pin_memory = False)\n",
      "    min_2: \"i64[1, 1]\" = torch.ops.aten.min.other(sub_2, zeros_like_3);  sub_2 = zeros_like_3 = None\n",
      "    neg: \"i64[1, 1]\" = torch.ops.aten.neg.default(min_2);  min_2 = None\n",
      "    lt_2: \"b8[1, 1]\" = torch.ops.aten.lt.Scalar(neg, 16)\n",
      "    to_36: \"f32[1, 1]\" = torch.ops.aten.to.dtype(neg, torch.float32)\n",
      "    div_2: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(to_36, 16);  to_36 = None\n",
      "    log_1: \"f32[1, 1]\" = torch.ops.aten.log.default(div_2);  div_2 = None\n",
      "    div_3: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(log_1, 2.0794415416798357);  log_1 = None\n",
      "    mul_93: \"f32[1, 1]\" = torch.ops.aten.mul.Tensor(div_3, 16);  div_3 = None\n",
      "    to_37: \"i64[1, 1]\" = torch.ops.aten.to.dtype(mul_93, torch.int64);  mul_93 = None\n",
      "    add_107: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(to_37, 16);  to_37 = None\n",
      "    full_like_4: \"i64[1, 1]\" = torch.ops.aten.full_like.default(add_107, 31, pin_memory = False)\n",
      "    min_3: \"i64[1, 1]\" = torch.ops.aten.min.other(add_107, full_like_4);  add_107 = full_like_4 = None\n",
      "    where_4: \"i64[1, 1]\" = torch.ops.aten.where.self(lt_2, neg, min_3);  lt_2 = neg = min_3 = None\n",
      "    add_108: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(where_4, 0);  where_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_7: \"f32[1, 1, 6]\" = torch.ops.aten.embedding.default(arg587_1, add_108);  arg587_1 = add_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_13: \"f32[6, 1, 1]\" = torch.ops.aten.permute.default(embedding_7, [2, 0, 1]);  embedding_7 = None\n",
      "    unsqueeze_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.unsqueeze.default(permute_13, 0);  permute_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_49: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_31);  unsqueeze_31 = None\n",
      "    slice_50: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_49, 1);  slice_49 = None\n",
      "    slice_51: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_50, 2, -1);  slice_50 = None\n",
      "    slice_52: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_51, 3);  slice_51 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_53: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(expand_4);  expand_4 = None\n",
      "    slice_54: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_53, 1);  slice_53 = None\n",
      "    slice_55: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_54, 2);  slice_54 = None\n",
      "    slice_56: \"f32[1, 1, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_55, 3, None, 1);  slice_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_109: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add.Tensor(slice_52, slice_56);  slice_52 = slice_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__9: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_17, add_109);  matmul_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_38: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__9, torch.float32);  add__9 = None\n",
      "    softmax_19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_38, -1)\n",
      "    type_as_8: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_19, to_38);  softmax_19 = to_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_86: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_8, 0.1, False);  type_as_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_86, cat_2);  dropout_86 = cat_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_18, 1, 2);  matmul_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_102: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_149, [1, -1, 384]);  transpose_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_163: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_102, arg586_1);  view_102 = arg586_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_87: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_163, 0.1, False);  linear_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_110: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_34, dropout_87);  to_34 = dropout_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_31: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_111: \"i64[]\" = torch.ops.aten.add.Tensor(select_31, 1);  select_31 = add_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_39: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_110, torch.float32);  add_110 = None\n",
      "    pow_27: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_39, 2)\n",
      "    mean_29: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_27, [-1], True);  pow_27 = None\n",
      "    add_112: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_29, 1e-06);  mean_29 = None\n",
      "    rsqrt_18: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_112);  add_112 = None\n",
      "    mul_94: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_39, rsqrt_18);  rsqrt_18 = None\n",
      "    mul_95: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg593_1, mul_94);  arg593_1 = mul_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_164: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_95, arg589_1);  mul_95 = arg589_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_103: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_164, [1, -1, 6, 64]);  linear_164 = None\n",
      "    transpose_150: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_103, 1, 2);  view_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_165: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg590_1);  arg590_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_166: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg591_1);  arg591_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_104: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_165, [1, -1, 6, 64]);  linear_165 = None\n",
      "    transpose_151: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_104, 1, 2);  view_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_105: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_166, [1, -1, 6, 64]);  linear_166 = None\n",
      "    transpose_152: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_105, 1, 2);  view_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant5 = self._tensor_constant5\n",
      "    lift_fresh_copy_5: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
      "    detach__5: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_5);  lift_fresh_copy_5 = None\n",
      "    _tensor_constant6 = self._tensor_constant6\n",
      "    lift_fresh_copy_6: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
      "    detach__6: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_6);  lift_fresh_copy_6 = None\n",
      "    cat_3: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__5, transpose_151], -2);  detach__5 = transpose_151 = None\n",
      "    cat_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__6, transpose_152], -2);  detach__6 = transpose_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_153: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_3, 3, 2);  cat_3 = None\n",
      "    matmul_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_150, transpose_153);  transpose_150 = transpose_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:535 in forward, code: position_bias = torch.zeros(\n",
      "    zeros_2: \"f32[1, 6, 1, 109]\" = torch.ops.aten.zeros.default([1, 6, 1, 109], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_57: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_90);  mul_90 = None\n",
      "    slice_58: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_57, 1);  slice_57 = None\n",
      "    slice_59: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_58, 2);  slice_58 = None\n",
      "    slice_60: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_59, 3, None, 109);  slice_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_113: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add.Tensor(zeros_2, slice_60);  zeros_2 = slice_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__10: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_19, add_113);  matmul_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_40: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__10, torch.float32);  add__10 = None\n",
      "    softmax_20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_40, -1)\n",
      "    type_as_9: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_20, to_40);  softmax_20 = to_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_88: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_9, 0.1, False);  type_as_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_20: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_88, cat_4);  dropout_88 = cat_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_154: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_20, 1, 2);  matmul_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_106: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_154, [1, -1, 384]);  transpose_154 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_167: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_106, arg592_1);  view_106 = arg592_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_89: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_167, 0.1, False);  linear_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_114: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_39, dropout_89);  to_39 = dropout_89 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_41: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_114, torch.float32);  add_114 = None\n",
      "    pow_28: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_41, 2)\n",
      "    mean_30: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_28, [-1], True);  pow_28 = None\n",
      "    add_115: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_30, 1e-06);  mean_30 = None\n",
      "    rsqrt_19: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_115);  add_115 = None\n",
      "    mul_96: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_41, rsqrt_19);  rsqrt_19 = None\n",
      "    mul_97: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg597_1, mul_96);  arg597_1 = mul_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_168: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg594_1);  arg594_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_98: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_168, 0.5)\n",
      "    pow_29: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_168, 3.0)\n",
      "    mul_99: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_29, 0.044715);  pow_29 = None\n",
      "    add_116: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_168, mul_99);  linear_168 = mul_99 = None\n",
      "    mul_100: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_116, 0.7978845608028654);  add_116 = None\n",
      "    tanh_8: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_100);  mul_100 = None\n",
      "    add_117: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_8, 1.0);  tanh_8 = None\n",
      "    mul_101: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_98, add_117);  mul_98 = add_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_169: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg595_1);  mul_97 = arg595_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_102: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_101, linear_169);  mul_101 = linear_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_90: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_102, 0.1, False);  mul_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_170: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_90, arg596_1);  dropout_90 = arg596_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_91: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_170, 0.1, False);  linear_170 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_118: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_41, dropout_91);  to_41 = dropout_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_42: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_118, torch.float32);  add_118 = None\n",
      "    pow_30: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_42, 2)\n",
      "    mean_31: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_30, [-1], True);  pow_30 = None\n",
      "    add_119: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_31, 1e-06);  mean_31 = None\n",
      "    rsqrt_20: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_119);  add_119 = None\n",
      "    mul_103: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_42, rsqrt_20);  rsqrt_20 = None\n",
      "    mul_104: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg602_1, mul_103);  arg602_1 = mul_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_171: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg598_1);  arg598_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_107: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_171, [1, -1, 6, 64]);  linear_171 = None\n",
      "    transpose_155: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_107, 1, 2);  view_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_172: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg599_1);  arg599_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_173: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg600_1);  mul_104 = arg600_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_108: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_172, [1, -1, 6, 64]);  linear_172 = None\n",
      "    transpose_156: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_108, 1, 2);  view_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_109: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_173, [1, -1, 6, 64]);  linear_173 = None\n",
      "    transpose_157: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_109, 1, 2);  view_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant7 = self._tensor_constant7\n",
      "    lift_fresh_copy_7: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None\n",
      "    detach__7: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_7);  lift_fresh_copy_7 = None\n",
      "    _tensor_constant8 = self._tensor_constant8\n",
      "    lift_fresh_copy_8: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None\n",
      "    detach__8: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_8);  lift_fresh_copy_8 = None\n",
      "    cat_5: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__7, transpose_156], -2);  detach__7 = transpose_156 = None\n",
      "    cat_6: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__8, transpose_157], -2);  detach__8 = transpose_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_158: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_5, 3, 2);  cat_5 = None\n",
      "    matmul_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_155, transpose_158);  transpose_155 = transpose_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__11: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_21, add_109);  matmul_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_43: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__11, torch.float32);  add__11 = None\n",
      "    softmax_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_43, -1)\n",
      "    type_as_10: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_21, to_43);  softmax_21 = to_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_92: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_10, 0.1, False);  type_as_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_92, cat_6);  dropout_92 = cat_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_22, 1, 2);  matmul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_110: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_159, [1, -1, 384]);  transpose_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_174: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_110, arg601_1);  view_110 = arg601_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_93: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_174, 0.1, False);  linear_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_120: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_42, dropout_93);  to_42 = dropout_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_32: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_121: \"i64[]\" = torch.ops.aten.add.Tensor(select_32, 1);  select_32 = add_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_44: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_120, torch.float32);  add_120 = None\n",
      "    pow_31: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_44, 2)\n",
      "    mean_32: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_31, [-1], True);  pow_31 = None\n",
      "    add_122: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_32, 1e-06);  mean_32 = None\n",
      "    rsqrt_21: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_122);  add_122 = None\n",
      "    mul_105: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_44, rsqrt_21);  rsqrt_21 = None\n",
      "    mul_106: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg607_1, mul_105);  arg607_1 = mul_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_175: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_106, arg603_1);  mul_106 = arg603_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_111: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_175, [1, -1, 6, 64]);  linear_175 = None\n",
      "    transpose_160: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_111, 1, 2);  view_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_176: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg604_1);  arg604_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_177: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg605_1);  arg605_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_112: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_176, [1, -1, 6, 64]);  linear_176 = None\n",
      "    transpose_161: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_112, 1, 2);  view_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_113: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_177, [1, -1, 6, 64]);  linear_177 = None\n",
      "    transpose_162: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant9 = self._tensor_constant9\n",
      "    lift_fresh_copy_9: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None\n",
      "    detach__9: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_9);  lift_fresh_copy_9 = None\n",
      "    _tensor_constant10 = self._tensor_constant10\n",
      "    lift_fresh_copy_10: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant10);  _tensor_constant10 = None\n",
      "    detach__10: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_10);  lift_fresh_copy_10 = None\n",
      "    cat_7: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__9, transpose_161], -2);  detach__9 = transpose_161 = None\n",
      "    cat_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__10, transpose_162], -2);  detach__10 = transpose_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_163: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_7, 3, 2);  cat_7 = None\n",
      "    matmul_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_160, transpose_163);  transpose_160 = transpose_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__12: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_23, add_113);  matmul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_45: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__12, torch.float32);  add__12 = None\n",
      "    softmax_22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_45, -1)\n",
      "    type_as_11: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_22, to_45);  softmax_22 = to_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_94: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_11, 0.1, False);  type_as_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_24: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_94, cat_8);  dropout_94 = cat_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_164: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_24, 1, 2);  matmul_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_114: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_164, [1, -1, 384]);  transpose_164 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_178: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_114, arg606_1);  view_114 = arg606_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_95: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_178, 0.1, False);  linear_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_123: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_44, dropout_95);  to_44 = dropout_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_46: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_123, torch.float32);  add_123 = None\n",
      "    pow_32: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_46, 2)\n",
      "    mean_33: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_32, [-1], True);  pow_32 = None\n",
      "    add_124: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_33, 1e-06);  mean_33 = None\n",
      "    rsqrt_22: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_124);  add_124 = None\n",
      "    mul_107: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_46, rsqrt_22);  rsqrt_22 = None\n",
      "    mul_108: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg611_1, mul_107);  arg611_1 = mul_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_179: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg608_1);  arg608_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_109: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_179, 0.5)\n",
      "    pow_33: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_179, 3.0)\n",
      "    mul_110: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_33, 0.044715);  pow_33 = None\n",
      "    add_125: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_179, mul_110);  linear_179 = mul_110 = None\n",
      "    mul_111: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_125, 0.7978845608028654);  add_125 = None\n",
      "    tanh_9: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_111);  mul_111 = None\n",
      "    add_126: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_9, 1.0);  tanh_9 = None\n",
      "    mul_112: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_109, add_126);  mul_109 = add_126 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_180: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg609_1);  mul_108 = arg609_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_113: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_112, linear_180);  mul_112 = linear_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_96: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_113, 0.1, False);  mul_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_181: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_96, arg610_1);  dropout_96 = arg610_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_97: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_181, 0.1, False);  linear_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_127: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_46, dropout_97);  to_46 = dropout_97 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_47: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_127, torch.float32);  add_127 = None\n",
      "    pow_34: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_47, 2)\n",
      "    mean_34: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_34, [-1], True);  pow_34 = None\n",
      "    add_128: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_34, 1e-06);  mean_34 = None\n",
      "    rsqrt_23: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_128);  add_128 = None\n",
      "    mul_114: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_47, rsqrt_23);  rsqrt_23 = None\n",
      "    mul_115: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg616_1, mul_114);  arg616_1 = mul_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_182: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg612_1);  arg612_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_115: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_182, [1, -1, 6, 64]);  linear_182 = None\n",
      "    transpose_165: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_115, 1, 2);  view_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_183: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg613_1);  arg613_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_184: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg614_1);  mul_115 = arg614_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_116: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_183, [1, -1, 6, 64]);  linear_183 = None\n",
      "    transpose_166: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_116, 1, 2);  view_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_117: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_184, [1, -1, 6, 64]);  linear_184 = None\n",
      "    transpose_167: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_117, 1, 2);  view_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant11 = self._tensor_constant11\n",
      "    lift_fresh_copy_11: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant11);  _tensor_constant11 = None\n",
      "    detach__11: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_11);  lift_fresh_copy_11 = None\n",
      "    _tensor_constant12 = self._tensor_constant12\n",
      "    lift_fresh_copy_12: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant12);  _tensor_constant12 = None\n",
      "    detach__12: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_12);  lift_fresh_copy_12 = None\n",
      "    cat_9: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__11, transpose_166], -2);  detach__11 = transpose_166 = None\n",
      "    cat_10: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__12, transpose_167], -2);  detach__12 = transpose_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_168: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_9, 3, 2);  cat_9 = None\n",
      "    matmul_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_165, transpose_168);  transpose_165 = transpose_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__13: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_25, add_109);  matmul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_48: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__13, torch.float32);  add__13 = None\n",
      "    softmax_23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_48, -1)\n",
      "    type_as_12: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_23, to_48);  softmax_23 = to_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_98: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_12, 0.1, False);  type_as_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_98, cat_10);  dropout_98 = cat_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_169: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_26, 1, 2);  matmul_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_118: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_169, [1, -1, 384]);  transpose_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_185: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_118, arg615_1);  view_118 = arg615_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_99: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_185, 0.1, False);  linear_185 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_129: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_47, dropout_99);  to_47 = dropout_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_33: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_130: \"i64[]\" = torch.ops.aten.add.Tensor(select_33, 1);  select_33 = add_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_49: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_129, torch.float32);  add_129 = None\n",
      "    pow_35: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_49, 2)\n",
      "    mean_35: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_35, [-1], True);  pow_35 = None\n",
      "    add_131: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_35, 1e-06);  mean_35 = None\n",
      "    rsqrt_24: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_131);  add_131 = None\n",
      "    mul_116: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_49, rsqrt_24);  rsqrt_24 = None\n",
      "    mul_117: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg621_1, mul_116);  arg621_1 = mul_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_186: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_117, arg617_1);  mul_117 = arg617_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_119: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_186, [1, -1, 6, 64]);  linear_186 = None\n",
      "    transpose_170: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_119, 1, 2);  view_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_187: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg618_1);  arg618_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_188: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg619_1);  arg619_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_120: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_187, [1, -1, 6, 64]);  linear_187 = None\n",
      "    transpose_171: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_121: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_188, [1, -1, 6, 64]);  linear_188 = None\n",
      "    transpose_172: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_121, 1, 2);  view_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant13 = self._tensor_constant13\n",
      "    lift_fresh_copy_13: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant13);  _tensor_constant13 = None\n",
      "    detach__13: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_13);  lift_fresh_copy_13 = None\n",
      "    _tensor_constant14 = self._tensor_constant14\n",
      "    lift_fresh_copy_14: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant14);  _tensor_constant14 = None\n",
      "    detach__14: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_14);  lift_fresh_copy_14 = None\n",
      "    cat_11: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__13, transpose_171], -2);  detach__13 = transpose_171 = None\n",
      "    cat_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__14, transpose_172], -2);  detach__14 = transpose_172 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_173: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_11, 3, 2);  cat_11 = None\n",
      "    matmul_27: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_170, transpose_173);  transpose_170 = transpose_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__14: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_27, add_113);  matmul_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_50: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__14, torch.float32);  add__14 = None\n",
      "    softmax_24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_50, -1)\n",
      "    type_as_13: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_24, to_50);  softmax_24 = to_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_100: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_13, 0.1, False);  type_as_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_28: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_100, cat_12);  dropout_100 = cat_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_174: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_28, 1, 2);  matmul_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_122: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_174, [1, -1, 384]);  transpose_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_189: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_122, arg620_1);  view_122 = arg620_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_101: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_189, 0.1, False);  linear_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_132: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_49, dropout_101);  to_49 = dropout_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_51: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_132, torch.float32);  add_132 = None\n",
      "    pow_36: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_51, 2)\n",
      "    mean_36: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_36, [-1], True);  pow_36 = None\n",
      "    add_133: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_36, 1e-06);  mean_36 = None\n",
      "    rsqrt_25: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_133);  add_133 = None\n",
      "    mul_118: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_51, rsqrt_25);  rsqrt_25 = None\n",
      "    mul_119: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg625_1, mul_118);  arg625_1 = mul_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_190: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg622_1);  arg622_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_120: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_190, 0.5)\n",
      "    pow_37: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_190, 3.0)\n",
      "    mul_121: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_37, 0.044715);  pow_37 = None\n",
      "    add_134: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_190, mul_121);  linear_190 = mul_121 = None\n",
      "    mul_122: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_134, 0.7978845608028654);  add_134 = None\n",
      "    tanh_10: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_122);  mul_122 = None\n",
      "    add_135: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_10, 1.0);  tanh_10 = None\n",
      "    mul_123: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_120, add_135);  mul_120 = add_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_191: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg623_1);  mul_119 = arg623_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_124: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_123, linear_191);  mul_123 = linear_191 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_102: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_124, 0.1, False);  mul_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_192: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_102, arg624_1);  dropout_102 = arg624_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_103: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_192, 0.1, False);  linear_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_136: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_51, dropout_103);  to_51 = dropout_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_52: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_136, torch.float32);  add_136 = None\n",
      "    pow_38: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_52, 2)\n",
      "    mean_37: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_38, [-1], True);  pow_38 = None\n",
      "    add_137: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_37, 1e-06);  mean_37 = None\n",
      "    rsqrt_26: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_137);  add_137 = None\n",
      "    mul_125: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_52, rsqrt_26);  rsqrt_26 = None\n",
      "    mul_126: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg630_1, mul_125);  arg630_1 = mul_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_193: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg626_1);  arg626_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_123: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_193, [1, -1, 6, 64]);  linear_193 = None\n",
      "    transpose_175: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_123, 1, 2);  view_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_194: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg627_1);  arg627_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_195: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg628_1);  mul_126 = arg628_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_124: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_194, [1, -1, 6, 64]);  linear_194 = None\n",
      "    transpose_176: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_124, 1, 2);  view_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_125: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_195, [1, -1, 6, 64]);  linear_195 = None\n",
      "    transpose_177: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_125, 1, 2);  view_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant15 = self._tensor_constant15\n",
      "    lift_fresh_copy_15: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant15);  _tensor_constant15 = None\n",
      "    detach__15: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_15);  lift_fresh_copy_15 = None\n",
      "    _tensor_constant16 = self._tensor_constant16\n",
      "    lift_fresh_copy_16: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant16);  _tensor_constant16 = None\n",
      "    detach__16: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_16);  lift_fresh_copy_16 = None\n",
      "    cat_13: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__15, transpose_176], -2);  detach__15 = transpose_176 = None\n",
      "    cat_14: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__16, transpose_177], -2);  detach__16 = transpose_177 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_178: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_13, 3, 2);  cat_13 = None\n",
      "    matmul_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_175, transpose_178);  transpose_175 = transpose_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__15: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_29, add_109);  matmul_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_53: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__15, torch.float32);  add__15 = None\n",
      "    softmax_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_53, -1)\n",
      "    type_as_14: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_25, to_53);  softmax_25 = to_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_104: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_14, 0.1, False);  type_as_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_104, cat_14);  dropout_104 = cat_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_179: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_30, 1, 2);  matmul_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_126: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_179, [1, -1, 384]);  transpose_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_196: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_126, arg629_1);  view_126 = arg629_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_105: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_196, 0.1, False);  linear_196 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_138: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_52, dropout_105);  to_52 = dropout_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_34: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_139: \"i64[]\" = torch.ops.aten.add.Tensor(select_34, 1);  select_34 = add_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_54: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_138, torch.float32);  add_138 = None\n",
      "    pow_39: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_54, 2)\n",
      "    mean_38: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_39, [-1], True);  pow_39 = None\n",
      "    add_140: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_38, 1e-06);  mean_38 = None\n",
      "    rsqrt_27: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_140);  add_140 = None\n",
      "    mul_127: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_54, rsqrt_27);  rsqrt_27 = None\n",
      "    mul_128: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg635_1, mul_127);  arg635_1 = mul_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_197: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_128, arg631_1);  mul_128 = arg631_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_127: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_197, [1, -1, 6, 64]);  linear_197 = None\n",
      "    transpose_180: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_127, 1, 2);  view_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_198: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg632_1);  arg632_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_199: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg633_1);  arg633_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_128: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_198, [1, -1, 6, 64]);  linear_198 = None\n",
      "    transpose_181: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_128, 1, 2);  view_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_199, [1, -1, 6, 64]);  linear_199 = None\n",
      "    transpose_182: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_129, 1, 2);  view_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant17 = self._tensor_constant17\n",
      "    lift_fresh_copy_17: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant17);  _tensor_constant17 = None\n",
      "    detach__17: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_17);  lift_fresh_copy_17 = None\n",
      "    _tensor_constant18 = self._tensor_constant18\n",
      "    lift_fresh_copy_18: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant18);  _tensor_constant18 = None\n",
      "    detach__18: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_18);  lift_fresh_copy_18 = None\n",
      "    cat_15: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__17, transpose_181], -2);  detach__17 = transpose_181 = None\n",
      "    cat_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__18, transpose_182], -2);  detach__18 = transpose_182 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_183: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_15, 3, 2);  cat_15 = None\n",
      "    matmul_31: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_180, transpose_183);  transpose_180 = transpose_183 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__16: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_31, add_113);  matmul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_55: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__16, torch.float32);  add__16 = None\n",
      "    softmax_26: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_55, -1)\n",
      "    type_as_15: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_26, to_55);  softmax_26 = to_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_106: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_15, 0.1, False);  type_as_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_32: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_106, cat_16);  dropout_106 = cat_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_184: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_32, 1, 2);  matmul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_130: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_184, [1, -1, 384]);  transpose_184 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_200: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_130, arg634_1);  view_130 = arg634_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_107: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_200, 0.1, False);  linear_200 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_141: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_54, dropout_107);  to_54 = dropout_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_56: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_141, torch.float32);  add_141 = None\n",
      "    pow_40: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_56, 2)\n",
      "    mean_39: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_40, [-1], True);  pow_40 = None\n",
      "    add_142: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_39, 1e-06);  mean_39 = None\n",
      "    rsqrt_28: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_142);  add_142 = None\n",
      "    mul_129: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_56, rsqrt_28);  rsqrt_28 = None\n",
      "    mul_130: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg639_1, mul_129);  arg639_1 = mul_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_201: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg636_1);  arg636_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_131: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_201, 0.5)\n",
      "    pow_41: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_201, 3.0)\n",
      "    mul_132: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_41, 0.044715);  pow_41 = None\n",
      "    add_143: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_201, mul_132);  linear_201 = mul_132 = None\n",
      "    mul_133: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_143, 0.7978845608028654);  add_143 = None\n",
      "    tanh_11: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_133);  mul_133 = None\n",
      "    add_144: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_11, 1.0);  tanh_11 = None\n",
      "    mul_134: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_131, add_144);  mul_131 = add_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_202: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg637_1);  mul_130 = arg637_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_135: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_134, linear_202);  mul_134 = linear_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_108: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_135, 0.1, False);  mul_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_203: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_108, arg638_1);  dropout_108 = arg638_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_109: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_203, 0.1, False);  linear_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_145: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_56, dropout_109);  to_56 = dropout_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_57: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_145, torch.float32);  add_145 = None\n",
      "    pow_42: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_57, 2)\n",
      "    mean_40: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_42, [-1], True);  pow_42 = None\n",
      "    add_146: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_40, 1e-06);  mean_40 = None\n",
      "    rsqrt_29: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_146);  add_146 = None\n",
      "    mul_136: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_57, rsqrt_29);  rsqrt_29 = None\n",
      "    mul_137: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg644_1, mul_136);  arg644_1 = mul_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_204: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg640_1);  arg640_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_131: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_204, [1, -1, 6, 64]);  linear_204 = None\n",
      "    transpose_185: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_131, 1, 2);  view_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_205: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg641_1);  arg641_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_206: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg642_1);  mul_137 = arg642_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_132: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_205, [1, -1, 6, 64]);  linear_205 = None\n",
      "    transpose_186: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_132, 1, 2);  view_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_133: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_206, [1, -1, 6, 64]);  linear_206 = None\n",
      "    transpose_187: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_133, 1, 2);  view_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant19 = self._tensor_constant19\n",
      "    lift_fresh_copy_19: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant19);  _tensor_constant19 = None\n",
      "    detach__19: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_19);  lift_fresh_copy_19 = None\n",
      "    _tensor_constant20 = self._tensor_constant20\n",
      "    lift_fresh_copy_20: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant20);  _tensor_constant20 = None\n",
      "    detach__20: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_20);  lift_fresh_copy_20 = None\n",
      "    cat_17: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__19, transpose_186], -2);  detach__19 = transpose_186 = None\n",
      "    cat_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__20, transpose_187], -2);  detach__20 = transpose_187 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_188: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_17, 3, 2);  cat_17 = None\n",
      "    matmul_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_185, transpose_188);  transpose_185 = transpose_188 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_33, add_109);  matmul_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_58: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__17, torch.float32);  add__17 = None\n",
      "    softmax_27: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_58, -1)\n",
      "    type_as_16: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_27, to_58);  softmax_27 = to_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_110: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_16, 0.1, False);  type_as_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_34: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_110, cat_18);  dropout_110 = cat_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_189: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_34, 1, 2);  matmul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_134: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_189, [1, -1, 384]);  transpose_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_207: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_134, arg643_1);  view_134 = arg643_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_111: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_207, 0.1, False);  linear_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_147: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_57, dropout_111);  to_57 = dropout_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_35: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_148: \"i64[]\" = torch.ops.aten.add.Tensor(select_35, 1);  select_35 = add_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_59: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_147, torch.float32);  add_147 = None\n",
      "    pow_43: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_59, 2)\n",
      "    mean_41: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_43, [-1], True);  pow_43 = None\n",
      "    add_149: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_41, 1e-06);  mean_41 = None\n",
      "    rsqrt_30: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_149);  add_149 = None\n",
      "    mul_138: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_59, rsqrt_30);  rsqrt_30 = None\n",
      "    mul_139: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg649_1, mul_138);  arg649_1 = mul_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_208: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_139, arg645_1);  mul_139 = arg645_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_135: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_208, [1, -1, 6, 64]);  linear_208 = None\n",
      "    transpose_190: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_135, 1, 2);  view_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_209: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg646_1);  arg646_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_210: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg647_1);  arg647_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_136: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_209, [1, -1, 6, 64]);  linear_209 = None\n",
      "    transpose_191: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_136, 1, 2);  view_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_137: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_210, [1, -1, 6, 64]);  linear_210 = None\n",
      "    transpose_192: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_137, 1, 2);  view_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant21 = self._tensor_constant21\n",
      "    lift_fresh_copy_21: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant21);  _tensor_constant21 = None\n",
      "    detach__21: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_21);  lift_fresh_copy_21 = None\n",
      "    _tensor_constant22 = self._tensor_constant22\n",
      "    lift_fresh_copy_22: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant22);  _tensor_constant22 = None\n",
      "    detach__22: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_22);  lift_fresh_copy_22 = None\n",
      "    cat_19: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__21, transpose_191], -2);  detach__21 = transpose_191 = None\n",
      "    cat_20: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__22, transpose_192], -2);  detach__22 = transpose_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_193: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_19, 3, 2);  cat_19 = None\n",
      "    matmul_35: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_190, transpose_193);  transpose_190 = transpose_193 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__18: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_35, add_113);  matmul_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_60: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__18, torch.float32);  add__18 = None\n",
      "    softmax_28: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_60, -1)\n",
      "    type_as_17: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_28, to_60);  softmax_28 = to_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_112: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_17, 0.1, False);  type_as_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_36: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_112, cat_20);  dropout_112 = cat_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_194: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_36, 1, 2);  matmul_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_138: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_194, [1, -1, 384]);  transpose_194 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_211: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_138, arg648_1);  view_138 = arg648_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_113: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_211, 0.1, False);  linear_211 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_150: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_59, dropout_113);  to_59 = dropout_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_61: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_150, torch.float32);  add_150 = None\n",
      "    pow_44: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_61, 2)\n",
      "    mean_42: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_44, [-1], True);  pow_44 = None\n",
      "    add_151: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_42, 1e-06);  mean_42 = None\n",
      "    rsqrt_31: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_151);  add_151 = None\n",
      "    mul_140: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_61, rsqrt_31);  rsqrt_31 = None\n",
      "    mul_141: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg653_1, mul_140);  arg653_1 = mul_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_212: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg650_1);  arg650_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_142: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_212, 0.5)\n",
      "    pow_45: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_212, 3.0)\n",
      "    mul_143: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_45, 0.044715);  pow_45 = None\n",
      "    add_152: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_212, mul_143);  linear_212 = mul_143 = None\n",
      "    mul_144: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_152, 0.7978845608028654);  add_152 = None\n",
      "    tanh_12: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_144);  mul_144 = None\n",
      "    add_153: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_12, 1.0);  tanh_12 = None\n",
      "    mul_145: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_142, add_153);  mul_142 = add_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_213: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg651_1);  mul_141 = arg651_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_146: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_145, linear_213);  mul_145 = linear_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_114: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_146, 0.1, False);  mul_146 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_214: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_114, arg652_1);  dropout_114 = arg652_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_115: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_214, 0.1, False);  linear_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_154: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_61, dropout_115);  to_61 = dropout_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_62: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_154, torch.float32);  add_154 = None\n",
      "    pow_46: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_62, 2)\n",
      "    mean_43: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_46, [-1], True);  pow_46 = None\n",
      "    add_155: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_43, 1e-06);  mean_43 = None\n",
      "    rsqrt_32: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_155);  add_155 = None\n",
      "    mul_147: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_62, rsqrt_32);  rsqrt_32 = None\n",
      "    mul_148: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg658_1, mul_147);  arg658_1 = mul_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_215: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg654_1);  arg654_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_139: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_215, [1, -1, 6, 64]);  linear_215 = None\n",
      "    transpose_195: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_139, 1, 2);  view_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_216: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg655_1);  arg655_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_217: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg656_1);  mul_148 = arg656_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_140: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_216, [1, -1, 6, 64]);  linear_216 = None\n",
      "    transpose_196: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_140, 1, 2);  view_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_141: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_217, [1, -1, 6, 64]);  linear_217 = None\n",
      "    transpose_197: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_141, 1, 2);  view_141 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant23 = self._tensor_constant23\n",
      "    lift_fresh_copy_23: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant23);  _tensor_constant23 = None\n",
      "    detach__23: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_23);  lift_fresh_copy_23 = None\n",
      "    _tensor_constant24 = self._tensor_constant24\n",
      "    lift_fresh_copy_24: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant24);  _tensor_constant24 = None\n",
      "    detach__24: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_24);  lift_fresh_copy_24 = None\n",
      "    cat_21: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__23, transpose_196], -2);  detach__23 = transpose_196 = None\n",
      "    cat_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__24, transpose_197], -2);  detach__24 = transpose_197 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_198: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_21, 3, 2);  cat_21 = None\n",
      "    matmul_37: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_195, transpose_198);  transpose_195 = transpose_198 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_37, add_109);  matmul_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_63: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__19, torch.float32);  add__19 = None\n",
      "    softmax_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_63, -1)\n",
      "    type_as_18: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_29, to_63);  softmax_29 = to_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_116: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_18, 0.1, False);  type_as_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_38: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_116, cat_22);  dropout_116 = cat_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_199: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_38, 1, 2);  matmul_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_142: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_199, [1, -1, 384]);  transpose_199 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_218: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_142, arg657_1);  view_142 = arg657_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_117: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_218, 0.1, False);  linear_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_156: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_62, dropout_117);  to_62 = dropout_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_36: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_157: \"i64[]\" = torch.ops.aten.add.Tensor(select_36, 1);  select_36 = add_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_64: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_156, torch.float32);  add_156 = None\n",
      "    pow_47: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_64, 2)\n",
      "    mean_44: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_47, [-1], True);  pow_47 = None\n",
      "    add_158: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_44, 1e-06);  mean_44 = None\n",
      "    rsqrt_33: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_158);  add_158 = None\n",
      "    mul_149: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_64, rsqrt_33);  rsqrt_33 = None\n",
      "    mul_150: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg663_1, mul_149);  arg663_1 = mul_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_219: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_150, arg659_1);  mul_150 = arg659_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_143: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_219, [1, -1, 6, 64]);  linear_219 = None\n",
      "    transpose_200: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_143, 1, 2);  view_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_220: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg660_1);  arg660_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_221: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg661_1);  arg661_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_220, [1, -1, 6, 64]);  linear_220 = None\n",
      "    transpose_201: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_144, 1, 2);  view_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_145: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_221, [1, -1, 6, 64]);  linear_221 = None\n",
      "    transpose_202: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_145, 1, 2);  view_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant25 = self._tensor_constant25\n",
      "    lift_fresh_copy_25: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant25);  _tensor_constant25 = None\n",
      "    detach__25: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_25);  lift_fresh_copy_25 = None\n",
      "    _tensor_constant26 = self._tensor_constant26\n",
      "    lift_fresh_copy_26: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant26);  _tensor_constant26 = None\n",
      "    detach__26: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_26);  lift_fresh_copy_26 = None\n",
      "    cat_23: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__25, transpose_201], -2);  detach__25 = transpose_201 = None\n",
      "    cat_24: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__26, transpose_202], -2);  detach__26 = transpose_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_203: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_23, 3, 2);  cat_23 = None\n",
      "    matmul_39: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_200, transpose_203);  transpose_200 = transpose_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_39, add_113);  matmul_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_65: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__20, torch.float32);  add__20 = None\n",
      "    softmax_30: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_65, -1)\n",
      "    type_as_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_30, to_65);  softmax_30 = to_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_118: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_19, 0.1, False);  type_as_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_40: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_118, cat_24);  dropout_118 = cat_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_204: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_40, 1, 2);  matmul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_146: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_204, [1, -1, 384]);  transpose_204 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_222: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_146, arg662_1);  view_146 = arg662_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_119: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_222, 0.1, False);  linear_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_159: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_64, dropout_119);  to_64 = dropout_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_66: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_159, torch.float32);  add_159 = None\n",
      "    pow_48: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_66, 2)\n",
      "    mean_45: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_48, [-1], True);  pow_48 = None\n",
      "    add_160: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_45, 1e-06);  mean_45 = None\n",
      "    rsqrt_34: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_160);  add_160 = None\n",
      "    mul_151: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_66, rsqrt_34);  rsqrt_34 = None\n",
      "    mul_152: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg667_1, mul_151);  arg667_1 = mul_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_223: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg664_1);  arg664_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_153: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_223, 0.5)\n",
      "    pow_49: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_223, 3.0)\n",
      "    mul_154: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_49, 0.044715);  pow_49 = None\n",
      "    add_161: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_223, mul_154);  linear_223 = mul_154 = None\n",
      "    mul_155: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_161, 0.7978845608028654);  add_161 = None\n",
      "    tanh_13: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_155);  mul_155 = None\n",
      "    add_162: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_13, 1.0);  tanh_13 = None\n",
      "    mul_156: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_153, add_162);  mul_153 = add_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_224: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg665_1);  mul_152 = arg665_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_157: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_156, linear_224);  mul_156 = linear_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_120: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_157, 0.1, False);  mul_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_225: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_120, arg666_1);  dropout_120 = arg666_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_121: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_225, 0.1, False);  linear_225 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_163: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_66, dropout_121);  to_66 = dropout_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_67: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_163, torch.float32);  add_163 = None\n",
      "    pow_50: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_67, 2)\n",
      "    mean_46: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_50, [-1], True);  pow_50 = None\n",
      "    add_164: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_46, 1e-06);  mean_46 = None\n",
      "    rsqrt_35: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_164);  add_164 = None\n",
      "    mul_158: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_67, rsqrt_35);  rsqrt_35 = None\n",
      "    mul_159: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg672_1, mul_158);  arg672_1 = mul_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_226: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg668_1);  arg668_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_147: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_226, [1, -1, 6, 64]);  linear_226 = None\n",
      "    transpose_205: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_147, 1, 2);  view_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_227: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg669_1);  arg669_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_228: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg670_1);  mul_159 = arg670_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_148: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_227, [1, -1, 6, 64]);  linear_227 = None\n",
      "    transpose_206: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_148, 1, 2);  view_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_228, [1, -1, 6, 64]);  linear_228 = None\n",
      "    transpose_207: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_149, 1, 2);  view_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant27 = self._tensor_constant27\n",
      "    lift_fresh_copy_27: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant27);  _tensor_constant27 = None\n",
      "    detach__27: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_27);  lift_fresh_copy_27 = None\n",
      "    _tensor_constant28 = self._tensor_constant28\n",
      "    lift_fresh_copy_28: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant28);  _tensor_constant28 = None\n",
      "    detach__28: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_28);  lift_fresh_copy_28 = None\n",
      "    cat_25: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__27, transpose_206], -2);  detach__27 = transpose_206 = None\n",
      "    cat_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__28, transpose_207], -2);  detach__28 = transpose_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_208: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_25, 3, 2);  cat_25 = None\n",
      "    matmul_41: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_205, transpose_208);  transpose_205 = transpose_208 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_41, add_109);  matmul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_68: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__21, torch.float32);  add__21 = None\n",
      "    softmax_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_68, -1)\n",
      "    type_as_20: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_31, to_68);  softmax_31 = to_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_122: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_20, 0.1, False);  type_as_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_42: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_122, cat_26);  dropout_122 = cat_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_209: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_42, 1, 2);  matmul_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_150: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_209, [1, -1, 384]);  transpose_209 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_229: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_150, arg671_1);  view_150 = arg671_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_123: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_229, 0.1, False);  linear_229 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_165: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_67, dropout_123);  to_67 = dropout_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_37: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_166: \"i64[]\" = torch.ops.aten.add.Tensor(select_37, 1);  select_37 = add_166 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_69: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_165, torch.float32);  add_165 = None\n",
      "    pow_51: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_69, 2)\n",
      "    mean_47: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_51, [-1], True);  pow_51 = None\n",
      "    add_167: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_47, 1e-06);  mean_47 = None\n",
      "    rsqrt_36: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_167);  add_167 = None\n",
      "    mul_160: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_69, rsqrt_36);  rsqrt_36 = None\n",
      "    mul_161: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg677_1, mul_160);  arg677_1 = mul_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_230: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_161, arg673_1);  mul_161 = arg673_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_151: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_230, [1, -1, 6, 64]);  linear_230 = None\n",
      "    transpose_210: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_151, 1, 2);  view_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_231: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg674_1);  arg674_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_232: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg675_1);  arg675_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_152: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_231, [1, -1, 6, 64]);  linear_231 = None\n",
      "    transpose_211: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_153: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_232, [1, -1, 6, 64]);  linear_232 = None\n",
      "    transpose_212: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_153, 1, 2);  view_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant29 = self._tensor_constant29\n",
      "    lift_fresh_copy_29: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant29);  _tensor_constant29 = None\n",
      "    detach__29: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_29);  lift_fresh_copy_29 = None\n",
      "    _tensor_constant30 = self._tensor_constant30\n",
      "    lift_fresh_copy_30: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant30);  _tensor_constant30 = None\n",
      "    detach__30: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_30);  lift_fresh_copy_30 = None\n",
      "    cat_27: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__29, transpose_211], -2);  detach__29 = transpose_211 = None\n",
      "    cat_28: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__30, transpose_212], -2);  detach__30 = transpose_212 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_213: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_27, 3, 2);  cat_27 = None\n",
      "    matmul_43: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_210, transpose_213);  transpose_210 = transpose_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_43, add_113);  matmul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_70: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__22, torch.float32);  add__22 = None\n",
      "    softmax_32: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_70, -1)\n",
      "    type_as_21: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_32, to_70);  softmax_32 = to_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_124: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_21, 0.1, False);  type_as_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_44: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_124, cat_28);  dropout_124 = cat_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_214: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_44, 1, 2);  matmul_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_154: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_214, [1, -1, 384]);  transpose_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_233: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_154, arg676_1);  view_154 = arg676_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_125: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_233, 0.1, False);  linear_233 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_168: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_69, dropout_125);  to_69 = dropout_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_71: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_168, torch.float32);  add_168 = None\n",
      "    pow_52: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_71, 2)\n",
      "    mean_48: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_52, [-1], True);  pow_52 = None\n",
      "    add_169: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_48, 1e-06);  mean_48 = None\n",
      "    rsqrt_37: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_169);  add_169 = None\n",
      "    mul_162: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_71, rsqrt_37);  rsqrt_37 = None\n",
      "    mul_163: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg681_1, mul_162);  arg681_1 = mul_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_234: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg678_1);  arg678_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_164: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_234, 0.5)\n",
      "    pow_53: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_234, 3.0)\n",
      "    mul_165: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_53, 0.044715);  pow_53 = None\n",
      "    add_170: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_234, mul_165);  linear_234 = mul_165 = None\n",
      "    mul_166: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_170, 0.7978845608028654);  add_170 = None\n",
      "    tanh_14: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_166);  mul_166 = None\n",
      "    add_171: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_14, 1.0);  tanh_14 = None\n",
      "    mul_167: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_164, add_171);  mul_164 = add_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_235: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg679_1);  mul_163 = arg679_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_168: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_167, linear_235);  mul_167 = linear_235 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_126: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_168, 0.1, False);  mul_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_236: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_126, arg680_1);  dropout_126 = arg680_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_127: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_236, 0.1, False);  linear_236 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_172: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_71, dropout_127);  to_71 = dropout_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_72: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_172, torch.float32);  add_172 = None\n",
      "    pow_54: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_72, 2)\n",
      "    mean_49: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_54, [-1], True);  pow_54 = None\n",
      "    add_173: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_49, 1e-06);  mean_49 = None\n",
      "    rsqrt_38: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_173);  add_173 = None\n",
      "    mul_169: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_72, rsqrt_38);  rsqrt_38 = None\n",
      "    mul_170: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg686_1, mul_169);  arg686_1 = mul_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_237: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg682_1);  arg682_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_155: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_237, [1, -1, 6, 64]);  linear_237 = None\n",
      "    transpose_215: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_155, 1, 2);  view_155 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_238: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg683_1);  arg683_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_239: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg684_1);  mul_170 = arg684_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_156: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_238, [1, -1, 6, 64]);  linear_238 = None\n",
      "    transpose_216: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_156, 1, 2);  view_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_157: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_239, [1, -1, 6, 64]);  linear_239 = None\n",
      "    transpose_217: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_157, 1, 2);  view_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant31 = self._tensor_constant31\n",
      "    lift_fresh_copy_31: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant31);  _tensor_constant31 = None\n",
      "    detach__31: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_31);  lift_fresh_copy_31 = None\n",
      "    _tensor_constant32 = self._tensor_constant32\n",
      "    lift_fresh_copy_32: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant32);  _tensor_constant32 = None\n",
      "    detach__32: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_32);  lift_fresh_copy_32 = None\n",
      "    cat_29: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__31, transpose_216], -2);  detach__31 = transpose_216 = None\n",
      "    cat_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__32, transpose_217], -2);  detach__32 = transpose_217 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_218: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_29, 3, 2);  cat_29 = None\n",
      "    matmul_45: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_215, transpose_218);  transpose_215 = transpose_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_45, add_109);  matmul_45 = add_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_73: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__23, torch.float32);  add__23 = None\n",
      "    softmax_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_73, -1)\n",
      "    type_as_22: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_33, to_73);  softmax_33 = to_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_128: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_22, 0.1, False);  type_as_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_46: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_128, cat_30);  dropout_128 = cat_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_219: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_46, 1, 2);  matmul_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_158: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_219, [1, -1, 384]);  transpose_219 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_240: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_158, arg685_1);  view_158 = arg685_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_129: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_240, 0.1, False);  linear_240 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_174: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_72, dropout_129);  to_72 = dropout_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_38: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_175: \"i64[]\" = torch.ops.aten.add.Tensor(select_38, 1);  select_38 = add_175 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_74: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_174, torch.float32);  add_174 = None\n",
      "    pow_55: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_74, 2)\n",
      "    mean_50: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_55, [-1], True);  pow_55 = None\n",
      "    add_176: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_50, 1e-06);  mean_50 = None\n",
      "    rsqrt_39: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_176);  add_176 = None\n",
      "    mul_171: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_74, rsqrt_39);  rsqrt_39 = None\n",
      "    mul_172: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg691_1, mul_171);  arg691_1 = mul_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_241: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_172, arg687_1);  mul_172 = arg687_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_241, [1, -1, 6, 64]);  linear_241 = None\n",
      "    transpose_220: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_159, 1, 2);  view_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_242: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg688_1);  arg688_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_243: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg689_1);  dropout_84 = arg689_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_160: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_242, [1, -1, 6, 64]);  linear_242 = None\n",
      "    transpose_221: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_160, 1, 2);  view_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_161: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_243, [1, -1, 6, 64]);  linear_243 = None\n",
      "    transpose_222: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_161, 1, 2);  view_161 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant33 = self._tensor_constant33\n",
      "    lift_fresh_copy_33: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant33);  _tensor_constant33 = None\n",
      "    detach__33: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_33);  lift_fresh_copy_33 = None\n",
      "    _tensor_constant34 = self._tensor_constant34\n",
      "    lift_fresh_copy_34: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant34);  _tensor_constant34 = None\n",
      "    detach__34: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_34);  lift_fresh_copy_34 = None\n",
      "    cat_31: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__33, transpose_221], -2);  detach__33 = transpose_221 = None\n",
      "    cat_32: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__34, transpose_222], -2);  detach__34 = transpose_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_223: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_31, 3, 2);  cat_31 = None\n",
      "    matmul_47: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_220, transpose_223);  transpose_220 = transpose_223 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_47, add_113);  matmul_47 = add_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_75: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__24, torch.float32);  add__24 = None\n",
      "    softmax_34: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_75, -1)\n",
      "    type_as_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_34, to_75);  softmax_34 = to_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_130: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_23, 0.1, False);  type_as_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_48: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_130, cat_32);  dropout_130 = cat_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_224: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_48, 1, 2);  matmul_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_162: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_224, [1, -1, 384]);  transpose_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_244: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_162, arg690_1);  view_162 = arg690_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_131: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_244, 0.1, False);  linear_244 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_177: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_74, dropout_131);  to_74 = dropout_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_76: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_177, torch.float32);  add_177 = None\n",
      "    pow_56: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_76, 2)\n",
      "    mean_51: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_56, [-1], True);  pow_56 = None\n",
      "    add_178: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_51, 1e-06);  mean_51 = None\n",
      "    rsqrt_40: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_178);  add_178 = None\n",
      "    mul_173: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_76, rsqrt_40);  rsqrt_40 = None\n",
      "    mul_174: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg695_1, mul_173);  arg695_1 = mul_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_245: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg692_1);  arg692_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_175: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_245, 0.5)\n",
      "    pow_57: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_245, 3.0)\n",
      "    mul_176: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_57, 0.044715);  pow_57 = None\n",
      "    add_179: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_245, mul_176);  linear_245 = mul_176 = None\n",
      "    mul_177: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_179, 0.7978845608028654);  add_179 = None\n",
      "    tanh_15: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_177);  mul_177 = None\n",
      "    add_180: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_15, 1.0);  tanh_15 = None\n",
      "    mul_178: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_175, add_180);  mul_175 = add_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_246: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg693_1);  mul_174 = arg693_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_179: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_178, linear_246);  mul_178 = linear_246 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_132: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_179, 0.1, False);  mul_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_247: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_132, arg694_1);  dropout_132 = arg694_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_133: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_247, 0.1, False);  linear_247 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_181: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_76, dropout_133);  to_76 = dropout_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_77: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_181, torch.float32);  add_181 = None\n",
      "    pow_58: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_77, 2)\n",
      "    mean_52: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_58, [-1], True);  pow_58 = None\n",
      "    add_182: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_52, 1e-06);  mean_52 = None\n",
      "    rsqrt_41: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_182);  add_182 = None\n",
      "    mul_180: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_77, rsqrt_41);  to_77 = rsqrt_41 = None\n",
      "    mul_181: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg696_1, mul_180);  arg696_1 = mul_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_134: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(mul_181, 0.1, False);  mul_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_248: \"f32[1, 1, 32128]\" = torch.ops.aten.linear.default(dropout_134, arg697_1);  dropout_134 = arg697_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    slice_61: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, -1);  slice_41 = None\n",
      "    add_183: \"i64[1]\" = torch.ops.aten.add.Tensor(slice_61, 1);  slice_61 = add_183 = None\n",
      "    slice_62: \"f32[1, 1, 32128]\" = torch.ops.aten.slice.Tensor(linear_248);  linear_248 = None\n",
      "    select_39: \"f32[1, 32128]\" = torch.ops.aten.select.int(slice_62, 1, -1);  slice_62 = None\n",
      "    slice_63: \"f32[1, 32128]\" = torch.ops.aten.slice.Tensor(select_39, 1);  select_39 = None\n",
      "    to_78: \"f32[1, 32128]\" = torch.ops.aten.to.device(slice_63, device(type='cpu'), torch.float32, False, True);  slice_63 = None\n",
      "    argmax: \"i64[1]\" = torch.ops.aten.argmax.default(to_78, -1);  to_78 = None\n",
      "    mul_182: \"i64[1]\" = torch.ops.aten.mul.Tensor(argmax, ones_3);  argmax = None\n",
      "    rsub_2: \"i64[1]\" = torch.ops.aten.rsub.Scalar(ones_3, 1)\n",
      "    mul_183: \"i64[1]\" = torch.ops.aten.mul.Tensor(detach__1, rsub_2);  detach__1 = rsub_2 = None\n",
      "    add_184: \"i64[1]\" = torch.ops.aten.add.Tensor(mul_182, mul_183);  mul_182 = mul_183 = None\n",
      "    slice_64: \"i64[1]\" = torch.ops.aten.slice.Tensor(add_184, 0, 0, 9223372036854775807);  add_184 = None\n",
      "    unsqueeze_32: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_64, 1);  slice_64 = None\n",
      "    cat_33: \"i64[1, 2]\" = torch.ops.aten.cat.default([mul_89, unsqueeze_32], -1);  mul_89 = unsqueeze_32 = None\n",
      "    full_1: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    full_2: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    or_1: \"b8[1]\" = torch.ops.aten.__or__.Tensor(full_1, full_2);  full_1 = full_2 = None\n",
      "    to_79: \"i64[1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_24, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_24 = None\n",
      "    slice_65: \"i64[1, 2]\" = torch.ops.aten.slice.Tensor(cat_33);  cat_33 = None\n",
      "    select_40: \"i64[1]\" = torch.ops.aten.select.int(slice_65, 1, -1);  slice_65 = None\n",
      "    isin_1: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(select_40, to_79);  select_40 = to_79 = None\n",
      "    or_2: \"b8[1]\" = torch.ops.aten.__or__.Tensor(or_1, isin_1);  or_1 = isin_1 = None\n",
      "    bitwise_not: \"b8[1]\" = torch.ops.aten.bitwise_not.default(or_2);  or_2 = None\n",
      "    and_1: \"i64[1]\" = torch.ops.aten.__and__.Tensor(ones_3, bitwise_not);  ones_3 = bitwise_not = None\n",
      "    max_1: \"i64[]\" = torch.ops.aten.max.default(and_1);  and_1 = None\n",
      "    eq: \"b8[]\" = torch.ops.aten.eq.Scalar(max_1, 0);  max_1 = None\n",
      "    ne_2: \"b8[]\" = torch.ops.aten.ne.Scalar(eq, 0);  eq = None\n",
      "    item_2: \"Sym(Eq(u1, 1))\" = torch.ops.aten.item.default(ne_2);  ne_2 = item_2 = None\n",
      "    _set_grad_enabled_3 = torch._C._set_grad_enabled(False);  _set_grad_enabled_3 = None\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[1, 1, 192]\", arg1_1: \"f32[1, 197, 192]\", arg2_1: \"f32[192, 3, 16, 16]\", arg3_1: \"f32[192]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192]\", arg6_1: \"f32[576, 192]\", arg7_1: \"f32[576]\", arg8_1: \"f32[192, 192]\", arg9_1: \"f32[192]\", arg10_1: \"f32[192]\", arg11_1: \"f32[192]\", arg12_1: \"f32[768, 192]\", arg13_1: \"f32[768]\", arg14_1: \"f32[192, 768]\", arg15_1: \"f32[192]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192]\", arg18_1: \"f32[576, 192]\", arg19_1: \"f32[576]\", arg20_1: \"f32[192, 192]\", arg21_1: \"f32[192]\", arg22_1: \"f32[192]\", arg23_1: \"f32[192]\", arg24_1: \"f32[768, 192]\", arg25_1: \"f32[768]\", arg26_1: \"f32[192, 768]\", arg27_1: \"f32[192]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192]\", arg30_1: \"f32[576, 192]\", arg31_1: \"f32[576]\", arg32_1: \"f32[192, 192]\", arg33_1: \"f32[192]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192]\", arg36_1: \"f32[768, 192]\", arg37_1: \"f32[768]\", arg38_1: \"f32[192, 768]\", arg39_1: \"f32[192]\", arg40_1: \"f32[192]\", arg41_1: \"f32[192]\", arg42_1: \"f32[576, 192]\", arg43_1: \"f32[576]\", arg44_1: \"f32[192, 192]\", arg45_1: \"f32[192]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192]\", arg48_1: \"f32[768, 192]\", arg49_1: \"f32[768]\", arg50_1: \"f32[192, 768]\", arg51_1: \"f32[192]\", arg52_1: \"f32[192]\", arg53_1: \"f32[192]\", arg54_1: \"f32[576, 192]\", arg55_1: \"f32[576]\", arg56_1: \"f32[192, 192]\", arg57_1: \"f32[192]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192]\", arg60_1: \"f32[768, 192]\", arg61_1: \"f32[768]\", arg62_1: \"f32[192, 768]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[576, 192]\", arg67_1: \"f32[576]\", arg68_1: \"f32[192, 192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[768, 192]\", arg73_1: \"f32[768]\", arg74_1: \"f32[192, 768]\", arg75_1: \"f32[192]\", arg76_1: \"f32[192]\", arg77_1: \"f32[192]\", arg78_1: \"f32[576, 192]\", arg79_1: \"f32[576]\", arg80_1: \"f32[192, 192]\", arg81_1: \"f32[192]\", arg82_1: \"f32[192]\", arg83_1: \"f32[192]\", arg84_1: \"f32[768, 192]\", arg85_1: \"f32[768]\", arg86_1: \"f32[192, 768]\", arg87_1: \"f32[192]\", arg88_1: \"f32[192]\", arg89_1: \"f32[192]\", arg90_1: \"f32[576, 192]\", arg91_1: \"f32[576]\", arg92_1: \"f32[192, 192]\", arg93_1: \"f32[192]\", arg94_1: \"f32[192]\", arg95_1: \"f32[192]\", arg96_1: \"f32[768, 192]\", arg97_1: \"f32[768]\", arg98_1: \"f32[192, 768]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[576, 192]\", arg103_1: \"f32[576]\", arg104_1: \"f32[192, 192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[768, 192]\", arg109_1: \"f32[768]\", arg110_1: \"f32[192, 768]\", arg111_1: \"f32[192]\", arg112_1: \"f32[192]\", arg113_1: \"f32[192]\", arg114_1: \"f32[576, 192]\", arg115_1: \"f32[576]\", arg116_1: \"f32[192, 192]\", arg117_1: \"f32[192]\", arg118_1: \"f32[192]\", arg119_1: \"f32[192]\", arg120_1: \"f32[768, 192]\", arg121_1: \"f32[768]\", arg122_1: \"f32[192, 768]\", arg123_1: \"f32[192]\", arg124_1: \"f32[192]\", arg125_1: \"f32[192]\", arg126_1: \"f32[576, 192]\", arg127_1: \"f32[576]\", arg128_1: \"f32[192, 192]\", arg129_1: \"f32[192]\", arg130_1: \"f32[192]\", arg131_1: \"f32[192]\", arg132_1: \"f32[768, 192]\", arg133_1: \"f32[768]\", arg134_1: \"f32[192, 768]\", arg135_1: \"f32[192]\", arg136_1: \"f32[192]\", arg137_1: \"f32[192]\", arg138_1: \"f32[576, 192]\", arg139_1: \"f32[576]\", arg140_1: \"f32[192, 192]\", arg141_1: \"f32[192]\", arg142_1: \"f32[192]\", arg143_1: \"f32[192]\", arg144_1: \"f32[768, 192]\", arg145_1: \"f32[768]\", arg146_1: \"f32[192, 768]\", arg147_1: \"f32[192]\", arg148_1: \"f32[192]\", arg149_1: \"f32[192]\", arg150_1: \"f32[768, 192]\", arg151_1: \"f32[768]\", arg152_1: \"f32[32, 768]\", arg153_1: \"f32[30522, 768]\", arg154_1: \"f32[512, 768]\", arg155_1: \"f32[2, 768]\", arg156_1: \"f32[768]\", arg157_1: \"f32[768]\", arg158_1: \"f32[768, 768]\", arg159_1: \"f32[768]\", arg160_1: \"f32[768, 768]\", arg161_1: \"f32[768]\", arg162_1: \"f32[768, 768]\", arg163_1: \"f32[768]\", arg164_1: \"f32[768, 768]\", arg165_1: \"f32[768]\", arg166_1: \"f32[768]\", arg167_1: \"f32[768]\", arg168_1: \"f32[3072, 768]\", arg169_1: \"f32[3072]\", arg170_1: \"f32[768, 3072]\", arg171_1: \"f32[768]\", arg172_1: \"f32[768]\", arg173_1: \"f32[768]\", arg174_1: \"f32[768, 768]\", arg175_1: \"f32[768]\", arg176_1: \"f32[768, 768]\", arg177_1: \"f32[768]\", arg178_1: \"f32[768, 768]\", arg179_1: \"f32[768]\", arg180_1: \"f32[768, 768]\", arg181_1: \"f32[768]\", arg182_1: \"f32[768]\", arg183_1: \"f32[768]\", arg184_1: \"f32[3072, 768]\", arg185_1: \"f32[3072]\", arg186_1: \"f32[768, 3072]\", arg187_1: \"f32[768]\", arg188_1: \"f32[768]\", arg189_1: \"f32[768]\", arg190_1: \"f32[768, 768]\", arg191_1: \"f32[768]\", arg192_1: \"f32[768, 768]\", arg193_1: \"f32[768]\", arg194_1: \"f32[768, 768]\", arg195_1: \"f32[768]\", arg196_1: \"f32[768, 768]\", arg197_1: \"f32[768]\", arg198_1: \"f32[768]\", arg199_1: \"f32[768]\", arg200_1: \"f32[3072, 768]\", arg201_1: \"f32[3072]\", arg202_1: \"f32[768, 3072]\", arg203_1: \"f32[768]\", arg204_1: \"f32[768]\", arg205_1: \"f32[768]\", arg206_1: \"f32[768, 768]\", arg207_1: \"f32[768]\", arg208_1: \"f32[768, 768]\", arg209_1: \"f32[768]\", arg210_1: \"f32[768, 768]\", arg211_1: \"f32[768]\", arg212_1: \"f32[768, 768]\", arg213_1: \"f32[768]\", arg214_1: \"f32[768]\", arg215_1: \"f32[768]\", arg216_1: \"f32[3072, 768]\", arg217_1: \"f32[3072]\", arg218_1: \"f32[768, 3072]\", arg219_1: \"f32[768]\", arg220_1: \"f32[768]\", arg221_1: \"f32[768]\", arg222_1: \"f32[768, 768]\", arg223_1: \"f32[768]\", arg224_1: \"f32[768, 768]\", arg225_1: \"f32[768]\", arg226_1: \"f32[768, 768]\", arg227_1: \"f32[768]\", arg228_1: \"f32[768, 768]\", arg229_1: \"f32[768]\", arg230_1: \"f32[768]\", arg231_1: \"f32[768]\", arg232_1: \"f32[3072, 768]\", arg233_1: \"f32[3072]\", arg234_1: \"f32[768, 3072]\", arg235_1: \"f32[768]\", arg236_1: \"f32[768]\", arg237_1: \"f32[768]\", arg238_1: \"f32[768, 768]\", arg239_1: \"f32[768]\", arg240_1: \"f32[768, 768]\", arg241_1: \"f32[768]\", arg242_1: \"f32[768, 768]\", arg243_1: \"f32[768]\", arg244_1: \"f32[768, 768]\", arg245_1: \"f32[768]\", arg246_1: \"f32[768]\", arg247_1: \"f32[768]\", arg248_1: \"f32[3072, 768]\", arg249_1: \"f32[3072]\", arg250_1: \"f32[768, 3072]\", arg251_1: \"f32[768]\", arg252_1: \"f32[768]\", arg253_1: \"f32[768]\", arg254_1: \"f32[768, 768]\", arg255_1: \"f32[768]\", arg256_1: \"f32[768, 768]\", arg257_1: \"f32[768]\", arg258_1: \"f32[768, 768]\", arg259_1: \"f32[768]\", arg260_1: \"f32[768, 768]\", arg261_1: \"f32[768]\", arg262_1: \"f32[768]\", arg263_1: \"f32[768]\", arg264_1: \"f32[3072, 768]\", arg265_1: \"f32[3072]\", arg266_1: \"f32[768, 3072]\", arg267_1: \"f32[768]\", arg268_1: \"f32[768]\", arg269_1: \"f32[768]\", arg270_1: \"f32[768, 768]\", arg271_1: \"f32[768]\", arg272_1: \"f32[768, 768]\", arg273_1: \"f32[768]\", arg274_1: \"f32[768, 768]\", arg275_1: \"f32[768]\", arg276_1: \"f32[768, 768]\", arg277_1: \"f32[768]\", arg278_1: \"f32[768]\", arg279_1: \"f32[768]\", arg280_1: \"f32[3072, 768]\", arg281_1: \"f32[3072]\", arg282_1: \"f32[768, 3072]\", arg283_1: \"f32[768]\", arg284_1: \"f32[768]\", arg285_1: \"f32[768]\", arg286_1: \"f32[768, 768]\", arg287_1: \"f32[768]\", arg288_1: \"f32[768, 768]\", arg289_1: \"f32[768]\", arg290_1: \"f32[768, 768]\", arg291_1: \"f32[768]\", arg292_1: \"f32[768, 768]\", arg293_1: \"f32[768]\", arg294_1: \"f32[768]\", arg295_1: \"f32[768]\", arg296_1: \"f32[3072, 768]\", arg297_1: \"f32[3072]\", arg298_1: \"f32[768, 3072]\", arg299_1: \"f32[768]\", arg300_1: \"f32[768]\", arg301_1: \"f32[768]\", arg302_1: \"f32[768, 768]\", arg303_1: \"f32[768]\", arg304_1: \"f32[768, 768]\", arg305_1: \"f32[768]\", arg306_1: \"f32[768, 768]\", arg307_1: \"f32[768]\", arg308_1: \"f32[768, 768]\", arg309_1: \"f32[768]\", arg310_1: \"f32[768]\", arg311_1: \"f32[768]\", arg312_1: \"f32[3072, 768]\", arg313_1: \"f32[3072]\", arg314_1: \"f32[768, 3072]\", arg315_1: \"f32[768]\", arg316_1: \"f32[768]\", arg317_1: \"f32[768]\", arg318_1: \"f32[768, 768]\", arg319_1: \"f32[768]\", arg320_1: \"f32[768, 768]\", arg321_1: \"f32[768]\", arg322_1: \"f32[768, 768]\", arg323_1: \"f32[768]\", arg324_1: \"f32[768, 768]\", arg325_1: \"f32[768]\", arg326_1: \"f32[768]\", arg327_1: \"f32[768]\", arg328_1: \"f32[3072, 768]\", arg329_1: \"f32[3072]\", arg330_1: \"f32[768, 3072]\", arg331_1: \"f32[768]\", arg332_1: \"f32[768]\", arg333_1: \"f32[768]\", arg334_1: \"f32[768, 768]\", arg335_1: \"f32[768]\", arg336_1: \"f32[768, 768]\", arg337_1: \"f32[768]\", arg338_1: \"f32[768, 768]\", arg339_1: \"f32[768]\", arg340_1: \"f32[768, 768]\", arg341_1: \"f32[768]\", arg342_1: \"f32[768]\", arg343_1: \"f32[768]\", arg344_1: \"f32[3072, 768]\", arg345_1: \"f32[3072]\", arg346_1: \"f32[768, 3072]\", arg347_1: \"f32[768]\", arg348_1: \"f32[768]\", arg349_1: \"f32[768]\", arg350_1: \"f32[768, 768]\", arg351_1: \"f32[768]\", arg352_1: \"f32[2304, 768]\", arg353_1: \"f32[2304]\", arg354_1: \"f32[768, 768]\", arg355_1: \"f32[768]\", arg356_1: \"f32[768]\", arg357_1: \"f32[768]\", arg358_1: \"f32[3072, 768]\", arg359_1: \"f32[3072]\", arg360_1: \"f32[768, 3072]\", arg361_1: \"f32[768]\", arg362_1: \"f32[768]\", arg363_1: \"f32[768]\", arg364_1: \"f32[3072, 768]\", arg365_1: \"f32[3072]\", arg366_1: \"f32[768, 3072]\", arg367_1: \"f32[768]\", arg368_1: \"f32[768]\", arg369_1: \"f32[768]\", arg370_1: \"f32[2304, 768]\", arg371_1: \"f32[2304]\", arg372_1: \"f32[768, 768]\", arg373_1: \"f32[768]\", arg374_1: \"f32[768]\", arg375_1: \"f32[768]\", arg376_1: \"f32[2304, 768]\", arg377_1: \"f32[2304]\", arg378_1: \"f32[768, 768]\", arg379_1: \"f32[768]\", arg380_1: \"f32[768]\", arg381_1: \"f32[768]\", arg382_1: \"f32[3072, 768]\", arg383_1: \"f32[3072]\", arg384_1: \"f32[768, 3072]\", arg385_1: \"f32[768]\", arg386_1: \"f32[768]\", arg387_1: \"f32[768]\", arg388_1: \"f32[3072, 768]\", arg389_1: \"f32[3072]\", arg390_1: \"f32[768, 3072]\", arg391_1: \"f32[768]\", arg392_1: \"f32[768]\", arg393_1: \"f32[768]\", arg394_1: \"f32[2304, 768]\", arg395_1: \"f32[2304]\", arg396_1: \"f32[768, 768]\", arg397_1: \"f32[768]\", arg398_1: \"f32[768]\", arg399_1: \"f32[768]\", arg400_1: \"f32[3072, 768]\", arg401_1: \"f32[3072]\", arg402_1: \"f32[768, 3072]\", arg403_1: \"f32[768]\", arg404_1: \"f32[768]\", arg405_1: \"f32[768]\", arg406_1: \"f32[3072, 768]\", arg407_1: \"f32[3072]\", arg408_1: \"f32[768, 3072]\", arg409_1: \"f32[768]\", arg410_1: \"f32[768]\", arg411_1: \"f32[768]\", arg412_1: \"f32[2304, 768]\", arg413_1: \"f32[2304]\", arg414_1: \"f32[768, 768]\", arg415_1: \"f32[768]\", arg416_1: \"f32[768]\", arg417_1: \"f32[768]\", arg418_1: \"f32[2304, 768]\", arg419_1: \"f32[2304]\", arg420_1: \"f32[768, 768]\", arg421_1: \"f32[768]\", arg422_1: \"f32[768]\", arg423_1: \"f32[768]\", arg424_1: \"f32[3072, 768]\", arg425_1: \"f32[3072]\", arg426_1: \"f32[768, 3072]\", arg427_1: \"f32[768]\", arg428_1: \"f32[768]\", arg429_1: \"f32[768]\", arg430_1: \"f32[3072, 768]\", arg431_1: \"f32[3072]\", arg432_1: \"f32[768, 3072]\", arg433_1: \"f32[768]\", arg434_1: \"f32[768]\", arg435_1: \"f32[768]\", arg436_1: \"f32[2304, 768]\", arg437_1: \"f32[2304]\", arg438_1: \"f32[768, 768]\", arg439_1: \"f32[768]\", arg440_1: \"f32[768]\", arg441_1: \"f32[768]\", arg442_1: \"f32[3072, 768]\", arg443_1: \"f32[3072]\", arg444_1: \"f32[768, 3072]\", arg445_1: \"f32[768]\", arg446_1: \"f32[768]\", arg447_1: \"f32[768]\", arg448_1: \"f32[3072, 768]\", arg449_1: \"f32[3072]\", arg450_1: \"f32[768, 3072]\", arg451_1: \"f32[768]\", arg452_1: \"f32[768]\", arg453_1: \"f32[768]\", arg454_1: \"f32[2304, 768]\", arg455_1: \"f32[2304]\", arg456_1: \"f32[768, 768]\", arg457_1: \"f32[768]\", arg458_1: \"f32[768]\", arg459_1: \"f32[768]\", arg460_1: \"f32[2304, 768]\", arg461_1: \"f32[2304]\", arg462_1: \"f32[768, 768]\", arg463_1: \"f32[768]\", arg464_1: \"f32[768]\", arg465_1: \"f32[768]\", arg466_1: \"f32[3072, 768]\", arg467_1: \"f32[3072]\", arg468_1: \"f32[768, 3072]\", arg469_1: \"f32[768]\", arg470_1: \"f32[768]\", arg471_1: \"f32[768]\", arg472_1: \"f32[3072, 768]\", arg473_1: \"f32[3072]\", arg474_1: \"f32[768, 3072]\", arg475_1: \"f32[768]\", arg476_1: \"f32[768]\", arg477_1: \"f32[768]\", arg478_1: \"f32[2304, 768]\", arg479_1: \"f32[2304]\", arg480_1: \"f32[768, 768]\", arg481_1: \"f32[768]\", arg482_1: \"f32[768]\", arg483_1: \"f32[768]\", arg484_1: \"f32[3072, 768]\", arg485_1: \"f32[3072]\", arg486_1: \"f32[768, 3072]\", arg487_1: \"f32[768]\", arg488_1: \"f32[768]\", arg489_1: \"f32[768]\", arg490_1: \"f32[3072, 768]\", arg491_1: \"f32[3072]\", arg492_1: \"f32[768, 3072]\", arg493_1: \"f32[768]\", arg494_1: \"f32[768]\", arg495_1: \"f32[768]\", arg496_1: \"f32[2304, 768]\", arg497_1: \"f32[2304]\", arg498_1: \"f32[768, 768]\", arg499_1: \"f32[768]\", arg500_1: \"f32[768]\", arg501_1: \"f32[768]\", arg502_1: \"f32[30522, 768]\", arg503_1: \"f32[77, 768]\", arg504_1: \"f32[512, 768]\", arg505_1: \"f32[512]\", arg506_1: \"f32[32128, 512]\", arg507_1: \"f32[32128, 512]\", arg508_1: \"f32[384, 512]\", arg509_1: \"f32[384, 512]\", arg510_1: \"f32[384, 512]\", arg511_1: \"f32[512, 384]\", arg512_1: \"f32[32, 6]\", arg513_1: \"f32[512]\", arg514_1: \"f32[1024, 512]\", arg515_1: \"f32[1024, 512]\", arg516_1: \"f32[512, 1024]\", arg517_1: \"f32[512]\", arg518_1: \"f32[384, 512]\", arg519_1: \"f32[384, 512]\", arg520_1: \"f32[384, 512]\", arg521_1: \"f32[512, 384]\", arg522_1: \"f32[512]\", arg523_1: \"f32[1024, 512]\", arg524_1: \"f32[1024, 512]\", arg525_1: \"f32[512, 1024]\", arg526_1: \"f32[512]\", arg527_1: \"f32[384, 512]\", arg528_1: \"f32[384, 512]\", arg529_1: \"f32[384, 512]\", arg530_1: \"f32[512, 384]\", arg531_1: \"f32[512]\", arg532_1: \"f32[1024, 512]\", arg533_1: \"f32[1024, 512]\", arg534_1: \"f32[512, 1024]\", arg535_1: \"f32[512]\", arg536_1: \"f32[384, 512]\", arg537_1: \"f32[384, 512]\", arg538_1: \"f32[384, 512]\", arg539_1: \"f32[512, 384]\", arg540_1: \"f32[512]\", arg541_1: \"f32[1024, 512]\", arg542_1: \"f32[1024, 512]\", arg543_1: \"f32[512, 1024]\", arg544_1: \"f32[512]\", arg545_1: \"f32[384, 512]\", arg546_1: \"f32[384, 512]\", arg547_1: \"f32[384, 512]\", arg548_1: \"f32[512, 384]\", arg549_1: \"f32[512]\", arg550_1: \"f32[1024, 512]\", arg551_1: \"f32[1024, 512]\", arg552_1: \"f32[512, 1024]\", arg553_1: \"f32[512]\", arg554_1: \"f32[384, 512]\", arg555_1: \"f32[384, 512]\", arg556_1: \"f32[384, 512]\", arg557_1: \"f32[512, 384]\", arg558_1: \"f32[512]\", arg559_1: \"f32[1024, 512]\", arg560_1: \"f32[1024, 512]\", arg561_1: \"f32[512, 1024]\", arg562_1: \"f32[512]\", arg563_1: \"f32[384, 512]\", arg564_1: \"f32[384, 512]\", arg565_1: \"f32[384, 512]\", arg566_1: \"f32[512, 384]\", arg567_1: \"f32[512]\", arg568_1: \"f32[1024, 512]\", arg569_1: \"f32[1024, 512]\", arg570_1: \"f32[512, 1024]\", arg571_1: \"f32[512]\", arg572_1: \"f32[384, 512]\", arg573_1: \"f32[384, 512]\", arg574_1: \"f32[384, 512]\", arg575_1: \"f32[512, 384]\", arg576_1: \"f32[512]\", arg577_1: \"f32[1024, 512]\", arg578_1: \"f32[1024, 512]\", arg579_1: \"f32[512, 1024]\", arg580_1: \"f32[512]\", arg581_1: \"f32[512]\", arg582_1: \"f32[32128, 512]\", arg583_1: \"f32[384, 512]\", arg584_1: \"f32[384, 512]\", arg585_1: \"f32[384, 512]\", arg586_1: \"f32[512, 384]\", arg587_1: \"f32[32, 6]\", arg588_1: \"f32[512]\", arg589_1: \"f32[384, 512]\", arg590_1: \"f32[384, 512]\", arg591_1: \"f32[384, 512]\", arg592_1: \"f32[512, 384]\", arg593_1: \"f32[512]\", arg594_1: \"f32[1024, 512]\", arg595_1: \"f32[1024, 512]\", arg596_1: \"f32[512, 1024]\", arg597_1: \"f32[512]\", arg598_1: \"f32[384, 512]\", arg599_1: \"f32[384, 512]\", arg600_1: \"f32[384, 512]\", arg601_1: \"f32[512, 384]\", arg602_1: \"f32[512]\", arg603_1: \"f32[384, 512]\", arg604_1: \"f32[384, 512]\", arg605_1: \"f32[384, 512]\", arg606_1: \"f32[512, 384]\", arg607_1: \"f32[512]\", arg608_1: \"f32[1024, 512]\", arg609_1: \"f32[1024, 512]\", arg610_1: \"f32[512, 1024]\", arg611_1: \"f32[512]\", arg612_1: \"f32[384, 512]\", arg613_1: \"f32[384, 512]\", arg614_1: \"f32[384, 512]\", arg615_1: \"f32[512, 384]\", arg616_1: \"f32[512]\", arg617_1: \"f32[384, 512]\", arg618_1: \"f32[384, 512]\", arg619_1: \"f32[384, 512]\", arg620_1: \"f32[512, 384]\", arg621_1: \"f32[512]\", arg622_1: \"f32[1024, 512]\", arg623_1: \"f32[1024, 512]\", arg624_1: \"f32[512, 1024]\", arg625_1: \"f32[512]\", arg626_1: \"f32[384, 512]\", arg627_1: \"f32[384, 512]\", arg628_1: \"f32[384, 512]\", arg629_1: \"f32[512, 384]\", arg630_1: \"f32[512]\", arg631_1: \"f32[384, 512]\", arg632_1: \"f32[384, 512]\", arg633_1: \"f32[384, 512]\", arg634_1: \"f32[512, 384]\", arg635_1: \"f32[512]\", arg636_1: \"f32[1024, 512]\", arg637_1: \"f32[1024, 512]\", arg638_1: \"f32[512, 1024]\", arg639_1: \"f32[512]\", arg640_1: \"f32[384, 512]\", arg641_1: \"f32[384, 512]\", arg642_1: \"f32[384, 512]\", arg643_1: \"f32[512, 384]\", arg644_1: \"f32[512]\", arg645_1: \"f32[384, 512]\", arg646_1: \"f32[384, 512]\", arg647_1: \"f32[384, 512]\", arg648_1: \"f32[512, 384]\", arg649_1: \"f32[512]\", arg650_1: \"f32[1024, 512]\", arg651_1: \"f32[1024, 512]\", arg652_1: \"f32[512, 1024]\", arg653_1: \"f32[512]\", arg654_1: \"f32[384, 512]\", arg655_1: \"f32[384, 512]\", arg656_1: \"f32[384, 512]\", arg657_1: \"f32[512, 384]\", arg658_1: \"f32[512]\", arg659_1: \"f32[384, 512]\", arg660_1: \"f32[384, 512]\", arg661_1: \"f32[384, 512]\", arg662_1: \"f32[512, 384]\", arg663_1: \"f32[512]\", arg664_1: \"f32[1024, 512]\", arg665_1: \"f32[1024, 512]\", arg666_1: \"f32[512, 1024]\", arg667_1: \"f32[512]\", arg668_1: \"f32[384, 512]\", arg669_1: \"f32[384, 512]\", arg670_1: \"f32[384, 512]\", arg671_1: \"f32[512, 384]\", arg672_1: \"f32[512]\", arg673_1: \"f32[384, 512]\", arg674_1: \"f32[384, 512]\", arg675_1: \"f32[384, 512]\", arg676_1: \"f32[512, 384]\", arg677_1: \"f32[512]\", arg678_1: \"f32[1024, 512]\", arg679_1: \"f32[1024, 512]\", arg680_1: \"f32[512, 1024]\", arg681_1: \"f32[512]\", arg682_1: \"f32[384, 512]\", arg683_1: \"f32[384, 512]\", arg684_1: \"f32[384, 512]\", arg685_1: \"f32[512, 384]\", arg686_1: \"f32[512]\", arg687_1: \"f32[384, 512]\", arg688_1: \"f32[384, 512]\", arg689_1: \"f32[384, 512]\", arg690_1: \"f32[512, 384]\", arg691_1: \"f32[512]\", arg692_1: \"f32[1024, 512]\", arg693_1: \"f32[1024, 512]\", arg694_1: \"f32[512, 1024]\", arg695_1: \"f32[512]\", arg696_1: \"f32[512]\", arg697_1: \"f32[32128, 512]\", arg698_1: \"i64[1, 77]\", arg699_1: \"f32[109, 109]\", arg700_1: \"f32[109, 109]\", arg701_1: \"f32[109, 109]\", arg702_1: \"i64[1, 512]\", arg703_1: \"i64[1, 512]\", arg704_1: \"f32[1, 3, 224, 224]\", arg705_1: \"i64[1, 77]\", arg706_1: \"i64[1, 77]\", arg707_1: \"i64[]\"):\n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:554 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d: \"f32[1, 192, 14, 14]\" = torch.ops.aten.conv2d.default(arg704_1, arg2_1, arg3_1, [16, 16]);  arg704_1 = arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/patch_embed.py:133 in forward, code: x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
      "    flatten: \"f32[1, 192, 196]\" = torch.ops.aten.flatten.using_ints(conv2d, 2);  conv2d = None\n",
      "    transpose: \"f32[1, 196, 192]\" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:260 in forward, code: image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
      "    expand: \"f32[1, 1, 192]\" = torch.ops.aten.expand.default(arg0_1, [1, -1, -1]);  arg0_1 = None\n",
      "    cat: \"f32[1, 197, 192]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
      "    add: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(cat, arg1_1);  cat = arg1_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(add, 0.0, False);  add = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(dropout, [192], arg4_1, arg5_1, 1e-06);  arg4_1 = arg5_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm, arg6_1, arg7_1);  layer_norm = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear, [1, 197, 3, 3, 64]);  linear = None\n",
      "    permute: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape, [2, 0, 3, 1, 4]);  reshape = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind = torch.ops.aten.unbind.int(permute);  permute = None\n",
      "    getitem: \"f32[1, 3, 197, 64]\" = unbind[0]\n",
      "    getitem_1: \"f32[1, 3, 197, 64]\" = unbind[1]\n",
      "    getitem_2: \"f32[1, 3, 197, 64]\" = unbind[2];  unbind = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem, getitem_1, getitem_2);  getitem = getitem_1 = getitem_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_1: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
      "    reshape_1: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_1, [1, 197, 192]);  transpose_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_1: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_1, arg8_1, arg9_1);  reshape_1 = arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_1: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_1, 0.0, False);  linear_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_1: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(dropout, dropout_1);  dropout = dropout_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_1: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_1, [192], arg10_1, arg11_1, 1e-06);  arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_2: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_1, arg12_1, arg13_1);  layer_norm_1 = arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_2: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu, 0.0, False);  gelu = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_3: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_2, arg14_1, arg15_1);  dropout_2 = arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_3: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_2: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_1, dropout_3);  add_1 = dropout_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_2: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_2, [192], arg16_1, arg17_1, 1e-06);  arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_4: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_2, arg18_1, arg19_1);  layer_norm_2 = arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_2: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_4, [1, 197, 3, 3, 64]);  linear_4 = None\n",
      "    permute_1: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_2, [2, 0, 3, 1, 4]);  reshape_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_1 = torch.ops.aten.unbind.int(permute_1);  permute_1 = None\n",
      "    getitem_3: \"f32[1, 3, 197, 64]\" = unbind_1[0]\n",
      "    getitem_4: \"f32[1, 3, 197, 64]\" = unbind_1[1]\n",
      "    getitem_5: \"f32[1, 3, 197, 64]\" = unbind_1[2];  unbind_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_1: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_3, getitem_4, getitem_5);  getitem_3 = getitem_4 = getitem_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_2: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
      "    reshape_3: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_2, [1, 197, 192]);  transpose_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_5: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_3, arg20_1, arg21_1);  reshape_3 = arg20_1 = arg21_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_4: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_3: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_2, dropout_4);  add_2 = dropout_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_3: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_3, [192], arg22_1, arg23_1, 1e-06);  arg22_1 = arg23_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_6: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_3, arg24_1, arg25_1);  layer_norm_3 = arg24_1 = arg25_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_1: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_5: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_1, 0.0, False);  gelu_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_7: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_5, arg26_1, arg27_1);  dropout_5 = arg26_1 = arg27_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_6: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_7, 0.0, False);  linear_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_4: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_3, dropout_6);  add_3 = dropout_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_4: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_4, [192], arg28_1, arg29_1, 1e-06);  arg28_1 = arg29_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_8: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_4, arg30_1, arg31_1);  layer_norm_4 = arg30_1 = arg31_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_4: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_8, [1, 197, 3, 3, 64]);  linear_8 = None\n",
      "    permute_2: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_4, [2, 0, 3, 1, 4]);  reshape_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_2 = torch.ops.aten.unbind.int(permute_2);  permute_2 = None\n",
      "    getitem_6: \"f32[1, 3, 197, 64]\" = unbind_2[0]\n",
      "    getitem_7: \"f32[1, 3, 197, 64]\" = unbind_2[1]\n",
      "    getitem_8: \"f32[1, 3, 197, 64]\" = unbind_2[2];  unbind_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_2: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_6, getitem_7, getitem_8);  getitem_6 = getitem_7 = getitem_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_3: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
      "    reshape_5: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_3, [1, 197, 192]);  transpose_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_9: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_5, arg32_1, arg33_1);  reshape_5 = arg32_1 = arg33_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_7: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_9, 0.0, False);  linear_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_5: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_4, dropout_7);  add_4 = dropout_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_5: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_5, [192], arg34_1, arg35_1, 1e-06);  arg34_1 = arg35_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_10: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_5, arg36_1, arg37_1);  layer_norm_5 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_2: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_8: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_2, 0.0, False);  gelu_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_11: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_8, arg38_1, arg39_1);  dropout_8 = arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_9: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_6: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_5, dropout_9);  add_5 = dropout_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_6: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_6, [192], arg40_1, arg41_1, 1e-06);  arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_12: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_6, arg42_1, arg43_1);  layer_norm_6 = arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_6: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_12, [1, 197, 3, 3, 64]);  linear_12 = None\n",
      "    permute_3: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_6, [2, 0, 3, 1, 4]);  reshape_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_3 = torch.ops.aten.unbind.int(permute_3);  permute_3 = None\n",
      "    getitem_9: \"f32[1, 3, 197, 64]\" = unbind_3[0]\n",
      "    getitem_10: \"f32[1, 3, 197, 64]\" = unbind_3[1]\n",
      "    getitem_11: \"f32[1, 3, 197, 64]\" = unbind_3[2];  unbind_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_3: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_9, getitem_10, getitem_11);  getitem_9 = getitem_10 = getitem_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_4: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
      "    reshape_7: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_4, [1, 197, 192]);  transpose_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_13: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_7, arg44_1, arg45_1);  reshape_7 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_10: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_13, 0.0, False);  linear_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_7: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_6, dropout_10);  add_6 = dropout_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_7: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_7, [192], arg46_1, arg47_1, 1e-06);  arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_14: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_7, arg48_1, arg49_1);  layer_norm_7 = arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_3: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_11: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_3, 0.0, False);  gelu_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_15: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_11, arg50_1, arg51_1);  dropout_11 = arg50_1 = arg51_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_12: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_15, 0.0, False);  linear_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_8: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_7, dropout_12);  add_7 = dropout_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_8: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_8, [192], arg52_1, arg53_1, 1e-06);  arg52_1 = arg53_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_16: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_8, arg54_1, arg55_1);  layer_norm_8 = arg54_1 = arg55_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_8: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_16, [1, 197, 3, 3, 64]);  linear_16 = None\n",
      "    permute_4: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_8, [2, 0, 3, 1, 4]);  reshape_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_4 = torch.ops.aten.unbind.int(permute_4);  permute_4 = None\n",
      "    getitem_12: \"f32[1, 3, 197, 64]\" = unbind_4[0]\n",
      "    getitem_13: \"f32[1, 3, 197, 64]\" = unbind_4[1]\n",
      "    getitem_14: \"f32[1, 3, 197, 64]\" = unbind_4[2];  unbind_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_4: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_12, getitem_13, getitem_14);  getitem_12 = getitem_13 = getitem_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_5: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
      "    reshape_9: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_5, [1, 197, 192]);  transpose_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_17: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_9, arg56_1, arg57_1);  reshape_9 = arg56_1 = arg57_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_13: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_9: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_8, dropout_13);  add_8 = dropout_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_9: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_9, [192], arg58_1, arg59_1, 1e-06);  arg58_1 = arg59_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_18: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_9, arg60_1, arg61_1);  layer_norm_9 = arg60_1 = arg61_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_4: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_14: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_4, 0.0, False);  gelu_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_19: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_14, arg62_1, arg63_1);  dropout_14 = arg62_1 = arg63_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_15: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_19, 0.0, False);  linear_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_10: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_9, dropout_15);  add_9 = dropout_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_10: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_10, [192], arg64_1, arg65_1, 1e-06);  arg64_1 = arg65_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_20: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_10, arg66_1, arg67_1);  layer_norm_10 = arg66_1 = arg67_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_10: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_20, [1, 197, 3, 3, 64]);  linear_20 = None\n",
      "    permute_5: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_10, [2, 0, 3, 1, 4]);  reshape_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_5 = torch.ops.aten.unbind.int(permute_5);  permute_5 = None\n",
      "    getitem_15: \"f32[1, 3, 197, 64]\" = unbind_5[0]\n",
      "    getitem_16: \"f32[1, 3, 197, 64]\" = unbind_5[1]\n",
      "    getitem_17: \"f32[1, 3, 197, 64]\" = unbind_5[2];  unbind_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_5: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_15, getitem_16, getitem_17);  getitem_15 = getitem_16 = getitem_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_6: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
      "    reshape_11: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_6, [1, 197, 192]);  transpose_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_21: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_11, arg68_1, arg69_1);  reshape_11 = arg68_1 = arg69_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_16: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_21, 0.0, False);  linear_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_11: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_10, dropout_16);  add_10 = dropout_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_11: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_11, [192], arg70_1, arg71_1, 1e-06);  arg70_1 = arg71_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_22: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_11, arg72_1, arg73_1);  layer_norm_11 = arg72_1 = arg73_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_5: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_17: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_5, 0.0, False);  gelu_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_23: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_17, arg74_1, arg75_1);  dropout_17 = arg74_1 = arg75_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_18: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_12: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_11, dropout_18);  add_11 = dropout_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_12: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_12, [192], arg76_1, arg77_1, 1e-06);  arg76_1 = arg77_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_24: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_12, arg78_1, arg79_1);  layer_norm_12 = arg78_1 = arg79_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_12: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_24, [1, 197, 3, 3, 64]);  linear_24 = None\n",
      "    permute_6: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_12, [2, 0, 3, 1, 4]);  reshape_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_6 = torch.ops.aten.unbind.int(permute_6);  permute_6 = None\n",
      "    getitem_18: \"f32[1, 3, 197, 64]\" = unbind_6[0]\n",
      "    getitem_19: \"f32[1, 3, 197, 64]\" = unbind_6[1]\n",
      "    getitem_20: \"f32[1, 3, 197, 64]\" = unbind_6[2];  unbind_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_6: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_18, getitem_19, getitem_20);  getitem_18 = getitem_19 = getitem_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_7: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
      "    reshape_13: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_7, [1, 197, 192]);  transpose_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_25: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_13, arg80_1, arg81_1);  reshape_13 = arg80_1 = arg81_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_19: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_25, 0.0, False);  linear_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_13: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_12, dropout_19);  add_12 = dropout_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_13: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_13, [192], arg82_1, arg83_1, 1e-06);  arg82_1 = arg83_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_26: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_13, arg84_1, arg85_1);  layer_norm_13 = arg84_1 = arg85_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_6: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_20: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_6, 0.0, False);  gelu_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_27: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_20, arg86_1, arg87_1);  dropout_20 = arg86_1 = arg87_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_21: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_27, 0.0, False);  linear_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_14: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_13, dropout_21);  add_13 = dropout_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_14: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_14, [192], arg88_1, arg89_1, 1e-06);  arg88_1 = arg89_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_28: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_14, arg90_1, arg91_1);  layer_norm_14 = arg90_1 = arg91_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_14: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_28, [1, 197, 3, 3, 64]);  linear_28 = None\n",
      "    permute_7: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_14, [2, 0, 3, 1, 4]);  reshape_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_7 = torch.ops.aten.unbind.int(permute_7);  permute_7 = None\n",
      "    getitem_21: \"f32[1, 3, 197, 64]\" = unbind_7[0]\n",
      "    getitem_22: \"f32[1, 3, 197, 64]\" = unbind_7[1]\n",
      "    getitem_23: \"f32[1, 3, 197, 64]\" = unbind_7[2];  unbind_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_7: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_21, getitem_22, getitem_23);  getitem_21 = getitem_22 = getitem_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_8: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
      "    reshape_15: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_8, [1, 197, 192]);  transpose_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_29: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_15, arg92_1, arg93_1);  reshape_15 = arg92_1 = arg93_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_22: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_29, 0.0, False);  linear_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_15: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_14, dropout_22);  add_14 = dropout_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_15: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_15, [192], arg94_1, arg95_1, 1e-06);  arg94_1 = arg95_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_30: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_15, arg96_1, arg97_1);  layer_norm_15 = arg96_1 = arg97_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_7: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_23: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_31: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_23, arg98_1, arg99_1);  dropout_23 = arg98_1 = arg99_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_24: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_31, 0.0, False);  linear_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_16: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_15, dropout_24);  add_15 = dropout_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_16: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_16, [192], arg100_1, arg101_1, 1e-06);  arg100_1 = arg101_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_32: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_16, arg102_1, arg103_1);  layer_norm_16 = arg102_1 = arg103_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_16: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_32, [1, 197, 3, 3, 64]);  linear_32 = None\n",
      "    permute_8: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_16, [2, 0, 3, 1, 4]);  reshape_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_8 = torch.ops.aten.unbind.int(permute_8);  permute_8 = None\n",
      "    getitem_24: \"f32[1, 3, 197, 64]\" = unbind_8[0]\n",
      "    getitem_25: \"f32[1, 3, 197, 64]\" = unbind_8[1]\n",
      "    getitem_26: \"f32[1, 3, 197, 64]\" = unbind_8[2];  unbind_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_8: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_24, getitem_25, getitem_26);  getitem_24 = getitem_25 = getitem_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_9: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
      "    reshape_17: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_9, [1, 197, 192]);  transpose_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_33: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_17, arg104_1, arg105_1);  reshape_17 = arg104_1 = arg105_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_25: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_33, 0.0, False);  linear_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_17: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_16, dropout_25);  add_16 = dropout_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_17: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_17, [192], arg106_1, arg107_1, 1e-06);  arg106_1 = arg107_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_34: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_17, arg108_1, arg109_1);  layer_norm_17 = arg108_1 = arg109_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_8: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_26: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_8, 0.0, False);  gelu_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_35: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_26, arg110_1, arg111_1);  dropout_26 = arg110_1 = arg111_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_27: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_35, 0.0, False);  linear_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_18: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_17, dropout_27);  add_17 = dropout_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_18: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_18, [192], arg112_1, arg113_1, 1e-06);  arg112_1 = arg113_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_36: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_18, arg114_1, arg115_1);  layer_norm_18 = arg114_1 = arg115_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_18: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_36, [1, 197, 3, 3, 64]);  linear_36 = None\n",
      "    permute_9: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_18, [2, 0, 3, 1, 4]);  reshape_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_9 = torch.ops.aten.unbind.int(permute_9);  permute_9 = None\n",
      "    getitem_27: \"f32[1, 3, 197, 64]\" = unbind_9[0]\n",
      "    getitem_28: \"f32[1, 3, 197, 64]\" = unbind_9[1]\n",
      "    getitem_29: \"f32[1, 3, 197, 64]\" = unbind_9[2];  unbind_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_9: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_27, getitem_28, getitem_29);  getitem_27 = getitem_28 = getitem_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_10: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
      "    reshape_19: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_10, [1, 197, 192]);  transpose_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_37: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_19, arg116_1, arg117_1);  reshape_19 = arg116_1 = arg117_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_28: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_37, 0.0, False);  linear_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_19: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_18, dropout_28);  add_18 = dropout_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_19: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_19, [192], arg118_1, arg119_1, 1e-06);  arg118_1 = arg119_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_38: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_19, arg120_1, arg121_1);  layer_norm_19 = arg120_1 = arg121_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_9: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_29: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_39: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_29, arg122_1, arg123_1);  dropout_29 = arg122_1 = arg123_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_30: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_39, 0.0, False);  linear_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_20: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_19, dropout_30);  add_19 = dropout_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_20: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_20, [192], arg124_1, arg125_1, 1e-06);  arg124_1 = arg125_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_40: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_20, arg126_1, arg127_1);  layer_norm_20 = arg126_1 = arg127_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_20: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_40, [1, 197, 3, 3, 64]);  linear_40 = None\n",
      "    permute_10: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_20, [2, 0, 3, 1, 4]);  reshape_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_10 = torch.ops.aten.unbind.int(permute_10);  permute_10 = None\n",
      "    getitem_30: \"f32[1, 3, 197, 64]\" = unbind_10[0]\n",
      "    getitem_31: \"f32[1, 3, 197, 64]\" = unbind_10[1]\n",
      "    getitem_32: \"f32[1, 3, 197, 64]\" = unbind_10[2];  unbind_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_10: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_30, getitem_31, getitem_32);  getitem_30 = getitem_31 = getitem_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_11: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
      "    reshape_21: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_11, [1, 197, 192]);  transpose_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_41: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_21, arg128_1, arg129_1);  reshape_21 = arg128_1 = arg129_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_31: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_41, 0.0, False);  linear_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_21: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_20, dropout_31);  add_20 = dropout_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_21: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_21, [192], arg130_1, arg131_1, 1e-06);  arg130_1 = arg131_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_42: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_21, arg132_1, arg133_1);  layer_norm_21 = arg132_1 = arg133_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_10: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_32: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_10, 0.0, False);  gelu_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_43: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_32, arg134_1, arg135_1);  dropout_32 = arg134_1 = arg135_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_33: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_43, 0.0, False);  linear_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_22: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_21, dropout_33);  add_21 = dropout_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_22: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_22, [192], arg136_1, arg137_1, 1e-06);  arg136_1 = arg137_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_44: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_22, arg138_1, arg139_1);  layer_norm_22 = arg138_1 = arg139_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_22: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_44, [1, 197, 3, 3, 64]);  linear_44 = None\n",
      "    permute_11: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_22, [2, 0, 3, 1, 4]);  reshape_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_11 = torch.ops.aten.unbind.int(permute_11);  permute_11 = None\n",
      "    getitem_33: \"f32[1, 3, 197, 64]\" = unbind_11[0]\n",
      "    getitem_34: \"f32[1, 3, 197, 64]\" = unbind_11[1]\n",
      "    getitem_35: \"f32[1, 3, 197, 64]\" = unbind_11[2];  unbind_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_11: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_33, getitem_34, getitem_35);  getitem_33 = getitem_34 = getitem_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_12: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
      "    reshape_23: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_12, [1, 197, 192]);  transpose_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_45: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_23, arg140_1, arg141_1);  reshape_23 = arg140_1 = arg141_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_34: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_45, 0.0, False);  linear_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_23: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_22, dropout_34);  add_22 = dropout_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_23: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_23, [192], arg142_1, arg143_1, 1e-06);  arg142_1 = arg143_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_46: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_23, arg144_1, arg145_1);  layer_norm_23 = arg144_1 = arg145_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_11: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_35: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_47: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_35, arg146_1, arg147_1);  dropout_35 = arg146_1 = arg147_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_36: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_47, 0.0, False);  linear_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_24: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_23, dropout_36);  add_23 = dropout_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_24: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_24, [192], arg148_1, arg149_1, 1e-06);  add_24 = arg148_1 = arg149_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_48: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_24, arg150_1, arg151_1);  layer_norm_24 = arg150_1 = arg151_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:263 in forward, code: cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    item: \"Sym(77)\" = torch.ops.aten.item.default(arg707_1)\n",
      "    zeros: \"i64[77]\" = torch.ops.aten.zeros.default([item], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:264 in forward, code: dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    item_1: \"Sym(77)\" = torch.ops.aten.item.default(arg707_1);  arg707_1 = item_1 = None\n",
      "    zeros_1: \"i64[77]\" = torch.ops.aten.zeros.default([item], dtype = torch.int64, device = device(type='cpu'), pin_memory = False);  item = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:144 in forward, code: learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
      "    unsqueeze: \"f32[1, 32, 768]\" = torch.ops.aten.unsqueeze.default(arg152_1, 0);  arg152_1 = None\n",
      "    expand_1: \"f32[1, 32, 768]\" = torch.ops.aten.expand.default(unsqueeze, [1, -1, -1]);  unsqueeze = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros);  zeros = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_2: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1])\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_1: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_2);  expand_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_25: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_2: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros_1);  zeros_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_3: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1]);  arg698_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_3: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_3);  arg503_1 = expand_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_26: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding_2, embedding_3);  embedding_2 = embedding_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:105 in forward, code: itc_query_embds = query_embds.clone()\n",
      "    clone: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:106 in forward, code: itm_query_embds = query_embds.clone()\n",
      "    clone_1: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  clone_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:107 in forward, code: itg_query_embds = query_embds.clone()\n",
      "    clone_2: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  expand_1 = clone_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:109 in forward, code: itc_text_embds = cls_text_embds.clone()\n",
      "    clone_3: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:110 in forward, code: itm_text_embds = cls_text_embds.clone()\n",
      "    clone_4: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25);  add_25 = clone_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:111 in forward, code: itg_text_embds = dec_text_embds.clone()\n",
      "    clone_5: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_26);  add_26 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:117 in forward, code: itc_add = to_additive_mask(self.itc_attn_mask, device=device, dtype=dtype)\n",
      "    to: \"f32[109, 109]\" = torch.ops.aten.to.device(arg699_1, device(type='cpu'), torch.float32);  arg699_1 = None\n",
      "    gt: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to, 0)\n",
      "    zeros_like: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to, pin_memory = False)\n",
      "    full_like: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to, -inf, pin_memory = False);  to = None\n",
      "    where: \"f32[109, 109]\" = torch.ops.aten.where.self(gt, zeros_like, full_like);  gt = zeros_like = full_like = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:118 in forward, code: itm_add = to_additive_mask(self.itm_attn_mask, device=device, dtype=dtype)\n",
      "    to_1: \"f32[109, 109]\" = torch.ops.aten.to.device(arg700_1, device(type='cpu'), torch.float32);  arg700_1 = None\n",
      "    gt_1: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_1, 0)\n",
      "    zeros_like_1: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_1, pin_memory = False)\n",
      "    full_like_1: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_1, -inf, pin_memory = False);  to_1 = None\n",
      "    where_1: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_1, zeros_like_1, full_like_1);  gt_1 = zeros_like_1 = full_like_1 = where_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:119 in forward, code: itg_add = to_additive_mask(self.itg_attn_mask, device=device, dtype=dtype)\n",
      "    to_2: \"f32[109, 109]\" = torch.ops.aten.to.device(arg701_1, device(type='cpu'), torch.float32);  arg701_1 = None\n",
      "    gt_2: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_2, 0)\n",
      "    zeros_like_2: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_2, pin_memory = False)\n",
      "    full_like_2: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_2, -inf, pin_memory = False);  to_2 = None\n",
      "    where_2: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_2, zeros_like_2, full_like_2);  gt_2 = zeros_like_2 = full_like_2 = where_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([clone, clone_3], 1);  clone = clone_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_13: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_49: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_13, arg352_1, arg353_1);  transpose_13 = arg352_1 = arg353_1 = None\n",
      "    unflatten: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_49, -1, [3, 768]);  linear_49 = None\n",
      "    unsqueeze_1: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten, 0);  unflatten = None\n",
      "    transpose_14: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_1, 0, -2);  unsqueeze_1 = None\n",
      "    squeeze: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_14, -2);  transpose_14 = None\n",
      "    contiguous: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze);  squeeze = None\n",
      "    select: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 0)\n",
      "    select_1: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 1)\n",
      "    select_2: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 2);  contiguous = None\n",
      "    unsqueeze_2: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select, [109, 12, 64]);  select = None\n",
      "    transpose_15: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view, 0, 1);  view = None\n",
      "    view_1: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_1, [109, 12, 64]);  select_1 = None\n",
      "    transpose_16: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_1, 0, 1);  view_1 = None\n",
      "    view_2: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_2, [109, 12, 64]);  select_2 = None\n",
      "    transpose_17: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_2, 0, 1);  view_2 = None\n",
      "    mul: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_15, 0.125);  transpose_15 = None\n",
      "    transpose_18: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_16, -2, -1);  transpose_16 = None\n",
      "    baddbmm: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_2, mul, transpose_18);  unsqueeze_2 = mul = transpose_18 = None\n",
      "    softmax: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm, -1);  baddbmm = None\n",
      "    bmm: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax, transpose_17);  transpose_17 = None\n",
      "    transpose_19: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm, 0, 1);  bmm = None\n",
      "    contiguous_1: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_19);  transpose_19 = None\n",
      "    view_3: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_1, [109, 768]);  contiguous_1 = None\n",
      "    linear_50: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_3, arg354_1, arg355_1);  view_3 = arg354_1 = arg355_1 = None\n",
      "    view_4: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_50, [109, 1, 768]);  linear_50 = None\n",
      "    view_5: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax, [1, 12, 109, 109]);  softmax = None\n",
      "    mean: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_5, [1]);  view_5 = mean = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_20: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_4, 1, 0);  view_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_27: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat, transpose_20);  concat = transpose_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_25: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_27, [768], arg356_1, arg357_1);  add_27 = arg356_1 = arg357_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_1: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25)\n",
      "    slice_2: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_1, 1, None, 32);  slice_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_3: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25);  layer_norm_25 = None\n",
      "    slice_4: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_3, 1, 32);  slice_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_21: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_2, 1, 0)\n",
      "    transpose_22: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(arg370_1, [768, 1536]);  arg370_1 = None\n",
      "    getitem_36: \"f32[768, 768]\" = split_with_sizes[0]\n",
      "    getitem_37: \"f32[1536, 768]\" = split_with_sizes[1];  split_with_sizes = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(arg371_1, [768, 1536]);  arg371_1 = None\n",
      "    getitem_38: \"f32[768]\" = split_with_sizes_1[0]\n",
      "    getitem_39: \"f32[1536]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
      "    linear_51: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_21, getitem_36, getitem_38);  transpose_21 = getitem_36 = getitem_38 = None\n",
      "    linear_52: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_22, getitem_37, getitem_39);  transpose_22 = getitem_37 = getitem_39 = None\n",
      "    unflatten_1: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_52, -1, [2, 768]);  linear_52 = None\n",
      "    unsqueeze_3: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_1, 0);  unflatten_1 = None\n",
      "    transpose_23: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_3, 0, -2);  unsqueeze_3 = None\n",
      "    squeeze_1: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_23, -2);  transpose_23 = None\n",
      "    contiguous_2: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_1);  squeeze_1 = None\n",
      "    select_3: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 0)\n",
      "    select_4: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 1);  contiguous_2 = None\n",
      "    view_6: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_51, [32, 12, 64]);  linear_51 = None\n",
      "    transpose_24: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_6, 0, 1);  view_6 = None\n",
      "    view_7: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_3, [197, 12, 64]);  select_3 = None\n",
      "    transpose_25: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_7, 0, 1);  view_7 = None\n",
      "    view_8: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_4, [197, 12, 64]);  select_4 = None\n",
      "    transpose_26: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_8, 0, 1);  view_8 = None\n",
      "    mul_1: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_24, 0.125);  transpose_24 = None\n",
      "    transpose_27: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_25, -2, -1);  transpose_25 = None\n",
      "    bmm_1: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_1, transpose_27);  mul_1 = transpose_27 = None\n",
      "    softmax_1: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_1, -1);  bmm_1 = None\n",
      "    bmm_2: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_1, transpose_26);  transpose_26 = None\n",
      "    transpose_28: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_2, 0, 1);  bmm_2 = None\n",
      "    contiguous_3: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_28);  transpose_28 = None\n",
      "    view_9: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_3, [32, 768]);  contiguous_3 = None\n",
      "    linear_53: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_9, arg372_1, arg373_1);  view_9 = arg372_1 = arg373_1 = None\n",
      "    view_10: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_53, [32, 1, 768]);  linear_53 = None\n",
      "    view_11: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_1, [1, 12, 32, 197]);  softmax_1 = None\n",
      "    mean_1: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_11, [1]);  view_11 = mean_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_29: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_10, 1, 0);  view_10 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_28: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_2, transpose_29);  slice_2 = transpose_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_26: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_28, [768], arg374_1, arg375_1);  add_28 = arg374_1 = arg375_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_54: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_26, arg358_1, arg359_1);  arg358_1 = arg359_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_12: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_54);  linear_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_55: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_12, arg360_1, arg361_1);  gelu_12 = arg360_1 = arg361_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_37: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_55, 0.1, False);  linear_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_29: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_37, layer_norm_26);  dropout_37 = layer_norm_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_27: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_29, [768], arg362_1, arg363_1, 1e-12);  add_29 = arg362_1 = arg363_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_56: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_4, arg364_1, arg365_1);  arg364_1 = arg365_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_13: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_56);  linear_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_57: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_13, arg366_1, arg367_1);  gelu_13 = arg366_1 = arg367_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_38: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_57, 0.1, False);  linear_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_30: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_38, slice_4);  dropout_38 = slice_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_28: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_30, [768], arg368_1, arg369_1, 1e-12);  add_30 = arg368_1 = arg369_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_1: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_27, layer_norm_28], 1);  layer_norm_27 = layer_norm_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_30: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_1, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_58: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_30, arg376_1, arg377_1);  transpose_30 = arg376_1 = arg377_1 = None\n",
      "    unflatten_2: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_58, -1, [3, 768]);  linear_58 = None\n",
      "    unsqueeze_4: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_2, 0);  unflatten_2 = None\n",
      "    transpose_31: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_4, 0, -2);  unsqueeze_4 = None\n",
      "    squeeze_2: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_31, -2);  transpose_31 = None\n",
      "    contiguous_4: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_2);  squeeze_2 = None\n",
      "    select_5: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 0)\n",
      "    select_6: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 1)\n",
      "    select_7: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 2);  contiguous_4 = None\n",
      "    unsqueeze_5: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_12: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_5, [109, 12, 64]);  select_5 = None\n",
      "    transpose_32: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_12, 0, 1);  view_12 = None\n",
      "    view_13: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_6, [109, 12, 64]);  select_6 = None\n",
      "    transpose_33: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_13, 0, 1);  view_13 = None\n",
      "    view_14: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_7, [109, 12, 64]);  select_7 = None\n",
      "    transpose_34: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_14, 0, 1);  view_14 = None\n",
      "    mul_2: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_32, 0.125);  transpose_32 = None\n",
      "    transpose_35: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_33, -2, -1);  transpose_33 = None\n",
      "    baddbmm_1: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_5, mul_2, transpose_35);  unsqueeze_5 = mul_2 = transpose_35 = None\n",
      "    softmax_2: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_1, -1);  baddbmm_1 = None\n",
      "    bmm_3: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_2, transpose_34);  transpose_34 = None\n",
      "    transpose_36: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_3, 0, 1);  bmm_3 = None\n",
      "    contiguous_5: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
      "    view_15: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_5, [109, 768]);  contiguous_5 = None\n",
      "    linear_59: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_15, arg378_1, arg379_1);  view_15 = arg378_1 = arg379_1 = None\n",
      "    view_16: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_59, [109, 1, 768]);  linear_59 = None\n",
      "    view_17: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_2, [1, 12, 109, 109]);  softmax_2 = None\n",
      "    mean_2: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_17, [1]);  view_17 = mean_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_37: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_16, 1, 0);  view_16 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_31: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_1, transpose_37);  concat_1 = transpose_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_29: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_31, [768], arg380_1, arg381_1);  add_31 = arg380_1 = arg381_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_5: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29)\n",
      "    slice_6: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_5, 1, None, 32);  slice_5 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_7: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29);  layer_norm_29 = None\n",
      "    slice_8: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_7, 1, 32);  slice_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_60: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_6, arg382_1, arg383_1);  arg382_1 = arg383_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_14: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_60);  linear_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_61: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_14, arg384_1, arg385_1);  gelu_14 = arg384_1 = arg385_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_39: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_61, 0.1, False);  linear_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_32: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_39, slice_6);  dropout_39 = slice_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_30: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_32, [768], arg386_1, arg387_1, 1e-12);  add_32 = arg386_1 = arg387_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_62: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_8, arg388_1, arg389_1);  arg388_1 = arg389_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_15: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_62);  linear_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_63: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_15, arg390_1, arg391_1);  gelu_15 = arg390_1 = arg391_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_40: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_63, 0.1, False);  linear_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_33: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_40, slice_8);  dropout_40 = slice_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_31: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_33, [768], arg392_1, arg393_1, 1e-12);  add_33 = arg392_1 = arg393_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_2: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_30, layer_norm_31], 1);  layer_norm_30 = layer_norm_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_38: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_2, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_64: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_38, arg394_1, arg395_1);  transpose_38 = arg394_1 = arg395_1 = None\n",
      "    unflatten_3: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_64, -1, [3, 768]);  linear_64 = None\n",
      "    unsqueeze_6: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_3, 0);  unflatten_3 = None\n",
      "    transpose_39: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_6, 0, -2);  unsqueeze_6 = None\n",
      "    squeeze_3: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_39, -2);  transpose_39 = None\n",
      "    contiguous_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_3);  squeeze_3 = None\n",
      "    select_8: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 0)\n",
      "    select_9: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 1)\n",
      "    select_10: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 2);  contiguous_6 = None\n",
      "    unsqueeze_7: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_18: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_8, [109, 12, 64]);  select_8 = None\n",
      "    transpose_40: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_18, 0, 1);  view_18 = None\n",
      "    view_19: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_9, [109, 12, 64]);  select_9 = None\n",
      "    transpose_41: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_19, 0, 1);  view_19 = None\n",
      "    view_20: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_10, [109, 12, 64]);  select_10 = None\n",
      "    transpose_42: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_20, 0, 1);  view_20 = None\n",
      "    mul_3: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_40, 0.125);  transpose_40 = None\n",
      "    transpose_43: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_41, -2, -1);  transpose_41 = None\n",
      "    baddbmm_2: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_7, mul_3, transpose_43);  unsqueeze_7 = mul_3 = transpose_43 = None\n",
      "    softmax_3: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_2, -1);  baddbmm_2 = None\n",
      "    bmm_4: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_3, transpose_42);  transpose_42 = None\n",
      "    transpose_44: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_4, 0, 1);  bmm_4 = None\n",
      "    contiguous_7: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_44);  transpose_44 = None\n",
      "    view_21: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_7, [109, 768]);  contiguous_7 = None\n",
      "    linear_65: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_21, arg396_1, arg397_1);  view_21 = arg396_1 = arg397_1 = None\n",
      "    view_22: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_65, [109, 1, 768]);  linear_65 = None\n",
      "    view_23: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_3, [1, 12, 109, 109]);  softmax_3 = None\n",
      "    mean_3: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_23, [1]);  view_23 = mean_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_45: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_22, 1, 0);  view_22 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_34: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_2, transpose_45);  concat_2 = transpose_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_32: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_34, [768], arg398_1, arg399_1);  add_34 = arg398_1 = arg399_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_9: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32)\n",
      "    slice_10: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_9, 1, None, 32);  slice_9 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_11: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32);  layer_norm_32 = None\n",
      "    slice_12: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_11, 1, 32);  slice_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_46: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_10, 1, 0)\n",
      "    transpose_47: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(arg412_1, [768, 1536]);  arg412_1 = None\n",
      "    getitem_40: \"f32[768, 768]\" = split_with_sizes_2[0]\n",
      "    getitem_41: \"f32[1536, 768]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
      "    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(arg413_1, [768, 1536]);  arg413_1 = None\n",
      "    getitem_42: \"f32[768]\" = split_with_sizes_3[0]\n",
      "    getitem_43: \"f32[1536]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
      "    linear_66: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_46, getitem_40, getitem_42);  transpose_46 = getitem_40 = getitem_42 = None\n",
      "    linear_67: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_47, getitem_41, getitem_43);  transpose_47 = getitem_41 = getitem_43 = None\n",
      "    unflatten_4: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_67, -1, [2, 768]);  linear_67 = None\n",
      "    unsqueeze_8: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_4, 0);  unflatten_4 = None\n",
      "    transpose_48: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_8, 0, -2);  unsqueeze_8 = None\n",
      "    squeeze_4: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_48, -2);  transpose_48 = None\n",
      "    contiguous_8: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_4);  squeeze_4 = None\n",
      "    select_11: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 0)\n",
      "    select_12: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 1);  contiguous_8 = None\n",
      "    view_24: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_66, [32, 12, 64]);  linear_66 = None\n",
      "    transpose_49: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_24, 0, 1);  view_24 = None\n",
      "    view_25: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_11, [197, 12, 64]);  select_11 = None\n",
      "    transpose_50: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_25, 0, 1);  view_25 = None\n",
      "    view_26: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_12, [197, 12, 64]);  select_12 = None\n",
      "    transpose_51: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_26, 0, 1);  view_26 = None\n",
      "    mul_4: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_49, 0.125);  transpose_49 = None\n",
      "    transpose_52: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_50, -2, -1);  transpose_50 = None\n",
      "    bmm_5: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_4, transpose_52);  mul_4 = transpose_52 = None\n",
      "    softmax_4: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_5, -1);  bmm_5 = None\n",
      "    bmm_6: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_4, transpose_51);  transpose_51 = None\n",
      "    transpose_53: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_6, 0, 1);  bmm_6 = None\n",
      "    contiguous_9: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_53);  transpose_53 = None\n",
      "    view_27: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_9, [32, 768]);  contiguous_9 = None\n",
      "    linear_68: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_27, arg414_1, arg415_1);  view_27 = arg414_1 = arg415_1 = None\n",
      "    view_28: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_68, [32, 1, 768]);  linear_68 = None\n",
      "    view_29: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_4, [1, 12, 32, 197]);  softmax_4 = None\n",
      "    mean_4: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_29, [1]);  view_29 = mean_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_54: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_28, 1, 0);  view_28 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_35: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_10, transpose_54);  slice_10 = transpose_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_33: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_35, [768], arg416_1, arg417_1);  add_35 = arg416_1 = arg417_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_69: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_33, arg400_1, arg401_1);  arg400_1 = arg401_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_16: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_69);  linear_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_70: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_16, arg402_1, arg403_1);  gelu_16 = arg402_1 = arg403_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_41: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_70, 0.1, False);  linear_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_36: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_41, layer_norm_33);  dropout_41 = layer_norm_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_34: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_36, [768], arg404_1, arg405_1, 1e-12);  add_36 = arg404_1 = arg405_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_71: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_12, arg406_1, arg407_1);  arg406_1 = arg407_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_17: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_71);  linear_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_72: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_17, arg408_1, arg409_1);  gelu_17 = arg408_1 = arg409_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_42: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_72, 0.1, False);  linear_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_37: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_42, slice_12);  dropout_42 = slice_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_35: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_37, [768], arg410_1, arg411_1, 1e-12);  add_37 = arg410_1 = arg411_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_3: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_34, layer_norm_35], 1);  layer_norm_34 = layer_norm_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_55: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_3, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_73: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_55, arg418_1, arg419_1);  transpose_55 = arg418_1 = arg419_1 = None\n",
      "    unflatten_5: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_73, -1, [3, 768]);  linear_73 = None\n",
      "    unsqueeze_9: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_5, 0);  unflatten_5 = None\n",
      "    transpose_56: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_9, 0, -2);  unsqueeze_9 = None\n",
      "    squeeze_5: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_56, -2);  transpose_56 = None\n",
      "    contiguous_10: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_5);  squeeze_5 = None\n",
      "    select_13: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 0)\n",
      "    select_14: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 1)\n",
      "    select_15: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 2);  contiguous_10 = None\n",
      "    unsqueeze_10: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_30: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_13, [109, 12, 64]);  select_13 = None\n",
      "    transpose_57: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None\n",
      "    view_31: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_14, [109, 12, 64]);  select_14 = None\n",
      "    transpose_58: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_31, 0, 1);  view_31 = None\n",
      "    view_32: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_15, [109, 12, 64]);  select_15 = None\n",
      "    transpose_59: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_32, 0, 1);  view_32 = None\n",
      "    mul_5: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_57, 0.125);  transpose_57 = None\n",
      "    transpose_60: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_58, -2, -1);  transpose_58 = None\n",
      "    baddbmm_3: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_10, mul_5, transpose_60);  unsqueeze_10 = mul_5 = transpose_60 = None\n",
      "    softmax_5: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_3, -1);  baddbmm_3 = None\n",
      "    bmm_7: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_5, transpose_59);  transpose_59 = None\n",
      "    transpose_61: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_7, 0, 1);  bmm_7 = None\n",
      "    contiguous_11: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_61);  transpose_61 = None\n",
      "    view_33: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_11, [109, 768]);  contiguous_11 = None\n",
      "    linear_74: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_33, arg420_1, arg421_1);  view_33 = arg420_1 = arg421_1 = None\n",
      "    view_34: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_74, [109, 1, 768]);  linear_74 = None\n",
      "    view_35: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_5, [1, 12, 109, 109]);  softmax_5 = None\n",
      "    mean_5: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_35, [1]);  view_35 = mean_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_62: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_34, 1, 0);  view_34 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_38: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_3, transpose_62);  concat_3 = transpose_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_36: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_38, [768], arg422_1, arg423_1);  add_38 = arg422_1 = arg423_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_13: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36)\n",
      "    slice_14: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_13, 1, None, 32);  slice_13 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_15: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36);  layer_norm_36 = None\n",
      "    slice_16: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_15, 1, 32);  slice_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_75: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_14, arg424_1, arg425_1);  arg424_1 = arg425_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_18: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_75);  linear_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_76: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_18, arg426_1, arg427_1);  gelu_18 = arg426_1 = arg427_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_43: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_76, 0.1, False);  linear_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_39: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_43, slice_14);  dropout_43 = slice_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_37: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_39, [768], arg428_1, arg429_1, 1e-12);  add_39 = arg428_1 = arg429_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_77: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_16, arg430_1, arg431_1);  arg430_1 = arg431_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_19: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_77);  linear_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_78: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_19, arg432_1, arg433_1);  gelu_19 = arg432_1 = arg433_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_44: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_78, 0.1, False);  linear_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_40: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_44, slice_16);  dropout_44 = slice_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_38: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_40, [768], arg434_1, arg435_1, 1e-12);  add_40 = arg434_1 = arg435_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_4: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_37, layer_norm_38], 1);  layer_norm_37 = layer_norm_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_63: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_4, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_79: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_63, arg436_1, arg437_1);  transpose_63 = arg436_1 = arg437_1 = None\n",
      "    unflatten_6: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_79, -1, [3, 768]);  linear_79 = None\n",
      "    unsqueeze_11: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_6, 0);  unflatten_6 = None\n",
      "    transpose_64: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_11, 0, -2);  unsqueeze_11 = None\n",
      "    squeeze_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_64, -2);  transpose_64 = None\n",
      "    contiguous_12: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_6);  squeeze_6 = None\n",
      "    select_16: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 0)\n",
      "    select_17: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 1)\n",
      "    select_18: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 2);  contiguous_12 = None\n",
      "    unsqueeze_12: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_36: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_16, [109, 12, 64]);  select_16 = None\n",
      "    transpose_65: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_36, 0, 1);  view_36 = None\n",
      "    view_37: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_17, [109, 12, 64]);  select_17 = None\n",
      "    transpose_66: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_37, 0, 1);  view_37 = None\n",
      "    view_38: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_18, [109, 12, 64]);  select_18 = None\n",
      "    transpose_67: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_38, 0, 1);  view_38 = None\n",
      "    mul_6: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_65, 0.125);  transpose_65 = None\n",
      "    transpose_68: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_66, -2, -1);  transpose_66 = None\n",
      "    baddbmm_4: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_12, mul_6, transpose_68);  unsqueeze_12 = mul_6 = transpose_68 = None\n",
      "    softmax_6: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_4, -1);  baddbmm_4 = None\n",
      "    bmm_8: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_6, transpose_67);  transpose_67 = None\n",
      "    transpose_69: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_8, 0, 1);  bmm_8 = None\n",
      "    contiguous_13: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_69);  transpose_69 = None\n",
      "    view_39: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_13, [109, 768]);  contiguous_13 = None\n",
      "    linear_80: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_39, arg438_1, arg439_1);  view_39 = arg438_1 = arg439_1 = None\n",
      "    view_40: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_80, [109, 1, 768]);  linear_80 = None\n",
      "    view_41: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_6, [1, 12, 109, 109]);  softmax_6 = None\n",
      "    mean_6: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_41, [1]);  view_41 = mean_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_70: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_40, 1, 0);  view_40 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_41: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_4, transpose_70);  concat_4 = transpose_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_39: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_41, [768], arg440_1, arg441_1);  add_41 = arg440_1 = arg441_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_17: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39)\n",
      "    slice_18: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_17, 1, None, 32);  slice_17 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_19: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39);  layer_norm_39 = None\n",
      "    slice_20: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_19, 1, 32);  slice_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_71: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_18, 1, 0)\n",
      "    transpose_72: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(arg454_1, [768, 1536]);  arg454_1 = None\n",
      "    getitem_44: \"f32[768, 768]\" = split_with_sizes_4[0]\n",
      "    getitem_45: \"f32[1536, 768]\" = split_with_sizes_4[1];  split_with_sizes_4 = None\n",
      "    split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(arg455_1, [768, 1536]);  arg455_1 = None\n",
      "    getitem_46: \"f32[768]\" = split_with_sizes_5[0]\n",
      "    getitem_47: \"f32[1536]\" = split_with_sizes_5[1];  split_with_sizes_5 = None\n",
      "    linear_81: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_71, getitem_44, getitem_46);  transpose_71 = getitem_44 = getitem_46 = None\n",
      "    linear_82: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_72, getitem_45, getitem_47);  transpose_72 = getitem_45 = getitem_47 = None\n",
      "    unflatten_7: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_82, -1, [2, 768]);  linear_82 = None\n",
      "    unsqueeze_13: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_7, 0);  unflatten_7 = None\n",
      "    transpose_73: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_13, 0, -2);  unsqueeze_13 = None\n",
      "    squeeze_7: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_73, -2);  transpose_73 = None\n",
      "    contiguous_14: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_7);  squeeze_7 = None\n",
      "    select_19: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 0)\n",
      "    select_20: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 1);  contiguous_14 = None\n",
      "    view_42: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_81, [32, 12, 64]);  linear_81 = None\n",
      "    transpose_74: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_42, 0, 1);  view_42 = None\n",
      "    view_43: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_19, [197, 12, 64]);  select_19 = None\n",
      "    transpose_75: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_43, 0, 1);  view_43 = None\n",
      "    view_44: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_20, [197, 12, 64]);  select_20 = None\n",
      "    transpose_76: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_44, 0, 1);  view_44 = None\n",
      "    mul_7: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_74, 0.125);  transpose_74 = None\n",
      "    transpose_77: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_75, -2, -1);  transpose_75 = None\n",
      "    bmm_9: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_7, transpose_77);  mul_7 = transpose_77 = None\n",
      "    softmax_7: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_9, -1);  bmm_9 = None\n",
      "    bmm_10: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_7, transpose_76);  transpose_76 = None\n",
      "    transpose_78: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_10, 0, 1);  bmm_10 = None\n",
      "    contiguous_15: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_78);  transpose_78 = None\n",
      "    view_45: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_15, [32, 768]);  contiguous_15 = None\n",
      "    linear_83: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_45, arg456_1, arg457_1);  view_45 = arg456_1 = arg457_1 = None\n",
      "    view_46: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_83, [32, 1, 768]);  linear_83 = None\n",
      "    view_47: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_7, [1, 12, 32, 197]);  softmax_7 = None\n",
      "    mean_7: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_47, [1]);  view_47 = mean_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_79: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_46, 1, 0);  view_46 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_42: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_18, transpose_79);  slice_18 = transpose_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_40: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_42, [768], arg458_1, arg459_1);  add_42 = arg458_1 = arg459_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_84: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_40, arg442_1, arg443_1);  arg442_1 = arg443_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_20: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_84);  linear_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_85: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_20, arg444_1, arg445_1);  gelu_20 = arg444_1 = arg445_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_45: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_85, 0.1, False);  linear_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_43: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_45, layer_norm_40);  dropout_45 = layer_norm_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_41: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_43, [768], arg446_1, arg447_1, 1e-12);  add_43 = arg446_1 = arg447_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_86: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_20, arg448_1, arg449_1);  arg448_1 = arg449_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_21: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_86);  linear_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_87: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_21, arg450_1, arg451_1);  gelu_21 = arg450_1 = arg451_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_46: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_87, 0.1, False);  linear_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_44: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_46, slice_20);  dropout_46 = slice_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_42: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_44, [768], arg452_1, arg453_1, 1e-12);  add_44 = arg452_1 = arg453_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_5: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_41, layer_norm_42], 1);  layer_norm_41 = layer_norm_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_80: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_5, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_88: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_80, arg460_1, arg461_1);  transpose_80 = arg460_1 = arg461_1 = None\n",
      "    unflatten_8: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_88, -1, [3, 768]);  linear_88 = None\n",
      "    unsqueeze_14: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_8, 0);  unflatten_8 = None\n",
      "    transpose_81: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_14, 0, -2);  unsqueeze_14 = None\n",
      "    squeeze_8: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_81, -2);  transpose_81 = None\n",
      "    contiguous_16: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_8);  squeeze_8 = None\n",
      "    select_21: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 0)\n",
      "    select_22: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 1)\n",
      "    select_23: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 2);  contiguous_16 = None\n",
      "    unsqueeze_15: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_48: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_21, [109, 12, 64]);  select_21 = None\n",
      "    transpose_82: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_48, 0, 1);  view_48 = None\n",
      "    view_49: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_22, [109, 12, 64]);  select_22 = None\n",
      "    transpose_83: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_49, 0, 1);  view_49 = None\n",
      "    view_50: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_23, [109, 12, 64]);  select_23 = None\n",
      "    transpose_84: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_50, 0, 1);  view_50 = None\n",
      "    mul_8: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_82, 0.125);  transpose_82 = None\n",
      "    transpose_85: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_83, -2, -1);  transpose_83 = None\n",
      "    baddbmm_5: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_15, mul_8, transpose_85);  unsqueeze_15 = mul_8 = transpose_85 = None\n",
      "    softmax_8: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_5, -1);  baddbmm_5 = None\n",
      "    bmm_11: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_8, transpose_84);  transpose_84 = None\n",
      "    transpose_86: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_11, 0, 1);  bmm_11 = None\n",
      "    contiguous_17: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_86);  transpose_86 = None\n",
      "    view_51: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_17, [109, 768]);  contiguous_17 = None\n",
      "    linear_89: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_51, arg462_1, arg463_1);  view_51 = arg462_1 = arg463_1 = None\n",
      "    view_52: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_89, [109, 1, 768]);  linear_89 = None\n",
      "    view_53: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_8, [1, 12, 109, 109]);  softmax_8 = None\n",
      "    mean_8: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_53, [1]);  view_53 = mean_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_87: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_52, 1, 0);  view_52 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_45: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_5, transpose_87);  concat_5 = transpose_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_43: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_45, [768], arg464_1, arg465_1);  add_45 = arg464_1 = arg465_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_21: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43)\n",
      "    slice_22: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_21, 1, None, 32);  slice_21 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_23: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43);  layer_norm_43 = None\n",
      "    slice_24: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_23, 1, 32);  slice_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_90: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_22, arg466_1, arg467_1);  arg466_1 = arg467_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_22: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_90);  linear_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_91: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_22, arg468_1, arg469_1);  gelu_22 = arg468_1 = arg469_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_47: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_91, 0.1, False);  linear_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_46: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_47, slice_22);  dropout_47 = slice_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_44: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_46, [768], arg470_1, arg471_1, 1e-12);  add_46 = arg470_1 = arg471_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_92: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_24, arg472_1, arg473_1);  arg472_1 = arg473_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_23: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_92);  linear_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_93: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_23, arg474_1, arg475_1);  gelu_23 = arg474_1 = arg475_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_48: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_93, 0.1, False);  linear_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_47: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_48, slice_24);  dropout_48 = slice_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_45: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_47, [768], arg476_1, arg477_1, 1e-12);  add_47 = arg476_1 = arg477_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_6: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_44, layer_norm_45], 1);  layer_norm_44 = layer_norm_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_88: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_6, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_94: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_88, arg478_1, arg479_1);  transpose_88 = arg478_1 = arg479_1 = None\n",
      "    unflatten_9: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_94, -1, [3, 768]);  linear_94 = None\n",
      "    unsqueeze_16: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_9, 0);  unflatten_9 = None\n",
      "    transpose_89: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_16, 0, -2);  unsqueeze_16 = None\n",
      "    squeeze_9: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_89, -2);  transpose_89 = None\n",
      "    contiguous_18: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_9);  squeeze_9 = None\n",
      "    select_24: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 0)\n",
      "    select_25: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 1)\n",
      "    select_26: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 2);  contiguous_18 = None\n",
      "    unsqueeze_17: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0);  where = None\n",
      "    view_54: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_24, [109, 12, 64]);  select_24 = None\n",
      "    transpose_90: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_54, 0, 1);  view_54 = None\n",
      "    view_55: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_25, [109, 12, 64]);  select_25 = None\n",
      "    transpose_91: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_55, 0, 1);  view_55 = None\n",
      "    view_56: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_26, [109, 12, 64]);  select_26 = None\n",
      "    transpose_92: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_56, 0, 1);  view_56 = None\n",
      "    mul_9: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_90, 0.125);  transpose_90 = None\n",
      "    transpose_93: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_91, -2, -1);  transpose_91 = None\n",
      "    baddbmm_6: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_17, mul_9, transpose_93);  unsqueeze_17 = mul_9 = transpose_93 = None\n",
      "    softmax_9: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_6, -1);  baddbmm_6 = None\n",
      "    bmm_12: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_9, transpose_92);  transpose_92 = None\n",
      "    transpose_94: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_12, 0, 1);  bmm_12 = None\n",
      "    contiguous_19: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_94);  transpose_94 = None\n",
      "    view_57: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_19, [109, 768]);  contiguous_19 = None\n",
      "    linear_95: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_57, arg480_1, arg481_1);  view_57 = arg480_1 = arg481_1 = None\n",
      "    view_58: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_95, [109, 1, 768]);  linear_95 = None\n",
      "    view_59: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_9, [1, 12, 109, 109]);  softmax_9 = None\n",
      "    mean_9: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_59, [1]);  view_59 = mean_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_95: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_58, 1, 0);  view_58 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_48: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_6, transpose_95);  concat_6 = transpose_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_46: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_48, [768], arg482_1, arg483_1);  add_48 = arg482_1 = arg483_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_25: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46)\n",
      "    slice_26: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_25, 1, None, 32);  slice_25 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_27: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46);  layer_norm_46 = None\n",
      "    slice_28: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_27, 1, 32);  slice_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_96: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_26, 1, 0)\n",
      "    transpose_97: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0);  linear_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(arg496_1, [768, 1536]);  arg496_1 = None\n",
      "    getitem_48: \"f32[768, 768]\" = split_with_sizes_6[0]\n",
      "    getitem_49: \"f32[1536, 768]\" = split_with_sizes_6[1];  split_with_sizes_6 = None\n",
      "    split_with_sizes_7 = torch.ops.aten.split_with_sizes.default(arg497_1, [768, 1536]);  arg497_1 = None\n",
      "    getitem_50: \"f32[768]\" = split_with_sizes_7[0]\n",
      "    getitem_51: \"f32[1536]\" = split_with_sizes_7[1];  split_with_sizes_7 = None\n",
      "    linear_96: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_96, getitem_48, getitem_50);  transpose_96 = getitem_48 = getitem_50 = None\n",
      "    linear_97: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_97, getitem_49, getitem_51);  transpose_97 = getitem_49 = getitem_51 = None\n",
      "    unflatten_10: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_97, -1, [2, 768]);  linear_97 = None\n",
      "    unsqueeze_18: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_10, 0);  unflatten_10 = None\n",
      "    transpose_98: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_18, 0, -2);  unsqueeze_18 = None\n",
      "    squeeze_10: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_98, -2);  transpose_98 = None\n",
      "    contiguous_20: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_10);  squeeze_10 = None\n",
      "    select_27: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 0)\n",
      "    select_28: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 1);  contiguous_20 = None\n",
      "    view_60: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_96, [32, 12, 64]);  linear_96 = None\n",
      "    transpose_99: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_60, 0, 1);  view_60 = None\n",
      "    view_61: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_27, [197, 12, 64]);  select_27 = None\n",
      "    transpose_100: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_61, 0, 1);  view_61 = None\n",
      "    view_62: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_28, [197, 12, 64]);  select_28 = None\n",
      "    transpose_101: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_62, 0, 1);  view_62 = None\n",
      "    mul_10: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_99, 0.125);  transpose_99 = None\n",
      "    transpose_102: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_100, -2, -1);  transpose_100 = None\n",
      "    bmm_13: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_10, transpose_102);  mul_10 = transpose_102 = None\n",
      "    softmax_10: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_13, -1);  bmm_13 = None\n",
      "    bmm_14: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_10, transpose_101);  transpose_101 = None\n",
      "    transpose_103: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_14, 0, 1);  bmm_14 = None\n",
      "    contiguous_21: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_103);  transpose_103 = None\n",
      "    view_63: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_21, [32, 768]);  contiguous_21 = None\n",
      "    linear_98: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_63, arg498_1, arg499_1);  view_63 = arg498_1 = arg499_1 = None\n",
      "    view_64: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_98, [32, 1, 768]);  linear_98 = None\n",
      "    view_65: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_10, [1, 12, 32, 197]);  softmax_10 = None\n",
      "    mean_10: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_65, [1]);  view_65 = mean_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_104: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_64, 1, 0);  view_64 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_49: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_26, transpose_104);  slice_26 = transpose_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_47: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_49, [768], arg500_1, arg501_1);  add_49 = arg500_1 = arg501_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_99: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_47, arg484_1, arg485_1);  arg484_1 = arg485_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_24: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_99);  linear_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_100: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_24, arg486_1, arg487_1);  gelu_24 = arg486_1 = arg487_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_49: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_100, 0.1, False);  linear_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_50: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_49, layer_norm_47);  dropout_49 = layer_norm_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_48: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_50, [768], arg488_1, arg489_1, 1e-12);  add_50 = arg488_1 = arg489_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_101: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_28, arg490_1, arg491_1);  arg490_1 = arg491_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_25: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_101);  linear_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_102: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_25, arg492_1, arg493_1);  gelu_25 = arg492_1 = arg493_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_50: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_102, 0.1, False);  linear_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_51: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_50, slice_28);  dropout_50 = slice_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_49: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_51, [768], arg494_1, arg495_1, 1e-12);  add_51 = arg494_1 = arg495_1 = layer_norm_49 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:155 in forward, code: itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
      "    numpy_t: \"f32[768, 30522]\" = torch.ops.aten.numpy_T.default(arg502_1);  arg502_1 = None\n",
      "    matmul: \"f32[1, 77, 30522]\" = torch.ops.aten.matmul.default(clone_5, numpy_t);  clone_5 = numpy_t = matmul = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_103: \"f32[1, 32, 512]\" = torch.ops.aten.linear.default(layer_norm_48, arg504_1, arg505_1);  layer_norm_48 = arg504_1 = arg505_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_4: \"f32[1, 77, 512]\" = torch.ops.aten.embedding.default(arg582_1, arg705_1);  arg705_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_1 = torch._C._set_grad_enabled(False);  _set_grad_enabled_1 = None\n",
      "    concat_7: \"f32[1, 109, 512]\" = torch.ops.aten.concat.default([linear_103, embedding_4], 1);  linear_103 = embedding_4 = None\n",
      "    ones: \"i64[1, 32]\" = torch.ops.aten.ones.default([1, 32], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    concat_8: \"i64[1, 109]\" = torch.ops.aten.concat.default([ones, arg706_1], 1);  ones = arg706_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1025 in forward, code: cache_position = torch.arange(\n",
      "    arange: \"i64[109]\" = torch.ops.aten.arange.start(0, 109, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1045 in forward, code: causal_mask = attention_mask[:, None, None, :]\n",
      "    slice_29: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_19: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_29, 1);  slice_29 = None\n",
      "    unsqueeze_20: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_19, 2);  unsqueeze_19 = None\n",
      "    slice_30: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_20, 3, 0, 9223372036854775807);  unsqueeze_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1046 in forward, code: causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n",
      "    to_3: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_30, torch.float32);  slice_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1047 in forward, code: causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n",
      "    rsub: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_3, 1.0);  to_3 = None\n",
      "    mul_11: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub, -3.4028234663852886e+38);  rsub = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_51: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(concat_7, 0.1, False);  concat_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_4: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(dropout_51, torch.float32);  dropout_51 = None\n",
      "    pow_1: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_4, 2)\n",
      "    mean_11: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n",
      "    add_52: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None\n",
      "    rsqrt: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_52);  add_52 = None\n",
      "    mul_12: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_4, rsqrt);  rsqrt = None\n",
      "    mul_13: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg513_1, mul_12);  arg513_1 = mul_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_104: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg508_1);  arg508_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_66: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_104, [1, -1, 6, 64]);  linear_104 = None\n",
      "    transpose_105: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_66, 1, 2);  view_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_105: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg509_1);  arg509_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_106: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg510_1);  mul_13 = arg510_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_67: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_105, [1, -1, 6, 64]);  linear_105 = None\n",
      "    transpose_106: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_67, 1, 2);  view_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_68: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_106, [1, -1, 6, 64]);  linear_106 = None\n",
      "    transpose_107: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_108: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_106, 3, 2);  transpose_106 = None\n",
      "    matmul_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_105, transpose_108);  transpose_105 = transpose_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_29: \"i64[]\" = torch.ops.aten.select.int(arange, 0, -1)\n",
      "    add_53: \"i64[]\" = torch.ops.aten.add.Tensor(select_29, 1);  select_29 = add_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_31: \"i64[109]\" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807);  arange = None\n",
      "    unsqueeze_21: \"i64[109, 1]\" = torch.ops.aten.unsqueeze.default(slice_31, 1);  slice_31 = None\n",
      "    to_5: \"i64[109, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_21, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_21 = None\n",
      "    arange_1: \"i64[109]\" = torch.ops.aten.arange.default(109, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_22: \"i64[1, 109]\" = torch.ops.aten.unsqueeze.default(arange_1, 0);  arange_1 = None\n",
      "    slice_32: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_22, 1, 0, 9223372036854775807);  unsqueeze_22 = None\n",
      "    sub: \"i64[109, 109]\" = torch.ops.aten.sub.Tensor(slice_32, to_5);  slice_32 = to_5 = None\n",
      "    gt_3: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(sub, 0)\n",
      "    to_6: \"i64[109, 109]\" = torch.ops.aten.to.dtype(gt_3, torch.int64);  gt_3 = None\n",
      "    mul_14: \"i64[109, 109]\" = torch.ops.aten.mul.Tensor(to_6, 16);  to_6 = None\n",
      "    add_54: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(mul_14, 0);  mul_14 = None\n",
      "    abs_1: \"i64[109, 109]\" = torch.ops.aten.abs.default(sub);  sub = None\n",
      "    lt: \"b8[109, 109]\" = torch.ops.aten.lt.Scalar(abs_1, 8)\n",
      "    to_7: \"f32[109, 109]\" = torch.ops.aten.to.dtype(abs_1, torch.float32)\n",
      "    div: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(to_7, 8);  to_7 = None\n",
      "    log: \"f32[109, 109]\" = torch.ops.aten.log.default(div);  div = None\n",
      "    div_1: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(log, 2.772588722239781);  log = None\n",
      "    mul_15: \"f32[109, 109]\" = torch.ops.aten.mul.Tensor(div_1, 8);  div_1 = None\n",
      "    to_8: \"i64[109, 109]\" = torch.ops.aten.to.dtype(mul_15, torch.int64);  mul_15 = None\n",
      "    add_55: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(to_8, 8);  to_8 = None\n",
      "    full_like_3: \"i64[109, 109]\" = torch.ops.aten.full_like.default(add_55, 15, pin_memory = False)\n",
      "    min_1: \"i64[109, 109]\" = torch.ops.aten.min.other(add_55, full_like_3);  add_55 = full_like_3 = None\n",
      "    where_3: \"i64[109, 109]\" = torch.ops.aten.where.self(lt, abs_1, min_1);  lt = abs_1 = min_1 = None\n",
      "    add_: \"i64[109, 109]\" = torch.ops.aten.add_.Tensor(add_54, where_3);  add_54 = where_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_5: \"f32[109, 109, 6]\" = torch.ops.aten.embedding.default(arg512_1, add_);  arg512_1 = add_ = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_12: \"f32[6, 109, 109]\" = torch.ops.aten.permute.default(embedding_5, [2, 0, 1]);  embedding_5 = None\n",
      "    unsqueeze_23: \"f32[1, 6, 109, 109]\" = torch.ops.aten.unsqueeze.default(permute_12, 0);  permute_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_33: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_23);  unsqueeze_23 = None\n",
      "    slice_34: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_33, 1);  slice_33 = None\n",
      "    slice_35: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_34, 2, -109);  slice_34 = None\n",
      "    slice_36: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_35, 3);  slice_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_37: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_11);  mul_11 = None\n",
      "    slice_38: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_37, 1);  slice_37 = None\n",
      "    slice_39: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_38, 2);  slice_38 = None\n",
      "    slice_40: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_39, 3, None, 109);  slice_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add.Tensor(slice_36, slice_40);  slice_36 = slice_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_1, add_56);  matmul_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__1, torch.float32);  add__1 = None\n",
      "    softmax_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_9, -1)\n",
      "    type_as: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_11, to_9);  softmax_11 = to_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_52: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as, 0.1, False);  type_as = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_2: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_52, transpose_107);  dropout_52 = transpose_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_109: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_2, 1, 2);  matmul_2 = None\n",
      "    contiguous_22: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_109);  transpose_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_69: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_22, [1, -1, 384]);  contiguous_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_107: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_69, arg511_1);  view_69 = arg511_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_53: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_107, 0.1, False);  linear_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_57: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_4, dropout_53);  to_4 = dropout_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_10: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_57, torch.float32);  add_57 = None\n",
      "    pow_2: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)\n",
      "    mean_12: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None\n",
      "    add_58: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None\n",
      "    rsqrt_1: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_58);  add_58 = None\n",
      "    mul_16: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_10, rsqrt_1);  rsqrt_1 = None\n",
      "    mul_17: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg517_1, mul_16);  arg517_1 = mul_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_108: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg514_1);  arg514_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_18: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_108, 0.5)\n",
      "    pow_3: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_108, 3.0)\n",
      "    mul_19: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_3, 0.044715);  pow_3 = None\n",
      "    add_59: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_108, mul_19);  linear_108 = mul_19 = None\n",
      "    mul_20: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_59, 0.7978845608028654);  add_59 = None\n",
      "    tanh: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_20);  mul_20 = None\n",
      "    add_60: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh, 1.0);  tanh = None\n",
      "    mul_21: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_18, add_60);  mul_18 = add_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_109: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg515_1);  mul_17 = arg515_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_22: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_21, linear_109);  mul_21 = linear_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_54: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_22, 0.1, False);  mul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_110: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_54, arg516_1);  dropout_54 = arg516_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_55: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_110, 0.1, False);  linear_110 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_61: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_10, dropout_55);  to_10 = dropout_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_11: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_61, torch.float32);  add_61 = None\n",
      "    pow_4: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_11, 2)\n",
      "    mean_13: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None\n",
      "    add_62: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None\n",
      "    rsqrt_2: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_62);  add_62 = None\n",
      "    mul_23: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_11, rsqrt_2);  rsqrt_2 = None\n",
      "    mul_24: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg522_1, mul_23);  arg522_1 = mul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_111: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg518_1);  arg518_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_70: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_111, [1, -1, 6, 64]);  linear_111 = None\n",
      "    transpose_110: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_70, 1, 2);  view_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_112: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg519_1);  arg519_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_113: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg520_1);  mul_24 = arg520_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_71: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_112, [1, -1, 6, 64]);  linear_112 = None\n",
      "    transpose_111: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_72: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_113, [1, -1, 6, 64]);  linear_113 = None\n",
      "    transpose_112: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_113: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_111, 3, 2);  transpose_111 = None\n",
      "    matmul_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_110, transpose_113);  transpose_110 = transpose_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_3, add_56);  matmul_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__2, torch.float32);  add__2 = None\n",
      "    softmax_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_12, -1)\n",
      "    type_as_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_12, to_12);  softmax_12 = to_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_1, 0.1, False);  type_as_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_56, transpose_112);  dropout_56 = transpose_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_114: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_4, 1, 2);  matmul_4 = None\n",
      "    contiguous_23: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_114);  transpose_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_73: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_23, [1, -1, 384]);  contiguous_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_114: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_73, arg521_1);  view_73 = arg521_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_57: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_114, 0.1, False);  linear_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_63: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_11, dropout_57);  to_11 = dropout_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_13: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_63, torch.float32);  add_63 = None\n",
      "    pow_5: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_13, 2)\n",
      "    mean_14: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None\n",
      "    add_64: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None\n",
      "    rsqrt_3: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_64);  add_64 = None\n",
      "    mul_25: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_13, rsqrt_3);  rsqrt_3 = None\n",
      "    mul_26: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg526_1, mul_25);  arg526_1 = mul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_115: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg523_1);  arg523_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_27: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_115, 0.5)\n",
      "    pow_6: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_115, 3.0)\n",
      "    mul_28: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_6, 0.044715);  pow_6 = None\n",
      "    add_65: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_115, mul_28);  linear_115 = mul_28 = None\n",
      "    mul_29: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_65, 0.7978845608028654);  add_65 = None\n",
      "    tanh_1: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_29);  mul_29 = None\n",
      "    add_66: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_1, 1.0);  tanh_1 = None\n",
      "    mul_30: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_27, add_66);  mul_27 = add_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_116: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg524_1);  mul_26 = arg524_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_31: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_30, linear_116);  mul_30 = linear_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_58: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_31, 0.1, False);  mul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_117: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_58, arg525_1);  dropout_58 = arg525_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_59: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_117, 0.1, False);  linear_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_67: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_13, dropout_59);  to_13 = dropout_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_14: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_67, torch.float32);  add_67 = None\n",
      "    pow_7: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)\n",
      "    mean_15: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_7, [-1], True);  pow_7 = None\n",
      "    add_68: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None\n",
      "    rsqrt_4: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_68);  add_68 = None\n",
      "    mul_32: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_14, rsqrt_4);  rsqrt_4 = None\n",
      "    mul_33: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg531_1, mul_32);  arg531_1 = mul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_118: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg527_1);  arg527_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_74: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_118, [1, -1, 6, 64]);  linear_118 = None\n",
      "    transpose_115: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_119: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg528_1);  arg528_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_120: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg529_1);  mul_33 = arg529_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_75: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_119, [1, -1, 6, 64]);  linear_119 = None\n",
      "    transpose_116: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_75, 1, 2);  view_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_76: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_120, [1, -1, 6, 64]);  linear_120 = None\n",
      "    transpose_117: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_76, 1, 2);  view_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_118: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_116, 3, 2);  transpose_116 = None\n",
      "    matmul_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_115, transpose_118);  transpose_115 = transpose_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_5, add_56);  matmul_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__3, torch.float32);  add__3 = None\n",
      "    softmax_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_15, -1)\n",
      "    type_as_2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_13, to_15);  softmax_13 = to_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_60: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_2, 0.1, False);  type_as_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_6: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_60, transpose_117);  dropout_60 = transpose_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_119: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_6, 1, 2);  matmul_6 = None\n",
      "    contiguous_24: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_119);  transpose_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_77: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_24, [1, -1, 384]);  contiguous_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_121: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_77, arg530_1);  view_77 = arg530_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_61: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_121, 0.1, False);  linear_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_69: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_14, dropout_61);  to_14 = dropout_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_16: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_69, torch.float32);  add_69 = None\n",
      "    pow_8: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_16, 2)\n",
      "    mean_16: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_8, [-1], True);  pow_8 = None\n",
      "    add_70: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None\n",
      "    rsqrt_5: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_70);  add_70 = None\n",
      "    mul_34: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_16, rsqrt_5);  rsqrt_5 = None\n",
      "    mul_35: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg535_1, mul_34);  arg535_1 = mul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_122: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg532_1);  arg532_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_36: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_122, 0.5)\n",
      "    pow_9: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_122, 3.0)\n",
      "    mul_37: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_9, 0.044715);  pow_9 = None\n",
      "    add_71: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_122, mul_37);  linear_122 = mul_37 = None\n",
      "    mul_38: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_71, 0.7978845608028654);  add_71 = None\n",
      "    tanh_2: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_38);  mul_38 = None\n",
      "    add_72: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_2, 1.0);  tanh_2 = None\n",
      "    mul_39: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_36, add_72);  mul_36 = add_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_123: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg533_1);  mul_35 = arg533_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_40: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_39, linear_123);  mul_39 = linear_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_62: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_40, 0.1, False);  mul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_124: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_62, arg534_1);  dropout_62 = arg534_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_63: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_124, 0.1, False);  linear_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_73: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_16, dropout_63);  to_16 = dropout_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_17: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_73, torch.float32);  add_73 = None\n",
      "    pow_10: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_17, 2)\n",
      "    mean_17: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_10, [-1], True);  pow_10 = None\n",
      "    add_74: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None\n",
      "    rsqrt_6: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_74);  add_74 = None\n",
      "    mul_41: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_17, rsqrt_6);  rsqrt_6 = None\n",
      "    mul_42: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg540_1, mul_41);  arg540_1 = mul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_125: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg536_1);  arg536_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_78: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_125, [1, -1, 6, 64]);  linear_125 = None\n",
      "    transpose_120: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_126: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg537_1);  arg537_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_127: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg538_1);  mul_42 = arg538_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_79: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_126, [1, -1, 6, 64]);  linear_126 = None\n",
      "    transpose_121: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_80: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_127, [1, -1, 6, 64]);  linear_127 = None\n",
      "    transpose_122: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_123: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_121, 3, 2);  transpose_121 = None\n",
      "    matmul_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_120, transpose_123);  transpose_120 = transpose_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_7, add_56);  matmul_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__4, torch.float32);  add__4 = None\n",
      "    softmax_14: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_18, -1)\n",
      "    type_as_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_14, to_18);  softmax_14 = to_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_64: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_3, 0.1, False);  type_as_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_64, transpose_122);  dropout_64 = transpose_122 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_124: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_8, 1, 2);  matmul_8 = None\n",
      "    contiguous_25: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_124);  transpose_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_81: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_25, [1, -1, 384]);  contiguous_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_128: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_81, arg539_1);  view_81 = arg539_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_65: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_128, 0.1, False);  linear_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_75: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_17, dropout_65);  to_17 = dropout_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_19: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_75, torch.float32);  add_75 = None\n",
      "    pow_11: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_19, 2)\n",
      "    mean_18: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_11, [-1], True);  pow_11 = None\n",
      "    add_76: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None\n",
      "    rsqrt_7: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_76);  add_76 = None\n",
      "    mul_43: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_19, rsqrt_7);  rsqrt_7 = None\n",
      "    mul_44: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg544_1, mul_43);  arg544_1 = mul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_129: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg541_1);  arg541_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_45: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_129, 0.5)\n",
      "    pow_12: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_129, 3.0)\n",
      "    mul_46: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_12, 0.044715);  pow_12 = None\n",
      "    add_77: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_129, mul_46);  linear_129 = mul_46 = None\n",
      "    mul_47: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_77, 0.7978845608028654);  add_77 = None\n",
      "    tanh_3: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_47);  mul_47 = None\n",
      "    add_78: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_3, 1.0);  tanh_3 = None\n",
      "    mul_48: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_45, add_78);  mul_45 = add_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_130: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg542_1);  mul_44 = arg542_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_49: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_48, linear_130);  mul_48 = linear_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_66: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_49, 0.1, False);  mul_49 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_131: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_66, arg543_1);  dropout_66 = arg543_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_67: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_131, 0.1, False);  linear_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_79: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_19, dropout_67);  to_19 = dropout_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_20: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_79, torch.float32);  add_79 = None\n",
      "    pow_13: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_20, 2)\n",
      "    mean_19: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_13, [-1], True);  pow_13 = None\n",
      "    add_80: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None\n",
      "    rsqrt_8: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_80);  add_80 = None\n",
      "    mul_50: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_20, rsqrt_8);  rsqrt_8 = None\n",
      "    mul_51: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg549_1, mul_50);  arg549_1 = mul_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_132: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg545_1);  arg545_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_82: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_132, [1, -1, 6, 64]);  linear_132 = None\n",
      "    transpose_125: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_82, 1, 2);  view_82 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_133: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg546_1);  arg546_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_134: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg547_1);  mul_51 = arg547_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_83: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_133, [1, -1, 6, 64]);  linear_133 = None\n",
      "    transpose_126: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_84: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_134, [1, -1, 6, 64]);  linear_134 = None\n",
      "    transpose_127: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_84, 1, 2);  view_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_128: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_126, 3, 2);  transpose_126 = None\n",
      "    matmul_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_125, transpose_128);  transpose_125 = transpose_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_9, add_56);  matmul_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_21: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__5, torch.float32);  add__5 = None\n",
      "    softmax_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_21, -1)\n",
      "    type_as_4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_15, to_21);  softmax_15 = to_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_68: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_4, 0.1, False);  type_as_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_10: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_68, transpose_127);  dropout_68 = transpose_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_10, 1, 2);  matmul_10 = None\n",
      "    contiguous_26: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_129);  transpose_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_85: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_26, [1, -1, 384]);  contiguous_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_135: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_85, arg548_1);  view_85 = arg548_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_69: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_135, 0.1, False);  linear_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_81: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_20, dropout_69);  to_20 = dropout_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_22: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_81, torch.float32);  add_81 = None\n",
      "    pow_14: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_22, 2)\n",
      "    mean_20: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_14, [-1], True);  pow_14 = None\n",
      "    add_82: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_20, 1e-06);  mean_20 = None\n",
      "    rsqrt_9: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_82);  add_82 = None\n",
      "    mul_52: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_22, rsqrt_9);  rsqrt_9 = None\n",
      "    mul_53: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg553_1, mul_52);  arg553_1 = mul_52 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_136: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg550_1);  arg550_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_54: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_136, 0.5)\n",
      "    pow_15: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_136, 3.0)\n",
      "    mul_55: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_15, 0.044715);  pow_15 = None\n",
      "    add_83: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_136, mul_55);  linear_136 = mul_55 = None\n",
      "    mul_56: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_83, 0.7978845608028654);  add_83 = None\n",
      "    tanh_4: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_56);  mul_56 = None\n",
      "    add_84: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_4, 1.0);  tanh_4 = None\n",
      "    mul_57: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_54, add_84);  mul_54 = add_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_137: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg551_1);  mul_53 = arg551_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_58: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_57, linear_137);  mul_57 = linear_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_70: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_58, 0.1, False);  mul_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_138: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_70, arg552_1);  dropout_70 = arg552_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_71: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_138, 0.1, False);  linear_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_85: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_22, dropout_71);  to_22 = dropout_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_23: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_85, torch.float32);  add_85 = None\n",
      "    pow_16: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_23, 2)\n",
      "    mean_21: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_16, [-1], True);  pow_16 = None\n",
      "    add_86: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_21, 1e-06);  mean_21 = None\n",
      "    rsqrt_10: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_86);  add_86 = None\n",
      "    mul_59: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_23, rsqrt_10);  rsqrt_10 = None\n",
      "    mul_60: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg558_1, mul_59);  arg558_1 = mul_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_139: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg554_1);  arg554_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_86: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_139, [1, -1, 6, 64]);  linear_139 = None\n",
      "    transpose_130: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_86, 1, 2);  view_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_140: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg555_1);  arg555_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_141: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg556_1);  mul_60 = arg556_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_87: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_140, [1, -1, 6, 64]);  linear_140 = None\n",
      "    transpose_131: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_87, 1, 2);  view_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_88: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_141, [1, -1, 6, 64]);  linear_141 = None\n",
      "    transpose_132: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_88, 1, 2);  view_88 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_133: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_131, 3, 2);  transpose_131 = None\n",
      "    matmul_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_130, transpose_133);  transpose_130 = transpose_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_11, add_56);  matmul_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_24: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__6, torch.float32);  add__6 = None\n",
      "    softmax_16: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_24, -1)\n",
      "    type_as_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_16, to_24);  softmax_16 = to_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_72: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_5, 0.1, False);  type_as_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_72, transpose_132);  dropout_72 = transpose_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_134: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_12, 1, 2);  matmul_12 = None\n",
      "    contiguous_27: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_134);  transpose_134 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_89: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_27, [1, -1, 384]);  contiguous_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_142: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_89, arg557_1);  view_89 = arg557_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_73: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_142, 0.1, False);  linear_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_87: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_23, dropout_73);  to_23 = dropout_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_25: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_87, torch.float32);  add_87 = None\n",
      "    pow_17: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_25, 2)\n",
      "    mean_22: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_17, [-1], True);  pow_17 = None\n",
      "    add_88: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_22, 1e-06);  mean_22 = None\n",
      "    rsqrt_11: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_88);  add_88 = None\n",
      "    mul_61: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_25, rsqrt_11);  rsqrt_11 = None\n",
      "    mul_62: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg562_1, mul_61);  arg562_1 = mul_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_143: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg559_1);  arg559_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_63: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_143, 0.5)\n",
      "    pow_18: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_143, 3.0)\n",
      "    mul_64: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_18, 0.044715);  pow_18 = None\n",
      "    add_89: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_143, mul_64);  linear_143 = mul_64 = None\n",
      "    mul_65: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_89, 0.7978845608028654);  add_89 = None\n",
      "    tanh_5: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_65);  mul_65 = None\n",
      "    add_90: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_5, 1.0);  tanh_5 = None\n",
      "    mul_66: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_63, add_90);  mul_63 = add_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_144: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg560_1);  mul_62 = arg560_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_67: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_66, linear_144);  mul_66 = linear_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_74: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_67, 0.1, False);  mul_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_145: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_74, arg561_1);  dropout_74 = arg561_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_75: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_145, 0.1, False);  linear_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_91: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_25, dropout_75);  to_25 = dropout_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_26: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_91, torch.float32);  add_91 = None\n",
      "    pow_19: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_26, 2)\n",
      "    mean_23: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_19, [-1], True);  pow_19 = None\n",
      "    add_92: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_23, 1e-06);  mean_23 = None\n",
      "    rsqrt_12: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_92);  add_92 = None\n",
      "    mul_68: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_26, rsqrt_12);  rsqrt_12 = None\n",
      "    mul_69: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg567_1, mul_68);  arg567_1 = mul_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_146: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg563_1);  arg563_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_90: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_146, [1, -1, 6, 64]);  linear_146 = None\n",
      "    transpose_135: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_90, 1, 2);  view_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_147: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg564_1);  arg564_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_148: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg565_1);  mul_69 = arg565_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_91: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_147, [1, -1, 6, 64]);  linear_147 = None\n",
      "    transpose_136: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_91, 1, 2);  view_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_92: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_148, [1, -1, 6, 64]);  linear_148 = None\n",
      "    transpose_137: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_138: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_136, 3, 2);  transpose_136 = None\n",
      "    matmul_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_135, transpose_138);  transpose_135 = transpose_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_13, add_56);  matmul_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_27: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__7, torch.float32);  add__7 = None\n",
      "    softmax_17: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_27, -1)\n",
      "    type_as_6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_17, to_27);  softmax_17 = to_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_76: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_6, 0.1, False);  type_as_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_14: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_76, transpose_137);  dropout_76 = transpose_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_139: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_14, 1, 2);  matmul_14 = None\n",
      "    contiguous_28: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_139);  transpose_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_93: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_28, [1, -1, 384]);  contiguous_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_149: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_93, arg566_1);  view_93 = arg566_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_77: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_149, 0.1, False);  linear_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_93: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_26, dropout_77);  to_26 = dropout_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_28: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_93, torch.float32);  add_93 = None\n",
      "    pow_20: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_28, 2)\n",
      "    mean_24: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_20, [-1], True);  pow_20 = None\n",
      "    add_94: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_24, 1e-06);  mean_24 = None\n",
      "    rsqrt_13: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_94);  add_94 = None\n",
      "    mul_70: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_28, rsqrt_13);  rsqrt_13 = None\n",
      "    mul_71: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg571_1, mul_70);  arg571_1 = mul_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_150: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg568_1);  arg568_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_72: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_150, 0.5)\n",
      "    pow_21: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_150, 3.0)\n",
      "    mul_73: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_21, 0.044715);  pow_21 = None\n",
      "    add_95: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_150, mul_73);  linear_150 = mul_73 = None\n",
      "    mul_74: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_95, 0.7978845608028654);  add_95 = None\n",
      "    tanh_6: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_74);  mul_74 = None\n",
      "    add_96: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_6, 1.0);  tanh_6 = None\n",
      "    mul_75: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_72, add_96);  mul_72 = add_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_151: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg569_1);  mul_71 = arg569_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_76: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_75, linear_151);  mul_75 = linear_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_78: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_76, 0.1, False);  mul_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_152: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_78, arg570_1);  dropout_78 = arg570_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_79: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_152, 0.1, False);  linear_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_97: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_28, dropout_79);  to_28 = dropout_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_29: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_97, torch.float32);  add_97 = None\n",
      "    pow_22: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_29, 2)\n",
      "    mean_25: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_22, [-1], True);  pow_22 = None\n",
      "    add_98: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_25, 1e-06);  mean_25 = None\n",
      "    rsqrt_14: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_98);  add_98 = None\n",
      "    mul_77: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_29, rsqrt_14);  rsqrt_14 = None\n",
      "    mul_78: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg576_1, mul_77);  arg576_1 = mul_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_153: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg572_1);  arg572_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_94: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_153, [1, -1, 6, 64]);  linear_153 = None\n",
      "    transpose_140: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_94, 1, 2);  view_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_154: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg573_1);  arg573_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_155: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg574_1);  mul_78 = arg574_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_95: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_154, [1, -1, 6, 64]);  linear_154 = None\n",
      "    transpose_141: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_95, 1, 2);  view_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_96: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_155, [1, -1, 6, 64]);  linear_155 = None\n",
      "    transpose_142: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_96, 1, 2);  view_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_143: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_141, 3, 2);  transpose_141 = None\n",
      "    matmul_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_140, transpose_143);  transpose_140 = transpose_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__8: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_15, add_56);  matmul_15 = add_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_30: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__8, torch.float32);  add__8 = None\n",
      "    softmax_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_30, -1)\n",
      "    type_as_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_18, to_30);  softmax_18 = to_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_80: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_7, 0.1, False);  type_as_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_80, transpose_142);  dropout_80 = transpose_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_16, 1, 2);  matmul_16 = None\n",
      "    contiguous_29: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_144);  transpose_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_97: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_29, [1, -1, 384]);  contiguous_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_156: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_97, arg575_1);  view_97 = arg575_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_81: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_156, 0.1, False);  linear_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_99: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_29, dropout_81);  to_29 = dropout_81 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_31: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_99, torch.float32);  add_99 = None\n",
      "    pow_23: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_31, 2)\n",
      "    mean_26: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_23, [-1], True);  pow_23 = None\n",
      "    add_100: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_26, 1e-06);  mean_26 = None\n",
      "    rsqrt_15: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_100);  add_100 = None\n",
      "    mul_79: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_31, rsqrt_15);  rsqrt_15 = None\n",
      "    mul_80: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg580_1, mul_79);  arg580_1 = mul_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_157: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg577_1);  arg577_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_81: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_157, 0.5)\n",
      "    pow_24: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_157, 3.0)\n",
      "    mul_82: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_24, 0.044715);  pow_24 = None\n",
      "    add_101: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_157, mul_82);  linear_157 = mul_82 = None\n",
      "    mul_83: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_101, 0.7978845608028654);  add_101 = None\n",
      "    tanh_7: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_83);  mul_83 = None\n",
      "    add_102: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_7, 1.0);  tanh_7 = None\n",
      "    mul_84: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_81, add_102);  mul_81 = add_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_158: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg578_1);  mul_80 = arg578_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_85: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_84, linear_158);  mul_84 = linear_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_82: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_85, 0.1, False);  mul_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_159: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_82, arg579_1);  dropout_82 = arg579_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_83: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_159, 0.1, False);  linear_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_103: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_31, dropout_83);  to_31 = dropout_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_32: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_103, torch.float32);  add_103 = None\n",
      "    pow_25: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_32, 2)\n",
      "    mean_27: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_25, [-1], True);  pow_25 = None\n",
      "    add_104: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_27, 1e-06);  mean_27 = None\n",
      "    rsqrt_16: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_104);  add_104 = None\n",
      "    mul_86: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_32, rsqrt_16);  to_32 = rsqrt_16 = None\n",
      "    mul_87: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg581_1, mul_86);  arg581_1 = mul_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_84: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(mul_87, 0.1, False);  mul_87 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_2 = torch._C._set_grad_enabled(False);  _set_grad_enabled_2 = None\n",
      "    ones_1: \"i64[1, 109]\" = torch.ops.aten.ones.default([1, 109], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_88: \"i64[1, 109]\" = torch.ops.aten.mul.Tensor(ones_1, -100);  ones_1 = mul_88 = None\n",
      "    _tensor_constant0 = self._tensor_constant0\n",
      "    lift_fresh_copy: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "    detach_: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n",
      "    _tensor_constant1 = self._tensor_constant1\n",
      "    lift_fresh_copy_1: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
      "    detach__1: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None\n",
      "    _tensor_constant2 = self._tensor_constant2\n",
      "    lift_fresh_copy_2: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
      "    detach__2: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_2);  lift_fresh_copy_2 = None\n",
      "    unsqueeze_24: \"i64[1]\" = torch.ops.aten.unsqueeze.default(detach_, 0);  detach_ = None\n",
      "    isin: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(unsqueeze_24, detach__1)\n",
      "    any_1: \"b8[]\" = torch.ops.aten.any.default(isin);  isin = None\n",
      "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(any_1, 0);  any_1 = ne = None\n",
      "    lt_1: \"b8[1]\" = torch.ops.aten.lt.Scalar(unsqueeze_24, 0)\n",
      "    any_2: \"b8[]\" = torch.ops.aten.any.default(lt_1);  lt_1 = None\n",
      "    ne_1: \"b8[]\" = torch.ops.aten.ne.Scalar(any_2, 0);  any_2 = ne_1 = None\n",
      "    ones_2: \"i64[1, 1]\" = torch.ops.aten.ones.default([1, 1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_89: \"i64[1, 1]\" = torch.ops.aten.mul.Tensor(ones_2, detach__2);  ones_2 = detach__2 = None\n",
      "    ones_3: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    ones_4: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    cumsum: \"i64[1]\" = torch.ops.aten.cumsum.default(ones_4, 0);  ones_4 = None\n",
      "    sub_1: \"i64[1]\" = torch.ops.aten.sub.Tensor(cumsum, 1);  cumsum = None\n",
      "    slice_41: \"i64[1]\" = torch.ops.aten.slice.Tensor(sub_1, 0, 0);  sub_1 = None\n",
      "    slice_42: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(mul_89, 0, 0, 9223372036854775807)\n",
      "    index: \"i64[1, 1]\" = torch.ops.aten.index.Tensor(slice_42, [None, slice_41]);  slice_42 = None\n",
      "    clone_6: \"i64[1, 1]\" = torch.ops.aten.clone.default(index, memory_format = torch.contiguous_format);  index = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:985 in forward, code: input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    view_98: \"i64[1, 1]\" = torch.ops.aten.view.default(clone_6, [-1, 1]);  clone_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_6: \"f32[1, 1, 512]\" = torch.ops.aten.embedding.default(arg582_1, view_98);  arg582_1 = view_98 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1035 in forward, code: causal_mask = self._update_causal_mask(\n",
      "    full: \"f32[1, 2]\" = torch.ops.aten.full.default([1, 2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_2: \"i64[2]\" = torch.ops.aten.arange.default(2, device = device(type='cpu'), pin_memory = False)\n",
      "    reshape_24: \"i64[1, 1]\" = torch.ops.aten.reshape.default(slice_41, [-1, 1])\n",
      "    gt_4: \"b8[1, 2]\" = torch.ops.aten.gt.Tensor(arange_2, reshape_24);  arange_2 = reshape_24 = None\n",
      "    mul_: \"f32[1, 2]\" = torch.ops.aten.mul_.Tensor(full, gt_4);  full = gt_4 = None\n",
      "    unsqueeze_25: \"f32[1, 1, 2]\" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None\n",
      "    unsqueeze_26: \"f32[1, 1, 1, 2]\" = torch.ops.aten.unsqueeze.default(unsqueeze_25, 1);  unsqueeze_25 = None\n",
      "    slice_43: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(unsqueeze_26, 2, 0, 9223372036854775807);  unsqueeze_26 = None\n",
      "    slice_44: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_43, 3, 0, 9223372036854775807);  slice_43 = None\n",
      "    expand_4: \"f32[1, 1, 1, 2]\" = torch.ops.aten.expand.default(slice_44, [1, 1, -1, -1]);  slice_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1060 in forward, code: encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
      "    slice_45: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807);  concat_8 = None\n",
      "    unsqueeze_27: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_45, 1);  slice_45 = None\n",
      "    unsqueeze_28: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_27, 2);  unsqueeze_27 = None\n",
      "    slice_46: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_28, 3, 0, 9223372036854775807);  unsqueeze_28 = None\n",
      "    to_33: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_46, torch.float32);  slice_46 = None\n",
      "    rsub_1: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_33, 1.0);  to_33 = None\n",
      "    mul_90: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub_1, -3.4028234663852886e+38);  rsub_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_85: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(embedding_6, 0.1, False);  embedding_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_34: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(dropout_85, torch.float32);  dropout_85 = None\n",
      "    pow_26: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_34, 2)\n",
      "    mean_28: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_26, [-1], True);  pow_26 = None\n",
      "    add_105: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_28, 1e-06);  mean_28 = None\n",
      "    rsqrt_17: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_105);  add_105 = None\n",
      "    mul_91: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_34, rsqrt_17);  rsqrt_17 = None\n",
      "    mul_92: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg588_1, mul_91);  arg588_1 = mul_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_160: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg583_1);  arg583_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_99: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_160, [1, -1, 6, 64]);  linear_160 = None\n",
      "    transpose_145: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_161: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg584_1);  arg584_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_162: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg585_1);  mul_92 = arg585_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_100: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_161, [1, -1, 6, 64]);  linear_161 = None\n",
      "    transpose_146: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_100, 1, 2);  view_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_101: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_162, [1, -1, 6, 64]);  linear_162 = None\n",
      "    transpose_147: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_101, 1, 2);  view_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant3 = self._tensor_constant3\n",
      "    lift_fresh_copy_3: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
      "    detach__3: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_3);  lift_fresh_copy_3 = None\n",
      "    _tensor_constant4 = self._tensor_constant4\n",
      "    lift_fresh_copy_4: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
      "    detach__4: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_4);  lift_fresh_copy_4 = None\n",
      "    cat_1: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__3, transpose_146], -2);  detach__3 = transpose_146 = None\n",
      "    cat_2: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__4, transpose_147], -2);  detach__4 = transpose_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_148: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_1, 3, 2);  cat_1 = None\n",
      "    matmul_17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_145, transpose_148);  transpose_145 = transpose_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_30: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_106: \"i64[]\" = torch.ops.aten.add.Tensor(select_30, 1);  select_30 = add_106 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_47: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_29: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_47, 1);  slice_47 = None\n",
      "    to_35: \"i64[1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_29, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_29 = None\n",
      "    arange_3: \"i64[1]\" = torch.ops.aten.arange.default(1, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_30: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None\n",
      "    slice_48: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_30, 1, 0, 9223372036854775807);  unsqueeze_30 = None\n",
      "    sub_2: \"i64[1, 1]\" = torch.ops.aten.sub.Tensor(slice_48, to_35);  slice_48 = to_35 = None\n",
      "    zeros_like_3: \"i64[1, 1]\" = torch.ops.aten.zeros_like.default(sub_2, pin_memory = False)\n",
      "    min_2: \"i64[1, 1]\" = torch.ops.aten.min.other(sub_2, zeros_like_3);  sub_2 = zeros_like_3 = None\n",
      "    neg: \"i64[1, 1]\" = torch.ops.aten.neg.default(min_2);  min_2 = None\n",
      "    lt_2: \"b8[1, 1]\" = torch.ops.aten.lt.Scalar(neg, 16)\n",
      "    to_36: \"f32[1, 1]\" = torch.ops.aten.to.dtype(neg, torch.float32)\n",
      "    div_2: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(to_36, 16);  to_36 = None\n",
      "    log_1: \"f32[1, 1]\" = torch.ops.aten.log.default(div_2);  div_2 = None\n",
      "    div_3: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(log_1, 2.0794415416798357);  log_1 = None\n",
      "    mul_93: \"f32[1, 1]\" = torch.ops.aten.mul.Tensor(div_3, 16);  div_3 = None\n",
      "    to_37: \"i64[1, 1]\" = torch.ops.aten.to.dtype(mul_93, torch.int64);  mul_93 = None\n",
      "    add_107: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(to_37, 16);  to_37 = None\n",
      "    full_like_4: \"i64[1, 1]\" = torch.ops.aten.full_like.default(add_107, 31, pin_memory = False)\n",
      "    min_3: \"i64[1, 1]\" = torch.ops.aten.min.other(add_107, full_like_4);  add_107 = full_like_4 = None\n",
      "    where_4: \"i64[1, 1]\" = torch.ops.aten.where.self(lt_2, neg, min_3);  lt_2 = neg = min_3 = None\n",
      "    add_108: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(where_4, 0);  where_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_7: \"f32[1, 1, 6]\" = torch.ops.aten.embedding.default(arg587_1, add_108);  arg587_1 = add_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_13: \"f32[6, 1, 1]\" = torch.ops.aten.permute.default(embedding_7, [2, 0, 1]);  embedding_7 = None\n",
      "    unsqueeze_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.unsqueeze.default(permute_13, 0);  permute_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_49: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_31);  unsqueeze_31 = None\n",
      "    slice_50: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_49, 1);  slice_49 = None\n",
      "    slice_51: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_50, 2, -1);  slice_50 = None\n",
      "    slice_52: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_51, 3);  slice_51 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_53: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(expand_4);  expand_4 = None\n",
      "    slice_54: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_53, 1);  slice_53 = None\n",
      "    slice_55: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_54, 2);  slice_54 = None\n",
      "    slice_56: \"f32[1, 1, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_55, 3, None, 1);  slice_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_109: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add.Tensor(slice_52, slice_56);  slice_52 = slice_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__9: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_17, add_109);  matmul_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_38: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__9, torch.float32);  add__9 = None\n",
      "    softmax_19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_38, -1)\n",
      "    type_as_8: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_19, to_38);  softmax_19 = to_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_86: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_8, 0.1, False);  type_as_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_86, cat_2);  dropout_86 = cat_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_18, 1, 2);  matmul_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_102: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_149, [1, -1, 384]);  transpose_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_163: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_102, arg586_1);  view_102 = arg586_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_87: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_163, 0.1, False);  linear_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_110: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_34, dropout_87);  to_34 = dropout_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_31: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_111: \"i64[]\" = torch.ops.aten.add.Tensor(select_31, 1);  select_31 = add_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_39: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_110, torch.float32);  add_110 = None\n",
      "    pow_27: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_39, 2)\n",
      "    mean_29: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_27, [-1], True);  pow_27 = None\n",
      "    add_112: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_29, 1e-06);  mean_29 = None\n",
      "    rsqrt_18: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_112);  add_112 = None\n",
      "    mul_94: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_39, rsqrt_18);  rsqrt_18 = None\n",
      "    mul_95: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg593_1, mul_94);  arg593_1 = mul_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_164: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_95, arg589_1);  mul_95 = arg589_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_103: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_164, [1, -1, 6, 64]);  linear_164 = None\n",
      "    transpose_150: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_103, 1, 2);  view_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_165: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg590_1);  arg590_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_166: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg591_1);  arg591_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_104: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_165, [1, -1, 6, 64]);  linear_165 = None\n",
      "    transpose_151: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_104, 1, 2);  view_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_105: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_166, [1, -1, 6, 64]);  linear_166 = None\n",
      "    transpose_152: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_105, 1, 2);  view_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant5 = self._tensor_constant5\n",
      "    lift_fresh_copy_5: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
      "    detach__5: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_5);  lift_fresh_copy_5 = None\n",
      "    _tensor_constant6 = self._tensor_constant6\n",
      "    lift_fresh_copy_6: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
      "    detach__6: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_6);  lift_fresh_copy_6 = None\n",
      "    cat_3: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__5, transpose_151], -2);  detach__5 = transpose_151 = None\n",
      "    cat_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__6, transpose_152], -2);  detach__6 = transpose_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_153: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_3, 3, 2);  cat_3 = None\n",
      "    matmul_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_150, transpose_153);  transpose_150 = transpose_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:535 in forward, code: position_bias = torch.zeros(\n",
      "    zeros_2: \"f32[1, 6, 1, 109]\" = torch.ops.aten.zeros.default([1, 6, 1, 109], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_57: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_90);  mul_90 = None\n",
      "    slice_58: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_57, 1);  slice_57 = None\n",
      "    slice_59: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_58, 2);  slice_58 = None\n",
      "    slice_60: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_59, 3, None, 109);  slice_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_113: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add.Tensor(zeros_2, slice_60);  zeros_2 = slice_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__10: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_19, add_113);  matmul_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_40: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__10, torch.float32);  add__10 = None\n",
      "    softmax_20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_40, -1)\n",
      "    type_as_9: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_20, to_40);  softmax_20 = to_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_88: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_9, 0.1, False);  type_as_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_20: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_88, cat_4);  dropout_88 = cat_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_154: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_20, 1, 2);  matmul_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_106: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_154, [1, -1, 384]);  transpose_154 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_167: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_106, arg592_1);  view_106 = arg592_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_89: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_167, 0.1, False);  linear_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_114: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_39, dropout_89);  to_39 = dropout_89 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_41: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_114, torch.float32);  add_114 = None\n",
      "    pow_28: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_41, 2)\n",
      "    mean_30: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_28, [-1], True);  pow_28 = None\n",
      "    add_115: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_30, 1e-06);  mean_30 = None\n",
      "    rsqrt_19: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_115);  add_115 = None\n",
      "    mul_96: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_41, rsqrt_19);  rsqrt_19 = None\n",
      "    mul_97: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg597_1, mul_96);  arg597_1 = mul_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_168: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg594_1);  arg594_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_98: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_168, 0.5)\n",
      "    pow_29: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_168, 3.0)\n",
      "    mul_99: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_29, 0.044715);  pow_29 = None\n",
      "    add_116: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_168, mul_99);  linear_168 = mul_99 = None\n",
      "    mul_100: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_116, 0.7978845608028654);  add_116 = None\n",
      "    tanh_8: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_100);  mul_100 = None\n",
      "    add_117: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_8, 1.0);  tanh_8 = None\n",
      "    mul_101: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_98, add_117);  mul_98 = add_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_169: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg595_1);  mul_97 = arg595_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_102: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_101, linear_169);  mul_101 = linear_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_90: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_102, 0.1, False);  mul_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_170: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_90, arg596_1);  dropout_90 = arg596_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_91: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_170, 0.1, False);  linear_170 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_118: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_41, dropout_91);  to_41 = dropout_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_42: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_118, torch.float32);  add_118 = None\n",
      "    pow_30: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_42, 2)\n",
      "    mean_31: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_30, [-1], True);  pow_30 = None\n",
      "    add_119: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_31, 1e-06);  mean_31 = None\n",
      "    rsqrt_20: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_119);  add_119 = None\n",
      "    mul_103: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_42, rsqrt_20);  rsqrt_20 = None\n",
      "    mul_104: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg602_1, mul_103);  arg602_1 = mul_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_171: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg598_1);  arg598_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_107: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_171, [1, -1, 6, 64]);  linear_171 = None\n",
      "    transpose_155: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_107, 1, 2);  view_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_172: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg599_1);  arg599_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_173: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg600_1);  mul_104 = arg600_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_108: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_172, [1, -1, 6, 64]);  linear_172 = None\n",
      "    transpose_156: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_108, 1, 2);  view_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_109: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_173, [1, -1, 6, 64]);  linear_173 = None\n",
      "    transpose_157: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_109, 1, 2);  view_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant7 = self._tensor_constant7\n",
      "    lift_fresh_copy_7: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None\n",
      "    detach__7: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_7);  lift_fresh_copy_7 = None\n",
      "    _tensor_constant8 = self._tensor_constant8\n",
      "    lift_fresh_copy_8: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None\n",
      "    detach__8: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_8);  lift_fresh_copy_8 = None\n",
      "    cat_5: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__7, transpose_156], -2);  detach__7 = transpose_156 = None\n",
      "    cat_6: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__8, transpose_157], -2);  detach__8 = transpose_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_158: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_5, 3, 2);  cat_5 = None\n",
      "    matmul_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_155, transpose_158);  transpose_155 = transpose_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__11: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_21, add_109);  matmul_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_43: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__11, torch.float32);  add__11 = None\n",
      "    softmax_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_43, -1)\n",
      "    type_as_10: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_21, to_43);  softmax_21 = to_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_92: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_10, 0.1, False);  type_as_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_92, cat_6);  dropout_92 = cat_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_22, 1, 2);  matmul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_110: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_159, [1, -1, 384]);  transpose_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_174: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_110, arg601_1);  view_110 = arg601_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_93: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_174, 0.1, False);  linear_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_120: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_42, dropout_93);  to_42 = dropout_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_32: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_121: \"i64[]\" = torch.ops.aten.add.Tensor(select_32, 1);  select_32 = add_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_44: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_120, torch.float32);  add_120 = None\n",
      "    pow_31: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_44, 2)\n",
      "    mean_32: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_31, [-1], True);  pow_31 = None\n",
      "    add_122: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_32, 1e-06);  mean_32 = None\n",
      "    rsqrt_21: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_122);  add_122 = None\n",
      "    mul_105: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_44, rsqrt_21);  rsqrt_21 = None\n",
      "    mul_106: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg607_1, mul_105);  arg607_1 = mul_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_175: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_106, arg603_1);  mul_106 = arg603_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_111: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_175, [1, -1, 6, 64]);  linear_175 = None\n",
      "    transpose_160: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_111, 1, 2);  view_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_176: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg604_1);  arg604_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_177: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg605_1);  arg605_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_112: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_176, [1, -1, 6, 64]);  linear_176 = None\n",
      "    transpose_161: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_112, 1, 2);  view_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_113: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_177, [1, -1, 6, 64]);  linear_177 = None\n",
      "    transpose_162: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant9 = self._tensor_constant9\n",
      "    lift_fresh_copy_9: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None\n",
      "    detach__9: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_9);  lift_fresh_copy_9 = None\n",
      "    _tensor_constant10 = self._tensor_constant10\n",
      "    lift_fresh_copy_10: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant10);  _tensor_constant10 = None\n",
      "    detach__10: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_10);  lift_fresh_copy_10 = None\n",
      "    cat_7: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__9, transpose_161], -2);  detach__9 = transpose_161 = None\n",
      "    cat_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__10, transpose_162], -2);  detach__10 = transpose_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_163: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_7, 3, 2);  cat_7 = None\n",
      "    matmul_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_160, transpose_163);  transpose_160 = transpose_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__12: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_23, add_113);  matmul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_45: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__12, torch.float32);  add__12 = None\n",
      "    softmax_22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_45, -1)\n",
      "    type_as_11: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_22, to_45);  softmax_22 = to_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_94: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_11, 0.1, False);  type_as_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_24: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_94, cat_8);  dropout_94 = cat_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_164: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_24, 1, 2);  matmul_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_114: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_164, [1, -1, 384]);  transpose_164 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_178: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_114, arg606_1);  view_114 = arg606_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_95: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_178, 0.1, False);  linear_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_123: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_44, dropout_95);  to_44 = dropout_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_46: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_123, torch.float32);  add_123 = None\n",
      "    pow_32: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_46, 2)\n",
      "    mean_33: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_32, [-1], True);  pow_32 = None\n",
      "    add_124: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_33, 1e-06);  mean_33 = None\n",
      "    rsqrt_22: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_124);  add_124 = None\n",
      "    mul_107: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_46, rsqrt_22);  rsqrt_22 = None\n",
      "    mul_108: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg611_1, mul_107);  arg611_1 = mul_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_179: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg608_1);  arg608_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_109: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_179, 0.5)\n",
      "    pow_33: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_179, 3.0)\n",
      "    mul_110: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_33, 0.044715);  pow_33 = None\n",
      "    add_125: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_179, mul_110);  linear_179 = mul_110 = None\n",
      "    mul_111: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_125, 0.7978845608028654);  add_125 = None\n",
      "    tanh_9: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_111);  mul_111 = None\n",
      "    add_126: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_9, 1.0);  tanh_9 = None\n",
      "    mul_112: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_109, add_126);  mul_109 = add_126 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_180: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg609_1);  mul_108 = arg609_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_113: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_112, linear_180);  mul_112 = linear_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_96: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_113, 0.1, False);  mul_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_181: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_96, arg610_1);  dropout_96 = arg610_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_97: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_181, 0.1, False);  linear_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_127: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_46, dropout_97);  to_46 = dropout_97 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_47: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_127, torch.float32);  add_127 = None\n",
      "    pow_34: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_47, 2)\n",
      "    mean_34: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_34, [-1], True);  pow_34 = None\n",
      "    add_128: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_34, 1e-06);  mean_34 = None\n",
      "    rsqrt_23: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_128);  add_128 = None\n",
      "    mul_114: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_47, rsqrt_23);  rsqrt_23 = None\n",
      "    mul_115: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg616_1, mul_114);  arg616_1 = mul_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_182: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg612_1);  arg612_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_115: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_182, [1, -1, 6, 64]);  linear_182 = None\n",
      "    transpose_165: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_115, 1, 2);  view_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_183: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg613_1);  arg613_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_184: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg614_1);  mul_115 = arg614_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_116: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_183, [1, -1, 6, 64]);  linear_183 = None\n",
      "    transpose_166: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_116, 1, 2);  view_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_117: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_184, [1, -1, 6, 64]);  linear_184 = None\n",
      "    transpose_167: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_117, 1, 2);  view_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant11 = self._tensor_constant11\n",
      "    lift_fresh_copy_11: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant11);  _tensor_constant11 = None\n",
      "    detach__11: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_11);  lift_fresh_copy_11 = None\n",
      "    _tensor_constant12 = self._tensor_constant12\n",
      "    lift_fresh_copy_12: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant12);  _tensor_constant12 = None\n",
      "    detach__12: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_12);  lift_fresh_copy_12 = None\n",
      "    cat_9: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__11, transpose_166], -2);  detach__11 = transpose_166 = None\n",
      "    cat_10: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__12, transpose_167], -2);  detach__12 = transpose_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_168: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_9, 3, 2);  cat_9 = None\n",
      "    matmul_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_165, transpose_168);  transpose_165 = transpose_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__13: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_25, add_109);  matmul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_48: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__13, torch.float32);  add__13 = None\n",
      "    softmax_23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_48, -1)\n",
      "    type_as_12: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_23, to_48);  softmax_23 = to_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_98: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_12, 0.1, False);  type_as_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_98, cat_10);  dropout_98 = cat_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_169: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_26, 1, 2);  matmul_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_118: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_169, [1, -1, 384]);  transpose_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_185: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_118, arg615_1);  view_118 = arg615_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_99: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_185, 0.1, False);  linear_185 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_129: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_47, dropout_99);  to_47 = dropout_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_33: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_130: \"i64[]\" = torch.ops.aten.add.Tensor(select_33, 1);  select_33 = add_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_49: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_129, torch.float32);  add_129 = None\n",
      "    pow_35: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_49, 2)\n",
      "    mean_35: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_35, [-1], True);  pow_35 = None\n",
      "    add_131: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_35, 1e-06);  mean_35 = None\n",
      "    rsqrt_24: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_131);  add_131 = None\n",
      "    mul_116: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_49, rsqrt_24);  rsqrt_24 = None\n",
      "    mul_117: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg621_1, mul_116);  arg621_1 = mul_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_186: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_117, arg617_1);  mul_117 = arg617_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_119: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_186, [1, -1, 6, 64]);  linear_186 = None\n",
      "    transpose_170: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_119, 1, 2);  view_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_187: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg618_1);  arg618_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_188: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg619_1);  arg619_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_120: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_187, [1, -1, 6, 64]);  linear_187 = None\n",
      "    transpose_171: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_121: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_188, [1, -1, 6, 64]);  linear_188 = None\n",
      "    transpose_172: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_121, 1, 2);  view_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant13 = self._tensor_constant13\n",
      "    lift_fresh_copy_13: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant13);  _tensor_constant13 = None\n",
      "    detach__13: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_13);  lift_fresh_copy_13 = None\n",
      "    _tensor_constant14 = self._tensor_constant14\n",
      "    lift_fresh_copy_14: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant14);  _tensor_constant14 = None\n",
      "    detach__14: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_14);  lift_fresh_copy_14 = None\n",
      "    cat_11: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__13, transpose_171], -2);  detach__13 = transpose_171 = None\n",
      "    cat_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__14, transpose_172], -2);  detach__14 = transpose_172 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_173: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_11, 3, 2);  cat_11 = None\n",
      "    matmul_27: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_170, transpose_173);  transpose_170 = transpose_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__14: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_27, add_113);  matmul_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_50: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__14, torch.float32);  add__14 = None\n",
      "    softmax_24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_50, -1)\n",
      "    type_as_13: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_24, to_50);  softmax_24 = to_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_100: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_13, 0.1, False);  type_as_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_28: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_100, cat_12);  dropout_100 = cat_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_174: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_28, 1, 2);  matmul_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_122: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_174, [1, -1, 384]);  transpose_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_189: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_122, arg620_1);  view_122 = arg620_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_101: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_189, 0.1, False);  linear_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_132: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_49, dropout_101);  to_49 = dropout_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_51: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_132, torch.float32);  add_132 = None\n",
      "    pow_36: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_51, 2)\n",
      "    mean_36: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_36, [-1], True);  pow_36 = None\n",
      "    add_133: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_36, 1e-06);  mean_36 = None\n",
      "    rsqrt_25: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_133);  add_133 = None\n",
      "    mul_118: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_51, rsqrt_25);  rsqrt_25 = None\n",
      "    mul_119: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg625_1, mul_118);  arg625_1 = mul_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_190: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg622_1);  arg622_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_120: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_190, 0.5)\n",
      "    pow_37: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_190, 3.0)\n",
      "    mul_121: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_37, 0.044715);  pow_37 = None\n",
      "    add_134: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_190, mul_121);  linear_190 = mul_121 = None\n",
      "    mul_122: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_134, 0.7978845608028654);  add_134 = None\n",
      "    tanh_10: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_122);  mul_122 = None\n",
      "    add_135: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_10, 1.0);  tanh_10 = None\n",
      "    mul_123: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_120, add_135);  mul_120 = add_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_191: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg623_1);  mul_119 = arg623_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_124: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_123, linear_191);  mul_123 = linear_191 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_102: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_124, 0.1, False);  mul_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_192: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_102, arg624_1);  dropout_102 = arg624_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_103: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_192, 0.1, False);  linear_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_136: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_51, dropout_103);  to_51 = dropout_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_52: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_136, torch.float32);  add_136 = None\n",
      "    pow_38: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_52, 2)\n",
      "    mean_37: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_38, [-1], True);  pow_38 = None\n",
      "    add_137: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_37, 1e-06);  mean_37 = None\n",
      "    rsqrt_26: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_137);  add_137 = None\n",
      "    mul_125: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_52, rsqrt_26);  rsqrt_26 = None\n",
      "    mul_126: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg630_1, mul_125);  arg630_1 = mul_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_193: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg626_1);  arg626_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_123: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_193, [1, -1, 6, 64]);  linear_193 = None\n",
      "    transpose_175: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_123, 1, 2);  view_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_194: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg627_1);  arg627_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_195: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg628_1);  mul_126 = arg628_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_124: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_194, [1, -1, 6, 64]);  linear_194 = None\n",
      "    transpose_176: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_124, 1, 2);  view_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_125: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_195, [1, -1, 6, 64]);  linear_195 = None\n",
      "    transpose_177: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_125, 1, 2);  view_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant15 = self._tensor_constant15\n",
      "    lift_fresh_copy_15: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant15);  _tensor_constant15 = None\n",
      "    detach__15: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_15);  lift_fresh_copy_15 = None\n",
      "    _tensor_constant16 = self._tensor_constant16\n",
      "    lift_fresh_copy_16: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant16);  _tensor_constant16 = None\n",
      "    detach__16: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_16);  lift_fresh_copy_16 = None\n",
      "    cat_13: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__15, transpose_176], -2);  detach__15 = transpose_176 = None\n",
      "    cat_14: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__16, transpose_177], -2);  detach__16 = transpose_177 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_178: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_13, 3, 2);  cat_13 = None\n",
      "    matmul_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_175, transpose_178);  transpose_175 = transpose_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__15: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_29, add_109);  matmul_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_53: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__15, torch.float32);  add__15 = None\n",
      "    softmax_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_53, -1)\n",
      "    type_as_14: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_25, to_53);  softmax_25 = to_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_104: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_14, 0.1, False);  type_as_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_104, cat_14);  dropout_104 = cat_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_179: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_30, 1, 2);  matmul_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_126: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_179, [1, -1, 384]);  transpose_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_196: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_126, arg629_1);  view_126 = arg629_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_105: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_196, 0.1, False);  linear_196 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_138: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_52, dropout_105);  to_52 = dropout_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_34: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_139: \"i64[]\" = torch.ops.aten.add.Tensor(select_34, 1);  select_34 = add_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_54: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_138, torch.float32);  add_138 = None\n",
      "    pow_39: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_54, 2)\n",
      "    mean_38: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_39, [-1], True);  pow_39 = None\n",
      "    add_140: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_38, 1e-06);  mean_38 = None\n",
      "    rsqrt_27: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_140);  add_140 = None\n",
      "    mul_127: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_54, rsqrt_27);  rsqrt_27 = None\n",
      "    mul_128: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg635_1, mul_127);  arg635_1 = mul_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_197: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_128, arg631_1);  mul_128 = arg631_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_127: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_197, [1, -1, 6, 64]);  linear_197 = None\n",
      "    transpose_180: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_127, 1, 2);  view_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_198: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg632_1);  arg632_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_199: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg633_1);  arg633_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_128: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_198, [1, -1, 6, 64]);  linear_198 = None\n",
      "    transpose_181: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_128, 1, 2);  view_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_199, [1, -1, 6, 64]);  linear_199 = None\n",
      "    transpose_182: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_129, 1, 2);  view_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant17 = self._tensor_constant17\n",
      "    lift_fresh_copy_17: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant17);  _tensor_constant17 = None\n",
      "    detach__17: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_17);  lift_fresh_copy_17 = None\n",
      "    _tensor_constant18 = self._tensor_constant18\n",
      "    lift_fresh_copy_18: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant18);  _tensor_constant18 = None\n",
      "    detach__18: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_18);  lift_fresh_copy_18 = None\n",
      "    cat_15: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__17, transpose_181], -2);  detach__17 = transpose_181 = None\n",
      "    cat_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__18, transpose_182], -2);  detach__18 = transpose_182 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_183: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_15, 3, 2);  cat_15 = None\n",
      "    matmul_31: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_180, transpose_183);  transpose_180 = transpose_183 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__16: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_31, add_113);  matmul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_55: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__16, torch.float32);  add__16 = None\n",
      "    softmax_26: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_55, -1)\n",
      "    type_as_15: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_26, to_55);  softmax_26 = to_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_106: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_15, 0.1, False);  type_as_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_32: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_106, cat_16);  dropout_106 = cat_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_184: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_32, 1, 2);  matmul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_130: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_184, [1, -1, 384]);  transpose_184 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_200: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_130, arg634_1);  view_130 = arg634_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_107: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_200, 0.1, False);  linear_200 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_141: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_54, dropout_107);  to_54 = dropout_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_56: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_141, torch.float32);  add_141 = None\n",
      "    pow_40: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_56, 2)\n",
      "    mean_39: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_40, [-1], True);  pow_40 = None\n",
      "    add_142: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_39, 1e-06);  mean_39 = None\n",
      "    rsqrt_28: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_142);  add_142 = None\n",
      "    mul_129: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_56, rsqrt_28);  rsqrt_28 = None\n",
      "    mul_130: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg639_1, mul_129);  arg639_1 = mul_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_201: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg636_1);  arg636_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_131: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_201, 0.5)\n",
      "    pow_41: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_201, 3.0)\n",
      "    mul_132: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_41, 0.044715);  pow_41 = None\n",
      "    add_143: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_201, mul_132);  linear_201 = mul_132 = None\n",
      "    mul_133: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_143, 0.7978845608028654);  add_143 = None\n",
      "    tanh_11: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_133);  mul_133 = None\n",
      "    add_144: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_11, 1.0);  tanh_11 = None\n",
      "    mul_134: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_131, add_144);  mul_131 = add_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_202: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg637_1);  mul_130 = arg637_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_135: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_134, linear_202);  mul_134 = linear_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_108: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_135, 0.1, False);  mul_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_203: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_108, arg638_1);  dropout_108 = arg638_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_109: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_203, 0.1, False);  linear_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_145: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_56, dropout_109);  to_56 = dropout_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_57: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_145, torch.float32);  add_145 = None\n",
      "    pow_42: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_57, 2)\n",
      "    mean_40: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_42, [-1], True);  pow_42 = None\n",
      "    add_146: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_40, 1e-06);  mean_40 = None\n",
      "    rsqrt_29: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_146);  add_146 = None\n",
      "    mul_136: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_57, rsqrt_29);  rsqrt_29 = None\n",
      "    mul_137: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg644_1, mul_136);  arg644_1 = mul_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_204: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg640_1);  arg640_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_131: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_204, [1, -1, 6, 64]);  linear_204 = None\n",
      "    transpose_185: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_131, 1, 2);  view_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_205: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg641_1);  arg641_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_206: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg642_1);  mul_137 = arg642_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_132: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_205, [1, -1, 6, 64]);  linear_205 = None\n",
      "    transpose_186: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_132, 1, 2);  view_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_133: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_206, [1, -1, 6, 64]);  linear_206 = None\n",
      "    transpose_187: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_133, 1, 2);  view_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant19 = self._tensor_constant19\n",
      "    lift_fresh_copy_19: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant19);  _tensor_constant19 = None\n",
      "    detach__19: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_19);  lift_fresh_copy_19 = None\n",
      "    _tensor_constant20 = self._tensor_constant20\n",
      "    lift_fresh_copy_20: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant20);  _tensor_constant20 = None\n",
      "    detach__20: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_20);  lift_fresh_copy_20 = None\n",
      "    cat_17: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__19, transpose_186], -2);  detach__19 = transpose_186 = None\n",
      "    cat_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__20, transpose_187], -2);  detach__20 = transpose_187 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_188: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_17, 3, 2);  cat_17 = None\n",
      "    matmul_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_185, transpose_188);  transpose_185 = transpose_188 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_33, add_109);  matmul_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_58: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__17, torch.float32);  add__17 = None\n",
      "    softmax_27: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_58, -1)\n",
      "    type_as_16: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_27, to_58);  softmax_27 = to_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_110: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_16, 0.1, False);  type_as_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_34: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_110, cat_18);  dropout_110 = cat_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_189: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_34, 1, 2);  matmul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_134: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_189, [1, -1, 384]);  transpose_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_207: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_134, arg643_1);  view_134 = arg643_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_111: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_207, 0.1, False);  linear_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_147: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_57, dropout_111);  to_57 = dropout_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_35: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_148: \"i64[]\" = torch.ops.aten.add.Tensor(select_35, 1);  select_35 = add_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_59: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_147, torch.float32);  add_147 = None\n",
      "    pow_43: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_59, 2)\n",
      "    mean_41: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_43, [-1], True);  pow_43 = None\n",
      "    add_149: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_41, 1e-06);  mean_41 = None\n",
      "    rsqrt_30: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_149);  add_149 = None\n",
      "    mul_138: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_59, rsqrt_30);  rsqrt_30 = None\n",
      "    mul_139: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg649_1, mul_138);  arg649_1 = mul_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_208: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_139, arg645_1);  mul_139 = arg645_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_135: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_208, [1, -1, 6, 64]);  linear_208 = None\n",
      "    transpose_190: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_135, 1, 2);  view_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_209: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg646_1);  arg646_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_210: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg647_1);  arg647_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_136: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_209, [1, -1, 6, 64]);  linear_209 = None\n",
      "    transpose_191: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_136, 1, 2);  view_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_137: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_210, [1, -1, 6, 64]);  linear_210 = None\n",
      "    transpose_192: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_137, 1, 2);  view_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant21 = self._tensor_constant21\n",
      "    lift_fresh_copy_21: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant21);  _tensor_constant21 = None\n",
      "    detach__21: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_21);  lift_fresh_copy_21 = None\n",
      "    _tensor_constant22 = self._tensor_constant22\n",
      "    lift_fresh_copy_22: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant22);  _tensor_constant22 = None\n",
      "    detach__22: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_22);  lift_fresh_copy_22 = None\n",
      "    cat_19: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__21, transpose_191], -2);  detach__21 = transpose_191 = None\n",
      "    cat_20: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__22, transpose_192], -2);  detach__22 = transpose_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_193: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_19, 3, 2);  cat_19 = None\n",
      "    matmul_35: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_190, transpose_193);  transpose_190 = transpose_193 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__18: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_35, add_113);  matmul_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_60: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__18, torch.float32);  add__18 = None\n",
      "    softmax_28: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_60, -1)\n",
      "    type_as_17: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_28, to_60);  softmax_28 = to_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_112: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_17, 0.1, False);  type_as_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_36: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_112, cat_20);  dropout_112 = cat_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_194: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_36, 1, 2);  matmul_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_138: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_194, [1, -1, 384]);  transpose_194 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_211: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_138, arg648_1);  view_138 = arg648_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_113: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_211, 0.1, False);  linear_211 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_150: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_59, dropout_113);  to_59 = dropout_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_61: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_150, torch.float32);  add_150 = None\n",
      "    pow_44: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_61, 2)\n",
      "    mean_42: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_44, [-1], True);  pow_44 = None\n",
      "    add_151: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_42, 1e-06);  mean_42 = None\n",
      "    rsqrt_31: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_151);  add_151 = None\n",
      "    mul_140: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_61, rsqrt_31);  rsqrt_31 = None\n",
      "    mul_141: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg653_1, mul_140);  arg653_1 = mul_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_212: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg650_1);  arg650_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_142: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_212, 0.5)\n",
      "    pow_45: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_212, 3.0)\n",
      "    mul_143: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_45, 0.044715);  pow_45 = None\n",
      "    add_152: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_212, mul_143);  linear_212 = mul_143 = None\n",
      "    mul_144: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_152, 0.7978845608028654);  add_152 = None\n",
      "    tanh_12: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_144);  mul_144 = None\n",
      "    add_153: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_12, 1.0);  tanh_12 = None\n",
      "    mul_145: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_142, add_153);  mul_142 = add_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_213: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg651_1);  mul_141 = arg651_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_146: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_145, linear_213);  mul_145 = linear_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_114: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_146, 0.1, False);  mul_146 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_214: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_114, arg652_1);  dropout_114 = arg652_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_115: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_214, 0.1, False);  linear_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_154: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_61, dropout_115);  to_61 = dropout_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_62: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_154, torch.float32);  add_154 = None\n",
      "    pow_46: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_62, 2)\n",
      "    mean_43: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_46, [-1], True);  pow_46 = None\n",
      "    add_155: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_43, 1e-06);  mean_43 = None\n",
      "    rsqrt_32: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_155);  add_155 = None\n",
      "    mul_147: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_62, rsqrt_32);  rsqrt_32 = None\n",
      "    mul_148: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg658_1, mul_147);  arg658_1 = mul_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_215: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg654_1);  arg654_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_139: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_215, [1, -1, 6, 64]);  linear_215 = None\n",
      "    transpose_195: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_139, 1, 2);  view_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_216: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg655_1);  arg655_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_217: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg656_1);  mul_148 = arg656_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_140: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_216, [1, -1, 6, 64]);  linear_216 = None\n",
      "    transpose_196: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_140, 1, 2);  view_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_141: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_217, [1, -1, 6, 64]);  linear_217 = None\n",
      "    transpose_197: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_141, 1, 2);  view_141 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant23 = self._tensor_constant23\n",
      "    lift_fresh_copy_23: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant23);  _tensor_constant23 = None\n",
      "    detach__23: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_23);  lift_fresh_copy_23 = None\n",
      "    _tensor_constant24 = self._tensor_constant24\n",
      "    lift_fresh_copy_24: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant24);  _tensor_constant24 = None\n",
      "    detach__24: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_24);  lift_fresh_copy_24 = None\n",
      "    cat_21: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__23, transpose_196], -2);  detach__23 = transpose_196 = None\n",
      "    cat_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__24, transpose_197], -2);  detach__24 = transpose_197 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_198: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_21, 3, 2);  cat_21 = None\n",
      "    matmul_37: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_195, transpose_198);  transpose_195 = transpose_198 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_37, add_109);  matmul_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_63: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__19, torch.float32);  add__19 = None\n",
      "    softmax_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_63, -1)\n",
      "    type_as_18: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_29, to_63);  softmax_29 = to_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_116: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_18, 0.1, False);  type_as_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_38: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_116, cat_22);  dropout_116 = cat_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_199: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_38, 1, 2);  matmul_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_142: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_199, [1, -1, 384]);  transpose_199 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_218: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_142, arg657_1);  view_142 = arg657_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_117: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_218, 0.1, False);  linear_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_156: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_62, dropout_117);  to_62 = dropout_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_36: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_157: \"i64[]\" = torch.ops.aten.add.Tensor(select_36, 1);  select_36 = add_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_64: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_156, torch.float32);  add_156 = None\n",
      "    pow_47: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_64, 2)\n",
      "    mean_44: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_47, [-1], True);  pow_47 = None\n",
      "    add_158: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_44, 1e-06);  mean_44 = None\n",
      "    rsqrt_33: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_158);  add_158 = None\n",
      "    mul_149: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_64, rsqrt_33);  rsqrt_33 = None\n",
      "    mul_150: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg663_1, mul_149);  arg663_1 = mul_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_219: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_150, arg659_1);  mul_150 = arg659_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_143: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_219, [1, -1, 6, 64]);  linear_219 = None\n",
      "    transpose_200: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_143, 1, 2);  view_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_220: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg660_1);  arg660_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_221: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg661_1);  arg661_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_220, [1, -1, 6, 64]);  linear_220 = None\n",
      "    transpose_201: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_144, 1, 2);  view_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_145: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_221, [1, -1, 6, 64]);  linear_221 = None\n",
      "    transpose_202: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_145, 1, 2);  view_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant25 = self._tensor_constant25\n",
      "    lift_fresh_copy_25: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant25);  _tensor_constant25 = None\n",
      "    detach__25: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_25);  lift_fresh_copy_25 = None\n",
      "    _tensor_constant26 = self._tensor_constant26\n",
      "    lift_fresh_copy_26: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant26);  _tensor_constant26 = None\n",
      "    detach__26: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_26);  lift_fresh_copy_26 = None\n",
      "    cat_23: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__25, transpose_201], -2);  detach__25 = transpose_201 = None\n",
      "    cat_24: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__26, transpose_202], -2);  detach__26 = transpose_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_203: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_23, 3, 2);  cat_23 = None\n",
      "    matmul_39: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_200, transpose_203);  transpose_200 = transpose_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_39, add_113);  matmul_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_65: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__20, torch.float32);  add__20 = None\n",
      "    softmax_30: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_65, -1)\n",
      "    type_as_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_30, to_65);  softmax_30 = to_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_118: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_19, 0.1, False);  type_as_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_40: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_118, cat_24);  dropout_118 = cat_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_204: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_40, 1, 2);  matmul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_146: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_204, [1, -1, 384]);  transpose_204 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_222: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_146, arg662_1);  view_146 = arg662_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_119: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_222, 0.1, False);  linear_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_159: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_64, dropout_119);  to_64 = dropout_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_66: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_159, torch.float32);  add_159 = None\n",
      "    pow_48: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_66, 2)\n",
      "    mean_45: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_48, [-1], True);  pow_48 = None\n",
      "    add_160: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_45, 1e-06);  mean_45 = None\n",
      "    rsqrt_34: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_160);  add_160 = None\n",
      "    mul_151: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_66, rsqrt_34);  rsqrt_34 = None\n",
      "    mul_152: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg667_1, mul_151);  arg667_1 = mul_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_223: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg664_1);  arg664_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_153: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_223, 0.5)\n",
      "    pow_49: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_223, 3.0)\n",
      "    mul_154: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_49, 0.044715);  pow_49 = None\n",
      "    add_161: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_223, mul_154);  linear_223 = mul_154 = None\n",
      "    mul_155: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_161, 0.7978845608028654);  add_161 = None\n",
      "    tanh_13: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_155);  mul_155 = None\n",
      "    add_162: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_13, 1.0);  tanh_13 = None\n",
      "    mul_156: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_153, add_162);  mul_153 = add_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_224: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg665_1);  mul_152 = arg665_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_157: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_156, linear_224);  mul_156 = linear_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_120: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_157, 0.1, False);  mul_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_225: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_120, arg666_1);  dropout_120 = arg666_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_121: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_225, 0.1, False);  linear_225 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_163: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_66, dropout_121);  to_66 = dropout_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_67: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_163, torch.float32);  add_163 = None\n",
      "    pow_50: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_67, 2)\n",
      "    mean_46: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_50, [-1], True);  pow_50 = None\n",
      "    add_164: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_46, 1e-06);  mean_46 = None\n",
      "    rsqrt_35: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_164);  add_164 = None\n",
      "    mul_158: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_67, rsqrt_35);  rsqrt_35 = None\n",
      "    mul_159: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg672_1, mul_158);  arg672_1 = mul_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_226: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg668_1);  arg668_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_147: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_226, [1, -1, 6, 64]);  linear_226 = None\n",
      "    transpose_205: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_147, 1, 2);  view_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_227: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg669_1);  arg669_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_228: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg670_1);  mul_159 = arg670_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_148: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_227, [1, -1, 6, 64]);  linear_227 = None\n",
      "    transpose_206: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_148, 1, 2);  view_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_228, [1, -1, 6, 64]);  linear_228 = None\n",
      "    transpose_207: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_149, 1, 2);  view_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant27 = self._tensor_constant27\n",
      "    lift_fresh_copy_27: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant27);  _tensor_constant27 = None\n",
      "    detach__27: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_27);  lift_fresh_copy_27 = None\n",
      "    _tensor_constant28 = self._tensor_constant28\n",
      "    lift_fresh_copy_28: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant28);  _tensor_constant28 = None\n",
      "    detach__28: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_28);  lift_fresh_copy_28 = None\n",
      "    cat_25: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__27, transpose_206], -2);  detach__27 = transpose_206 = None\n",
      "    cat_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__28, transpose_207], -2);  detach__28 = transpose_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_208: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_25, 3, 2);  cat_25 = None\n",
      "    matmul_41: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_205, transpose_208);  transpose_205 = transpose_208 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_41, add_109);  matmul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_68: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__21, torch.float32);  add__21 = None\n",
      "    softmax_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_68, -1)\n",
      "    type_as_20: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_31, to_68);  softmax_31 = to_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_122: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_20, 0.1, False);  type_as_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_42: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_122, cat_26);  dropout_122 = cat_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_209: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_42, 1, 2);  matmul_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_150: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_209, [1, -1, 384]);  transpose_209 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_229: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_150, arg671_1);  view_150 = arg671_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_123: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_229, 0.1, False);  linear_229 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_165: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_67, dropout_123);  to_67 = dropout_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_37: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_166: \"i64[]\" = torch.ops.aten.add.Tensor(select_37, 1);  select_37 = add_166 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_69: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_165, torch.float32);  add_165 = None\n",
      "    pow_51: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_69, 2)\n",
      "    mean_47: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_51, [-1], True);  pow_51 = None\n",
      "    add_167: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_47, 1e-06);  mean_47 = None\n",
      "    rsqrt_36: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_167);  add_167 = None\n",
      "    mul_160: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_69, rsqrt_36);  rsqrt_36 = None\n",
      "    mul_161: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg677_1, mul_160);  arg677_1 = mul_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_230: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_161, arg673_1);  mul_161 = arg673_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_151: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_230, [1, -1, 6, 64]);  linear_230 = None\n",
      "    transpose_210: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_151, 1, 2);  view_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_231: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg674_1);  arg674_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_232: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg675_1);  arg675_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_152: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_231, [1, -1, 6, 64]);  linear_231 = None\n",
      "    transpose_211: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_153: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_232, [1, -1, 6, 64]);  linear_232 = None\n",
      "    transpose_212: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_153, 1, 2);  view_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant29 = self._tensor_constant29\n",
      "    lift_fresh_copy_29: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant29);  _tensor_constant29 = None\n",
      "    detach__29: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_29);  lift_fresh_copy_29 = None\n",
      "    _tensor_constant30 = self._tensor_constant30\n",
      "    lift_fresh_copy_30: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant30);  _tensor_constant30 = None\n",
      "    detach__30: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_30);  lift_fresh_copy_30 = None\n",
      "    cat_27: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__29, transpose_211], -2);  detach__29 = transpose_211 = None\n",
      "    cat_28: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__30, transpose_212], -2);  detach__30 = transpose_212 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_213: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_27, 3, 2);  cat_27 = None\n",
      "    matmul_43: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_210, transpose_213);  transpose_210 = transpose_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_43, add_113);  matmul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_70: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__22, torch.float32);  add__22 = None\n",
      "    softmax_32: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_70, -1)\n",
      "    type_as_21: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_32, to_70);  softmax_32 = to_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_124: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_21, 0.1, False);  type_as_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_44: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_124, cat_28);  dropout_124 = cat_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_214: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_44, 1, 2);  matmul_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_154: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_214, [1, -1, 384]);  transpose_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_233: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_154, arg676_1);  view_154 = arg676_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_125: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_233, 0.1, False);  linear_233 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_168: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_69, dropout_125);  to_69 = dropout_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_71: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_168, torch.float32);  add_168 = None\n",
      "    pow_52: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_71, 2)\n",
      "    mean_48: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_52, [-1], True);  pow_52 = None\n",
      "    add_169: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_48, 1e-06);  mean_48 = None\n",
      "    rsqrt_37: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_169);  add_169 = None\n",
      "    mul_162: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_71, rsqrt_37);  rsqrt_37 = None\n",
      "    mul_163: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg681_1, mul_162);  arg681_1 = mul_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_234: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg678_1);  arg678_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_164: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_234, 0.5)\n",
      "    pow_53: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_234, 3.0)\n",
      "    mul_165: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_53, 0.044715);  pow_53 = None\n",
      "    add_170: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_234, mul_165);  linear_234 = mul_165 = None\n",
      "    mul_166: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_170, 0.7978845608028654);  add_170 = None\n",
      "    tanh_14: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_166);  mul_166 = None\n",
      "    add_171: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_14, 1.0);  tanh_14 = None\n",
      "    mul_167: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_164, add_171);  mul_164 = add_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_235: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg679_1);  mul_163 = arg679_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_168: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_167, linear_235);  mul_167 = linear_235 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_126: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_168, 0.1, False);  mul_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_236: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_126, arg680_1);  dropout_126 = arg680_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_127: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_236, 0.1, False);  linear_236 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_172: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_71, dropout_127);  to_71 = dropout_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_72: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_172, torch.float32);  add_172 = None\n",
      "    pow_54: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_72, 2)\n",
      "    mean_49: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_54, [-1], True);  pow_54 = None\n",
      "    add_173: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_49, 1e-06);  mean_49 = None\n",
      "    rsqrt_38: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_173);  add_173 = None\n",
      "    mul_169: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_72, rsqrt_38);  rsqrt_38 = None\n",
      "    mul_170: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg686_1, mul_169);  arg686_1 = mul_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_237: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg682_1);  arg682_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_155: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_237, [1, -1, 6, 64]);  linear_237 = None\n",
      "    transpose_215: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_155, 1, 2);  view_155 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_238: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg683_1);  arg683_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_239: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg684_1);  mul_170 = arg684_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_156: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_238, [1, -1, 6, 64]);  linear_238 = None\n",
      "    transpose_216: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_156, 1, 2);  view_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_157: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_239, [1, -1, 6, 64]);  linear_239 = None\n",
      "    transpose_217: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_157, 1, 2);  view_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant31 = self._tensor_constant31\n",
      "    lift_fresh_copy_31: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant31);  _tensor_constant31 = None\n",
      "    detach__31: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_31);  lift_fresh_copy_31 = None\n",
      "    _tensor_constant32 = self._tensor_constant32\n",
      "    lift_fresh_copy_32: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant32);  _tensor_constant32 = None\n",
      "    detach__32: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_32);  lift_fresh_copy_32 = None\n",
      "    cat_29: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__31, transpose_216], -2);  detach__31 = transpose_216 = None\n",
      "    cat_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__32, transpose_217], -2);  detach__32 = transpose_217 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_218: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_29, 3, 2);  cat_29 = None\n",
      "    matmul_45: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_215, transpose_218);  transpose_215 = transpose_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_45, add_109);  matmul_45 = add_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_73: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__23, torch.float32);  add__23 = None\n",
      "    softmax_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_73, -1)\n",
      "    type_as_22: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_33, to_73);  softmax_33 = to_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_128: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_22, 0.1, False);  type_as_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_46: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_128, cat_30);  dropout_128 = cat_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_219: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_46, 1, 2);  matmul_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_158: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_219, [1, -1, 384]);  transpose_219 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_240: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_158, arg685_1);  view_158 = arg685_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_129: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_240, 0.1, False);  linear_240 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_174: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_72, dropout_129);  to_72 = dropout_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_38: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_175: \"i64[]\" = torch.ops.aten.add.Tensor(select_38, 1);  select_38 = add_175 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_74: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_174, torch.float32);  add_174 = None\n",
      "    pow_55: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_74, 2)\n",
      "    mean_50: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_55, [-1], True);  pow_55 = None\n",
      "    add_176: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_50, 1e-06);  mean_50 = None\n",
      "    rsqrt_39: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_176);  add_176 = None\n",
      "    mul_171: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_74, rsqrt_39);  rsqrt_39 = None\n",
      "    mul_172: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg691_1, mul_171);  arg691_1 = mul_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_241: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_172, arg687_1);  mul_172 = arg687_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_241, [1, -1, 6, 64]);  linear_241 = None\n",
      "    transpose_220: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_159, 1, 2);  view_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_242: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg688_1);  arg688_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_243: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg689_1);  dropout_84 = arg689_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_160: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_242, [1, -1, 6, 64]);  linear_242 = None\n",
      "    transpose_221: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_160, 1, 2);  view_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_161: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_243, [1, -1, 6, 64]);  linear_243 = None\n",
      "    transpose_222: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_161, 1, 2);  view_161 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant33 = self._tensor_constant33\n",
      "    lift_fresh_copy_33: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant33);  _tensor_constant33 = None\n",
      "    detach__33: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_33);  lift_fresh_copy_33 = None\n",
      "    _tensor_constant34 = self._tensor_constant34\n",
      "    lift_fresh_copy_34: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant34);  _tensor_constant34 = None\n",
      "    detach__34: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_34);  lift_fresh_copy_34 = None\n",
      "    cat_31: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__33, transpose_221], -2);  detach__33 = transpose_221 = None\n",
      "    cat_32: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__34, transpose_222], -2);  detach__34 = transpose_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_223: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_31, 3, 2);  cat_31 = None\n",
      "    matmul_47: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_220, transpose_223);  transpose_220 = transpose_223 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_47, add_113);  matmul_47 = add_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_75: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__24, torch.float32);  add__24 = None\n",
      "    softmax_34: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_75, -1)\n",
      "    type_as_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_34, to_75);  softmax_34 = to_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_130: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_23, 0.1, False);  type_as_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_48: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_130, cat_32);  dropout_130 = cat_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_224: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_48, 1, 2);  matmul_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_162: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_224, [1, -1, 384]);  transpose_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_244: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_162, arg690_1);  view_162 = arg690_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_131: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_244, 0.1, False);  linear_244 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_177: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_74, dropout_131);  to_74 = dropout_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_76: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_177, torch.float32);  add_177 = None\n",
      "    pow_56: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_76, 2)\n",
      "    mean_51: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_56, [-1], True);  pow_56 = None\n",
      "    add_178: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_51, 1e-06);  mean_51 = None\n",
      "    rsqrt_40: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_178);  add_178 = None\n",
      "    mul_173: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_76, rsqrt_40);  rsqrt_40 = None\n",
      "    mul_174: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg695_1, mul_173);  arg695_1 = mul_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_245: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg692_1);  arg692_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_175: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_245, 0.5)\n",
      "    pow_57: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_245, 3.0)\n",
      "    mul_176: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_57, 0.044715);  pow_57 = None\n",
      "    add_179: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_245, mul_176);  linear_245 = mul_176 = None\n",
      "    mul_177: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_179, 0.7978845608028654);  add_179 = None\n",
      "    tanh_15: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_177);  mul_177 = None\n",
      "    add_180: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_15, 1.0);  tanh_15 = None\n",
      "    mul_178: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_175, add_180);  mul_175 = add_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_246: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg693_1);  mul_174 = arg693_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_179: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_178, linear_246);  mul_178 = linear_246 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_132: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_179, 0.1, False);  mul_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_247: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_132, arg694_1);  dropout_132 = arg694_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_133: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_247, 0.1, False);  linear_247 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_181: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_76, dropout_133);  to_76 = dropout_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_77: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_181, torch.float32);  add_181 = None\n",
      "    pow_58: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_77, 2)\n",
      "    mean_52: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_58, [-1], True);  pow_58 = None\n",
      "    add_182: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_52, 1e-06);  mean_52 = None\n",
      "    rsqrt_41: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_182);  add_182 = None\n",
      "    mul_180: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_77, rsqrt_41);  to_77 = rsqrt_41 = None\n",
      "    mul_181: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg696_1, mul_180);  arg696_1 = mul_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_134: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(mul_181, 0.1, False);  mul_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_248: \"f32[1, 1, 32128]\" = torch.ops.aten.linear.default(dropout_134, arg697_1);  dropout_134 = arg697_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    slice_61: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, -1);  slice_41 = None\n",
      "    add_183: \"i64[1]\" = torch.ops.aten.add.Tensor(slice_61, 1);  slice_61 = add_183 = None\n",
      "    slice_62: \"f32[1, 1, 32128]\" = torch.ops.aten.slice.Tensor(linear_248);  linear_248 = None\n",
      "    select_39: \"f32[1, 32128]\" = torch.ops.aten.select.int(slice_62, 1, -1);  slice_62 = None\n",
      "    slice_63: \"f32[1, 32128]\" = torch.ops.aten.slice.Tensor(select_39, 1);  select_39 = None\n",
      "    to_78: \"f32[1, 32128]\" = torch.ops.aten.to.device(slice_63, device(type='cpu'), torch.float32, False, True);  slice_63 = None\n",
      "    argmax: \"i64[1]\" = torch.ops.aten.argmax.default(to_78, -1);  to_78 = None\n",
      "    mul_182: \"i64[1]\" = torch.ops.aten.mul.Tensor(argmax, ones_3);  argmax = None\n",
      "    rsub_2: \"i64[1]\" = torch.ops.aten.rsub.Scalar(ones_3, 1)\n",
      "    mul_183: \"i64[1]\" = torch.ops.aten.mul.Tensor(detach__1, rsub_2);  detach__1 = rsub_2 = None\n",
      "    add_184: \"i64[1]\" = torch.ops.aten.add.Tensor(mul_182, mul_183);  mul_182 = mul_183 = None\n",
      "    slice_64: \"i64[1]\" = torch.ops.aten.slice.Tensor(add_184, 0, 0, 9223372036854775807);  add_184 = None\n",
      "    unsqueeze_32: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_64, 1);  slice_64 = None\n",
      "    cat_33: \"i64[1, 2]\" = torch.ops.aten.cat.default([mul_89, unsqueeze_32], -1);  mul_89 = unsqueeze_32 = None\n",
      "    full_1: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    full_2: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    or_1: \"b8[1]\" = torch.ops.aten.__or__.Tensor(full_1, full_2);  full_1 = full_2 = None\n",
      "    to_79: \"i64[1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_24, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_24 = None\n",
      "    slice_65: \"i64[1, 2]\" = torch.ops.aten.slice.Tensor(cat_33);  cat_33 = None\n",
      "    select_40: \"i64[1]\" = torch.ops.aten.select.int(slice_65, 1, -1);  slice_65 = None\n",
      "    isin_1: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(select_40, to_79);  select_40 = to_79 = None\n",
      "    or_2: \"b8[1]\" = torch.ops.aten.__or__.Tensor(or_1, isin_1);  or_1 = isin_1 = None\n",
      "    bitwise_not: \"b8[1]\" = torch.ops.aten.bitwise_not.default(or_2);  or_2 = None\n",
      "    and_1: \"i64[1]\" = torch.ops.aten.__and__.Tensor(ones_3, bitwise_not);  ones_3 = bitwise_not = None\n",
      "    max_1: \"i64[]\" = torch.ops.aten.max.default(and_1);  and_1 = None\n",
      "    eq: \"b8[]\" = torch.ops.aten.eq.Scalar(max_1, 0);  max_1 = None\n",
      "    ne_2: \"b8[]\" = torch.ops.aten.ne.Scalar(eq, 0);  eq = None\n",
      "    item_2: \"Sym(Eq(u1, 1))\" = torch.ops.aten.item.default(ne_2);  ne_2 = item_2 = None\n",
      "    _set_grad_enabled_3 = torch._C._set_grad_enabled(False);  _set_grad_enabled_3 = None\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Blip2Model([...]` with `torch.export.export(..., strict=False)`... \n",
      "[torch.onnx] Obtain model graph for `Blip2Model([...]` with `torch.export.export(..., strict=True)`...\n",
      "[torch.onnx] Obtain model graph for `Blip2Model([...]` with `torch.export.export(..., strict=True)`... \n",
      "[torch.onnx] Obtain model graph for `Blip2Model([...]` with `torch.export draft_export`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 13:14:40.745000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u1, 1)) -> 0\n",
      "W0914 13:14:41.817000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u2, 1)) -> 0\n",
      "W0914 13:14:42.888000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u3, 1)) -> 0\n",
      "W0914 13:14:43.957000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u4, 1)) -> 0\n",
      "W0914 13:14:45.020000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u5, 1)) -> 0\n",
      "W0914 13:14:46.456000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u6, 1)) -> 0\n",
      "W0914 13:14:47.519000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u7, 1)) -> 0\n",
      "W0914 13:14:48.595000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u8, 1)) -> 0\n",
      "W0914 13:14:49.669000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u9, 1)) -> 0\n",
      "W0914 13:14:50.743000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u10, 1)) -> 0\n",
      "W0914 13:14:51.818000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u11, 1)) -> 0\n",
      "W0914 13:14:52.887000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u12, 1)) -> 0\n",
      "W0914 13:14:53.975000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u13, 1)) -> 0\n",
      "W0914 13:14:55.067000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u14, 1)) -> 0\n",
      "W0914 13:14:56.148000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u15, 1)) -> 0\n",
      "W0914 13:14:57.234000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u16, 1)) -> 0\n",
      "W0914 13:14:58.314000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u17, 1)) -> 0\n",
      "W0914 13:14:59.410000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u18, 1)) -> 0\n",
      "W0914 13:15:00.491000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u19, 1)) -> 0\n",
      "W0914 13:15:01.562000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u20, 1)) -> 0\n",
      "W0914 13:15:02.642000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u21, 1)) -> 0\n",
      "W0914 13:15:03.719000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u22, 1)) -> 0\n",
      "W0914 13:15:04.805000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u23, 1)) -> 0\n",
      "W0914 13:15:05.886000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u24, 1)) -> 0\n",
      "W0914 13:15:06.971000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u25, 1)) -> 0\n",
      "W0914 13:15:08.447000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u26, 1)) -> 0\n",
      "W0914 13:15:09.529000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u27, 1)) -> 0\n",
      "W0914 13:15:10.600000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u28, 1)) -> 0\n",
      "W0914 13:15:11.687000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u29, 1)) -> 0\n",
      "W0914 13:15:12.773000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u30, 1)) -> 1\n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497] \n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497] ###################################################################################################\n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497] WARNING: 1 issue(s) found during export, and it was not able to soundly produce a graph.\n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497] To view the report of failures in an html page, please run the command:\n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497]     `tlparse /tmp/export_ubuntu/dedicated_log_torch_trace_wnh6ilrz.log --export`\n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497] Or, you can view the errors in python by inspecting `print(ep._report)`.\n",
      "W0914 13:15:26.413000 1989 torch/export/_draft_export.py:497] #################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Draft Export report:\n",
      "\u001b[93m\n",
      "###################################################################################################\n",
      "WARNING: 1 issue(s) found during export, and it was not able to soundly produce a graph.\n",
      "Please follow the instructions to fix the errors.\n",
      "###################################################################################################\n",
      "\n",
      "1. Data dependent error.\n",
      "    When exporting, we were unable to evaluate the value of `Eq(u1, 1)`.\n",
      "    This was encountered 30 times.\n",
      "    This occurred at the following user stacktrace: \n",
      "        File /usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py, lineno 2539, in generate\n",
      "        File /usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py, lineno 2858, in _sample\n",
      "        File /usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py, lineno 2676, in _has_unfinished_sequences\n",
      "            elif this_peer_finished:\n",
      "        \n",
      "        Locals:\n",
      "            this_peer_finished: ['Tensor(shape: torch.Size([]), stride: (), storage_offset: 0)']\n",
      "\n",
      "    And the following framework stacktrace: \n",
      "        File /usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py, lineno 1327, in __torch_function__\n",
      "        File /usr/local/lib/python3.12/dist-packages/torch/fx/experimental/proxy_tensor.py, lineno 1374, in __torch_function__\n",
      "            return func(*args, **kwargs)\n",
      "\n",
      "    As a result, it was specialized to a constant (e.g. `0` in the 1st occurrence), and asserts were inserted into the graph.\n",
      "\n",
      "    Please add `torch._check(...)` to the original code to assert this data-dependent assumption.\n",
      "    Please refer to https://docs.google.com/document/d/1kZ_BbB3JnoLbUZleDT6635dHs88ZVYId8jT-yTFgf3A/edit#heading=h.boi2xurpqa0o for more details.\n",
      "\n",
      "\u001b[0m\n",
      "[torch.onnx] Obtain model graph for `Blip2Model([...]` with `torch.export draft_export`... \n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... \n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "/usr/local/lib/python3.12/dist-packages/onnx/reference/ops/op_log.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  return (np.log(x).astype(x.dtype),)\n",
      "/usr/local/lib/python3.12/dist-packages/onnx/reference/ops/op_cast.py:149: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 404 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ONNX model saved & validated as model.onnx\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Load model from model.onnx failed:Node (node_Concat_4464) Op (Concat) [ShapeInferenceError] axis must be in [-rank, rank-1].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFail\u001b[39m                                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m ONNX model saved & validated as model.onnx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# ----- 4) Quick ORT inference test -----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m sess = \u001b[43mort\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m out = sess.run(\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     37\u001b[39m     {\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     },\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mORT outputs:\u001b[39m\u001b[33m\"\u001b[39m, [np.asarray(o).shape \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:472\u001b[39m, in \u001b[36mInferenceSession.__init__\u001b[39m\u001b[34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m disabled_optimizers = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdisabled_optimizers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:550\u001b[39m, in \u001b[36mInferenceSession._create_inference_session\u001b[39m\u001b[34m(self, providers, provider_options, disabled_optimizers)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28mself\u001b[39m._register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_path:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     sess = \u001b[43mC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    552\u001b[39m     sess = C.InferenceSession(session_options, \u001b[38;5;28mself\u001b[39m._model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._read_config_from_model)\n",
      "\u001b[31mFail\u001b[39m: [ONNXRuntimeError] : 1 : FAIL : Load model from model.onnx failed:Node (node_Concat_4464) Op (Concat) [ShapeInferenceError] axis must be in [-rank, rank-1]."
     ]
    }
   ],
   "source": [
    "import os, torch, onnx, onnxruntime as ort, numpy as np\n",
    "\n",
    "# Silence HF tokenizers parallelism warning (safe)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ----- 1) Prep model & dummy inputs -----\n",
    "model.cpu().eval()\n",
    "with torch.no_grad():\n",
    "    B, C, H, W, S = 1, 3, 224, 224, 77\n",
    "    img = torch.randn(B, C, H, W, dtype=torch.float32)          # float32\n",
    "    input_token = torch.zeros(B, S, dtype=torch.long)           # int64\n",
    "    enc_mask = torch.ones(B, S, dtype=torch.long)               # int64 (or bool)\n",
    "    dummy_input_size = torch.tensor(S, dtype=torch.int64)       # pass as tensor (safer for dynamo)\n",
    "\n",
    "    # ----- 2) Export with dynamo path -----\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (img, input_token, enc_mask, dummy_input_size),\n",
    "        \"model.onnx\",\n",
    "        opset_version=18,          # 18 is robust\n",
    "        dynamo=True,               # new exporter\n",
    "        export_params=True,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"img\", \"input_token\", \"enc_mask\", \"dummy_input_size\"],\n",
    "        output_names=[\"output\"],   # add more names if your model returns multiple tensors\n",
    "        # NOTE: don't pass dynamic_axes with dynamo=True\n",
    "    )\n",
    "\n",
    "# ----- 3) Sanity-check the ONNX -----\n",
    "onnx.checker.check_model(\"model.onnx\")\n",
    "print(\" ONNX model saved & validated as model.onnx\")\n",
    "\n",
    "# ----- 4) Quick ORT inference test -----\n",
    "sess = ort.InferenceSession(\"model.onnx\")\n",
    "out = sess.run(\n",
    "    None,\n",
    "    {\n",
    "        \"img\": np.random.randn(1, 3, 224, 224).astype(np.float32),\n",
    "        \"input_token\": np.zeros((1, S), dtype=np.int64),\n",
    "        \"enc_mask\": np.ones((1, S), dtype=np.int64),\n",
    "        \"dummy_input_size\": np.array(S, dtype=np.int64),  # scalar input to ORT\n",
    "    },\n",
    ")\n",
    "print(\"ORT outputs:\", [np.asarray(o).shape for o in out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model.train()  - 100.8 ms\n",
    "model.eval()   - 80.9 ms\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ExportWrapper([...]` with `torch.export.export(..., strict=False)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 13:25:47.090000 1989 torch/fx/experimental/symbolic_shapes.py:7903] Unable to find user code corresponding to {u0}\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[1, 1, 192]\", arg1_1: \"f32[1, 197, 192]\", arg2_1: \"f32[192, 3, 16, 16]\", arg3_1: \"f32[192]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192]\", arg6_1: \"f32[576, 192]\", arg7_1: \"f32[576]\", arg8_1: \"f32[192, 192]\", arg9_1: \"f32[192]\", arg10_1: \"f32[192]\", arg11_1: \"f32[192]\", arg12_1: \"f32[768, 192]\", arg13_1: \"f32[768]\", arg14_1: \"f32[192, 768]\", arg15_1: \"f32[192]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192]\", arg18_1: \"f32[576, 192]\", arg19_1: \"f32[576]\", arg20_1: \"f32[192, 192]\", arg21_1: \"f32[192]\", arg22_1: \"f32[192]\", arg23_1: \"f32[192]\", arg24_1: \"f32[768, 192]\", arg25_1: \"f32[768]\", arg26_1: \"f32[192, 768]\", arg27_1: \"f32[192]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192]\", arg30_1: \"f32[576, 192]\", arg31_1: \"f32[576]\", arg32_1: \"f32[192, 192]\", arg33_1: \"f32[192]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192]\", arg36_1: \"f32[768, 192]\", arg37_1: \"f32[768]\", arg38_1: \"f32[192, 768]\", arg39_1: \"f32[192]\", arg40_1: \"f32[192]\", arg41_1: \"f32[192]\", arg42_1: \"f32[576, 192]\", arg43_1: \"f32[576]\", arg44_1: \"f32[192, 192]\", arg45_1: \"f32[192]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192]\", arg48_1: \"f32[768, 192]\", arg49_1: \"f32[768]\", arg50_1: \"f32[192, 768]\", arg51_1: \"f32[192]\", arg52_1: \"f32[192]\", arg53_1: \"f32[192]\", arg54_1: \"f32[576, 192]\", arg55_1: \"f32[576]\", arg56_1: \"f32[192, 192]\", arg57_1: \"f32[192]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192]\", arg60_1: \"f32[768, 192]\", arg61_1: \"f32[768]\", arg62_1: \"f32[192, 768]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[576, 192]\", arg67_1: \"f32[576]\", arg68_1: \"f32[192, 192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[768, 192]\", arg73_1: \"f32[768]\", arg74_1: \"f32[192, 768]\", arg75_1: \"f32[192]\", arg76_1: \"f32[192]\", arg77_1: \"f32[192]\", arg78_1: \"f32[576, 192]\", arg79_1: \"f32[576]\", arg80_1: \"f32[192, 192]\", arg81_1: \"f32[192]\", arg82_1: \"f32[192]\", arg83_1: \"f32[192]\", arg84_1: \"f32[768, 192]\", arg85_1: \"f32[768]\", arg86_1: \"f32[192, 768]\", arg87_1: \"f32[192]\", arg88_1: \"f32[192]\", arg89_1: \"f32[192]\", arg90_1: \"f32[576, 192]\", arg91_1: \"f32[576]\", arg92_1: \"f32[192, 192]\", arg93_1: \"f32[192]\", arg94_1: \"f32[192]\", arg95_1: \"f32[192]\", arg96_1: \"f32[768, 192]\", arg97_1: \"f32[768]\", arg98_1: \"f32[192, 768]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[576, 192]\", arg103_1: \"f32[576]\", arg104_1: \"f32[192, 192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[768, 192]\", arg109_1: \"f32[768]\", arg110_1: \"f32[192, 768]\", arg111_1: \"f32[192]\", arg112_1: \"f32[192]\", arg113_1: \"f32[192]\", arg114_1: \"f32[576, 192]\", arg115_1: \"f32[576]\", arg116_1: \"f32[192, 192]\", arg117_1: \"f32[192]\", arg118_1: \"f32[192]\", arg119_1: \"f32[192]\", arg120_1: \"f32[768, 192]\", arg121_1: \"f32[768]\", arg122_1: \"f32[192, 768]\", arg123_1: \"f32[192]\", arg124_1: \"f32[192]\", arg125_1: \"f32[192]\", arg126_1: \"f32[576, 192]\", arg127_1: \"f32[576]\", arg128_1: \"f32[192, 192]\", arg129_1: \"f32[192]\", arg130_1: \"f32[192]\", arg131_1: \"f32[192]\", arg132_1: \"f32[768, 192]\", arg133_1: \"f32[768]\", arg134_1: \"f32[192, 768]\", arg135_1: \"f32[192]\", arg136_1: \"f32[192]\", arg137_1: \"f32[192]\", arg138_1: \"f32[576, 192]\", arg139_1: \"f32[576]\", arg140_1: \"f32[192, 192]\", arg141_1: \"f32[192]\", arg142_1: \"f32[192]\", arg143_1: \"f32[192]\", arg144_1: \"f32[768, 192]\", arg145_1: \"f32[768]\", arg146_1: \"f32[192, 768]\", arg147_1: \"f32[192]\", arg148_1: \"f32[192]\", arg149_1: \"f32[192]\", arg150_1: \"f32[768, 192]\", arg151_1: \"f32[768]\", arg152_1: \"f32[32, 768]\", arg153_1: \"f32[30522, 768]\", arg154_1: \"f32[512, 768]\", arg155_1: \"f32[2, 768]\", arg156_1: \"f32[768]\", arg157_1: \"f32[768]\", arg158_1: \"f32[768, 768]\", arg159_1: \"f32[768]\", arg160_1: \"f32[768, 768]\", arg161_1: \"f32[768]\", arg162_1: \"f32[768, 768]\", arg163_1: \"f32[768]\", arg164_1: \"f32[768, 768]\", arg165_1: \"f32[768]\", arg166_1: \"f32[768]\", arg167_1: \"f32[768]\", arg168_1: \"f32[3072, 768]\", arg169_1: \"f32[3072]\", arg170_1: \"f32[768, 3072]\", arg171_1: \"f32[768]\", arg172_1: \"f32[768]\", arg173_1: \"f32[768]\", arg174_1: \"f32[768, 768]\", arg175_1: \"f32[768]\", arg176_1: \"f32[768, 768]\", arg177_1: \"f32[768]\", arg178_1: \"f32[768, 768]\", arg179_1: \"f32[768]\", arg180_1: \"f32[768, 768]\", arg181_1: \"f32[768]\", arg182_1: \"f32[768]\", arg183_1: \"f32[768]\", arg184_1: \"f32[3072, 768]\", arg185_1: \"f32[3072]\", arg186_1: \"f32[768, 3072]\", arg187_1: \"f32[768]\", arg188_1: \"f32[768]\", arg189_1: \"f32[768]\", arg190_1: \"f32[768, 768]\", arg191_1: \"f32[768]\", arg192_1: \"f32[768, 768]\", arg193_1: \"f32[768]\", arg194_1: \"f32[768, 768]\", arg195_1: \"f32[768]\", arg196_1: \"f32[768, 768]\", arg197_1: \"f32[768]\", arg198_1: \"f32[768]\", arg199_1: \"f32[768]\", arg200_1: \"f32[3072, 768]\", arg201_1: \"f32[3072]\", arg202_1: \"f32[768, 3072]\", arg203_1: \"f32[768]\", arg204_1: \"f32[768]\", arg205_1: \"f32[768]\", arg206_1: \"f32[768, 768]\", arg207_1: \"f32[768]\", arg208_1: \"f32[768, 768]\", arg209_1: \"f32[768]\", arg210_1: \"f32[768, 768]\", arg211_1: \"f32[768]\", arg212_1: \"f32[768, 768]\", arg213_1: \"f32[768]\", arg214_1: \"f32[768]\", arg215_1: \"f32[768]\", arg216_1: \"f32[3072, 768]\", arg217_1: \"f32[3072]\", arg218_1: \"f32[768, 3072]\", arg219_1: \"f32[768]\", arg220_1: \"f32[768]\", arg221_1: \"f32[768]\", arg222_1: \"f32[768, 768]\", arg223_1: \"f32[768]\", arg224_1: \"f32[768, 768]\", arg225_1: \"f32[768]\", arg226_1: \"f32[768, 768]\", arg227_1: \"f32[768]\", arg228_1: \"f32[768, 768]\", arg229_1: \"f32[768]\", arg230_1: \"f32[768]\", arg231_1: \"f32[768]\", arg232_1: \"f32[3072, 768]\", arg233_1: \"f32[3072]\", arg234_1: \"f32[768, 3072]\", arg235_1: \"f32[768]\", arg236_1: \"f32[768]\", arg237_1: \"f32[768]\", arg238_1: \"f32[768, 768]\", arg239_1: \"f32[768]\", arg240_1: \"f32[768, 768]\", arg241_1: \"f32[768]\", arg242_1: \"f32[768, 768]\", arg243_1: \"f32[768]\", arg244_1: \"f32[768, 768]\", arg245_1: \"f32[768]\", arg246_1: \"f32[768]\", arg247_1: \"f32[768]\", arg248_1: \"f32[3072, 768]\", arg249_1: \"f32[3072]\", arg250_1: \"f32[768, 3072]\", arg251_1: \"f32[768]\", arg252_1: \"f32[768]\", arg253_1: \"f32[768]\", arg254_1: \"f32[768, 768]\", arg255_1: \"f32[768]\", arg256_1: \"f32[768, 768]\", arg257_1: \"f32[768]\", arg258_1: \"f32[768, 768]\", arg259_1: \"f32[768]\", arg260_1: \"f32[768, 768]\", arg261_1: \"f32[768]\", arg262_1: \"f32[768]\", arg263_1: \"f32[768]\", arg264_1: \"f32[3072, 768]\", arg265_1: \"f32[3072]\", arg266_1: \"f32[768, 3072]\", arg267_1: \"f32[768]\", arg268_1: \"f32[768]\", arg269_1: \"f32[768]\", arg270_1: \"f32[768, 768]\", arg271_1: \"f32[768]\", arg272_1: \"f32[768, 768]\", arg273_1: \"f32[768]\", arg274_1: \"f32[768, 768]\", arg275_1: \"f32[768]\", arg276_1: \"f32[768, 768]\", arg277_1: \"f32[768]\", arg278_1: \"f32[768]\", arg279_1: \"f32[768]\", arg280_1: \"f32[3072, 768]\", arg281_1: \"f32[3072]\", arg282_1: \"f32[768, 3072]\", arg283_1: \"f32[768]\", arg284_1: \"f32[768]\", arg285_1: \"f32[768]\", arg286_1: \"f32[768, 768]\", arg287_1: \"f32[768]\", arg288_1: \"f32[768, 768]\", arg289_1: \"f32[768]\", arg290_1: \"f32[768, 768]\", arg291_1: \"f32[768]\", arg292_1: \"f32[768, 768]\", arg293_1: \"f32[768]\", arg294_1: \"f32[768]\", arg295_1: \"f32[768]\", arg296_1: \"f32[3072, 768]\", arg297_1: \"f32[3072]\", arg298_1: \"f32[768, 3072]\", arg299_1: \"f32[768]\", arg300_1: \"f32[768]\", arg301_1: \"f32[768]\", arg302_1: \"f32[768, 768]\", arg303_1: \"f32[768]\", arg304_1: \"f32[768, 768]\", arg305_1: \"f32[768]\", arg306_1: \"f32[768, 768]\", arg307_1: \"f32[768]\", arg308_1: \"f32[768, 768]\", arg309_1: \"f32[768]\", arg310_1: \"f32[768]\", arg311_1: \"f32[768]\", arg312_1: \"f32[3072, 768]\", arg313_1: \"f32[3072]\", arg314_1: \"f32[768, 3072]\", arg315_1: \"f32[768]\", arg316_1: \"f32[768]\", arg317_1: \"f32[768]\", arg318_1: \"f32[768, 768]\", arg319_1: \"f32[768]\", arg320_1: \"f32[768, 768]\", arg321_1: \"f32[768]\", arg322_1: \"f32[768, 768]\", arg323_1: \"f32[768]\", arg324_1: \"f32[768, 768]\", arg325_1: \"f32[768]\", arg326_1: \"f32[768]\", arg327_1: \"f32[768]\", arg328_1: \"f32[3072, 768]\", arg329_1: \"f32[3072]\", arg330_1: \"f32[768, 3072]\", arg331_1: \"f32[768]\", arg332_1: \"f32[768]\", arg333_1: \"f32[768]\", arg334_1: \"f32[768, 768]\", arg335_1: \"f32[768]\", arg336_1: \"f32[768, 768]\", arg337_1: \"f32[768]\", arg338_1: \"f32[768, 768]\", arg339_1: \"f32[768]\", arg340_1: \"f32[768, 768]\", arg341_1: \"f32[768]\", arg342_1: \"f32[768]\", arg343_1: \"f32[768]\", arg344_1: \"f32[3072, 768]\", arg345_1: \"f32[3072]\", arg346_1: \"f32[768, 3072]\", arg347_1: \"f32[768]\", arg348_1: \"f32[768]\", arg349_1: \"f32[768]\", arg350_1: \"f32[768, 768]\", arg351_1: \"f32[768]\", arg352_1: \"f32[2304, 768]\", arg353_1: \"f32[2304]\", arg354_1: \"f32[768, 768]\", arg355_1: \"f32[768]\", arg356_1: \"f32[768]\", arg357_1: \"f32[768]\", arg358_1: \"f32[3072, 768]\", arg359_1: \"f32[3072]\", arg360_1: \"f32[768, 3072]\", arg361_1: \"f32[768]\", arg362_1: \"f32[768]\", arg363_1: \"f32[768]\", arg364_1: \"f32[3072, 768]\", arg365_1: \"f32[3072]\", arg366_1: \"f32[768, 3072]\", arg367_1: \"f32[768]\", arg368_1: \"f32[768]\", arg369_1: \"f32[768]\", arg370_1: \"f32[2304, 768]\", arg371_1: \"f32[2304]\", arg372_1: \"f32[768, 768]\", arg373_1: \"f32[768]\", arg374_1: \"f32[768]\", arg375_1: \"f32[768]\", arg376_1: \"f32[2304, 768]\", arg377_1: \"f32[2304]\", arg378_1: \"f32[768, 768]\", arg379_1: \"f32[768]\", arg380_1: \"f32[768]\", arg381_1: \"f32[768]\", arg382_1: \"f32[3072, 768]\", arg383_1: \"f32[3072]\", arg384_1: \"f32[768, 3072]\", arg385_1: \"f32[768]\", arg386_1: \"f32[768]\", arg387_1: \"f32[768]\", arg388_1: \"f32[3072, 768]\", arg389_1: \"f32[3072]\", arg390_1: \"f32[768, 3072]\", arg391_1: \"f32[768]\", arg392_1: \"f32[768]\", arg393_1: \"f32[768]\", arg394_1: \"f32[2304, 768]\", arg395_1: \"f32[2304]\", arg396_1: \"f32[768, 768]\", arg397_1: \"f32[768]\", arg398_1: \"f32[768]\", arg399_1: \"f32[768]\", arg400_1: \"f32[3072, 768]\", arg401_1: \"f32[3072]\", arg402_1: \"f32[768, 3072]\", arg403_1: \"f32[768]\", arg404_1: \"f32[768]\", arg405_1: \"f32[768]\", arg406_1: \"f32[3072, 768]\", arg407_1: \"f32[3072]\", arg408_1: \"f32[768, 3072]\", arg409_1: \"f32[768]\", arg410_1: \"f32[768]\", arg411_1: \"f32[768]\", arg412_1: \"f32[2304, 768]\", arg413_1: \"f32[2304]\", arg414_1: \"f32[768, 768]\", arg415_1: \"f32[768]\", arg416_1: \"f32[768]\", arg417_1: \"f32[768]\", arg418_1: \"f32[2304, 768]\", arg419_1: \"f32[2304]\", arg420_1: \"f32[768, 768]\", arg421_1: \"f32[768]\", arg422_1: \"f32[768]\", arg423_1: \"f32[768]\", arg424_1: \"f32[3072, 768]\", arg425_1: \"f32[3072]\", arg426_1: \"f32[768, 3072]\", arg427_1: \"f32[768]\", arg428_1: \"f32[768]\", arg429_1: \"f32[768]\", arg430_1: \"f32[3072, 768]\", arg431_1: \"f32[3072]\", arg432_1: \"f32[768, 3072]\", arg433_1: \"f32[768]\", arg434_1: \"f32[768]\", arg435_1: \"f32[768]\", arg436_1: \"f32[2304, 768]\", arg437_1: \"f32[2304]\", arg438_1: \"f32[768, 768]\", arg439_1: \"f32[768]\", arg440_1: \"f32[768]\", arg441_1: \"f32[768]\", arg442_1: \"f32[3072, 768]\", arg443_1: \"f32[3072]\", arg444_1: \"f32[768, 3072]\", arg445_1: \"f32[768]\", arg446_1: \"f32[768]\", arg447_1: \"f32[768]\", arg448_1: \"f32[3072, 768]\", arg449_1: \"f32[3072]\", arg450_1: \"f32[768, 3072]\", arg451_1: \"f32[768]\", arg452_1: \"f32[768]\", arg453_1: \"f32[768]\", arg454_1: \"f32[2304, 768]\", arg455_1: \"f32[2304]\", arg456_1: \"f32[768, 768]\", arg457_1: \"f32[768]\", arg458_1: \"f32[768]\", arg459_1: \"f32[768]\", arg460_1: \"f32[2304, 768]\", arg461_1: \"f32[2304]\", arg462_1: \"f32[768, 768]\", arg463_1: \"f32[768]\", arg464_1: \"f32[768]\", arg465_1: \"f32[768]\", arg466_1: \"f32[3072, 768]\", arg467_1: \"f32[3072]\", arg468_1: \"f32[768, 3072]\", arg469_1: \"f32[768]\", arg470_1: \"f32[768]\", arg471_1: \"f32[768]\", arg472_1: \"f32[3072, 768]\", arg473_1: \"f32[3072]\", arg474_1: \"f32[768, 3072]\", arg475_1: \"f32[768]\", arg476_1: \"f32[768]\", arg477_1: \"f32[768]\", arg478_1: \"f32[2304, 768]\", arg479_1: \"f32[2304]\", arg480_1: \"f32[768, 768]\", arg481_1: \"f32[768]\", arg482_1: \"f32[768]\", arg483_1: \"f32[768]\", arg484_1: \"f32[3072, 768]\", arg485_1: \"f32[3072]\", arg486_1: \"f32[768, 3072]\", arg487_1: \"f32[768]\", arg488_1: \"f32[768]\", arg489_1: \"f32[768]\", arg490_1: \"f32[3072, 768]\", arg491_1: \"f32[3072]\", arg492_1: \"f32[768, 3072]\", arg493_1: \"f32[768]\", arg494_1: \"f32[768]\", arg495_1: \"f32[768]\", arg496_1: \"f32[2304, 768]\", arg497_1: \"f32[2304]\", arg498_1: \"f32[768, 768]\", arg499_1: \"f32[768]\", arg500_1: \"f32[768]\", arg501_1: \"f32[768]\", arg502_1: \"f32[30522, 768]\", arg503_1: \"f32[77, 768]\", arg504_1: \"f32[512, 768]\", arg505_1: \"f32[512]\", arg506_1: \"f32[32128, 512]\", arg507_1: \"f32[32128, 512]\", arg508_1: \"f32[384, 512]\", arg509_1: \"f32[384, 512]\", arg510_1: \"f32[384, 512]\", arg511_1: \"f32[512, 384]\", arg512_1: \"f32[32, 6]\", arg513_1: \"f32[512]\", arg514_1: \"f32[1024, 512]\", arg515_1: \"f32[1024, 512]\", arg516_1: \"f32[512, 1024]\", arg517_1: \"f32[512]\", arg518_1: \"f32[384, 512]\", arg519_1: \"f32[384, 512]\", arg520_1: \"f32[384, 512]\", arg521_1: \"f32[512, 384]\", arg522_1: \"f32[512]\", arg523_1: \"f32[1024, 512]\", arg524_1: \"f32[1024, 512]\", arg525_1: \"f32[512, 1024]\", arg526_1: \"f32[512]\", arg527_1: \"f32[384, 512]\", arg528_1: \"f32[384, 512]\", arg529_1: \"f32[384, 512]\", arg530_1: \"f32[512, 384]\", arg531_1: \"f32[512]\", arg532_1: \"f32[1024, 512]\", arg533_1: \"f32[1024, 512]\", arg534_1: \"f32[512, 1024]\", arg535_1: \"f32[512]\", arg536_1: \"f32[384, 512]\", arg537_1: \"f32[384, 512]\", arg538_1: \"f32[384, 512]\", arg539_1: \"f32[512, 384]\", arg540_1: \"f32[512]\", arg541_1: \"f32[1024, 512]\", arg542_1: \"f32[1024, 512]\", arg543_1: \"f32[512, 1024]\", arg544_1: \"f32[512]\", arg545_1: \"f32[384, 512]\", arg546_1: \"f32[384, 512]\", arg547_1: \"f32[384, 512]\", arg548_1: \"f32[512, 384]\", arg549_1: \"f32[512]\", arg550_1: \"f32[1024, 512]\", arg551_1: \"f32[1024, 512]\", arg552_1: \"f32[512, 1024]\", arg553_1: \"f32[512]\", arg554_1: \"f32[384, 512]\", arg555_1: \"f32[384, 512]\", arg556_1: \"f32[384, 512]\", arg557_1: \"f32[512, 384]\", arg558_1: \"f32[512]\", arg559_1: \"f32[1024, 512]\", arg560_1: \"f32[1024, 512]\", arg561_1: \"f32[512, 1024]\", arg562_1: \"f32[512]\", arg563_1: \"f32[384, 512]\", arg564_1: \"f32[384, 512]\", arg565_1: \"f32[384, 512]\", arg566_1: \"f32[512, 384]\", arg567_1: \"f32[512]\", arg568_1: \"f32[1024, 512]\", arg569_1: \"f32[1024, 512]\", arg570_1: \"f32[512, 1024]\", arg571_1: \"f32[512]\", arg572_1: \"f32[384, 512]\", arg573_1: \"f32[384, 512]\", arg574_1: \"f32[384, 512]\", arg575_1: \"f32[512, 384]\", arg576_1: \"f32[512]\", arg577_1: \"f32[1024, 512]\", arg578_1: \"f32[1024, 512]\", arg579_1: \"f32[512, 1024]\", arg580_1: \"f32[512]\", arg581_1: \"f32[512]\", arg582_1: \"f32[32128, 512]\", arg583_1: \"f32[384, 512]\", arg584_1: \"f32[384, 512]\", arg585_1: \"f32[384, 512]\", arg586_1: \"f32[512, 384]\", arg587_1: \"f32[32, 6]\", arg588_1: \"f32[512]\", arg589_1: \"f32[384, 512]\", arg590_1: \"f32[384, 512]\", arg591_1: \"f32[384, 512]\", arg592_1: \"f32[512, 384]\", arg593_1: \"f32[512]\", arg594_1: \"f32[1024, 512]\", arg595_1: \"f32[1024, 512]\", arg596_1: \"f32[512, 1024]\", arg597_1: \"f32[512]\", arg598_1: \"f32[384, 512]\", arg599_1: \"f32[384, 512]\", arg600_1: \"f32[384, 512]\", arg601_1: \"f32[512, 384]\", arg602_1: \"f32[512]\", arg603_1: \"f32[384, 512]\", arg604_1: \"f32[384, 512]\", arg605_1: \"f32[384, 512]\", arg606_1: \"f32[512, 384]\", arg607_1: \"f32[512]\", arg608_1: \"f32[1024, 512]\", arg609_1: \"f32[1024, 512]\", arg610_1: \"f32[512, 1024]\", arg611_1: \"f32[512]\", arg612_1: \"f32[384, 512]\", arg613_1: \"f32[384, 512]\", arg614_1: \"f32[384, 512]\", arg615_1: \"f32[512, 384]\", arg616_1: \"f32[512]\", arg617_1: \"f32[384, 512]\", arg618_1: \"f32[384, 512]\", arg619_1: \"f32[384, 512]\", arg620_1: \"f32[512, 384]\", arg621_1: \"f32[512]\", arg622_1: \"f32[1024, 512]\", arg623_1: \"f32[1024, 512]\", arg624_1: \"f32[512, 1024]\", arg625_1: \"f32[512]\", arg626_1: \"f32[384, 512]\", arg627_1: \"f32[384, 512]\", arg628_1: \"f32[384, 512]\", arg629_1: \"f32[512, 384]\", arg630_1: \"f32[512]\", arg631_1: \"f32[384, 512]\", arg632_1: \"f32[384, 512]\", arg633_1: \"f32[384, 512]\", arg634_1: \"f32[512, 384]\", arg635_1: \"f32[512]\", arg636_1: \"f32[1024, 512]\", arg637_1: \"f32[1024, 512]\", arg638_1: \"f32[512, 1024]\", arg639_1: \"f32[512]\", arg640_1: \"f32[384, 512]\", arg641_1: \"f32[384, 512]\", arg642_1: \"f32[384, 512]\", arg643_1: \"f32[512, 384]\", arg644_1: \"f32[512]\", arg645_1: \"f32[384, 512]\", arg646_1: \"f32[384, 512]\", arg647_1: \"f32[384, 512]\", arg648_1: \"f32[512, 384]\", arg649_1: \"f32[512]\", arg650_1: \"f32[1024, 512]\", arg651_1: \"f32[1024, 512]\", arg652_1: \"f32[512, 1024]\", arg653_1: \"f32[512]\", arg654_1: \"f32[384, 512]\", arg655_1: \"f32[384, 512]\", arg656_1: \"f32[384, 512]\", arg657_1: \"f32[512, 384]\", arg658_1: \"f32[512]\", arg659_1: \"f32[384, 512]\", arg660_1: \"f32[384, 512]\", arg661_1: \"f32[384, 512]\", arg662_1: \"f32[512, 384]\", arg663_1: \"f32[512]\", arg664_1: \"f32[1024, 512]\", arg665_1: \"f32[1024, 512]\", arg666_1: \"f32[512, 1024]\", arg667_1: \"f32[512]\", arg668_1: \"f32[384, 512]\", arg669_1: \"f32[384, 512]\", arg670_1: \"f32[384, 512]\", arg671_1: \"f32[512, 384]\", arg672_1: \"f32[512]\", arg673_1: \"f32[384, 512]\", arg674_1: \"f32[384, 512]\", arg675_1: \"f32[384, 512]\", arg676_1: \"f32[512, 384]\", arg677_1: \"f32[512]\", arg678_1: \"f32[1024, 512]\", arg679_1: \"f32[1024, 512]\", arg680_1: \"f32[512, 1024]\", arg681_1: \"f32[512]\", arg682_1: \"f32[384, 512]\", arg683_1: \"f32[384, 512]\", arg684_1: \"f32[384, 512]\", arg685_1: \"f32[512, 384]\", arg686_1: \"f32[512]\", arg687_1: \"f32[384, 512]\", arg688_1: \"f32[384, 512]\", arg689_1: \"f32[384, 512]\", arg690_1: \"f32[512, 384]\", arg691_1: \"f32[512]\", arg692_1: \"f32[1024, 512]\", arg693_1: \"f32[1024, 512]\", arg694_1: \"f32[512, 1024]\", arg695_1: \"f32[512]\", arg696_1: \"f32[512]\", arg697_1: \"f32[32128, 512]\", arg698_1: \"i64[1, 77]\", arg699_1: \"f32[109, 109]\", arg700_1: \"f32[109, 109]\", arg701_1: \"f32[109, 109]\", arg702_1: \"i64[1, 512]\", arg703_1: \"i64[1, 512]\", arg704_1: \"f32[1, 3, 224, 224]\", arg705_1: \"i64[1, 77]\", arg706_1: \"i64[1, 77]\"):\n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:554 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d: \"f32[1, 192, 14, 14]\" = torch.ops.aten.conv2d.default(arg704_1, arg2_1, arg3_1, [16, 16]);  arg704_1 = arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/patch_embed.py:133 in forward, code: x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
      "    flatten: \"f32[1, 192, 196]\" = torch.ops.aten.flatten.using_ints(conv2d, 2);  conv2d = None\n",
      "    transpose: \"f32[1, 196, 192]\" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:260 in forward, code: image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
      "    expand: \"f32[1, 1, 192]\" = torch.ops.aten.expand.default(arg0_1, [1, -1, -1]);  arg0_1 = None\n",
      "    cat: \"f32[1, 197, 192]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
      "    add: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(cat, arg1_1);  cat = arg1_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(add, 0.0, False);  add = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(dropout, [192], arg4_1, arg5_1, 1e-06);  arg4_1 = arg5_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm, arg6_1, arg7_1);  layer_norm = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear, [1, 197, 3, 3, 64]);  linear = None\n",
      "    permute: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape, [2, 0, 3, 1, 4]);  reshape = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind = torch.ops.aten.unbind.int(permute);  permute = None\n",
      "    getitem: \"f32[1, 3, 197, 64]\" = unbind[0]\n",
      "    getitem_1: \"f32[1, 3, 197, 64]\" = unbind[1]\n",
      "    getitem_2: \"f32[1, 3, 197, 64]\" = unbind[2];  unbind = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem, getitem_1, getitem_2);  getitem = getitem_1 = getitem_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_1: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
      "    reshape_1: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_1, [1, 197, 192]);  transpose_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_1: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_1, arg8_1, arg9_1);  reshape_1 = arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_1: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_1, 0.0, False);  linear_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_1: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(dropout, dropout_1);  dropout = dropout_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_1: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_1, [192], arg10_1, arg11_1, 1e-06);  arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_2: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_1, arg12_1, arg13_1);  layer_norm_1 = arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_2: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu, 0.0, False);  gelu = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_3: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_2, arg14_1, arg15_1);  dropout_2 = arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_3: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_2: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_1, dropout_3);  add_1 = dropout_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_2: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_2, [192], arg16_1, arg17_1, 1e-06);  arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_4: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_2, arg18_1, arg19_1);  layer_norm_2 = arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_2: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_4, [1, 197, 3, 3, 64]);  linear_4 = None\n",
      "    permute_1: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_2, [2, 0, 3, 1, 4]);  reshape_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_1 = torch.ops.aten.unbind.int(permute_1);  permute_1 = None\n",
      "    getitem_3: \"f32[1, 3, 197, 64]\" = unbind_1[0]\n",
      "    getitem_4: \"f32[1, 3, 197, 64]\" = unbind_1[1]\n",
      "    getitem_5: \"f32[1, 3, 197, 64]\" = unbind_1[2];  unbind_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_1: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_3, getitem_4, getitem_5);  getitem_3 = getitem_4 = getitem_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_2: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
      "    reshape_3: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_2, [1, 197, 192]);  transpose_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_5: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_3, arg20_1, arg21_1);  reshape_3 = arg20_1 = arg21_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_4: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_3: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_2, dropout_4);  add_2 = dropout_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_3: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_3, [192], arg22_1, arg23_1, 1e-06);  arg22_1 = arg23_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_6: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_3, arg24_1, arg25_1);  layer_norm_3 = arg24_1 = arg25_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_1: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_5: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_1, 0.0, False);  gelu_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_7: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_5, arg26_1, arg27_1);  dropout_5 = arg26_1 = arg27_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_6: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_7, 0.0, False);  linear_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_4: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_3, dropout_6);  add_3 = dropout_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_4: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_4, [192], arg28_1, arg29_1, 1e-06);  arg28_1 = arg29_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_8: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_4, arg30_1, arg31_1);  layer_norm_4 = arg30_1 = arg31_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_4: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_8, [1, 197, 3, 3, 64]);  linear_8 = None\n",
      "    permute_2: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_4, [2, 0, 3, 1, 4]);  reshape_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_2 = torch.ops.aten.unbind.int(permute_2);  permute_2 = None\n",
      "    getitem_6: \"f32[1, 3, 197, 64]\" = unbind_2[0]\n",
      "    getitem_7: \"f32[1, 3, 197, 64]\" = unbind_2[1]\n",
      "    getitem_8: \"f32[1, 3, 197, 64]\" = unbind_2[2];  unbind_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_2: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_6, getitem_7, getitem_8);  getitem_6 = getitem_7 = getitem_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_3: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
      "    reshape_5: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_3, [1, 197, 192]);  transpose_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_9: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_5, arg32_1, arg33_1);  reshape_5 = arg32_1 = arg33_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_7: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_9, 0.0, False);  linear_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_5: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_4, dropout_7);  add_4 = dropout_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_5: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_5, [192], arg34_1, arg35_1, 1e-06);  arg34_1 = arg35_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_10: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_5, arg36_1, arg37_1);  layer_norm_5 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_2: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_8: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_2, 0.0, False);  gelu_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_11: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_8, arg38_1, arg39_1);  dropout_8 = arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_9: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_6: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_5, dropout_9);  add_5 = dropout_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_6: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_6, [192], arg40_1, arg41_1, 1e-06);  arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_12: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_6, arg42_1, arg43_1);  layer_norm_6 = arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_6: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_12, [1, 197, 3, 3, 64]);  linear_12 = None\n",
      "    permute_3: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_6, [2, 0, 3, 1, 4]);  reshape_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_3 = torch.ops.aten.unbind.int(permute_3);  permute_3 = None\n",
      "    getitem_9: \"f32[1, 3, 197, 64]\" = unbind_3[0]\n",
      "    getitem_10: \"f32[1, 3, 197, 64]\" = unbind_3[1]\n",
      "    getitem_11: \"f32[1, 3, 197, 64]\" = unbind_3[2];  unbind_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_3: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_9, getitem_10, getitem_11);  getitem_9 = getitem_10 = getitem_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_4: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
      "    reshape_7: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_4, [1, 197, 192]);  transpose_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_13: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_7, arg44_1, arg45_1);  reshape_7 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_10: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_13, 0.0, False);  linear_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_7: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_6, dropout_10);  add_6 = dropout_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_7: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_7, [192], arg46_1, arg47_1, 1e-06);  arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_14: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_7, arg48_1, arg49_1);  layer_norm_7 = arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_3: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_11: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_3, 0.0, False);  gelu_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_15: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_11, arg50_1, arg51_1);  dropout_11 = arg50_1 = arg51_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_12: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_15, 0.0, False);  linear_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_8: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_7, dropout_12);  add_7 = dropout_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_8: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_8, [192], arg52_1, arg53_1, 1e-06);  arg52_1 = arg53_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_16: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_8, arg54_1, arg55_1);  layer_norm_8 = arg54_1 = arg55_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_8: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_16, [1, 197, 3, 3, 64]);  linear_16 = None\n",
      "    permute_4: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_8, [2, 0, 3, 1, 4]);  reshape_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_4 = torch.ops.aten.unbind.int(permute_4);  permute_4 = None\n",
      "    getitem_12: \"f32[1, 3, 197, 64]\" = unbind_4[0]\n",
      "    getitem_13: \"f32[1, 3, 197, 64]\" = unbind_4[1]\n",
      "    getitem_14: \"f32[1, 3, 197, 64]\" = unbind_4[2];  unbind_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_4: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_12, getitem_13, getitem_14);  getitem_12 = getitem_13 = getitem_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_5: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
      "    reshape_9: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_5, [1, 197, 192]);  transpose_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_17: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_9, arg56_1, arg57_1);  reshape_9 = arg56_1 = arg57_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_13: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_9: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_8, dropout_13);  add_8 = dropout_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_9: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_9, [192], arg58_1, arg59_1, 1e-06);  arg58_1 = arg59_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_18: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_9, arg60_1, arg61_1);  layer_norm_9 = arg60_1 = arg61_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_4: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_14: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_4, 0.0, False);  gelu_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_19: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_14, arg62_1, arg63_1);  dropout_14 = arg62_1 = arg63_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_15: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_19, 0.0, False);  linear_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_10: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_9, dropout_15);  add_9 = dropout_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_10: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_10, [192], arg64_1, arg65_1, 1e-06);  arg64_1 = arg65_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_20: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_10, arg66_1, arg67_1);  layer_norm_10 = arg66_1 = arg67_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_10: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_20, [1, 197, 3, 3, 64]);  linear_20 = None\n",
      "    permute_5: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_10, [2, 0, 3, 1, 4]);  reshape_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_5 = torch.ops.aten.unbind.int(permute_5);  permute_5 = None\n",
      "    getitem_15: \"f32[1, 3, 197, 64]\" = unbind_5[0]\n",
      "    getitem_16: \"f32[1, 3, 197, 64]\" = unbind_5[1]\n",
      "    getitem_17: \"f32[1, 3, 197, 64]\" = unbind_5[2];  unbind_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_5: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_15, getitem_16, getitem_17);  getitem_15 = getitem_16 = getitem_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_6: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
      "    reshape_11: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_6, [1, 197, 192]);  transpose_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_21: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_11, arg68_1, arg69_1);  reshape_11 = arg68_1 = arg69_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_16: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_21, 0.0, False);  linear_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_11: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_10, dropout_16);  add_10 = dropout_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_11: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_11, [192], arg70_1, arg71_1, 1e-06);  arg70_1 = arg71_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_22: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_11, arg72_1, arg73_1);  layer_norm_11 = arg72_1 = arg73_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_5: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_17: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_5, 0.0, False);  gelu_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_23: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_17, arg74_1, arg75_1);  dropout_17 = arg74_1 = arg75_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_18: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_12: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_11, dropout_18);  add_11 = dropout_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_12: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_12, [192], arg76_1, arg77_1, 1e-06);  arg76_1 = arg77_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_24: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_12, arg78_1, arg79_1);  layer_norm_12 = arg78_1 = arg79_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_12: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_24, [1, 197, 3, 3, 64]);  linear_24 = None\n",
      "    permute_6: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_12, [2, 0, 3, 1, 4]);  reshape_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_6 = torch.ops.aten.unbind.int(permute_6);  permute_6 = None\n",
      "    getitem_18: \"f32[1, 3, 197, 64]\" = unbind_6[0]\n",
      "    getitem_19: \"f32[1, 3, 197, 64]\" = unbind_6[1]\n",
      "    getitem_20: \"f32[1, 3, 197, 64]\" = unbind_6[2];  unbind_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_6: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_18, getitem_19, getitem_20);  getitem_18 = getitem_19 = getitem_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_7: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
      "    reshape_13: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_7, [1, 197, 192]);  transpose_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_25: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_13, arg80_1, arg81_1);  reshape_13 = arg80_1 = arg81_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_19: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_25, 0.0, False);  linear_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_13: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_12, dropout_19);  add_12 = dropout_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_13: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_13, [192], arg82_1, arg83_1, 1e-06);  arg82_1 = arg83_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_26: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_13, arg84_1, arg85_1);  layer_norm_13 = arg84_1 = arg85_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_6: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_20: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_6, 0.0, False);  gelu_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_27: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_20, arg86_1, arg87_1);  dropout_20 = arg86_1 = arg87_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_21: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_27, 0.0, False);  linear_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_14: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_13, dropout_21);  add_13 = dropout_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_14: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_14, [192], arg88_1, arg89_1, 1e-06);  arg88_1 = arg89_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_28: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_14, arg90_1, arg91_1);  layer_norm_14 = arg90_1 = arg91_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_14: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_28, [1, 197, 3, 3, 64]);  linear_28 = None\n",
      "    permute_7: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_14, [2, 0, 3, 1, 4]);  reshape_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_7 = torch.ops.aten.unbind.int(permute_7);  permute_7 = None\n",
      "    getitem_21: \"f32[1, 3, 197, 64]\" = unbind_7[0]\n",
      "    getitem_22: \"f32[1, 3, 197, 64]\" = unbind_7[1]\n",
      "    getitem_23: \"f32[1, 3, 197, 64]\" = unbind_7[2];  unbind_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_7: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_21, getitem_22, getitem_23);  getitem_21 = getitem_22 = getitem_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_8: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
      "    reshape_15: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_8, [1, 197, 192]);  transpose_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_29: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_15, arg92_1, arg93_1);  reshape_15 = arg92_1 = arg93_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_22: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_29, 0.0, False);  linear_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_15: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_14, dropout_22);  add_14 = dropout_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_15: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_15, [192], arg94_1, arg95_1, 1e-06);  arg94_1 = arg95_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_30: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_15, arg96_1, arg97_1);  layer_norm_15 = arg96_1 = arg97_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_7: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_23: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_31: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_23, arg98_1, arg99_1);  dropout_23 = arg98_1 = arg99_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_24: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_31, 0.0, False);  linear_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_16: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_15, dropout_24);  add_15 = dropout_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_16: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_16, [192], arg100_1, arg101_1, 1e-06);  arg100_1 = arg101_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_32: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_16, arg102_1, arg103_1);  layer_norm_16 = arg102_1 = arg103_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_16: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_32, [1, 197, 3, 3, 64]);  linear_32 = None\n",
      "    permute_8: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_16, [2, 0, 3, 1, 4]);  reshape_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_8 = torch.ops.aten.unbind.int(permute_8);  permute_8 = None\n",
      "    getitem_24: \"f32[1, 3, 197, 64]\" = unbind_8[0]\n",
      "    getitem_25: \"f32[1, 3, 197, 64]\" = unbind_8[1]\n",
      "    getitem_26: \"f32[1, 3, 197, 64]\" = unbind_8[2];  unbind_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_8: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_24, getitem_25, getitem_26);  getitem_24 = getitem_25 = getitem_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_9: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
      "    reshape_17: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_9, [1, 197, 192]);  transpose_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_33: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_17, arg104_1, arg105_1);  reshape_17 = arg104_1 = arg105_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_25: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_33, 0.0, False);  linear_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_17: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_16, dropout_25);  add_16 = dropout_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_17: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_17, [192], arg106_1, arg107_1, 1e-06);  arg106_1 = arg107_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_34: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_17, arg108_1, arg109_1);  layer_norm_17 = arg108_1 = arg109_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_8: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_26: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_8, 0.0, False);  gelu_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_35: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_26, arg110_1, arg111_1);  dropout_26 = arg110_1 = arg111_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_27: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_35, 0.0, False);  linear_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_18: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_17, dropout_27);  add_17 = dropout_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_18: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_18, [192], arg112_1, arg113_1, 1e-06);  arg112_1 = arg113_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_36: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_18, arg114_1, arg115_1);  layer_norm_18 = arg114_1 = arg115_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_18: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_36, [1, 197, 3, 3, 64]);  linear_36 = None\n",
      "    permute_9: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_18, [2, 0, 3, 1, 4]);  reshape_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_9 = torch.ops.aten.unbind.int(permute_9);  permute_9 = None\n",
      "    getitem_27: \"f32[1, 3, 197, 64]\" = unbind_9[0]\n",
      "    getitem_28: \"f32[1, 3, 197, 64]\" = unbind_9[1]\n",
      "    getitem_29: \"f32[1, 3, 197, 64]\" = unbind_9[2];  unbind_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_9: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_27, getitem_28, getitem_29);  getitem_27 = getitem_28 = getitem_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_10: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
      "    reshape_19: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_10, [1, 197, 192]);  transpose_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_37: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_19, arg116_1, arg117_1);  reshape_19 = arg116_1 = arg117_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_28: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_37, 0.0, False);  linear_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_19: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_18, dropout_28);  add_18 = dropout_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_19: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_19, [192], arg118_1, arg119_1, 1e-06);  arg118_1 = arg119_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_38: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_19, arg120_1, arg121_1);  layer_norm_19 = arg120_1 = arg121_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_9: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_29: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_39: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_29, arg122_1, arg123_1);  dropout_29 = arg122_1 = arg123_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_30: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_39, 0.0, False);  linear_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_20: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_19, dropout_30);  add_19 = dropout_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_20: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_20, [192], arg124_1, arg125_1, 1e-06);  arg124_1 = arg125_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_40: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_20, arg126_1, arg127_1);  layer_norm_20 = arg126_1 = arg127_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_20: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_40, [1, 197, 3, 3, 64]);  linear_40 = None\n",
      "    permute_10: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_20, [2, 0, 3, 1, 4]);  reshape_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_10 = torch.ops.aten.unbind.int(permute_10);  permute_10 = None\n",
      "    getitem_30: \"f32[1, 3, 197, 64]\" = unbind_10[0]\n",
      "    getitem_31: \"f32[1, 3, 197, 64]\" = unbind_10[1]\n",
      "    getitem_32: \"f32[1, 3, 197, 64]\" = unbind_10[2];  unbind_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_10: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_30, getitem_31, getitem_32);  getitem_30 = getitem_31 = getitem_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_11: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
      "    reshape_21: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_11, [1, 197, 192]);  transpose_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_41: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_21, arg128_1, arg129_1);  reshape_21 = arg128_1 = arg129_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_31: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_41, 0.0, False);  linear_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_21: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_20, dropout_31);  add_20 = dropout_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_21: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_21, [192], arg130_1, arg131_1, 1e-06);  arg130_1 = arg131_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_42: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_21, arg132_1, arg133_1);  layer_norm_21 = arg132_1 = arg133_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_10: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_32: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_10, 0.0, False);  gelu_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_43: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_32, arg134_1, arg135_1);  dropout_32 = arg134_1 = arg135_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_33: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_43, 0.0, False);  linear_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_22: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_21, dropout_33);  add_21 = dropout_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_22: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_22, [192], arg136_1, arg137_1, 1e-06);  arg136_1 = arg137_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_44: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_22, arg138_1, arg139_1);  layer_norm_22 = arg138_1 = arg139_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_22: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_44, [1, 197, 3, 3, 64]);  linear_44 = None\n",
      "    permute_11: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_22, [2, 0, 3, 1, 4]);  reshape_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_11 = torch.ops.aten.unbind.int(permute_11);  permute_11 = None\n",
      "    getitem_33: \"f32[1, 3, 197, 64]\" = unbind_11[0]\n",
      "    getitem_34: \"f32[1, 3, 197, 64]\" = unbind_11[1]\n",
      "    getitem_35: \"f32[1, 3, 197, 64]\" = unbind_11[2];  unbind_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_11: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_33, getitem_34, getitem_35);  getitem_33 = getitem_34 = getitem_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_12: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
      "    reshape_23: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_12, [1, 197, 192]);  transpose_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_45: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_23, arg140_1, arg141_1);  reshape_23 = arg140_1 = arg141_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_34: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_45, 0.0, False);  linear_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_23: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_22, dropout_34);  add_22 = dropout_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_23: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_23, [192], arg142_1, arg143_1, 1e-06);  arg142_1 = arg143_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_46: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_23, arg144_1, arg145_1);  layer_norm_23 = arg144_1 = arg145_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_11: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_35: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_47: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_35, arg146_1, arg147_1);  dropout_35 = arg146_1 = arg147_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_36: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_47, 0.0, False);  linear_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_24: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_23, dropout_36);  add_23 = dropout_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_24: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_24, [192], arg148_1, arg149_1, 1e-06);  add_24 = arg148_1 = arg149_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_48: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_24, arg150_1, arg151_1);  layer_norm_24 = arg150_1 = arg151_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:263 in forward, code: cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    zeros: \"i64[77]\" = torch.ops.aten.zeros.default([77], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:264 in forward, code: dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    zeros_1: \"i64[77]\" = torch.ops.aten.zeros.default([77], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:144 in forward, code: learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
      "    unsqueeze: \"f32[1, 32, 768]\" = torch.ops.aten.unsqueeze.default(arg152_1, 0);  arg152_1 = None\n",
      "    expand_1: \"f32[1, 32, 768]\" = torch.ops.aten.expand.default(unsqueeze, [1, -1, -1]);  unsqueeze = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros);  zeros = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_2: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1])\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_1: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_2);  expand_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_25: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_2: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros_1);  zeros_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_3: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1]);  arg698_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_3: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_3);  arg503_1 = expand_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_26: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding_2, embedding_3);  embedding_2 = embedding_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:105 in forward, code: itc_query_embds = query_embds.clone()\n",
      "    clone: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:106 in forward, code: itm_query_embds = query_embds.clone()\n",
      "    clone_1: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  clone_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:107 in forward, code: itg_query_embds = query_embds.clone()\n",
      "    clone_2: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  expand_1 = clone_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:109 in forward, code: itc_text_embds = cls_text_embds.clone()\n",
      "    clone_3: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:110 in forward, code: itm_text_embds = cls_text_embds.clone()\n",
      "    clone_4: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25);  add_25 = clone_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:111 in forward, code: itg_text_embds = dec_text_embds.clone()\n",
      "    clone_5: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_26);  add_26 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:117 in forward, code: itc_add = to_additive_mask(self.itc_attn_mask, device=device, dtype=dtype)\n",
      "    to: \"f32[109, 109]\" = torch.ops.aten.to.device(arg699_1, device(type='cpu'), torch.float32);  arg699_1 = None\n",
      "    gt: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to, 0)\n",
      "    zeros_like: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to, pin_memory = False)\n",
      "    full_like: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to, -inf, pin_memory = False);  to = None\n",
      "    where: \"f32[109, 109]\" = torch.ops.aten.where.self(gt, zeros_like, full_like);  gt = zeros_like = full_like = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:118 in forward, code: itm_add = to_additive_mask(self.itm_attn_mask, device=device, dtype=dtype)\n",
      "    to_1: \"f32[109, 109]\" = torch.ops.aten.to.device(arg700_1, device(type='cpu'), torch.float32);  arg700_1 = None\n",
      "    gt_1: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_1, 0)\n",
      "    zeros_like_1: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_1, pin_memory = False)\n",
      "    full_like_1: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_1, -inf, pin_memory = False);  to_1 = None\n",
      "    where_1: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_1, zeros_like_1, full_like_1);  gt_1 = zeros_like_1 = full_like_1 = where_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:119 in forward, code: itg_add = to_additive_mask(self.itg_attn_mask, device=device, dtype=dtype)\n",
      "    to_2: \"f32[109, 109]\" = torch.ops.aten.to.device(arg701_1, device(type='cpu'), torch.float32);  arg701_1 = None\n",
      "    gt_2: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_2, 0)\n",
      "    zeros_like_2: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_2, pin_memory = False)\n",
      "    full_like_2: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_2, -inf, pin_memory = False);  to_2 = None\n",
      "    where_2: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_2, zeros_like_2, full_like_2);  gt_2 = zeros_like_2 = full_like_2 = where_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([clone, clone_3], 1);  clone = clone_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_13: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_49: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_13, arg352_1, arg353_1);  transpose_13 = arg352_1 = arg353_1 = None\n",
      "    unflatten: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_49, -1, [3, 768]);  linear_49 = None\n",
      "    unsqueeze_1: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten, 0);  unflatten = None\n",
      "    transpose_14: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_1, 0, -2);  unsqueeze_1 = None\n",
      "    squeeze: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_14, -2);  transpose_14 = None\n",
      "    contiguous: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze);  squeeze = None\n",
      "    select: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 0)\n",
      "    select_1: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 1)\n",
      "    select_2: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 2);  contiguous = None\n",
      "    unsqueeze_2: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select, [109, 12, 64]);  select = None\n",
      "    transpose_15: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view, 0, 1);  view = None\n",
      "    view_1: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_1, [109, 12, 64]);  select_1 = None\n",
      "    transpose_16: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_1, 0, 1);  view_1 = None\n",
      "    view_2: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_2, [109, 12, 64]);  select_2 = None\n",
      "    transpose_17: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_2, 0, 1);  view_2 = None\n",
      "    mul: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_15, 0.125);  transpose_15 = None\n",
      "    transpose_18: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_16, -2, -1);  transpose_16 = None\n",
      "    baddbmm: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_2, mul, transpose_18);  unsqueeze_2 = mul = transpose_18 = None\n",
      "    softmax: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm, -1);  baddbmm = None\n",
      "    bmm: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax, transpose_17);  transpose_17 = None\n",
      "    transpose_19: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm, 0, 1);  bmm = None\n",
      "    contiguous_1: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_19);  transpose_19 = None\n",
      "    view_3: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_1, [109, 768]);  contiguous_1 = None\n",
      "    linear_50: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_3, arg354_1, arg355_1);  view_3 = arg354_1 = arg355_1 = None\n",
      "    view_4: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_50, [109, 1, 768]);  linear_50 = None\n",
      "    view_5: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax, [1, 12, 109, 109]);  softmax = None\n",
      "    mean: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_5, [1]);  view_5 = mean = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_20: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_4, 1, 0);  view_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_27: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat, transpose_20);  concat = transpose_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_25: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_27, [768], arg356_1, arg357_1);  add_27 = arg356_1 = arg357_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_1: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25)\n",
      "    slice_2: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_1, 1, None, 32);  slice_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_3: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25);  layer_norm_25 = None\n",
      "    slice_4: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_3, 1, 32);  slice_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_21: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_2, 1, 0)\n",
      "    transpose_22: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(arg370_1, [768, 1536]);  arg370_1 = None\n",
      "    getitem_36: \"f32[768, 768]\" = split_with_sizes[0]\n",
      "    getitem_37: \"f32[1536, 768]\" = split_with_sizes[1];  split_with_sizes = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(arg371_1, [768, 1536]);  arg371_1 = None\n",
      "    getitem_38: \"f32[768]\" = split_with_sizes_1[0]\n",
      "    getitem_39: \"f32[1536]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
      "    linear_51: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_21, getitem_36, getitem_38);  transpose_21 = getitem_36 = getitem_38 = None\n",
      "    linear_52: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_22, getitem_37, getitem_39);  transpose_22 = getitem_37 = getitem_39 = None\n",
      "    unflatten_1: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_52, -1, [2, 768]);  linear_52 = None\n",
      "    unsqueeze_3: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_1, 0);  unflatten_1 = None\n",
      "    transpose_23: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_3, 0, -2);  unsqueeze_3 = None\n",
      "    squeeze_1: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_23, -2);  transpose_23 = None\n",
      "    contiguous_2: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_1);  squeeze_1 = None\n",
      "    select_3: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 0)\n",
      "    select_4: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 1);  contiguous_2 = None\n",
      "    view_6: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_51, [32, 12, 64]);  linear_51 = None\n",
      "    transpose_24: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_6, 0, 1);  view_6 = None\n",
      "    view_7: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_3, [197, 12, 64]);  select_3 = None\n",
      "    transpose_25: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_7, 0, 1);  view_7 = None\n",
      "    view_8: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_4, [197, 12, 64]);  select_4 = None\n",
      "    transpose_26: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_8, 0, 1);  view_8 = None\n",
      "    mul_1: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_24, 0.125);  transpose_24 = None\n",
      "    transpose_27: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_25, -2, -1);  transpose_25 = None\n",
      "    bmm_1: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_1, transpose_27);  mul_1 = transpose_27 = None\n",
      "    softmax_1: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_1, -1);  bmm_1 = None\n",
      "    bmm_2: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_1, transpose_26);  transpose_26 = None\n",
      "    transpose_28: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_2, 0, 1);  bmm_2 = None\n",
      "    contiguous_3: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_28);  transpose_28 = None\n",
      "    view_9: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_3, [32, 768]);  contiguous_3 = None\n",
      "    linear_53: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_9, arg372_1, arg373_1);  view_9 = arg372_1 = arg373_1 = None\n",
      "    view_10: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_53, [32, 1, 768]);  linear_53 = None\n",
      "    view_11: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_1, [1, 12, 32, 197]);  softmax_1 = None\n",
      "    mean_1: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_11, [1]);  view_11 = mean_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_29: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_10, 1, 0);  view_10 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_28: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_2, transpose_29);  slice_2 = transpose_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_26: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_28, [768], arg374_1, arg375_1);  add_28 = arg374_1 = arg375_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_54: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_26, arg358_1, arg359_1);  arg358_1 = arg359_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_12: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_54);  linear_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_55: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_12, arg360_1, arg361_1);  gelu_12 = arg360_1 = arg361_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_37: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_55, 0.1, False);  linear_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_29: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_37, layer_norm_26);  dropout_37 = layer_norm_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_27: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_29, [768], arg362_1, arg363_1, 1e-12);  add_29 = arg362_1 = arg363_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_56: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_4, arg364_1, arg365_1);  arg364_1 = arg365_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_13: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_56);  linear_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_57: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_13, arg366_1, arg367_1);  gelu_13 = arg366_1 = arg367_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_38: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_57, 0.1, False);  linear_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_30: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_38, slice_4);  dropout_38 = slice_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_28: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_30, [768], arg368_1, arg369_1, 1e-12);  add_30 = arg368_1 = arg369_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_1: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_27, layer_norm_28], 1);  layer_norm_27 = layer_norm_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_30: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_1, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_58: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_30, arg376_1, arg377_1);  transpose_30 = arg376_1 = arg377_1 = None\n",
      "    unflatten_2: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_58, -1, [3, 768]);  linear_58 = None\n",
      "    unsqueeze_4: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_2, 0);  unflatten_2 = None\n",
      "    transpose_31: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_4, 0, -2);  unsqueeze_4 = None\n",
      "    squeeze_2: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_31, -2);  transpose_31 = None\n",
      "    contiguous_4: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_2);  squeeze_2 = None\n",
      "    select_5: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 0)\n",
      "    select_6: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 1)\n",
      "    select_7: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 2);  contiguous_4 = None\n",
      "    unsqueeze_5: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_12: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_5, [109, 12, 64]);  select_5 = None\n",
      "    transpose_32: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_12, 0, 1);  view_12 = None\n",
      "    view_13: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_6, [109, 12, 64]);  select_6 = None\n",
      "    transpose_33: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_13, 0, 1);  view_13 = None\n",
      "    view_14: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_7, [109, 12, 64]);  select_7 = None\n",
      "    transpose_34: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_14, 0, 1);  view_14 = None\n",
      "    mul_2: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_32, 0.125);  transpose_32 = None\n",
      "    transpose_35: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_33, -2, -1);  transpose_33 = None\n",
      "    baddbmm_1: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_5, mul_2, transpose_35);  unsqueeze_5 = mul_2 = transpose_35 = None\n",
      "    softmax_2: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_1, -1);  baddbmm_1 = None\n",
      "    bmm_3: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_2, transpose_34);  transpose_34 = None\n",
      "    transpose_36: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_3, 0, 1);  bmm_3 = None\n",
      "    contiguous_5: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
      "    view_15: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_5, [109, 768]);  contiguous_5 = None\n",
      "    linear_59: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_15, arg378_1, arg379_1);  view_15 = arg378_1 = arg379_1 = None\n",
      "    view_16: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_59, [109, 1, 768]);  linear_59 = None\n",
      "    view_17: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_2, [1, 12, 109, 109]);  softmax_2 = None\n",
      "    mean_2: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_17, [1]);  view_17 = mean_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_37: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_16, 1, 0);  view_16 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_31: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_1, transpose_37);  concat_1 = transpose_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_29: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_31, [768], arg380_1, arg381_1);  add_31 = arg380_1 = arg381_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_5: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29)\n",
      "    slice_6: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_5, 1, None, 32);  slice_5 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_7: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29);  layer_norm_29 = None\n",
      "    slice_8: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_7, 1, 32);  slice_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_60: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_6, arg382_1, arg383_1);  arg382_1 = arg383_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_14: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_60);  linear_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_61: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_14, arg384_1, arg385_1);  gelu_14 = arg384_1 = arg385_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_39: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_61, 0.1, False);  linear_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_32: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_39, slice_6);  dropout_39 = slice_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_30: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_32, [768], arg386_1, arg387_1, 1e-12);  add_32 = arg386_1 = arg387_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_62: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_8, arg388_1, arg389_1);  arg388_1 = arg389_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_15: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_62);  linear_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_63: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_15, arg390_1, arg391_1);  gelu_15 = arg390_1 = arg391_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_40: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_63, 0.1, False);  linear_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_33: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_40, slice_8);  dropout_40 = slice_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_31: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_33, [768], arg392_1, arg393_1, 1e-12);  add_33 = arg392_1 = arg393_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_2: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_30, layer_norm_31], 1);  layer_norm_30 = layer_norm_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_38: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_2, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_64: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_38, arg394_1, arg395_1);  transpose_38 = arg394_1 = arg395_1 = None\n",
      "    unflatten_3: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_64, -1, [3, 768]);  linear_64 = None\n",
      "    unsqueeze_6: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_3, 0);  unflatten_3 = None\n",
      "    transpose_39: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_6, 0, -2);  unsqueeze_6 = None\n",
      "    squeeze_3: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_39, -2);  transpose_39 = None\n",
      "    contiguous_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_3);  squeeze_3 = None\n",
      "    select_8: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 0)\n",
      "    select_9: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 1)\n",
      "    select_10: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 2);  contiguous_6 = None\n",
      "    unsqueeze_7: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_18: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_8, [109, 12, 64]);  select_8 = None\n",
      "    transpose_40: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_18, 0, 1);  view_18 = None\n",
      "    view_19: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_9, [109, 12, 64]);  select_9 = None\n",
      "    transpose_41: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_19, 0, 1);  view_19 = None\n",
      "    view_20: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_10, [109, 12, 64]);  select_10 = None\n",
      "    transpose_42: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_20, 0, 1);  view_20 = None\n",
      "    mul_3: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_40, 0.125);  transpose_40 = None\n",
      "    transpose_43: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_41, -2, -1);  transpose_41 = None\n",
      "    baddbmm_2: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_7, mul_3, transpose_43);  unsqueeze_7 = mul_3 = transpose_43 = None\n",
      "    softmax_3: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_2, -1);  baddbmm_2 = None\n",
      "    bmm_4: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_3, transpose_42);  transpose_42 = None\n",
      "    transpose_44: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_4, 0, 1);  bmm_4 = None\n",
      "    contiguous_7: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_44);  transpose_44 = None\n",
      "    view_21: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_7, [109, 768]);  contiguous_7 = None\n",
      "    linear_65: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_21, arg396_1, arg397_1);  view_21 = arg396_1 = arg397_1 = None\n",
      "    view_22: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_65, [109, 1, 768]);  linear_65 = None\n",
      "    view_23: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_3, [1, 12, 109, 109]);  softmax_3 = None\n",
      "    mean_3: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_23, [1]);  view_23 = mean_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_45: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_22, 1, 0);  view_22 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_34: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_2, transpose_45);  concat_2 = transpose_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_32: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_34, [768], arg398_1, arg399_1);  add_34 = arg398_1 = arg399_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_9: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32)\n",
      "    slice_10: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_9, 1, None, 32);  slice_9 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_11: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32);  layer_norm_32 = None\n",
      "    slice_12: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_11, 1, 32);  slice_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_46: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_10, 1, 0)\n",
      "    transpose_47: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(arg412_1, [768, 1536]);  arg412_1 = None\n",
      "    getitem_40: \"f32[768, 768]\" = split_with_sizes_2[0]\n",
      "    getitem_41: \"f32[1536, 768]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
      "    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(arg413_1, [768, 1536]);  arg413_1 = None\n",
      "    getitem_42: \"f32[768]\" = split_with_sizes_3[0]\n",
      "    getitem_43: \"f32[1536]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
      "    linear_66: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_46, getitem_40, getitem_42);  transpose_46 = getitem_40 = getitem_42 = None\n",
      "    linear_67: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_47, getitem_41, getitem_43);  transpose_47 = getitem_41 = getitem_43 = None\n",
      "    unflatten_4: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_67, -1, [2, 768]);  linear_67 = None\n",
      "    unsqueeze_8: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_4, 0);  unflatten_4 = None\n",
      "    transpose_48: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_8, 0, -2);  unsqueeze_8 = None\n",
      "    squeeze_4: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_48, -2);  transpose_48 = None\n",
      "    contiguous_8: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_4);  squeeze_4 = None\n",
      "    select_11: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 0)\n",
      "    select_12: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 1);  contiguous_8 = None\n",
      "    view_24: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_66, [32, 12, 64]);  linear_66 = None\n",
      "    transpose_49: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_24, 0, 1);  view_24 = None\n",
      "    view_25: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_11, [197, 12, 64]);  select_11 = None\n",
      "    transpose_50: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_25, 0, 1);  view_25 = None\n",
      "    view_26: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_12, [197, 12, 64]);  select_12 = None\n",
      "    transpose_51: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_26, 0, 1);  view_26 = None\n",
      "    mul_4: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_49, 0.125);  transpose_49 = None\n",
      "    transpose_52: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_50, -2, -1);  transpose_50 = None\n",
      "    bmm_5: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_4, transpose_52);  mul_4 = transpose_52 = None\n",
      "    softmax_4: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_5, -1);  bmm_5 = None\n",
      "    bmm_6: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_4, transpose_51);  transpose_51 = None\n",
      "    transpose_53: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_6, 0, 1);  bmm_6 = None\n",
      "    contiguous_9: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_53);  transpose_53 = None\n",
      "    view_27: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_9, [32, 768]);  contiguous_9 = None\n",
      "    linear_68: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_27, arg414_1, arg415_1);  view_27 = arg414_1 = arg415_1 = None\n",
      "    view_28: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_68, [32, 1, 768]);  linear_68 = None\n",
      "    view_29: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_4, [1, 12, 32, 197]);  softmax_4 = None\n",
      "    mean_4: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_29, [1]);  view_29 = mean_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_54: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_28, 1, 0);  view_28 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_35: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_10, transpose_54);  slice_10 = transpose_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_33: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_35, [768], arg416_1, arg417_1);  add_35 = arg416_1 = arg417_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_69: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_33, arg400_1, arg401_1);  arg400_1 = arg401_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_16: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_69);  linear_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_70: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_16, arg402_1, arg403_1);  gelu_16 = arg402_1 = arg403_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_41: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_70, 0.1, False);  linear_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_36: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_41, layer_norm_33);  dropout_41 = layer_norm_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_34: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_36, [768], arg404_1, arg405_1, 1e-12);  add_36 = arg404_1 = arg405_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_71: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_12, arg406_1, arg407_1);  arg406_1 = arg407_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_17: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_71);  linear_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_72: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_17, arg408_1, arg409_1);  gelu_17 = arg408_1 = arg409_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_42: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_72, 0.1, False);  linear_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_37: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_42, slice_12);  dropout_42 = slice_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_35: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_37, [768], arg410_1, arg411_1, 1e-12);  add_37 = arg410_1 = arg411_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_3: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_34, layer_norm_35], 1);  layer_norm_34 = layer_norm_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_55: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_3, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_73: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_55, arg418_1, arg419_1);  transpose_55 = arg418_1 = arg419_1 = None\n",
      "    unflatten_5: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_73, -1, [3, 768]);  linear_73 = None\n",
      "    unsqueeze_9: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_5, 0);  unflatten_5 = None\n",
      "    transpose_56: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_9, 0, -2);  unsqueeze_9 = None\n",
      "    squeeze_5: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_56, -2);  transpose_56 = None\n",
      "    contiguous_10: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_5);  squeeze_5 = None\n",
      "    select_13: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 0)\n",
      "    select_14: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 1)\n",
      "    select_15: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 2);  contiguous_10 = None\n",
      "    unsqueeze_10: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_30: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_13, [109, 12, 64]);  select_13 = None\n",
      "    transpose_57: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None\n",
      "    view_31: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_14, [109, 12, 64]);  select_14 = None\n",
      "    transpose_58: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_31, 0, 1);  view_31 = None\n",
      "    view_32: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_15, [109, 12, 64]);  select_15 = None\n",
      "    transpose_59: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_32, 0, 1);  view_32 = None\n",
      "    mul_5: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_57, 0.125);  transpose_57 = None\n",
      "    transpose_60: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_58, -2, -1);  transpose_58 = None\n",
      "    baddbmm_3: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_10, mul_5, transpose_60);  unsqueeze_10 = mul_5 = transpose_60 = None\n",
      "    softmax_5: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_3, -1);  baddbmm_3 = None\n",
      "    bmm_7: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_5, transpose_59);  transpose_59 = None\n",
      "    transpose_61: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_7, 0, 1);  bmm_7 = None\n",
      "    contiguous_11: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_61);  transpose_61 = None\n",
      "    view_33: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_11, [109, 768]);  contiguous_11 = None\n",
      "    linear_74: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_33, arg420_1, arg421_1);  view_33 = arg420_1 = arg421_1 = None\n",
      "    view_34: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_74, [109, 1, 768]);  linear_74 = None\n",
      "    view_35: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_5, [1, 12, 109, 109]);  softmax_5 = None\n",
      "    mean_5: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_35, [1]);  view_35 = mean_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_62: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_34, 1, 0);  view_34 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_38: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_3, transpose_62);  concat_3 = transpose_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_36: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_38, [768], arg422_1, arg423_1);  add_38 = arg422_1 = arg423_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_13: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36)\n",
      "    slice_14: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_13, 1, None, 32);  slice_13 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_15: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36);  layer_norm_36 = None\n",
      "    slice_16: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_15, 1, 32);  slice_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_75: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_14, arg424_1, arg425_1);  arg424_1 = arg425_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_18: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_75);  linear_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_76: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_18, arg426_1, arg427_1);  gelu_18 = arg426_1 = arg427_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_43: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_76, 0.1, False);  linear_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_39: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_43, slice_14);  dropout_43 = slice_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_37: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_39, [768], arg428_1, arg429_1, 1e-12);  add_39 = arg428_1 = arg429_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_77: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_16, arg430_1, arg431_1);  arg430_1 = arg431_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_19: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_77);  linear_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_78: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_19, arg432_1, arg433_1);  gelu_19 = arg432_1 = arg433_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_44: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_78, 0.1, False);  linear_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_40: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_44, slice_16);  dropout_44 = slice_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_38: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_40, [768], arg434_1, arg435_1, 1e-12);  add_40 = arg434_1 = arg435_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_4: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_37, layer_norm_38], 1);  layer_norm_37 = layer_norm_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_63: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_4, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_79: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_63, arg436_1, arg437_1);  transpose_63 = arg436_1 = arg437_1 = None\n",
      "    unflatten_6: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_79, -1, [3, 768]);  linear_79 = None\n",
      "    unsqueeze_11: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_6, 0);  unflatten_6 = None\n",
      "    transpose_64: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_11, 0, -2);  unsqueeze_11 = None\n",
      "    squeeze_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_64, -2);  transpose_64 = None\n",
      "    contiguous_12: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_6);  squeeze_6 = None\n",
      "    select_16: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 0)\n",
      "    select_17: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 1)\n",
      "    select_18: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 2);  contiguous_12 = None\n",
      "    unsqueeze_12: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_36: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_16, [109, 12, 64]);  select_16 = None\n",
      "    transpose_65: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_36, 0, 1);  view_36 = None\n",
      "    view_37: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_17, [109, 12, 64]);  select_17 = None\n",
      "    transpose_66: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_37, 0, 1);  view_37 = None\n",
      "    view_38: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_18, [109, 12, 64]);  select_18 = None\n",
      "    transpose_67: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_38, 0, 1);  view_38 = None\n",
      "    mul_6: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_65, 0.125);  transpose_65 = None\n",
      "    transpose_68: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_66, -2, -1);  transpose_66 = None\n",
      "    baddbmm_4: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_12, mul_6, transpose_68);  unsqueeze_12 = mul_6 = transpose_68 = None\n",
      "    softmax_6: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_4, -1);  baddbmm_4 = None\n",
      "    bmm_8: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_6, transpose_67);  transpose_67 = None\n",
      "    transpose_69: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_8, 0, 1);  bmm_8 = None\n",
      "    contiguous_13: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_69);  transpose_69 = None\n",
      "    view_39: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_13, [109, 768]);  contiguous_13 = None\n",
      "    linear_80: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_39, arg438_1, arg439_1);  view_39 = arg438_1 = arg439_1 = None\n",
      "    view_40: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_80, [109, 1, 768]);  linear_80 = None\n",
      "    view_41: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_6, [1, 12, 109, 109]);  softmax_6 = None\n",
      "    mean_6: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_41, [1]);  view_41 = mean_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_70: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_40, 1, 0);  view_40 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_41: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_4, transpose_70);  concat_4 = transpose_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_39: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_41, [768], arg440_1, arg441_1);  add_41 = arg440_1 = arg441_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_17: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39)\n",
      "    slice_18: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_17, 1, None, 32);  slice_17 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_19: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39);  layer_norm_39 = None\n",
      "    slice_20: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_19, 1, 32);  slice_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_71: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_18, 1, 0)\n",
      "    transpose_72: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(arg454_1, [768, 1536]);  arg454_1 = None\n",
      "    getitem_44: \"f32[768, 768]\" = split_with_sizes_4[0]\n",
      "    getitem_45: \"f32[1536, 768]\" = split_with_sizes_4[1];  split_with_sizes_4 = None\n",
      "    split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(arg455_1, [768, 1536]);  arg455_1 = None\n",
      "    getitem_46: \"f32[768]\" = split_with_sizes_5[0]\n",
      "    getitem_47: \"f32[1536]\" = split_with_sizes_5[1];  split_with_sizes_5 = None\n",
      "    linear_81: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_71, getitem_44, getitem_46);  transpose_71 = getitem_44 = getitem_46 = None\n",
      "    linear_82: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_72, getitem_45, getitem_47);  transpose_72 = getitem_45 = getitem_47 = None\n",
      "    unflatten_7: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_82, -1, [2, 768]);  linear_82 = None\n",
      "    unsqueeze_13: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_7, 0);  unflatten_7 = None\n",
      "    transpose_73: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_13, 0, -2);  unsqueeze_13 = None\n",
      "    squeeze_7: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_73, -2);  transpose_73 = None\n",
      "    contiguous_14: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_7);  squeeze_7 = None\n",
      "    select_19: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 0)\n",
      "    select_20: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 1);  contiguous_14 = None\n",
      "    view_42: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_81, [32, 12, 64]);  linear_81 = None\n",
      "    transpose_74: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_42, 0, 1);  view_42 = None\n",
      "    view_43: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_19, [197, 12, 64]);  select_19 = None\n",
      "    transpose_75: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_43, 0, 1);  view_43 = None\n",
      "    view_44: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_20, [197, 12, 64]);  select_20 = None\n",
      "    transpose_76: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_44, 0, 1);  view_44 = None\n",
      "    mul_7: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_74, 0.125);  transpose_74 = None\n",
      "    transpose_77: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_75, -2, -1);  transpose_75 = None\n",
      "    bmm_9: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_7, transpose_77);  mul_7 = transpose_77 = None\n",
      "    softmax_7: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_9, -1);  bmm_9 = None\n",
      "    bmm_10: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_7, transpose_76);  transpose_76 = None\n",
      "    transpose_78: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_10, 0, 1);  bmm_10 = None\n",
      "    contiguous_15: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_78);  transpose_78 = None\n",
      "    view_45: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_15, [32, 768]);  contiguous_15 = None\n",
      "    linear_83: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_45, arg456_1, arg457_1);  view_45 = arg456_1 = arg457_1 = None\n",
      "    view_46: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_83, [32, 1, 768]);  linear_83 = None\n",
      "    view_47: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_7, [1, 12, 32, 197]);  softmax_7 = None\n",
      "    mean_7: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_47, [1]);  view_47 = mean_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_79: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_46, 1, 0);  view_46 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_42: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_18, transpose_79);  slice_18 = transpose_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_40: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_42, [768], arg458_1, arg459_1);  add_42 = arg458_1 = arg459_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_84: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_40, arg442_1, arg443_1);  arg442_1 = arg443_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_20: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_84);  linear_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_85: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_20, arg444_1, arg445_1);  gelu_20 = arg444_1 = arg445_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_45: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_85, 0.1, False);  linear_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_43: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_45, layer_norm_40);  dropout_45 = layer_norm_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_41: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_43, [768], arg446_1, arg447_1, 1e-12);  add_43 = arg446_1 = arg447_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_86: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_20, arg448_1, arg449_1);  arg448_1 = arg449_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_21: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_86);  linear_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_87: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_21, arg450_1, arg451_1);  gelu_21 = arg450_1 = arg451_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_46: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_87, 0.1, False);  linear_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_44: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_46, slice_20);  dropout_46 = slice_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_42: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_44, [768], arg452_1, arg453_1, 1e-12);  add_44 = arg452_1 = arg453_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_5: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_41, layer_norm_42], 1);  layer_norm_41 = layer_norm_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_80: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_5, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_88: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_80, arg460_1, arg461_1);  transpose_80 = arg460_1 = arg461_1 = None\n",
      "    unflatten_8: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_88, -1, [3, 768]);  linear_88 = None\n",
      "    unsqueeze_14: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_8, 0);  unflatten_8 = None\n",
      "    transpose_81: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_14, 0, -2);  unsqueeze_14 = None\n",
      "    squeeze_8: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_81, -2);  transpose_81 = None\n",
      "    contiguous_16: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_8);  squeeze_8 = None\n",
      "    select_21: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 0)\n",
      "    select_22: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 1)\n",
      "    select_23: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 2);  contiguous_16 = None\n",
      "    unsqueeze_15: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_48: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_21, [109, 12, 64]);  select_21 = None\n",
      "    transpose_82: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_48, 0, 1);  view_48 = None\n",
      "    view_49: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_22, [109, 12, 64]);  select_22 = None\n",
      "    transpose_83: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_49, 0, 1);  view_49 = None\n",
      "    view_50: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_23, [109, 12, 64]);  select_23 = None\n",
      "    transpose_84: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_50, 0, 1);  view_50 = None\n",
      "    mul_8: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_82, 0.125);  transpose_82 = None\n",
      "    transpose_85: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_83, -2, -1);  transpose_83 = None\n",
      "    baddbmm_5: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_15, mul_8, transpose_85);  unsqueeze_15 = mul_8 = transpose_85 = None\n",
      "    softmax_8: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_5, -1);  baddbmm_5 = None\n",
      "    bmm_11: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_8, transpose_84);  transpose_84 = None\n",
      "    transpose_86: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_11, 0, 1);  bmm_11 = None\n",
      "    contiguous_17: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_86);  transpose_86 = None\n",
      "    view_51: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_17, [109, 768]);  contiguous_17 = None\n",
      "    linear_89: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_51, arg462_1, arg463_1);  view_51 = arg462_1 = arg463_1 = None\n",
      "    view_52: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_89, [109, 1, 768]);  linear_89 = None\n",
      "    view_53: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_8, [1, 12, 109, 109]);  softmax_8 = None\n",
      "    mean_8: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_53, [1]);  view_53 = mean_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_87: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_52, 1, 0);  view_52 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_45: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_5, transpose_87);  concat_5 = transpose_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_43: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_45, [768], arg464_1, arg465_1);  add_45 = arg464_1 = arg465_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_21: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43)\n",
      "    slice_22: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_21, 1, None, 32);  slice_21 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_23: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43);  layer_norm_43 = None\n",
      "    slice_24: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_23, 1, 32);  slice_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_90: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_22, arg466_1, arg467_1);  arg466_1 = arg467_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_22: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_90);  linear_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_91: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_22, arg468_1, arg469_1);  gelu_22 = arg468_1 = arg469_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_47: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_91, 0.1, False);  linear_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_46: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_47, slice_22);  dropout_47 = slice_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_44: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_46, [768], arg470_1, arg471_1, 1e-12);  add_46 = arg470_1 = arg471_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_92: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_24, arg472_1, arg473_1);  arg472_1 = arg473_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_23: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_92);  linear_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_93: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_23, arg474_1, arg475_1);  gelu_23 = arg474_1 = arg475_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_48: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_93, 0.1, False);  linear_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_47: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_48, slice_24);  dropout_48 = slice_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_45: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_47, [768], arg476_1, arg477_1, 1e-12);  add_47 = arg476_1 = arg477_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_6: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_44, layer_norm_45], 1);  layer_norm_44 = layer_norm_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_88: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_6, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_94: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_88, arg478_1, arg479_1);  transpose_88 = arg478_1 = arg479_1 = None\n",
      "    unflatten_9: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_94, -1, [3, 768]);  linear_94 = None\n",
      "    unsqueeze_16: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_9, 0);  unflatten_9 = None\n",
      "    transpose_89: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_16, 0, -2);  unsqueeze_16 = None\n",
      "    squeeze_9: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_89, -2);  transpose_89 = None\n",
      "    contiguous_18: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_9);  squeeze_9 = None\n",
      "    select_24: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 0)\n",
      "    select_25: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 1)\n",
      "    select_26: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 2);  contiguous_18 = None\n",
      "    unsqueeze_17: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0);  where = None\n",
      "    view_54: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_24, [109, 12, 64]);  select_24 = None\n",
      "    transpose_90: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_54, 0, 1);  view_54 = None\n",
      "    view_55: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_25, [109, 12, 64]);  select_25 = None\n",
      "    transpose_91: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_55, 0, 1);  view_55 = None\n",
      "    view_56: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_26, [109, 12, 64]);  select_26 = None\n",
      "    transpose_92: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_56, 0, 1);  view_56 = None\n",
      "    mul_9: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_90, 0.125);  transpose_90 = None\n",
      "    transpose_93: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_91, -2, -1);  transpose_91 = None\n",
      "    baddbmm_6: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_17, mul_9, transpose_93);  unsqueeze_17 = mul_9 = transpose_93 = None\n",
      "    softmax_9: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_6, -1);  baddbmm_6 = None\n",
      "    bmm_12: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_9, transpose_92);  transpose_92 = None\n",
      "    transpose_94: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_12, 0, 1);  bmm_12 = None\n",
      "    contiguous_19: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_94);  transpose_94 = None\n",
      "    view_57: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_19, [109, 768]);  contiguous_19 = None\n",
      "    linear_95: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_57, arg480_1, arg481_1);  view_57 = arg480_1 = arg481_1 = None\n",
      "    view_58: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_95, [109, 1, 768]);  linear_95 = None\n",
      "    view_59: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_9, [1, 12, 109, 109]);  softmax_9 = None\n",
      "    mean_9: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_59, [1]);  view_59 = mean_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_95: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_58, 1, 0);  view_58 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_48: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_6, transpose_95);  concat_6 = transpose_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_46: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_48, [768], arg482_1, arg483_1);  add_48 = arg482_1 = arg483_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_25: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46)\n",
      "    slice_26: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_25, 1, None, 32);  slice_25 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_27: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46);  layer_norm_46 = None\n",
      "    slice_28: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_27, 1, 32);  slice_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_96: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_26, 1, 0)\n",
      "    transpose_97: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0);  linear_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(arg496_1, [768, 1536]);  arg496_1 = None\n",
      "    getitem_48: \"f32[768, 768]\" = split_with_sizes_6[0]\n",
      "    getitem_49: \"f32[1536, 768]\" = split_with_sizes_6[1];  split_with_sizes_6 = None\n",
      "    split_with_sizes_7 = torch.ops.aten.split_with_sizes.default(arg497_1, [768, 1536]);  arg497_1 = None\n",
      "    getitem_50: \"f32[768]\" = split_with_sizes_7[0]\n",
      "    getitem_51: \"f32[1536]\" = split_with_sizes_7[1];  split_with_sizes_7 = None\n",
      "    linear_96: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_96, getitem_48, getitem_50);  transpose_96 = getitem_48 = getitem_50 = None\n",
      "    linear_97: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_97, getitem_49, getitem_51);  transpose_97 = getitem_49 = getitem_51 = None\n",
      "    unflatten_10: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_97, -1, [2, 768]);  linear_97 = None\n",
      "    unsqueeze_18: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_10, 0);  unflatten_10 = None\n",
      "    transpose_98: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_18, 0, -2);  unsqueeze_18 = None\n",
      "    squeeze_10: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_98, -2);  transpose_98 = None\n",
      "    contiguous_20: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_10);  squeeze_10 = None\n",
      "    select_27: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 0)\n",
      "    select_28: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 1);  contiguous_20 = None\n",
      "    view_60: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_96, [32, 12, 64]);  linear_96 = None\n",
      "    transpose_99: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_60, 0, 1);  view_60 = None\n",
      "    view_61: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_27, [197, 12, 64]);  select_27 = None\n",
      "    transpose_100: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_61, 0, 1);  view_61 = None\n",
      "    view_62: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_28, [197, 12, 64]);  select_28 = None\n",
      "    transpose_101: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_62, 0, 1);  view_62 = None\n",
      "    mul_10: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_99, 0.125);  transpose_99 = None\n",
      "    transpose_102: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_100, -2, -1);  transpose_100 = None\n",
      "    bmm_13: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_10, transpose_102);  mul_10 = transpose_102 = None\n",
      "    softmax_10: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_13, -1);  bmm_13 = None\n",
      "    bmm_14: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_10, transpose_101);  transpose_101 = None\n",
      "    transpose_103: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_14, 0, 1);  bmm_14 = None\n",
      "    contiguous_21: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_103);  transpose_103 = None\n",
      "    view_63: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_21, [32, 768]);  contiguous_21 = None\n",
      "    linear_98: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_63, arg498_1, arg499_1);  view_63 = arg498_1 = arg499_1 = None\n",
      "    view_64: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_98, [32, 1, 768]);  linear_98 = None\n",
      "    view_65: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_10, [1, 12, 32, 197]);  softmax_10 = None\n",
      "    mean_10: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_65, [1]);  view_65 = mean_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_104: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_64, 1, 0);  view_64 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_49: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_26, transpose_104);  slice_26 = transpose_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_47: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_49, [768], arg500_1, arg501_1);  add_49 = arg500_1 = arg501_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_99: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_47, arg484_1, arg485_1);  arg484_1 = arg485_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_24: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_99);  linear_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_100: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_24, arg486_1, arg487_1);  gelu_24 = arg486_1 = arg487_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_49: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_100, 0.1, False);  linear_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_50: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_49, layer_norm_47);  dropout_49 = layer_norm_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_48: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_50, [768], arg488_1, arg489_1, 1e-12);  add_50 = arg488_1 = arg489_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_101: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_28, arg490_1, arg491_1);  arg490_1 = arg491_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_25: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_101);  linear_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_102: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_25, arg492_1, arg493_1);  gelu_25 = arg492_1 = arg493_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_50: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_102, 0.1, False);  linear_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_51: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_50, slice_28);  dropout_50 = slice_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_49: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_51, [768], arg494_1, arg495_1, 1e-12);  add_51 = arg494_1 = arg495_1 = layer_norm_49 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:155 in forward, code: itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
      "    numpy_t: \"f32[768, 30522]\" = torch.ops.aten.numpy_T.default(arg502_1);  arg502_1 = None\n",
      "    matmul: \"f32[1, 77, 30522]\" = torch.ops.aten.matmul.default(clone_5, numpy_t);  clone_5 = numpy_t = matmul = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_103: \"f32[1, 32, 512]\" = torch.ops.aten.linear.default(layer_norm_48, arg504_1, arg505_1);  layer_norm_48 = arg504_1 = arg505_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_4: \"f32[1, 77, 512]\" = torch.ops.aten.embedding.default(arg582_1, arg705_1);  arg705_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_1 = torch._C._set_grad_enabled(False);  _set_grad_enabled_1 = None\n",
      "    concat_7: \"f32[1, 109, 512]\" = torch.ops.aten.concat.default([linear_103, embedding_4], 1);  linear_103 = embedding_4 = None\n",
      "    ones: \"i64[1, 32]\" = torch.ops.aten.ones.default([1, 32], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    concat_8: \"i64[1, 109]\" = torch.ops.aten.concat.default([ones, arg706_1], 1);  ones = arg706_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1025 in forward, code: cache_position = torch.arange(\n",
      "    arange: \"i64[109]\" = torch.ops.aten.arange.start(0, 109, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1045 in forward, code: causal_mask = attention_mask[:, None, None, :]\n",
      "    slice_29: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_19: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_29, 1);  slice_29 = None\n",
      "    unsqueeze_20: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_19, 2);  unsqueeze_19 = None\n",
      "    slice_30: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_20, 3, 0, 9223372036854775807);  unsqueeze_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1046 in forward, code: causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n",
      "    to_3: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_30, torch.float32);  slice_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1047 in forward, code: causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n",
      "    rsub: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_3, 1.0);  to_3 = None\n",
      "    mul_11: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub, -3.4028234663852886e+38);  rsub = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_51: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(concat_7, 0.1, False);  concat_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_4: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(dropout_51, torch.float32);  dropout_51 = None\n",
      "    pow_1: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_4, 2)\n",
      "    mean_11: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n",
      "    add_52: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None\n",
      "    rsqrt: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_52);  add_52 = None\n",
      "    mul_12: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_4, rsqrt);  rsqrt = None\n",
      "    mul_13: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg513_1, mul_12);  arg513_1 = mul_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_104: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg508_1);  arg508_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_66: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_104, [1, -1, 6, 64]);  linear_104 = None\n",
      "    transpose_105: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_66, 1, 2);  view_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_105: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg509_1);  arg509_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_106: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg510_1);  mul_13 = arg510_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_67: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_105, [1, -1, 6, 64]);  linear_105 = None\n",
      "    transpose_106: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_67, 1, 2);  view_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_68: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_106, [1, -1, 6, 64]);  linear_106 = None\n",
      "    transpose_107: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_108: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_106, 3, 2);  transpose_106 = None\n",
      "    matmul_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_105, transpose_108);  transpose_105 = transpose_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_29: \"i64[]\" = torch.ops.aten.select.int(arange, 0, -1)\n",
      "    add_53: \"i64[]\" = torch.ops.aten.add.Tensor(select_29, 1);  select_29 = add_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_31: \"i64[109]\" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807);  arange = None\n",
      "    unsqueeze_21: \"i64[109, 1]\" = torch.ops.aten.unsqueeze.default(slice_31, 1);  slice_31 = None\n",
      "    to_5: \"i64[109, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_21, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_21 = None\n",
      "    arange_1: \"i64[109]\" = torch.ops.aten.arange.default(109, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_22: \"i64[1, 109]\" = torch.ops.aten.unsqueeze.default(arange_1, 0);  arange_1 = None\n",
      "    slice_32: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_22, 1, 0, 9223372036854775807);  unsqueeze_22 = None\n",
      "    sub: \"i64[109, 109]\" = torch.ops.aten.sub.Tensor(slice_32, to_5);  slice_32 = to_5 = None\n",
      "    gt_3: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(sub, 0)\n",
      "    to_6: \"i64[109, 109]\" = torch.ops.aten.to.dtype(gt_3, torch.int64);  gt_3 = None\n",
      "    mul_14: \"i64[109, 109]\" = torch.ops.aten.mul.Tensor(to_6, 16);  to_6 = None\n",
      "    add_54: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(mul_14, 0);  mul_14 = None\n",
      "    abs_1: \"i64[109, 109]\" = torch.ops.aten.abs.default(sub);  sub = None\n",
      "    lt: \"b8[109, 109]\" = torch.ops.aten.lt.Scalar(abs_1, 8)\n",
      "    to_7: \"f32[109, 109]\" = torch.ops.aten.to.dtype(abs_1, torch.float32)\n",
      "    div: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(to_7, 8);  to_7 = None\n",
      "    log: \"f32[109, 109]\" = torch.ops.aten.log.default(div);  div = None\n",
      "    div_1: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(log, 2.772588722239781);  log = None\n",
      "    mul_15: \"f32[109, 109]\" = torch.ops.aten.mul.Tensor(div_1, 8);  div_1 = None\n",
      "    to_8: \"i64[109, 109]\" = torch.ops.aten.to.dtype(mul_15, torch.int64);  mul_15 = None\n",
      "    add_55: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(to_8, 8);  to_8 = None\n",
      "    full_like_3: \"i64[109, 109]\" = torch.ops.aten.full_like.default(add_55, 15, pin_memory = False)\n",
      "    min_1: \"i64[109, 109]\" = torch.ops.aten.min.other(add_55, full_like_3);  add_55 = full_like_3 = None\n",
      "    where_3: \"i64[109, 109]\" = torch.ops.aten.where.self(lt, abs_1, min_1);  lt = abs_1 = min_1 = None\n",
      "    add_: \"i64[109, 109]\" = torch.ops.aten.add_.Tensor(add_54, where_3);  add_54 = where_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_5: \"f32[109, 109, 6]\" = torch.ops.aten.embedding.default(arg512_1, add_);  arg512_1 = add_ = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_12: \"f32[6, 109, 109]\" = torch.ops.aten.permute.default(embedding_5, [2, 0, 1]);  embedding_5 = None\n",
      "    unsqueeze_23: \"f32[1, 6, 109, 109]\" = torch.ops.aten.unsqueeze.default(permute_12, 0);  permute_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_33: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_23);  unsqueeze_23 = None\n",
      "    slice_34: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_33, 1);  slice_33 = None\n",
      "    slice_35: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_34, 2, -109);  slice_34 = None\n",
      "    slice_36: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_35, 3);  slice_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_37: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_11);  mul_11 = None\n",
      "    slice_38: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_37, 1);  slice_37 = None\n",
      "    slice_39: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_38, 2);  slice_38 = None\n",
      "    slice_40: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_39, 3, None, 109);  slice_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add.Tensor(slice_36, slice_40);  slice_36 = slice_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_1, add_56);  matmul_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__1, torch.float32);  add__1 = None\n",
      "    softmax_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_9, -1)\n",
      "    type_as: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_11, to_9);  softmax_11 = to_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_52: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as, 0.1, False);  type_as = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_2: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_52, transpose_107);  dropout_52 = transpose_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_109: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_2, 1, 2);  matmul_2 = None\n",
      "    contiguous_22: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_109);  transpose_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_69: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_22, [1, -1, 384]);  contiguous_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_107: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_69, arg511_1);  view_69 = arg511_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_53: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_107, 0.1, False);  linear_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_57: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_4, dropout_53);  to_4 = dropout_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_10: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_57, torch.float32);  add_57 = None\n",
      "    pow_2: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)\n",
      "    mean_12: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None\n",
      "    add_58: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None\n",
      "    rsqrt_1: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_58);  add_58 = None\n",
      "    mul_16: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_10, rsqrt_1);  rsqrt_1 = None\n",
      "    mul_17: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg517_1, mul_16);  arg517_1 = mul_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_108: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg514_1);  arg514_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_18: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_108, 0.5)\n",
      "    pow_3: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_108, 3.0)\n",
      "    mul_19: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_3, 0.044715);  pow_3 = None\n",
      "    add_59: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_108, mul_19);  linear_108 = mul_19 = None\n",
      "    mul_20: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_59, 0.7978845608028654);  add_59 = None\n",
      "    tanh: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_20);  mul_20 = None\n",
      "    add_60: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh, 1.0);  tanh = None\n",
      "    mul_21: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_18, add_60);  mul_18 = add_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_109: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg515_1);  mul_17 = arg515_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_22: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_21, linear_109);  mul_21 = linear_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_54: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_22, 0.1, False);  mul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_110: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_54, arg516_1);  dropout_54 = arg516_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_55: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_110, 0.1, False);  linear_110 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_61: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_10, dropout_55);  to_10 = dropout_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_11: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_61, torch.float32);  add_61 = None\n",
      "    pow_4: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_11, 2)\n",
      "    mean_13: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None\n",
      "    add_62: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None\n",
      "    rsqrt_2: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_62);  add_62 = None\n",
      "    mul_23: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_11, rsqrt_2);  rsqrt_2 = None\n",
      "    mul_24: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg522_1, mul_23);  arg522_1 = mul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_111: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg518_1);  arg518_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_70: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_111, [1, -1, 6, 64]);  linear_111 = None\n",
      "    transpose_110: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_70, 1, 2);  view_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_112: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg519_1);  arg519_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_113: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg520_1);  mul_24 = arg520_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_71: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_112, [1, -1, 6, 64]);  linear_112 = None\n",
      "    transpose_111: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_72: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_113, [1, -1, 6, 64]);  linear_113 = None\n",
      "    transpose_112: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_113: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_111, 3, 2);  transpose_111 = None\n",
      "    matmul_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_110, transpose_113);  transpose_110 = transpose_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_3, add_56);  matmul_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__2, torch.float32);  add__2 = None\n",
      "    softmax_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_12, -1)\n",
      "    type_as_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_12, to_12);  softmax_12 = to_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_1, 0.1, False);  type_as_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_56, transpose_112);  dropout_56 = transpose_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_114: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_4, 1, 2);  matmul_4 = None\n",
      "    contiguous_23: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_114);  transpose_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_73: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_23, [1, -1, 384]);  contiguous_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_114: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_73, arg521_1);  view_73 = arg521_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_57: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_114, 0.1, False);  linear_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_63: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_11, dropout_57);  to_11 = dropout_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_13: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_63, torch.float32);  add_63 = None\n",
      "    pow_5: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_13, 2)\n",
      "    mean_14: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None\n",
      "    add_64: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None\n",
      "    rsqrt_3: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_64);  add_64 = None\n",
      "    mul_25: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_13, rsqrt_3);  rsqrt_3 = None\n",
      "    mul_26: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg526_1, mul_25);  arg526_1 = mul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_115: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg523_1);  arg523_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_27: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_115, 0.5)\n",
      "    pow_6: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_115, 3.0)\n",
      "    mul_28: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_6, 0.044715);  pow_6 = None\n",
      "    add_65: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_115, mul_28);  linear_115 = mul_28 = None\n",
      "    mul_29: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_65, 0.7978845608028654);  add_65 = None\n",
      "    tanh_1: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_29);  mul_29 = None\n",
      "    add_66: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_1, 1.0);  tanh_1 = None\n",
      "    mul_30: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_27, add_66);  mul_27 = add_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_116: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg524_1);  mul_26 = arg524_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_31: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_30, linear_116);  mul_30 = linear_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_58: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_31, 0.1, False);  mul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_117: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_58, arg525_1);  dropout_58 = arg525_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_59: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_117, 0.1, False);  linear_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_67: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_13, dropout_59);  to_13 = dropout_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_14: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_67, torch.float32);  add_67 = None\n",
      "    pow_7: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)\n",
      "    mean_15: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_7, [-1], True);  pow_7 = None\n",
      "    add_68: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None\n",
      "    rsqrt_4: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_68);  add_68 = None\n",
      "    mul_32: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_14, rsqrt_4);  rsqrt_4 = None\n",
      "    mul_33: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg531_1, mul_32);  arg531_1 = mul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_118: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg527_1);  arg527_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_74: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_118, [1, -1, 6, 64]);  linear_118 = None\n",
      "    transpose_115: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_119: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg528_1);  arg528_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_120: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg529_1);  mul_33 = arg529_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_75: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_119, [1, -1, 6, 64]);  linear_119 = None\n",
      "    transpose_116: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_75, 1, 2);  view_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_76: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_120, [1, -1, 6, 64]);  linear_120 = None\n",
      "    transpose_117: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_76, 1, 2);  view_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_118: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_116, 3, 2);  transpose_116 = None\n",
      "    matmul_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_115, transpose_118);  transpose_115 = transpose_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_5, add_56);  matmul_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__3, torch.float32);  add__3 = None\n",
      "    softmax_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_15, -1)\n",
      "    type_as_2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_13, to_15);  softmax_13 = to_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_60: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_2, 0.1, False);  type_as_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_6: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_60, transpose_117);  dropout_60 = transpose_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_119: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_6, 1, 2);  matmul_6 = None\n",
      "    contiguous_24: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_119);  transpose_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_77: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_24, [1, -1, 384]);  contiguous_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_121: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_77, arg530_1);  view_77 = arg530_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_61: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_121, 0.1, False);  linear_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_69: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_14, dropout_61);  to_14 = dropout_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_16: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_69, torch.float32);  add_69 = None\n",
      "    pow_8: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_16, 2)\n",
      "    mean_16: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_8, [-1], True);  pow_8 = None\n",
      "    add_70: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None\n",
      "    rsqrt_5: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_70);  add_70 = None\n",
      "    mul_34: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_16, rsqrt_5);  rsqrt_5 = None\n",
      "    mul_35: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg535_1, mul_34);  arg535_1 = mul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_122: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg532_1);  arg532_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_36: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_122, 0.5)\n",
      "    pow_9: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_122, 3.0)\n",
      "    mul_37: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_9, 0.044715);  pow_9 = None\n",
      "    add_71: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_122, mul_37);  linear_122 = mul_37 = None\n",
      "    mul_38: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_71, 0.7978845608028654);  add_71 = None\n",
      "    tanh_2: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_38);  mul_38 = None\n",
      "    add_72: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_2, 1.0);  tanh_2 = None\n",
      "    mul_39: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_36, add_72);  mul_36 = add_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_123: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg533_1);  mul_35 = arg533_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_40: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_39, linear_123);  mul_39 = linear_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_62: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_40, 0.1, False);  mul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_124: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_62, arg534_1);  dropout_62 = arg534_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_63: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_124, 0.1, False);  linear_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_73: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_16, dropout_63);  to_16 = dropout_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_17: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_73, torch.float32);  add_73 = None\n",
      "    pow_10: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_17, 2)\n",
      "    mean_17: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_10, [-1], True);  pow_10 = None\n",
      "    add_74: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None\n",
      "    rsqrt_6: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_74);  add_74 = None\n",
      "    mul_41: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_17, rsqrt_6);  rsqrt_6 = None\n",
      "    mul_42: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg540_1, mul_41);  arg540_1 = mul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_125: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg536_1);  arg536_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_78: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_125, [1, -1, 6, 64]);  linear_125 = None\n",
      "    transpose_120: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_126: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg537_1);  arg537_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_127: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg538_1);  mul_42 = arg538_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_79: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_126, [1, -1, 6, 64]);  linear_126 = None\n",
      "    transpose_121: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_80: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_127, [1, -1, 6, 64]);  linear_127 = None\n",
      "    transpose_122: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_123: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_121, 3, 2);  transpose_121 = None\n",
      "    matmul_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_120, transpose_123);  transpose_120 = transpose_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_7, add_56);  matmul_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__4, torch.float32);  add__4 = None\n",
      "    softmax_14: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_18, -1)\n",
      "    type_as_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_14, to_18);  softmax_14 = to_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_64: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_3, 0.1, False);  type_as_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_64, transpose_122);  dropout_64 = transpose_122 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_124: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_8, 1, 2);  matmul_8 = None\n",
      "    contiguous_25: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_124);  transpose_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_81: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_25, [1, -1, 384]);  contiguous_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_128: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_81, arg539_1);  view_81 = arg539_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_65: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_128, 0.1, False);  linear_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_75: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_17, dropout_65);  to_17 = dropout_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_19: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_75, torch.float32);  add_75 = None\n",
      "    pow_11: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_19, 2)\n",
      "    mean_18: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_11, [-1], True);  pow_11 = None\n",
      "    add_76: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None\n",
      "    rsqrt_7: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_76);  add_76 = None\n",
      "    mul_43: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_19, rsqrt_7);  rsqrt_7 = None\n",
      "    mul_44: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg544_1, mul_43);  arg544_1 = mul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_129: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg541_1);  arg541_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_45: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_129, 0.5)\n",
      "    pow_12: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_129, 3.0)\n",
      "    mul_46: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_12, 0.044715);  pow_12 = None\n",
      "    add_77: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_129, mul_46);  linear_129 = mul_46 = None\n",
      "    mul_47: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_77, 0.7978845608028654);  add_77 = None\n",
      "    tanh_3: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_47);  mul_47 = None\n",
      "    add_78: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_3, 1.0);  tanh_3 = None\n",
      "    mul_48: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_45, add_78);  mul_45 = add_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_130: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg542_1);  mul_44 = arg542_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_49: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_48, linear_130);  mul_48 = linear_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_66: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_49, 0.1, False);  mul_49 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_131: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_66, arg543_1);  dropout_66 = arg543_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_67: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_131, 0.1, False);  linear_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_79: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_19, dropout_67);  to_19 = dropout_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_20: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_79, torch.float32);  add_79 = None\n",
      "    pow_13: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_20, 2)\n",
      "    mean_19: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_13, [-1], True);  pow_13 = None\n",
      "    add_80: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None\n",
      "    rsqrt_8: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_80);  add_80 = None\n",
      "    mul_50: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_20, rsqrt_8);  rsqrt_8 = None\n",
      "    mul_51: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg549_1, mul_50);  arg549_1 = mul_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_132: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg545_1);  arg545_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_82: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_132, [1, -1, 6, 64]);  linear_132 = None\n",
      "    transpose_125: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_82, 1, 2);  view_82 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_133: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg546_1);  arg546_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_134: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg547_1);  mul_51 = arg547_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_83: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_133, [1, -1, 6, 64]);  linear_133 = None\n",
      "    transpose_126: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_84: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_134, [1, -1, 6, 64]);  linear_134 = None\n",
      "    transpose_127: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_84, 1, 2);  view_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_128: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_126, 3, 2);  transpose_126 = None\n",
      "    matmul_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_125, transpose_128);  transpose_125 = transpose_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_9, add_56);  matmul_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_21: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__5, torch.float32);  add__5 = None\n",
      "    softmax_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_21, -1)\n",
      "    type_as_4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_15, to_21);  softmax_15 = to_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_68: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_4, 0.1, False);  type_as_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_10: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_68, transpose_127);  dropout_68 = transpose_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_10, 1, 2);  matmul_10 = None\n",
      "    contiguous_26: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_129);  transpose_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_85: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_26, [1, -1, 384]);  contiguous_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_135: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_85, arg548_1);  view_85 = arg548_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_69: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_135, 0.1, False);  linear_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_81: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_20, dropout_69);  to_20 = dropout_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_22: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_81, torch.float32);  add_81 = None\n",
      "    pow_14: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_22, 2)\n",
      "    mean_20: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_14, [-1], True);  pow_14 = None\n",
      "    add_82: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_20, 1e-06);  mean_20 = None\n",
      "    rsqrt_9: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_82);  add_82 = None\n",
      "    mul_52: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_22, rsqrt_9);  rsqrt_9 = None\n",
      "    mul_53: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg553_1, mul_52);  arg553_1 = mul_52 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_136: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg550_1);  arg550_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_54: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_136, 0.5)\n",
      "    pow_15: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_136, 3.0)\n",
      "    mul_55: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_15, 0.044715);  pow_15 = None\n",
      "    add_83: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_136, mul_55);  linear_136 = mul_55 = None\n",
      "    mul_56: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_83, 0.7978845608028654);  add_83 = None\n",
      "    tanh_4: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_56);  mul_56 = None\n",
      "    add_84: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_4, 1.0);  tanh_4 = None\n",
      "    mul_57: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_54, add_84);  mul_54 = add_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_137: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg551_1);  mul_53 = arg551_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_58: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_57, linear_137);  mul_57 = linear_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_70: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_58, 0.1, False);  mul_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_138: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_70, arg552_1);  dropout_70 = arg552_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_71: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_138, 0.1, False);  linear_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_85: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_22, dropout_71);  to_22 = dropout_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_23: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_85, torch.float32);  add_85 = None\n",
      "    pow_16: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_23, 2)\n",
      "    mean_21: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_16, [-1], True);  pow_16 = None\n",
      "    add_86: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_21, 1e-06);  mean_21 = None\n",
      "    rsqrt_10: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_86);  add_86 = None\n",
      "    mul_59: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_23, rsqrt_10);  rsqrt_10 = None\n",
      "    mul_60: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg558_1, mul_59);  arg558_1 = mul_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_139: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg554_1);  arg554_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_86: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_139, [1, -1, 6, 64]);  linear_139 = None\n",
      "    transpose_130: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_86, 1, 2);  view_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_140: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg555_1);  arg555_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_141: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg556_1);  mul_60 = arg556_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_87: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_140, [1, -1, 6, 64]);  linear_140 = None\n",
      "    transpose_131: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_87, 1, 2);  view_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_88: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_141, [1, -1, 6, 64]);  linear_141 = None\n",
      "    transpose_132: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_88, 1, 2);  view_88 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_133: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_131, 3, 2);  transpose_131 = None\n",
      "    matmul_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_130, transpose_133);  transpose_130 = transpose_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_11, add_56);  matmul_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_24: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__6, torch.float32);  add__6 = None\n",
      "    softmax_16: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_24, -1)\n",
      "    type_as_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_16, to_24);  softmax_16 = to_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_72: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_5, 0.1, False);  type_as_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_72, transpose_132);  dropout_72 = transpose_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_134: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_12, 1, 2);  matmul_12 = None\n",
      "    contiguous_27: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_134);  transpose_134 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_89: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_27, [1, -1, 384]);  contiguous_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_142: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_89, arg557_1);  view_89 = arg557_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_73: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_142, 0.1, False);  linear_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_87: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_23, dropout_73);  to_23 = dropout_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_25: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_87, torch.float32);  add_87 = None\n",
      "    pow_17: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_25, 2)\n",
      "    mean_22: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_17, [-1], True);  pow_17 = None\n",
      "    add_88: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_22, 1e-06);  mean_22 = None\n",
      "    rsqrt_11: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_88);  add_88 = None\n",
      "    mul_61: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_25, rsqrt_11);  rsqrt_11 = None\n",
      "    mul_62: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg562_1, mul_61);  arg562_1 = mul_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_143: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg559_1);  arg559_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_63: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_143, 0.5)\n",
      "    pow_18: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_143, 3.0)\n",
      "    mul_64: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_18, 0.044715);  pow_18 = None\n",
      "    add_89: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_143, mul_64);  linear_143 = mul_64 = None\n",
      "    mul_65: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_89, 0.7978845608028654);  add_89 = None\n",
      "    tanh_5: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_65);  mul_65 = None\n",
      "    add_90: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_5, 1.0);  tanh_5 = None\n",
      "    mul_66: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_63, add_90);  mul_63 = add_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_144: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg560_1);  mul_62 = arg560_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_67: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_66, linear_144);  mul_66 = linear_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_74: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_67, 0.1, False);  mul_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_145: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_74, arg561_1);  dropout_74 = arg561_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_75: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_145, 0.1, False);  linear_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_91: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_25, dropout_75);  to_25 = dropout_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_26: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_91, torch.float32);  add_91 = None\n",
      "    pow_19: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_26, 2)\n",
      "    mean_23: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_19, [-1], True);  pow_19 = None\n",
      "    add_92: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_23, 1e-06);  mean_23 = None\n",
      "    rsqrt_12: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_92);  add_92 = None\n",
      "    mul_68: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_26, rsqrt_12);  rsqrt_12 = None\n",
      "    mul_69: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg567_1, mul_68);  arg567_1 = mul_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_146: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg563_1);  arg563_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_90: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_146, [1, -1, 6, 64]);  linear_146 = None\n",
      "    transpose_135: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_90, 1, 2);  view_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_147: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg564_1);  arg564_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_148: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg565_1);  mul_69 = arg565_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_91: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_147, [1, -1, 6, 64]);  linear_147 = None\n",
      "    transpose_136: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_91, 1, 2);  view_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_92: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_148, [1, -1, 6, 64]);  linear_148 = None\n",
      "    transpose_137: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_138: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_136, 3, 2);  transpose_136 = None\n",
      "    matmul_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_135, transpose_138);  transpose_135 = transpose_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_13, add_56);  matmul_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_27: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__7, torch.float32);  add__7 = None\n",
      "    softmax_17: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_27, -1)\n",
      "    type_as_6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_17, to_27);  softmax_17 = to_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_76: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_6, 0.1, False);  type_as_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_14: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_76, transpose_137);  dropout_76 = transpose_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_139: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_14, 1, 2);  matmul_14 = None\n",
      "    contiguous_28: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_139);  transpose_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_93: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_28, [1, -1, 384]);  contiguous_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_149: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_93, arg566_1);  view_93 = arg566_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_77: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_149, 0.1, False);  linear_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_93: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_26, dropout_77);  to_26 = dropout_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_28: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_93, torch.float32);  add_93 = None\n",
      "    pow_20: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_28, 2)\n",
      "    mean_24: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_20, [-1], True);  pow_20 = None\n",
      "    add_94: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_24, 1e-06);  mean_24 = None\n",
      "    rsqrt_13: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_94);  add_94 = None\n",
      "    mul_70: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_28, rsqrt_13);  rsqrt_13 = None\n",
      "    mul_71: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg571_1, mul_70);  arg571_1 = mul_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_150: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg568_1);  arg568_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_72: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_150, 0.5)\n",
      "    pow_21: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_150, 3.0)\n",
      "    mul_73: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_21, 0.044715);  pow_21 = None\n",
      "    add_95: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_150, mul_73);  linear_150 = mul_73 = None\n",
      "    mul_74: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_95, 0.7978845608028654);  add_95 = None\n",
      "    tanh_6: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_74);  mul_74 = None\n",
      "    add_96: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_6, 1.0);  tanh_6 = None\n",
      "    mul_75: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_72, add_96);  mul_72 = add_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_151: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg569_1);  mul_71 = arg569_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_76: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_75, linear_151);  mul_75 = linear_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_78: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_76, 0.1, False);  mul_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_152: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_78, arg570_1);  dropout_78 = arg570_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_79: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_152, 0.1, False);  linear_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_97: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_28, dropout_79);  to_28 = dropout_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_29: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_97, torch.float32);  add_97 = None\n",
      "    pow_22: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_29, 2)\n",
      "    mean_25: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_22, [-1], True);  pow_22 = None\n",
      "    add_98: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_25, 1e-06);  mean_25 = None\n",
      "    rsqrt_14: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_98);  add_98 = None\n",
      "    mul_77: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_29, rsqrt_14);  rsqrt_14 = None\n",
      "    mul_78: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg576_1, mul_77);  arg576_1 = mul_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_153: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg572_1);  arg572_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_94: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_153, [1, -1, 6, 64]);  linear_153 = None\n",
      "    transpose_140: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_94, 1, 2);  view_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_154: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg573_1);  arg573_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_155: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg574_1);  mul_78 = arg574_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_95: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_154, [1, -1, 6, 64]);  linear_154 = None\n",
      "    transpose_141: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_95, 1, 2);  view_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_96: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_155, [1, -1, 6, 64]);  linear_155 = None\n",
      "    transpose_142: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_96, 1, 2);  view_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_143: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_141, 3, 2);  transpose_141 = None\n",
      "    matmul_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_140, transpose_143);  transpose_140 = transpose_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__8: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_15, add_56);  matmul_15 = add_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_30: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__8, torch.float32);  add__8 = None\n",
      "    softmax_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_30, -1)\n",
      "    type_as_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_18, to_30);  softmax_18 = to_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_80: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_7, 0.1, False);  type_as_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_80, transpose_142);  dropout_80 = transpose_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_16, 1, 2);  matmul_16 = None\n",
      "    contiguous_29: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_144);  transpose_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_97: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_29, [1, -1, 384]);  contiguous_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_156: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_97, arg575_1);  view_97 = arg575_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_81: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_156, 0.1, False);  linear_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_99: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_29, dropout_81);  to_29 = dropout_81 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_31: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_99, torch.float32);  add_99 = None\n",
      "    pow_23: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_31, 2)\n",
      "    mean_26: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_23, [-1], True);  pow_23 = None\n",
      "    add_100: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_26, 1e-06);  mean_26 = None\n",
      "    rsqrt_15: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_100);  add_100 = None\n",
      "    mul_79: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_31, rsqrt_15);  rsqrt_15 = None\n",
      "    mul_80: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg580_1, mul_79);  arg580_1 = mul_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_157: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg577_1);  arg577_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_81: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_157, 0.5)\n",
      "    pow_24: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_157, 3.0)\n",
      "    mul_82: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_24, 0.044715);  pow_24 = None\n",
      "    add_101: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_157, mul_82);  linear_157 = mul_82 = None\n",
      "    mul_83: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_101, 0.7978845608028654);  add_101 = None\n",
      "    tanh_7: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_83);  mul_83 = None\n",
      "    add_102: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_7, 1.0);  tanh_7 = None\n",
      "    mul_84: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_81, add_102);  mul_81 = add_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_158: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg578_1);  mul_80 = arg578_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_85: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_84, linear_158);  mul_84 = linear_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_82: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_85, 0.1, False);  mul_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_159: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_82, arg579_1);  dropout_82 = arg579_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_83: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_159, 0.1, False);  linear_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_103: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_31, dropout_83);  to_31 = dropout_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_32: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_103, torch.float32);  add_103 = None\n",
      "    pow_25: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_32, 2)\n",
      "    mean_27: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_25, [-1], True);  pow_25 = None\n",
      "    add_104: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_27, 1e-06);  mean_27 = None\n",
      "    rsqrt_16: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_104);  add_104 = None\n",
      "    mul_86: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_32, rsqrt_16);  to_32 = rsqrt_16 = None\n",
      "    mul_87: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg581_1, mul_86);  arg581_1 = mul_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_84: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(mul_87, 0.1, False);  mul_87 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_2 = torch._C._set_grad_enabled(False);  _set_grad_enabled_2 = None\n",
      "    ones_1: \"i64[1, 109]\" = torch.ops.aten.ones.default([1, 109], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_88: \"i64[1, 109]\" = torch.ops.aten.mul.Tensor(ones_1, -100);  ones_1 = mul_88 = None\n",
      "    _tensor_constant0 = self._tensor_constant0\n",
      "    lift_fresh_copy: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "    detach_: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n",
      "    _tensor_constant1 = self._tensor_constant1\n",
      "    lift_fresh_copy_1: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
      "    detach__1: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None\n",
      "    _tensor_constant2 = self._tensor_constant2\n",
      "    lift_fresh_copy_2: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
      "    detach__2: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_2);  lift_fresh_copy_2 = None\n",
      "    unsqueeze_24: \"i64[1]\" = torch.ops.aten.unsqueeze.default(detach_, 0);  detach_ = None\n",
      "    isin: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(unsqueeze_24, detach__1)\n",
      "    any_1: \"b8[]\" = torch.ops.aten.any.default(isin);  isin = None\n",
      "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(any_1, 0);  any_1 = ne = None\n",
      "    lt_1: \"b8[1]\" = torch.ops.aten.lt.Scalar(unsqueeze_24, 0)\n",
      "    any_2: \"b8[]\" = torch.ops.aten.any.default(lt_1);  lt_1 = None\n",
      "    ne_1: \"b8[]\" = torch.ops.aten.ne.Scalar(any_2, 0);  any_2 = ne_1 = None\n",
      "    ones_2: \"i64[1, 1]\" = torch.ops.aten.ones.default([1, 1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_89: \"i64[1, 1]\" = torch.ops.aten.mul.Tensor(ones_2, detach__2);  ones_2 = detach__2 = None\n",
      "    ones_3: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    ones_4: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    cumsum: \"i64[1]\" = torch.ops.aten.cumsum.default(ones_4, 0);  ones_4 = None\n",
      "    sub_1: \"i64[1]\" = torch.ops.aten.sub.Tensor(cumsum, 1);  cumsum = None\n",
      "    slice_41: \"i64[1]\" = torch.ops.aten.slice.Tensor(sub_1, 0, 0);  sub_1 = None\n",
      "    slice_42: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(mul_89, 0, 0, 9223372036854775807)\n",
      "    index: \"i64[1, 1]\" = torch.ops.aten.index.Tensor(slice_42, [None, slice_41]);  slice_42 = None\n",
      "    clone_6: \"i64[1, 1]\" = torch.ops.aten.clone.default(index, memory_format = torch.contiguous_format);  index = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:985 in forward, code: input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    view_98: \"i64[1, 1]\" = torch.ops.aten.view.default(clone_6, [-1, 1]);  clone_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_6: \"f32[1, 1, 512]\" = torch.ops.aten.embedding.default(arg582_1, view_98);  arg582_1 = view_98 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1035 in forward, code: causal_mask = self._update_causal_mask(\n",
      "    full: \"f32[1, 2]\" = torch.ops.aten.full.default([1, 2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_2: \"i64[2]\" = torch.ops.aten.arange.default(2, device = device(type='cpu'), pin_memory = False)\n",
      "    reshape_24: \"i64[1, 1]\" = torch.ops.aten.reshape.default(slice_41, [-1, 1])\n",
      "    gt_4: \"b8[1, 2]\" = torch.ops.aten.gt.Tensor(arange_2, reshape_24);  arange_2 = reshape_24 = None\n",
      "    mul_: \"f32[1, 2]\" = torch.ops.aten.mul_.Tensor(full, gt_4);  full = gt_4 = None\n",
      "    unsqueeze_25: \"f32[1, 1, 2]\" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None\n",
      "    unsqueeze_26: \"f32[1, 1, 1, 2]\" = torch.ops.aten.unsqueeze.default(unsqueeze_25, 1);  unsqueeze_25 = None\n",
      "    slice_43: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(unsqueeze_26, 2, 0, 9223372036854775807);  unsqueeze_26 = None\n",
      "    slice_44: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_43, 3, 0, 9223372036854775807);  slice_43 = None\n",
      "    expand_4: \"f32[1, 1, 1, 2]\" = torch.ops.aten.expand.default(slice_44, [1, 1, -1, -1]);  slice_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1060 in forward, code: encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
      "    slice_45: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807);  concat_8 = None\n",
      "    unsqueeze_27: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_45, 1);  slice_45 = None\n",
      "    unsqueeze_28: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_27, 2);  unsqueeze_27 = None\n",
      "    slice_46: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_28, 3, 0, 9223372036854775807);  unsqueeze_28 = None\n",
      "    to_33: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_46, torch.float32);  slice_46 = None\n",
      "    rsub_1: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_33, 1.0);  to_33 = None\n",
      "    mul_90: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub_1, -3.4028234663852886e+38);  rsub_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_85: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(embedding_6, 0.1, False);  embedding_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_34: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(dropout_85, torch.float32);  dropout_85 = None\n",
      "    pow_26: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_34, 2)\n",
      "    mean_28: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_26, [-1], True);  pow_26 = None\n",
      "    add_105: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_28, 1e-06);  mean_28 = None\n",
      "    rsqrt_17: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_105);  add_105 = None\n",
      "    mul_91: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_34, rsqrt_17);  rsqrt_17 = None\n",
      "    mul_92: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg588_1, mul_91);  arg588_1 = mul_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_160: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg583_1);  arg583_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_99: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_160, [1, -1, 6, 64]);  linear_160 = None\n",
      "    transpose_145: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_161: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg584_1);  arg584_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_162: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg585_1);  mul_92 = arg585_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_100: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_161, [1, -1, 6, 64]);  linear_161 = None\n",
      "    transpose_146: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_100, 1, 2);  view_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_101: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_162, [1, -1, 6, 64]);  linear_162 = None\n",
      "    transpose_147: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_101, 1, 2);  view_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant3 = self._tensor_constant3\n",
      "    lift_fresh_copy_3: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
      "    detach__3: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_3);  lift_fresh_copy_3 = None\n",
      "    _tensor_constant4 = self._tensor_constant4\n",
      "    lift_fresh_copy_4: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
      "    detach__4: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_4);  lift_fresh_copy_4 = None\n",
      "    cat_1: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__3, transpose_146], -2);  detach__3 = transpose_146 = None\n",
      "    cat_2: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__4, transpose_147], -2);  detach__4 = transpose_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_148: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_1, 3, 2);  cat_1 = None\n",
      "    matmul_17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_145, transpose_148);  transpose_145 = transpose_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_30: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_106: \"i64[]\" = torch.ops.aten.add.Tensor(select_30, 1);  select_30 = add_106 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_47: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_29: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_47, 1);  slice_47 = None\n",
      "    to_35: \"i64[1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_29, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_29 = None\n",
      "    arange_3: \"i64[1]\" = torch.ops.aten.arange.default(1, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_30: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None\n",
      "    slice_48: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_30, 1, 0, 9223372036854775807);  unsqueeze_30 = None\n",
      "    sub_2: \"i64[1, 1]\" = torch.ops.aten.sub.Tensor(slice_48, to_35);  slice_48 = to_35 = None\n",
      "    zeros_like_3: \"i64[1, 1]\" = torch.ops.aten.zeros_like.default(sub_2, pin_memory = False)\n",
      "    min_2: \"i64[1, 1]\" = torch.ops.aten.min.other(sub_2, zeros_like_3);  sub_2 = zeros_like_3 = None\n",
      "    neg: \"i64[1, 1]\" = torch.ops.aten.neg.default(min_2);  min_2 = None\n",
      "    lt_2: \"b8[1, 1]\" = torch.ops.aten.lt.Scalar(neg, 16)\n",
      "    to_36: \"f32[1, 1]\" = torch.ops.aten.to.dtype(neg, torch.float32)\n",
      "    div_2: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(to_36, 16);  to_36 = None\n",
      "    log_1: \"f32[1, 1]\" = torch.ops.aten.log.default(div_2);  div_2 = None\n",
      "    div_3: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(log_1, 2.0794415416798357);  log_1 = None\n",
      "    mul_93: \"f32[1, 1]\" = torch.ops.aten.mul.Tensor(div_3, 16);  div_3 = None\n",
      "    to_37: \"i64[1, 1]\" = torch.ops.aten.to.dtype(mul_93, torch.int64);  mul_93 = None\n",
      "    add_107: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(to_37, 16);  to_37 = None\n",
      "    full_like_4: \"i64[1, 1]\" = torch.ops.aten.full_like.default(add_107, 31, pin_memory = False)\n",
      "    min_3: \"i64[1, 1]\" = torch.ops.aten.min.other(add_107, full_like_4);  add_107 = full_like_4 = None\n",
      "    where_4: \"i64[1, 1]\" = torch.ops.aten.where.self(lt_2, neg, min_3);  lt_2 = neg = min_3 = None\n",
      "    add_108: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(where_4, 0);  where_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_7: \"f32[1, 1, 6]\" = torch.ops.aten.embedding.default(arg587_1, add_108);  arg587_1 = add_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_13: \"f32[6, 1, 1]\" = torch.ops.aten.permute.default(embedding_7, [2, 0, 1]);  embedding_7 = None\n",
      "    unsqueeze_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.unsqueeze.default(permute_13, 0);  permute_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_49: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_31);  unsqueeze_31 = None\n",
      "    slice_50: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_49, 1);  slice_49 = None\n",
      "    slice_51: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_50, 2, -1);  slice_50 = None\n",
      "    slice_52: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_51, 3);  slice_51 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_53: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(expand_4);  expand_4 = None\n",
      "    slice_54: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_53, 1);  slice_53 = None\n",
      "    slice_55: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_54, 2);  slice_54 = None\n",
      "    slice_56: \"f32[1, 1, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_55, 3, None, 1);  slice_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_109: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add.Tensor(slice_52, slice_56);  slice_52 = slice_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__9: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_17, add_109);  matmul_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_38: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__9, torch.float32);  add__9 = None\n",
      "    softmax_19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_38, -1)\n",
      "    type_as_8: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_19, to_38);  softmax_19 = to_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_86: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_8, 0.1, False);  type_as_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_86, cat_2);  dropout_86 = cat_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_18, 1, 2);  matmul_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_102: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_149, [1, -1, 384]);  transpose_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_163: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_102, arg586_1);  view_102 = arg586_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_87: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_163, 0.1, False);  linear_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_110: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_34, dropout_87);  to_34 = dropout_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_31: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_111: \"i64[]\" = torch.ops.aten.add.Tensor(select_31, 1);  select_31 = add_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_39: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_110, torch.float32);  add_110 = None\n",
      "    pow_27: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_39, 2)\n",
      "    mean_29: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_27, [-1], True);  pow_27 = None\n",
      "    add_112: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_29, 1e-06);  mean_29 = None\n",
      "    rsqrt_18: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_112);  add_112 = None\n",
      "    mul_94: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_39, rsqrt_18);  rsqrt_18 = None\n",
      "    mul_95: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg593_1, mul_94);  arg593_1 = mul_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_164: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_95, arg589_1);  mul_95 = arg589_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_103: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_164, [1, -1, 6, 64]);  linear_164 = None\n",
      "    transpose_150: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_103, 1, 2);  view_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_165: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg590_1);  arg590_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_166: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg591_1);  arg591_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_104: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_165, [1, -1, 6, 64]);  linear_165 = None\n",
      "    transpose_151: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_104, 1, 2);  view_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_105: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_166, [1, -1, 6, 64]);  linear_166 = None\n",
      "    transpose_152: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_105, 1, 2);  view_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant5 = self._tensor_constant5\n",
      "    lift_fresh_copy_5: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
      "    detach__5: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_5);  lift_fresh_copy_5 = None\n",
      "    _tensor_constant6 = self._tensor_constant6\n",
      "    lift_fresh_copy_6: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
      "    detach__6: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_6);  lift_fresh_copy_6 = None\n",
      "    cat_3: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__5, transpose_151], -2);  detach__5 = transpose_151 = None\n",
      "    cat_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__6, transpose_152], -2);  detach__6 = transpose_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_153: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_3, 3, 2);  cat_3 = None\n",
      "    matmul_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_150, transpose_153);  transpose_150 = transpose_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:535 in forward, code: position_bias = torch.zeros(\n",
      "    zeros_2: \"f32[1, 6, 1, 109]\" = torch.ops.aten.zeros.default([1, 6, 1, 109], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_57: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_90);  mul_90 = None\n",
      "    slice_58: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_57, 1);  slice_57 = None\n",
      "    slice_59: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_58, 2);  slice_58 = None\n",
      "    slice_60: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_59, 3, None, 109);  slice_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_113: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add.Tensor(zeros_2, slice_60);  zeros_2 = slice_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__10: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_19, add_113);  matmul_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_40: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__10, torch.float32);  add__10 = None\n",
      "    softmax_20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_40, -1)\n",
      "    type_as_9: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_20, to_40);  softmax_20 = to_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_88: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_9, 0.1, False);  type_as_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_20: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_88, cat_4);  dropout_88 = cat_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_154: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_20, 1, 2);  matmul_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_106: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_154, [1, -1, 384]);  transpose_154 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_167: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_106, arg592_1);  view_106 = arg592_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_89: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_167, 0.1, False);  linear_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_114: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_39, dropout_89);  to_39 = dropout_89 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_41: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_114, torch.float32);  add_114 = None\n",
      "    pow_28: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_41, 2)\n",
      "    mean_30: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_28, [-1], True);  pow_28 = None\n",
      "    add_115: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_30, 1e-06);  mean_30 = None\n",
      "    rsqrt_19: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_115);  add_115 = None\n",
      "    mul_96: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_41, rsqrt_19);  rsqrt_19 = None\n",
      "    mul_97: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg597_1, mul_96);  arg597_1 = mul_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_168: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg594_1);  arg594_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_98: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_168, 0.5)\n",
      "    pow_29: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_168, 3.0)\n",
      "    mul_99: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_29, 0.044715);  pow_29 = None\n",
      "    add_116: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_168, mul_99);  linear_168 = mul_99 = None\n",
      "    mul_100: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_116, 0.7978845608028654);  add_116 = None\n",
      "    tanh_8: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_100);  mul_100 = None\n",
      "    add_117: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_8, 1.0);  tanh_8 = None\n",
      "    mul_101: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_98, add_117);  mul_98 = add_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_169: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg595_1);  mul_97 = arg595_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_102: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_101, linear_169);  mul_101 = linear_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_90: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_102, 0.1, False);  mul_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_170: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_90, arg596_1);  dropout_90 = arg596_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_91: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_170, 0.1, False);  linear_170 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_118: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_41, dropout_91);  to_41 = dropout_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_42: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_118, torch.float32);  add_118 = None\n",
      "    pow_30: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_42, 2)\n",
      "    mean_31: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_30, [-1], True);  pow_30 = None\n",
      "    add_119: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_31, 1e-06);  mean_31 = None\n",
      "    rsqrt_20: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_119);  add_119 = None\n",
      "    mul_103: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_42, rsqrt_20);  rsqrt_20 = None\n",
      "    mul_104: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg602_1, mul_103);  arg602_1 = mul_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_171: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg598_1);  arg598_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_107: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_171, [1, -1, 6, 64]);  linear_171 = None\n",
      "    transpose_155: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_107, 1, 2);  view_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_172: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg599_1);  arg599_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_173: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg600_1);  mul_104 = arg600_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_108: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_172, [1, -1, 6, 64]);  linear_172 = None\n",
      "    transpose_156: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_108, 1, 2);  view_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_109: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_173, [1, -1, 6, 64]);  linear_173 = None\n",
      "    transpose_157: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_109, 1, 2);  view_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant7 = self._tensor_constant7\n",
      "    lift_fresh_copy_7: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None\n",
      "    detach__7: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_7);  lift_fresh_copy_7 = None\n",
      "    _tensor_constant8 = self._tensor_constant8\n",
      "    lift_fresh_copy_8: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None\n",
      "    detach__8: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_8);  lift_fresh_copy_8 = None\n",
      "    cat_5: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__7, transpose_156], -2);  detach__7 = transpose_156 = None\n",
      "    cat_6: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__8, transpose_157], -2);  detach__8 = transpose_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_158: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_5, 3, 2);  cat_5 = None\n",
      "    matmul_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_155, transpose_158);  transpose_155 = transpose_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__11: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_21, add_109);  matmul_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_43: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__11, torch.float32);  add__11 = None\n",
      "    softmax_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_43, -1)\n",
      "    type_as_10: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_21, to_43);  softmax_21 = to_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_92: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_10, 0.1, False);  type_as_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_92, cat_6);  dropout_92 = cat_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_22, 1, 2);  matmul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_110: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_159, [1, -1, 384]);  transpose_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_174: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_110, arg601_1);  view_110 = arg601_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_93: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_174, 0.1, False);  linear_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_120: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_42, dropout_93);  to_42 = dropout_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_32: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_121: \"i64[]\" = torch.ops.aten.add.Tensor(select_32, 1);  select_32 = add_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_44: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_120, torch.float32);  add_120 = None\n",
      "    pow_31: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_44, 2)\n",
      "    mean_32: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_31, [-1], True);  pow_31 = None\n",
      "    add_122: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_32, 1e-06);  mean_32 = None\n",
      "    rsqrt_21: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_122);  add_122 = None\n",
      "    mul_105: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_44, rsqrt_21);  rsqrt_21 = None\n",
      "    mul_106: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg607_1, mul_105);  arg607_1 = mul_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_175: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_106, arg603_1);  mul_106 = arg603_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_111: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_175, [1, -1, 6, 64]);  linear_175 = None\n",
      "    transpose_160: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_111, 1, 2);  view_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_176: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg604_1);  arg604_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_177: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg605_1);  arg605_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_112: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_176, [1, -1, 6, 64]);  linear_176 = None\n",
      "    transpose_161: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_112, 1, 2);  view_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_113: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_177, [1, -1, 6, 64]);  linear_177 = None\n",
      "    transpose_162: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant9 = self._tensor_constant9\n",
      "    lift_fresh_copy_9: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None\n",
      "    detach__9: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_9);  lift_fresh_copy_9 = None\n",
      "    _tensor_constant10 = self._tensor_constant10\n",
      "    lift_fresh_copy_10: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant10);  _tensor_constant10 = None\n",
      "    detach__10: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_10);  lift_fresh_copy_10 = None\n",
      "    cat_7: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__9, transpose_161], -2);  detach__9 = transpose_161 = None\n",
      "    cat_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__10, transpose_162], -2);  detach__10 = transpose_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_163: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_7, 3, 2);  cat_7 = None\n",
      "    matmul_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_160, transpose_163);  transpose_160 = transpose_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__12: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_23, add_113);  matmul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_45: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__12, torch.float32);  add__12 = None\n",
      "    softmax_22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_45, -1)\n",
      "    type_as_11: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_22, to_45);  softmax_22 = to_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_94: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_11, 0.1, False);  type_as_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_24: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_94, cat_8);  dropout_94 = cat_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_164: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_24, 1, 2);  matmul_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_114: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_164, [1, -1, 384]);  transpose_164 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_178: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_114, arg606_1);  view_114 = arg606_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_95: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_178, 0.1, False);  linear_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_123: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_44, dropout_95);  to_44 = dropout_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_46: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_123, torch.float32);  add_123 = None\n",
      "    pow_32: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_46, 2)\n",
      "    mean_33: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_32, [-1], True);  pow_32 = None\n",
      "    add_124: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_33, 1e-06);  mean_33 = None\n",
      "    rsqrt_22: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_124);  add_124 = None\n",
      "    mul_107: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_46, rsqrt_22);  rsqrt_22 = None\n",
      "    mul_108: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg611_1, mul_107);  arg611_1 = mul_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_179: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg608_1);  arg608_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_109: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_179, 0.5)\n",
      "    pow_33: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_179, 3.0)\n",
      "    mul_110: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_33, 0.044715);  pow_33 = None\n",
      "    add_125: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_179, mul_110);  linear_179 = mul_110 = None\n",
      "    mul_111: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_125, 0.7978845608028654);  add_125 = None\n",
      "    tanh_9: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_111);  mul_111 = None\n",
      "    add_126: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_9, 1.0);  tanh_9 = None\n",
      "    mul_112: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_109, add_126);  mul_109 = add_126 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_180: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg609_1);  mul_108 = arg609_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_113: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_112, linear_180);  mul_112 = linear_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_96: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_113, 0.1, False);  mul_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_181: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_96, arg610_1);  dropout_96 = arg610_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_97: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_181, 0.1, False);  linear_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_127: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_46, dropout_97);  to_46 = dropout_97 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_47: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_127, torch.float32);  add_127 = None\n",
      "    pow_34: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_47, 2)\n",
      "    mean_34: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_34, [-1], True);  pow_34 = None\n",
      "    add_128: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_34, 1e-06);  mean_34 = None\n",
      "    rsqrt_23: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_128);  add_128 = None\n",
      "    mul_114: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_47, rsqrt_23);  rsqrt_23 = None\n",
      "    mul_115: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg616_1, mul_114);  arg616_1 = mul_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_182: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg612_1);  arg612_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_115: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_182, [1, -1, 6, 64]);  linear_182 = None\n",
      "    transpose_165: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_115, 1, 2);  view_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_183: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg613_1);  arg613_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_184: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg614_1);  mul_115 = arg614_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_116: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_183, [1, -1, 6, 64]);  linear_183 = None\n",
      "    transpose_166: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_116, 1, 2);  view_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_117: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_184, [1, -1, 6, 64]);  linear_184 = None\n",
      "    transpose_167: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_117, 1, 2);  view_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant11 = self._tensor_constant11\n",
      "    lift_fresh_copy_11: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant11);  _tensor_constant11 = None\n",
      "    detach__11: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_11);  lift_fresh_copy_11 = None\n",
      "    _tensor_constant12 = self._tensor_constant12\n",
      "    lift_fresh_copy_12: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant12);  _tensor_constant12 = None\n",
      "    detach__12: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_12);  lift_fresh_copy_12 = None\n",
      "    cat_9: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__11, transpose_166], -2);  detach__11 = transpose_166 = None\n",
      "    cat_10: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__12, transpose_167], -2);  detach__12 = transpose_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_168: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_9, 3, 2);  cat_9 = None\n",
      "    matmul_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_165, transpose_168);  transpose_165 = transpose_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__13: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_25, add_109);  matmul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_48: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__13, torch.float32);  add__13 = None\n",
      "    softmax_23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_48, -1)\n",
      "    type_as_12: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_23, to_48);  softmax_23 = to_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_98: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_12, 0.1, False);  type_as_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_98, cat_10);  dropout_98 = cat_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_169: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_26, 1, 2);  matmul_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_118: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_169, [1, -1, 384]);  transpose_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_185: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_118, arg615_1);  view_118 = arg615_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_99: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_185, 0.1, False);  linear_185 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_129: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_47, dropout_99);  to_47 = dropout_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_33: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_130: \"i64[]\" = torch.ops.aten.add.Tensor(select_33, 1);  select_33 = add_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_49: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_129, torch.float32);  add_129 = None\n",
      "    pow_35: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_49, 2)\n",
      "    mean_35: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_35, [-1], True);  pow_35 = None\n",
      "    add_131: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_35, 1e-06);  mean_35 = None\n",
      "    rsqrt_24: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_131);  add_131 = None\n",
      "    mul_116: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_49, rsqrt_24);  rsqrt_24 = None\n",
      "    mul_117: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg621_1, mul_116);  arg621_1 = mul_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_186: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_117, arg617_1);  mul_117 = arg617_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_119: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_186, [1, -1, 6, 64]);  linear_186 = None\n",
      "    transpose_170: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_119, 1, 2);  view_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_187: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg618_1);  arg618_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_188: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg619_1);  arg619_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_120: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_187, [1, -1, 6, 64]);  linear_187 = None\n",
      "    transpose_171: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_121: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_188, [1, -1, 6, 64]);  linear_188 = None\n",
      "    transpose_172: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_121, 1, 2);  view_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant13 = self._tensor_constant13\n",
      "    lift_fresh_copy_13: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant13);  _tensor_constant13 = None\n",
      "    detach__13: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_13);  lift_fresh_copy_13 = None\n",
      "    _tensor_constant14 = self._tensor_constant14\n",
      "    lift_fresh_copy_14: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant14);  _tensor_constant14 = None\n",
      "    detach__14: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_14);  lift_fresh_copy_14 = None\n",
      "    cat_11: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__13, transpose_171], -2);  detach__13 = transpose_171 = None\n",
      "    cat_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__14, transpose_172], -2);  detach__14 = transpose_172 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_173: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_11, 3, 2);  cat_11 = None\n",
      "    matmul_27: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_170, transpose_173);  transpose_170 = transpose_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__14: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_27, add_113);  matmul_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_50: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__14, torch.float32);  add__14 = None\n",
      "    softmax_24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_50, -1)\n",
      "    type_as_13: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_24, to_50);  softmax_24 = to_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_100: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_13, 0.1, False);  type_as_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_28: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_100, cat_12);  dropout_100 = cat_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_174: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_28, 1, 2);  matmul_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_122: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_174, [1, -1, 384]);  transpose_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_189: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_122, arg620_1);  view_122 = arg620_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_101: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_189, 0.1, False);  linear_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_132: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_49, dropout_101);  to_49 = dropout_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_51: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_132, torch.float32);  add_132 = None\n",
      "    pow_36: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_51, 2)\n",
      "    mean_36: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_36, [-1], True);  pow_36 = None\n",
      "    add_133: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_36, 1e-06);  mean_36 = None\n",
      "    rsqrt_25: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_133);  add_133 = None\n",
      "    mul_118: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_51, rsqrt_25);  rsqrt_25 = None\n",
      "    mul_119: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg625_1, mul_118);  arg625_1 = mul_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_190: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg622_1);  arg622_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_120: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_190, 0.5)\n",
      "    pow_37: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_190, 3.0)\n",
      "    mul_121: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_37, 0.044715);  pow_37 = None\n",
      "    add_134: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_190, mul_121);  linear_190 = mul_121 = None\n",
      "    mul_122: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_134, 0.7978845608028654);  add_134 = None\n",
      "    tanh_10: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_122);  mul_122 = None\n",
      "    add_135: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_10, 1.0);  tanh_10 = None\n",
      "    mul_123: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_120, add_135);  mul_120 = add_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_191: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg623_1);  mul_119 = arg623_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_124: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_123, linear_191);  mul_123 = linear_191 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_102: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_124, 0.1, False);  mul_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_192: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_102, arg624_1);  dropout_102 = arg624_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_103: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_192, 0.1, False);  linear_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_136: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_51, dropout_103);  to_51 = dropout_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_52: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_136, torch.float32);  add_136 = None\n",
      "    pow_38: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_52, 2)\n",
      "    mean_37: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_38, [-1], True);  pow_38 = None\n",
      "    add_137: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_37, 1e-06);  mean_37 = None\n",
      "    rsqrt_26: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_137);  add_137 = None\n",
      "    mul_125: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_52, rsqrt_26);  rsqrt_26 = None\n",
      "    mul_126: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg630_1, mul_125);  arg630_1 = mul_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_193: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg626_1);  arg626_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_123: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_193, [1, -1, 6, 64]);  linear_193 = None\n",
      "    transpose_175: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_123, 1, 2);  view_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_194: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg627_1);  arg627_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_195: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg628_1);  mul_126 = arg628_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_124: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_194, [1, -1, 6, 64]);  linear_194 = None\n",
      "    transpose_176: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_124, 1, 2);  view_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_125: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_195, [1, -1, 6, 64]);  linear_195 = None\n",
      "    transpose_177: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_125, 1, 2);  view_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant15 = self._tensor_constant15\n",
      "    lift_fresh_copy_15: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant15);  _tensor_constant15 = None\n",
      "    detach__15: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_15);  lift_fresh_copy_15 = None\n",
      "    _tensor_constant16 = self._tensor_constant16\n",
      "    lift_fresh_copy_16: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant16);  _tensor_constant16 = None\n",
      "    detach__16: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_16);  lift_fresh_copy_16 = None\n",
      "    cat_13: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__15, transpose_176], -2);  detach__15 = transpose_176 = None\n",
      "    cat_14: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__16, transpose_177], -2);  detach__16 = transpose_177 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_178: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_13, 3, 2);  cat_13 = None\n",
      "    matmul_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_175, transpose_178);  transpose_175 = transpose_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__15: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_29, add_109);  matmul_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_53: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__15, torch.float32);  add__15 = None\n",
      "    softmax_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_53, -1)\n",
      "    type_as_14: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_25, to_53);  softmax_25 = to_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_104: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_14, 0.1, False);  type_as_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_104, cat_14);  dropout_104 = cat_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_179: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_30, 1, 2);  matmul_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_126: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_179, [1, -1, 384]);  transpose_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_196: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_126, arg629_1);  view_126 = arg629_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_105: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_196, 0.1, False);  linear_196 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_138: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_52, dropout_105);  to_52 = dropout_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_34: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_139: \"i64[]\" = torch.ops.aten.add.Tensor(select_34, 1);  select_34 = add_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_54: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_138, torch.float32);  add_138 = None\n",
      "    pow_39: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_54, 2)\n",
      "    mean_38: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_39, [-1], True);  pow_39 = None\n",
      "    add_140: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_38, 1e-06);  mean_38 = None\n",
      "    rsqrt_27: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_140);  add_140 = None\n",
      "    mul_127: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_54, rsqrt_27);  rsqrt_27 = None\n",
      "    mul_128: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg635_1, mul_127);  arg635_1 = mul_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_197: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_128, arg631_1);  mul_128 = arg631_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_127: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_197, [1, -1, 6, 64]);  linear_197 = None\n",
      "    transpose_180: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_127, 1, 2);  view_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_198: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg632_1);  arg632_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_199: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg633_1);  arg633_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_128: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_198, [1, -1, 6, 64]);  linear_198 = None\n",
      "    transpose_181: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_128, 1, 2);  view_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_199, [1, -1, 6, 64]);  linear_199 = None\n",
      "    transpose_182: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_129, 1, 2);  view_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant17 = self._tensor_constant17\n",
      "    lift_fresh_copy_17: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant17);  _tensor_constant17 = None\n",
      "    detach__17: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_17);  lift_fresh_copy_17 = None\n",
      "    _tensor_constant18 = self._tensor_constant18\n",
      "    lift_fresh_copy_18: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant18);  _tensor_constant18 = None\n",
      "    detach__18: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_18);  lift_fresh_copy_18 = None\n",
      "    cat_15: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__17, transpose_181], -2);  detach__17 = transpose_181 = None\n",
      "    cat_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__18, transpose_182], -2);  detach__18 = transpose_182 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_183: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_15, 3, 2);  cat_15 = None\n",
      "    matmul_31: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_180, transpose_183);  transpose_180 = transpose_183 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__16: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_31, add_113);  matmul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_55: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__16, torch.float32);  add__16 = None\n",
      "    softmax_26: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_55, -1)\n",
      "    type_as_15: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_26, to_55);  softmax_26 = to_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_106: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_15, 0.1, False);  type_as_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_32: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_106, cat_16);  dropout_106 = cat_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_184: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_32, 1, 2);  matmul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_130: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_184, [1, -1, 384]);  transpose_184 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_200: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_130, arg634_1);  view_130 = arg634_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_107: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_200, 0.1, False);  linear_200 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_141: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_54, dropout_107);  to_54 = dropout_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_56: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_141, torch.float32);  add_141 = None\n",
      "    pow_40: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_56, 2)\n",
      "    mean_39: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_40, [-1], True);  pow_40 = None\n",
      "    add_142: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_39, 1e-06);  mean_39 = None\n",
      "    rsqrt_28: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_142);  add_142 = None\n",
      "    mul_129: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_56, rsqrt_28);  rsqrt_28 = None\n",
      "    mul_130: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg639_1, mul_129);  arg639_1 = mul_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_201: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg636_1);  arg636_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_131: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_201, 0.5)\n",
      "    pow_41: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_201, 3.0)\n",
      "    mul_132: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_41, 0.044715);  pow_41 = None\n",
      "    add_143: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_201, mul_132);  linear_201 = mul_132 = None\n",
      "    mul_133: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_143, 0.7978845608028654);  add_143 = None\n",
      "    tanh_11: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_133);  mul_133 = None\n",
      "    add_144: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_11, 1.0);  tanh_11 = None\n",
      "    mul_134: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_131, add_144);  mul_131 = add_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_202: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg637_1);  mul_130 = arg637_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_135: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_134, linear_202);  mul_134 = linear_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_108: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_135, 0.1, False);  mul_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_203: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_108, arg638_1);  dropout_108 = arg638_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_109: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_203, 0.1, False);  linear_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_145: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_56, dropout_109);  to_56 = dropout_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_57: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_145, torch.float32);  add_145 = None\n",
      "    pow_42: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_57, 2)\n",
      "    mean_40: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_42, [-1], True);  pow_42 = None\n",
      "    add_146: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_40, 1e-06);  mean_40 = None\n",
      "    rsqrt_29: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_146);  add_146 = None\n",
      "    mul_136: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_57, rsqrt_29);  rsqrt_29 = None\n",
      "    mul_137: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg644_1, mul_136);  arg644_1 = mul_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_204: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg640_1);  arg640_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_131: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_204, [1, -1, 6, 64]);  linear_204 = None\n",
      "    transpose_185: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_131, 1, 2);  view_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_205: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg641_1);  arg641_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_206: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg642_1);  mul_137 = arg642_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_132: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_205, [1, -1, 6, 64]);  linear_205 = None\n",
      "    transpose_186: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_132, 1, 2);  view_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_133: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_206, [1, -1, 6, 64]);  linear_206 = None\n",
      "    transpose_187: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_133, 1, 2);  view_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant19 = self._tensor_constant19\n",
      "    lift_fresh_copy_19: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant19);  _tensor_constant19 = None\n",
      "    detach__19: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_19);  lift_fresh_copy_19 = None\n",
      "    _tensor_constant20 = self._tensor_constant20\n",
      "    lift_fresh_copy_20: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant20);  _tensor_constant20 = None\n",
      "    detach__20: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_20);  lift_fresh_copy_20 = None\n",
      "    cat_17: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__19, transpose_186], -2);  detach__19 = transpose_186 = None\n",
      "    cat_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__20, transpose_187], -2);  detach__20 = transpose_187 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_188: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_17, 3, 2);  cat_17 = None\n",
      "    matmul_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_185, transpose_188);  transpose_185 = transpose_188 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_33, add_109);  matmul_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_58: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__17, torch.float32);  add__17 = None\n",
      "    softmax_27: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_58, -1)\n",
      "    type_as_16: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_27, to_58);  softmax_27 = to_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_110: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_16, 0.1, False);  type_as_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_34: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_110, cat_18);  dropout_110 = cat_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_189: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_34, 1, 2);  matmul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_134: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_189, [1, -1, 384]);  transpose_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_207: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_134, arg643_1);  view_134 = arg643_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_111: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_207, 0.1, False);  linear_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_147: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_57, dropout_111);  to_57 = dropout_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_35: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_148: \"i64[]\" = torch.ops.aten.add.Tensor(select_35, 1);  select_35 = add_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_59: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_147, torch.float32);  add_147 = None\n",
      "    pow_43: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_59, 2)\n",
      "    mean_41: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_43, [-1], True);  pow_43 = None\n",
      "    add_149: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_41, 1e-06);  mean_41 = None\n",
      "    rsqrt_30: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_149);  add_149 = None\n",
      "    mul_138: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_59, rsqrt_30);  rsqrt_30 = None\n",
      "    mul_139: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg649_1, mul_138);  arg649_1 = mul_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_208: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_139, arg645_1);  mul_139 = arg645_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_135: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_208, [1, -1, 6, 64]);  linear_208 = None\n",
      "    transpose_190: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_135, 1, 2);  view_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_209: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg646_1);  arg646_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_210: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg647_1);  arg647_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_136: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_209, [1, -1, 6, 64]);  linear_209 = None\n",
      "    transpose_191: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_136, 1, 2);  view_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_137: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_210, [1, -1, 6, 64]);  linear_210 = None\n",
      "    transpose_192: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_137, 1, 2);  view_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant21 = self._tensor_constant21\n",
      "    lift_fresh_copy_21: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant21);  _tensor_constant21 = None\n",
      "    detach__21: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_21);  lift_fresh_copy_21 = None\n",
      "    _tensor_constant22 = self._tensor_constant22\n",
      "    lift_fresh_copy_22: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant22);  _tensor_constant22 = None\n",
      "    detach__22: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_22);  lift_fresh_copy_22 = None\n",
      "    cat_19: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__21, transpose_191], -2);  detach__21 = transpose_191 = None\n",
      "    cat_20: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__22, transpose_192], -2);  detach__22 = transpose_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_193: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_19, 3, 2);  cat_19 = None\n",
      "    matmul_35: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_190, transpose_193);  transpose_190 = transpose_193 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__18: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_35, add_113);  matmul_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_60: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__18, torch.float32);  add__18 = None\n",
      "    softmax_28: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_60, -1)\n",
      "    type_as_17: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_28, to_60);  softmax_28 = to_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_112: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_17, 0.1, False);  type_as_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_36: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_112, cat_20);  dropout_112 = cat_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_194: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_36, 1, 2);  matmul_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_138: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_194, [1, -1, 384]);  transpose_194 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_211: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_138, arg648_1);  view_138 = arg648_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_113: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_211, 0.1, False);  linear_211 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_150: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_59, dropout_113);  to_59 = dropout_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_61: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_150, torch.float32);  add_150 = None\n",
      "    pow_44: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_61, 2)\n",
      "    mean_42: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_44, [-1], True);  pow_44 = None\n",
      "    add_151: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_42, 1e-06);  mean_42 = None\n",
      "    rsqrt_31: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_151);  add_151 = None\n",
      "    mul_140: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_61, rsqrt_31);  rsqrt_31 = None\n",
      "    mul_141: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg653_1, mul_140);  arg653_1 = mul_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_212: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg650_1);  arg650_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_142: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_212, 0.5)\n",
      "    pow_45: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_212, 3.0)\n",
      "    mul_143: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_45, 0.044715);  pow_45 = None\n",
      "    add_152: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_212, mul_143);  linear_212 = mul_143 = None\n",
      "    mul_144: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_152, 0.7978845608028654);  add_152 = None\n",
      "    tanh_12: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_144);  mul_144 = None\n",
      "    add_153: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_12, 1.0);  tanh_12 = None\n",
      "    mul_145: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_142, add_153);  mul_142 = add_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_213: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg651_1);  mul_141 = arg651_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_146: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_145, linear_213);  mul_145 = linear_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_114: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_146, 0.1, False);  mul_146 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_214: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_114, arg652_1);  dropout_114 = arg652_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_115: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_214, 0.1, False);  linear_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_154: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_61, dropout_115);  to_61 = dropout_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_62: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_154, torch.float32);  add_154 = None\n",
      "    pow_46: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_62, 2)\n",
      "    mean_43: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_46, [-1], True);  pow_46 = None\n",
      "    add_155: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_43, 1e-06);  mean_43 = None\n",
      "    rsqrt_32: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_155);  add_155 = None\n",
      "    mul_147: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_62, rsqrt_32);  rsqrt_32 = None\n",
      "    mul_148: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg658_1, mul_147);  arg658_1 = mul_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_215: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg654_1);  arg654_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_139: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_215, [1, -1, 6, 64]);  linear_215 = None\n",
      "    transpose_195: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_139, 1, 2);  view_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_216: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg655_1);  arg655_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_217: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg656_1);  mul_148 = arg656_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_140: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_216, [1, -1, 6, 64]);  linear_216 = None\n",
      "    transpose_196: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_140, 1, 2);  view_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_141: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_217, [1, -1, 6, 64]);  linear_217 = None\n",
      "    transpose_197: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_141, 1, 2);  view_141 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant23 = self._tensor_constant23\n",
      "    lift_fresh_copy_23: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant23);  _tensor_constant23 = None\n",
      "    detach__23: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_23);  lift_fresh_copy_23 = None\n",
      "    _tensor_constant24 = self._tensor_constant24\n",
      "    lift_fresh_copy_24: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant24);  _tensor_constant24 = None\n",
      "    detach__24: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_24);  lift_fresh_copy_24 = None\n",
      "    cat_21: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__23, transpose_196], -2);  detach__23 = transpose_196 = None\n",
      "    cat_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__24, transpose_197], -2);  detach__24 = transpose_197 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_198: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_21, 3, 2);  cat_21 = None\n",
      "    matmul_37: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_195, transpose_198);  transpose_195 = transpose_198 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_37, add_109);  matmul_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_63: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__19, torch.float32);  add__19 = None\n",
      "    softmax_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_63, -1)\n",
      "    type_as_18: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_29, to_63);  softmax_29 = to_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_116: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_18, 0.1, False);  type_as_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_38: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_116, cat_22);  dropout_116 = cat_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_199: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_38, 1, 2);  matmul_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_142: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_199, [1, -1, 384]);  transpose_199 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_218: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_142, arg657_1);  view_142 = arg657_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_117: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_218, 0.1, False);  linear_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_156: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_62, dropout_117);  to_62 = dropout_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_36: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_157: \"i64[]\" = torch.ops.aten.add.Tensor(select_36, 1);  select_36 = add_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_64: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_156, torch.float32);  add_156 = None\n",
      "    pow_47: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_64, 2)\n",
      "    mean_44: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_47, [-1], True);  pow_47 = None\n",
      "    add_158: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_44, 1e-06);  mean_44 = None\n",
      "    rsqrt_33: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_158);  add_158 = None\n",
      "    mul_149: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_64, rsqrt_33);  rsqrt_33 = None\n",
      "    mul_150: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg663_1, mul_149);  arg663_1 = mul_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_219: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_150, arg659_1);  mul_150 = arg659_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_143: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_219, [1, -1, 6, 64]);  linear_219 = None\n",
      "    transpose_200: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_143, 1, 2);  view_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_220: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg660_1);  arg660_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_221: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg661_1);  arg661_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_220, [1, -1, 6, 64]);  linear_220 = None\n",
      "    transpose_201: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_144, 1, 2);  view_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_145: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_221, [1, -1, 6, 64]);  linear_221 = None\n",
      "    transpose_202: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_145, 1, 2);  view_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant25 = self._tensor_constant25\n",
      "    lift_fresh_copy_25: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant25);  _tensor_constant25 = None\n",
      "    detach__25: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_25);  lift_fresh_copy_25 = None\n",
      "    _tensor_constant26 = self._tensor_constant26\n",
      "    lift_fresh_copy_26: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant26);  _tensor_constant26 = None\n",
      "    detach__26: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_26);  lift_fresh_copy_26 = None\n",
      "    cat_23: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__25, transpose_201], -2);  detach__25 = transpose_201 = None\n",
      "    cat_24: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__26, transpose_202], -2);  detach__26 = transpose_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_203: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_23, 3, 2);  cat_23 = None\n",
      "    matmul_39: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_200, transpose_203);  transpose_200 = transpose_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_39, add_113);  matmul_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_65: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__20, torch.float32);  add__20 = None\n",
      "    softmax_30: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_65, -1)\n",
      "    type_as_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_30, to_65);  softmax_30 = to_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_118: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_19, 0.1, False);  type_as_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_40: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_118, cat_24);  dropout_118 = cat_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_204: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_40, 1, 2);  matmul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_146: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_204, [1, -1, 384]);  transpose_204 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_222: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_146, arg662_1);  view_146 = arg662_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_119: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_222, 0.1, False);  linear_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_159: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_64, dropout_119);  to_64 = dropout_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_66: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_159, torch.float32);  add_159 = None\n",
      "    pow_48: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_66, 2)\n",
      "    mean_45: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_48, [-1], True);  pow_48 = None\n",
      "    add_160: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_45, 1e-06);  mean_45 = None\n",
      "    rsqrt_34: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_160);  add_160 = None\n",
      "    mul_151: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_66, rsqrt_34);  rsqrt_34 = None\n",
      "    mul_152: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg667_1, mul_151);  arg667_1 = mul_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_223: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg664_1);  arg664_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_153: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_223, 0.5)\n",
      "    pow_49: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_223, 3.0)\n",
      "    mul_154: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_49, 0.044715);  pow_49 = None\n",
      "    add_161: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_223, mul_154);  linear_223 = mul_154 = None\n",
      "    mul_155: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_161, 0.7978845608028654);  add_161 = None\n",
      "    tanh_13: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_155);  mul_155 = None\n",
      "    add_162: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_13, 1.0);  tanh_13 = None\n",
      "    mul_156: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_153, add_162);  mul_153 = add_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_224: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg665_1);  mul_152 = arg665_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_157: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_156, linear_224);  mul_156 = linear_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_120: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_157, 0.1, False);  mul_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_225: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_120, arg666_1);  dropout_120 = arg666_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_121: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_225, 0.1, False);  linear_225 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_163: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_66, dropout_121);  to_66 = dropout_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_67: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_163, torch.float32);  add_163 = None\n",
      "    pow_50: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_67, 2)\n",
      "    mean_46: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_50, [-1], True);  pow_50 = None\n",
      "    add_164: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_46, 1e-06);  mean_46 = None\n",
      "    rsqrt_35: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_164);  add_164 = None\n",
      "    mul_158: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_67, rsqrt_35);  rsqrt_35 = None\n",
      "    mul_159: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg672_1, mul_158);  arg672_1 = mul_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_226: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg668_1);  arg668_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_147: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_226, [1, -1, 6, 64]);  linear_226 = None\n",
      "    transpose_205: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_147, 1, 2);  view_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_227: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg669_1);  arg669_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_228: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg670_1);  mul_159 = arg670_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_148: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_227, [1, -1, 6, 64]);  linear_227 = None\n",
      "    transpose_206: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_148, 1, 2);  view_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_228, [1, -1, 6, 64]);  linear_228 = None\n",
      "    transpose_207: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_149, 1, 2);  view_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant27 = self._tensor_constant27\n",
      "    lift_fresh_copy_27: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant27);  _tensor_constant27 = None\n",
      "    detach__27: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_27);  lift_fresh_copy_27 = None\n",
      "    _tensor_constant28 = self._tensor_constant28\n",
      "    lift_fresh_copy_28: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant28);  _tensor_constant28 = None\n",
      "    detach__28: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_28);  lift_fresh_copy_28 = None\n",
      "    cat_25: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__27, transpose_206], -2);  detach__27 = transpose_206 = None\n",
      "    cat_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__28, transpose_207], -2);  detach__28 = transpose_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_208: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_25, 3, 2);  cat_25 = None\n",
      "    matmul_41: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_205, transpose_208);  transpose_205 = transpose_208 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_41, add_109);  matmul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_68: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__21, torch.float32);  add__21 = None\n",
      "    softmax_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_68, -1)\n",
      "    type_as_20: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_31, to_68);  softmax_31 = to_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_122: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_20, 0.1, False);  type_as_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_42: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_122, cat_26);  dropout_122 = cat_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_209: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_42, 1, 2);  matmul_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_150: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_209, [1, -1, 384]);  transpose_209 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_229: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_150, arg671_1);  view_150 = arg671_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_123: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_229, 0.1, False);  linear_229 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_165: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_67, dropout_123);  to_67 = dropout_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_37: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_166: \"i64[]\" = torch.ops.aten.add.Tensor(select_37, 1);  select_37 = add_166 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_69: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_165, torch.float32);  add_165 = None\n",
      "    pow_51: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_69, 2)\n",
      "    mean_47: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_51, [-1], True);  pow_51 = None\n",
      "    add_167: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_47, 1e-06);  mean_47 = None\n",
      "    rsqrt_36: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_167);  add_167 = None\n",
      "    mul_160: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_69, rsqrt_36);  rsqrt_36 = None\n",
      "    mul_161: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg677_1, mul_160);  arg677_1 = mul_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_230: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_161, arg673_1);  mul_161 = arg673_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_151: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_230, [1, -1, 6, 64]);  linear_230 = None\n",
      "    transpose_210: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_151, 1, 2);  view_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_231: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg674_1);  arg674_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_232: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg675_1);  arg675_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_152: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_231, [1, -1, 6, 64]);  linear_231 = None\n",
      "    transpose_211: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_153: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_232, [1, -1, 6, 64]);  linear_232 = None\n",
      "    transpose_212: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_153, 1, 2);  view_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant29 = self._tensor_constant29\n",
      "    lift_fresh_copy_29: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant29);  _tensor_constant29 = None\n",
      "    detach__29: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_29);  lift_fresh_copy_29 = None\n",
      "    _tensor_constant30 = self._tensor_constant30\n",
      "    lift_fresh_copy_30: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant30);  _tensor_constant30 = None\n",
      "    detach__30: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_30);  lift_fresh_copy_30 = None\n",
      "    cat_27: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__29, transpose_211], -2);  detach__29 = transpose_211 = None\n",
      "    cat_28: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__30, transpose_212], -2);  detach__30 = transpose_212 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_213: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_27, 3, 2);  cat_27 = None\n",
      "    matmul_43: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_210, transpose_213);  transpose_210 = transpose_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_43, add_113);  matmul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_70: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__22, torch.float32);  add__22 = None\n",
      "    softmax_32: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_70, -1)\n",
      "    type_as_21: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_32, to_70);  softmax_32 = to_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_124: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_21, 0.1, False);  type_as_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_44: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_124, cat_28);  dropout_124 = cat_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_214: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_44, 1, 2);  matmul_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_154: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_214, [1, -1, 384]);  transpose_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_233: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_154, arg676_1);  view_154 = arg676_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_125: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_233, 0.1, False);  linear_233 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_168: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_69, dropout_125);  to_69 = dropout_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_71: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_168, torch.float32);  add_168 = None\n",
      "    pow_52: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_71, 2)\n",
      "    mean_48: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_52, [-1], True);  pow_52 = None\n",
      "    add_169: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_48, 1e-06);  mean_48 = None\n",
      "    rsqrt_37: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_169);  add_169 = None\n",
      "    mul_162: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_71, rsqrt_37);  rsqrt_37 = None\n",
      "    mul_163: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg681_1, mul_162);  arg681_1 = mul_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_234: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg678_1);  arg678_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_164: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_234, 0.5)\n",
      "    pow_53: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_234, 3.0)\n",
      "    mul_165: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_53, 0.044715);  pow_53 = None\n",
      "    add_170: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_234, mul_165);  linear_234 = mul_165 = None\n",
      "    mul_166: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_170, 0.7978845608028654);  add_170 = None\n",
      "    tanh_14: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_166);  mul_166 = None\n",
      "    add_171: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_14, 1.0);  tanh_14 = None\n",
      "    mul_167: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_164, add_171);  mul_164 = add_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_235: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg679_1);  mul_163 = arg679_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_168: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_167, linear_235);  mul_167 = linear_235 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_126: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_168, 0.1, False);  mul_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_236: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_126, arg680_1);  dropout_126 = arg680_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_127: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_236, 0.1, False);  linear_236 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_172: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_71, dropout_127);  to_71 = dropout_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_72: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_172, torch.float32);  add_172 = None\n",
      "    pow_54: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_72, 2)\n",
      "    mean_49: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_54, [-1], True);  pow_54 = None\n",
      "    add_173: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_49, 1e-06);  mean_49 = None\n",
      "    rsqrt_38: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_173);  add_173 = None\n",
      "    mul_169: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_72, rsqrt_38);  rsqrt_38 = None\n",
      "    mul_170: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg686_1, mul_169);  arg686_1 = mul_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_237: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg682_1);  arg682_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_155: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_237, [1, -1, 6, 64]);  linear_237 = None\n",
      "    transpose_215: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_155, 1, 2);  view_155 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_238: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg683_1);  arg683_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_239: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg684_1);  mul_170 = arg684_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_156: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_238, [1, -1, 6, 64]);  linear_238 = None\n",
      "    transpose_216: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_156, 1, 2);  view_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_157: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_239, [1, -1, 6, 64]);  linear_239 = None\n",
      "    transpose_217: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_157, 1, 2);  view_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant31 = self._tensor_constant31\n",
      "    lift_fresh_copy_31: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant31);  _tensor_constant31 = None\n",
      "    detach__31: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_31);  lift_fresh_copy_31 = None\n",
      "    _tensor_constant32 = self._tensor_constant32\n",
      "    lift_fresh_copy_32: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant32);  _tensor_constant32 = None\n",
      "    detach__32: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_32);  lift_fresh_copy_32 = None\n",
      "    cat_29: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__31, transpose_216], -2);  detach__31 = transpose_216 = None\n",
      "    cat_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__32, transpose_217], -2);  detach__32 = transpose_217 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_218: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_29, 3, 2);  cat_29 = None\n",
      "    matmul_45: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_215, transpose_218);  transpose_215 = transpose_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_45, add_109);  matmul_45 = add_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_73: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__23, torch.float32);  add__23 = None\n",
      "    softmax_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_73, -1)\n",
      "    type_as_22: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_33, to_73);  softmax_33 = to_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_128: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_22, 0.1, False);  type_as_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_46: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_128, cat_30);  dropout_128 = cat_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_219: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_46, 1, 2);  matmul_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_158: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_219, [1, -1, 384]);  transpose_219 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_240: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_158, arg685_1);  view_158 = arg685_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_129: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_240, 0.1, False);  linear_240 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_174: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_72, dropout_129);  to_72 = dropout_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_38: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_175: \"i64[]\" = torch.ops.aten.add.Tensor(select_38, 1);  select_38 = add_175 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_74: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_174, torch.float32);  add_174 = None\n",
      "    pow_55: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_74, 2)\n",
      "    mean_50: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_55, [-1], True);  pow_55 = None\n",
      "    add_176: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_50, 1e-06);  mean_50 = None\n",
      "    rsqrt_39: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_176);  add_176 = None\n",
      "    mul_171: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_74, rsqrt_39);  rsqrt_39 = None\n",
      "    mul_172: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg691_1, mul_171);  arg691_1 = mul_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_241: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_172, arg687_1);  mul_172 = arg687_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_241, [1, -1, 6, 64]);  linear_241 = None\n",
      "    transpose_220: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_159, 1, 2);  view_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_242: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg688_1);  arg688_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_243: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg689_1);  dropout_84 = arg689_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_160: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_242, [1, -1, 6, 64]);  linear_242 = None\n",
      "    transpose_221: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_160, 1, 2);  view_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_161: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_243, [1, -1, 6, 64]);  linear_243 = None\n",
      "    transpose_222: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_161, 1, 2);  view_161 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant33 = self._tensor_constant33\n",
      "    lift_fresh_copy_33: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant33);  _tensor_constant33 = None\n",
      "    detach__33: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_33);  lift_fresh_copy_33 = None\n",
      "    _tensor_constant34 = self._tensor_constant34\n",
      "    lift_fresh_copy_34: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant34);  _tensor_constant34 = None\n",
      "    detach__34: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_34);  lift_fresh_copy_34 = None\n",
      "    cat_31: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__33, transpose_221], -2);  detach__33 = transpose_221 = None\n",
      "    cat_32: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__34, transpose_222], -2);  detach__34 = transpose_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_223: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_31, 3, 2);  cat_31 = None\n",
      "    matmul_47: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_220, transpose_223);  transpose_220 = transpose_223 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_47, add_113);  matmul_47 = add_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_75: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__24, torch.float32);  add__24 = None\n",
      "    softmax_34: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_75, -1)\n",
      "    type_as_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_34, to_75);  softmax_34 = to_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_130: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_23, 0.1, False);  type_as_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_48: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_130, cat_32);  dropout_130 = cat_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_224: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_48, 1, 2);  matmul_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_162: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_224, [1, -1, 384]);  transpose_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_244: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_162, arg690_1);  view_162 = arg690_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_131: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_244, 0.1, False);  linear_244 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_177: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_74, dropout_131);  to_74 = dropout_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_76: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_177, torch.float32);  add_177 = None\n",
      "    pow_56: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_76, 2)\n",
      "    mean_51: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_56, [-1], True);  pow_56 = None\n",
      "    add_178: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_51, 1e-06);  mean_51 = None\n",
      "    rsqrt_40: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_178);  add_178 = None\n",
      "    mul_173: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_76, rsqrt_40);  rsqrt_40 = None\n",
      "    mul_174: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg695_1, mul_173);  arg695_1 = mul_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_245: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg692_1);  arg692_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_175: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_245, 0.5)\n",
      "    pow_57: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_245, 3.0)\n",
      "    mul_176: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_57, 0.044715);  pow_57 = None\n",
      "    add_179: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_245, mul_176);  linear_245 = mul_176 = None\n",
      "    mul_177: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_179, 0.7978845608028654);  add_179 = None\n",
      "    tanh_15: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_177);  mul_177 = None\n",
      "    add_180: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_15, 1.0);  tanh_15 = None\n",
      "    mul_178: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_175, add_180);  mul_175 = add_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_246: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg693_1);  mul_174 = arg693_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_179: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_178, linear_246);  mul_178 = linear_246 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_132: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_179, 0.1, False);  mul_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_247: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_132, arg694_1);  dropout_132 = arg694_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_133: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_247, 0.1, False);  linear_247 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_181: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_76, dropout_133);  to_76 = dropout_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_77: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_181, torch.float32);  add_181 = None\n",
      "    pow_58: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_77, 2)\n",
      "    mean_52: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_58, [-1], True);  pow_58 = None\n",
      "    add_182: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_52, 1e-06);  mean_52 = None\n",
      "    rsqrt_41: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_182);  add_182 = None\n",
      "    mul_180: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_77, rsqrt_41);  to_77 = rsqrt_41 = None\n",
      "    mul_181: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg696_1, mul_180);  arg696_1 = mul_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_134: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(mul_181, 0.1, False);  mul_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_248: \"f32[1, 1, 32128]\" = torch.ops.aten.linear.default(dropout_134, arg697_1);  dropout_134 = arg697_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    slice_61: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, -1);  slice_41 = None\n",
      "    add_183: \"i64[1]\" = torch.ops.aten.add.Tensor(slice_61, 1);  slice_61 = add_183 = None\n",
      "    slice_62: \"f32[1, 1, 32128]\" = torch.ops.aten.slice.Tensor(linear_248);  linear_248 = None\n",
      "    select_39: \"f32[1, 32128]\" = torch.ops.aten.select.int(slice_62, 1, -1);  slice_62 = None\n",
      "    slice_63: \"f32[1, 32128]\" = torch.ops.aten.slice.Tensor(select_39, 1);  select_39 = None\n",
      "    to_78: \"f32[1, 32128]\" = torch.ops.aten.to.device(slice_63, device(type='cpu'), torch.float32, False, True);  slice_63 = None\n",
      "    argmax: \"i64[1]\" = torch.ops.aten.argmax.default(to_78, -1);  to_78 = None\n",
      "    mul_182: \"i64[1]\" = torch.ops.aten.mul.Tensor(argmax, ones_3);  argmax = None\n",
      "    rsub_2: \"i64[1]\" = torch.ops.aten.rsub.Scalar(ones_3, 1)\n",
      "    mul_183: \"i64[1]\" = torch.ops.aten.mul.Tensor(detach__1, rsub_2);  detach__1 = rsub_2 = None\n",
      "    add_184: \"i64[1]\" = torch.ops.aten.add.Tensor(mul_182, mul_183);  mul_182 = mul_183 = None\n",
      "    slice_64: \"i64[1]\" = torch.ops.aten.slice.Tensor(add_184, 0, 0, 9223372036854775807);  add_184 = None\n",
      "    unsqueeze_32: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_64, 1);  slice_64 = None\n",
      "    cat_33: \"i64[1, 2]\" = torch.ops.aten.cat.default([mul_89, unsqueeze_32], -1);  mul_89 = unsqueeze_32 = None\n",
      "    full_1: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    full_2: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    or_1: \"b8[1]\" = torch.ops.aten.__or__.Tensor(full_1, full_2);  full_1 = full_2 = None\n",
      "    to_79: \"i64[1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_24, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_24 = None\n",
      "    slice_65: \"i64[1, 2]\" = torch.ops.aten.slice.Tensor(cat_33);  cat_33 = None\n",
      "    select_40: \"i64[1]\" = torch.ops.aten.select.int(slice_65, 1, -1);  slice_65 = None\n",
      "    isin_1: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(select_40, to_79);  select_40 = to_79 = None\n",
      "    or_2: \"b8[1]\" = torch.ops.aten.__or__.Tensor(or_1, isin_1);  or_1 = isin_1 = None\n",
      "    bitwise_not: \"b8[1]\" = torch.ops.aten.bitwise_not.default(or_2);  or_2 = None\n",
      "    and_1: \"i64[1]\" = torch.ops.aten.__and__.Tensor(ones_3, bitwise_not);  ones_3 = bitwise_not = None\n",
      "    max_1: \"i64[]\" = torch.ops.aten.max.default(and_1);  and_1 = None\n",
      "    eq: \"b8[]\" = torch.ops.aten.eq.Scalar(max_1, 0);  max_1 = None\n",
      "    ne_2: \"b8[]\" = torch.ops.aten.ne.Scalar(eq, 0);  eq = None\n",
      "    item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne_2);  ne_2 = item = None\n",
      "    _set_grad_enabled_3 = torch._C._set_grad_enabled(False);  _set_grad_enabled_3 = None\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "def forward(self, arg0_1: \"f32[1, 1, 192]\", arg1_1: \"f32[1, 197, 192]\", arg2_1: \"f32[192, 3, 16, 16]\", arg3_1: \"f32[192]\", arg4_1: \"f32[192]\", arg5_1: \"f32[192]\", arg6_1: \"f32[576, 192]\", arg7_1: \"f32[576]\", arg8_1: \"f32[192, 192]\", arg9_1: \"f32[192]\", arg10_1: \"f32[192]\", arg11_1: \"f32[192]\", arg12_1: \"f32[768, 192]\", arg13_1: \"f32[768]\", arg14_1: \"f32[192, 768]\", arg15_1: \"f32[192]\", arg16_1: \"f32[192]\", arg17_1: \"f32[192]\", arg18_1: \"f32[576, 192]\", arg19_1: \"f32[576]\", arg20_1: \"f32[192, 192]\", arg21_1: \"f32[192]\", arg22_1: \"f32[192]\", arg23_1: \"f32[192]\", arg24_1: \"f32[768, 192]\", arg25_1: \"f32[768]\", arg26_1: \"f32[192, 768]\", arg27_1: \"f32[192]\", arg28_1: \"f32[192]\", arg29_1: \"f32[192]\", arg30_1: \"f32[576, 192]\", arg31_1: \"f32[576]\", arg32_1: \"f32[192, 192]\", arg33_1: \"f32[192]\", arg34_1: \"f32[192]\", arg35_1: \"f32[192]\", arg36_1: \"f32[768, 192]\", arg37_1: \"f32[768]\", arg38_1: \"f32[192, 768]\", arg39_1: \"f32[192]\", arg40_1: \"f32[192]\", arg41_1: \"f32[192]\", arg42_1: \"f32[576, 192]\", arg43_1: \"f32[576]\", arg44_1: \"f32[192, 192]\", arg45_1: \"f32[192]\", arg46_1: \"f32[192]\", arg47_1: \"f32[192]\", arg48_1: \"f32[768, 192]\", arg49_1: \"f32[768]\", arg50_1: \"f32[192, 768]\", arg51_1: \"f32[192]\", arg52_1: \"f32[192]\", arg53_1: \"f32[192]\", arg54_1: \"f32[576, 192]\", arg55_1: \"f32[576]\", arg56_1: \"f32[192, 192]\", arg57_1: \"f32[192]\", arg58_1: \"f32[192]\", arg59_1: \"f32[192]\", arg60_1: \"f32[768, 192]\", arg61_1: \"f32[768]\", arg62_1: \"f32[192, 768]\", arg63_1: \"f32[192]\", arg64_1: \"f32[192]\", arg65_1: \"f32[192]\", arg66_1: \"f32[576, 192]\", arg67_1: \"f32[576]\", arg68_1: \"f32[192, 192]\", arg69_1: \"f32[192]\", arg70_1: \"f32[192]\", arg71_1: \"f32[192]\", arg72_1: \"f32[768, 192]\", arg73_1: \"f32[768]\", arg74_1: \"f32[192, 768]\", arg75_1: \"f32[192]\", arg76_1: \"f32[192]\", arg77_1: \"f32[192]\", arg78_1: \"f32[576, 192]\", arg79_1: \"f32[576]\", arg80_1: \"f32[192, 192]\", arg81_1: \"f32[192]\", arg82_1: \"f32[192]\", arg83_1: \"f32[192]\", arg84_1: \"f32[768, 192]\", arg85_1: \"f32[768]\", arg86_1: \"f32[192, 768]\", arg87_1: \"f32[192]\", arg88_1: \"f32[192]\", arg89_1: \"f32[192]\", arg90_1: \"f32[576, 192]\", arg91_1: \"f32[576]\", arg92_1: \"f32[192, 192]\", arg93_1: \"f32[192]\", arg94_1: \"f32[192]\", arg95_1: \"f32[192]\", arg96_1: \"f32[768, 192]\", arg97_1: \"f32[768]\", arg98_1: \"f32[192, 768]\", arg99_1: \"f32[192]\", arg100_1: \"f32[192]\", arg101_1: \"f32[192]\", arg102_1: \"f32[576, 192]\", arg103_1: \"f32[576]\", arg104_1: \"f32[192, 192]\", arg105_1: \"f32[192]\", arg106_1: \"f32[192]\", arg107_1: \"f32[192]\", arg108_1: \"f32[768, 192]\", arg109_1: \"f32[768]\", arg110_1: \"f32[192, 768]\", arg111_1: \"f32[192]\", arg112_1: \"f32[192]\", arg113_1: \"f32[192]\", arg114_1: \"f32[576, 192]\", arg115_1: \"f32[576]\", arg116_1: \"f32[192, 192]\", arg117_1: \"f32[192]\", arg118_1: \"f32[192]\", arg119_1: \"f32[192]\", arg120_1: \"f32[768, 192]\", arg121_1: \"f32[768]\", arg122_1: \"f32[192, 768]\", arg123_1: \"f32[192]\", arg124_1: \"f32[192]\", arg125_1: \"f32[192]\", arg126_1: \"f32[576, 192]\", arg127_1: \"f32[576]\", arg128_1: \"f32[192, 192]\", arg129_1: \"f32[192]\", arg130_1: \"f32[192]\", arg131_1: \"f32[192]\", arg132_1: \"f32[768, 192]\", arg133_1: \"f32[768]\", arg134_1: \"f32[192, 768]\", arg135_1: \"f32[192]\", arg136_1: \"f32[192]\", arg137_1: \"f32[192]\", arg138_1: \"f32[576, 192]\", arg139_1: \"f32[576]\", arg140_1: \"f32[192, 192]\", arg141_1: \"f32[192]\", arg142_1: \"f32[192]\", arg143_1: \"f32[192]\", arg144_1: \"f32[768, 192]\", arg145_1: \"f32[768]\", arg146_1: \"f32[192, 768]\", arg147_1: \"f32[192]\", arg148_1: \"f32[192]\", arg149_1: \"f32[192]\", arg150_1: \"f32[768, 192]\", arg151_1: \"f32[768]\", arg152_1: \"f32[32, 768]\", arg153_1: \"f32[30522, 768]\", arg154_1: \"f32[512, 768]\", arg155_1: \"f32[2, 768]\", arg156_1: \"f32[768]\", arg157_1: \"f32[768]\", arg158_1: \"f32[768, 768]\", arg159_1: \"f32[768]\", arg160_1: \"f32[768, 768]\", arg161_1: \"f32[768]\", arg162_1: \"f32[768, 768]\", arg163_1: \"f32[768]\", arg164_1: \"f32[768, 768]\", arg165_1: \"f32[768]\", arg166_1: \"f32[768]\", arg167_1: \"f32[768]\", arg168_1: \"f32[3072, 768]\", arg169_1: \"f32[3072]\", arg170_1: \"f32[768, 3072]\", arg171_1: \"f32[768]\", arg172_1: \"f32[768]\", arg173_1: \"f32[768]\", arg174_1: \"f32[768, 768]\", arg175_1: \"f32[768]\", arg176_1: \"f32[768, 768]\", arg177_1: \"f32[768]\", arg178_1: \"f32[768, 768]\", arg179_1: \"f32[768]\", arg180_1: \"f32[768, 768]\", arg181_1: \"f32[768]\", arg182_1: \"f32[768]\", arg183_1: \"f32[768]\", arg184_1: \"f32[3072, 768]\", arg185_1: \"f32[3072]\", arg186_1: \"f32[768, 3072]\", arg187_1: \"f32[768]\", arg188_1: \"f32[768]\", arg189_1: \"f32[768]\", arg190_1: \"f32[768, 768]\", arg191_1: \"f32[768]\", arg192_1: \"f32[768, 768]\", arg193_1: \"f32[768]\", arg194_1: \"f32[768, 768]\", arg195_1: \"f32[768]\", arg196_1: \"f32[768, 768]\", arg197_1: \"f32[768]\", arg198_1: \"f32[768]\", arg199_1: \"f32[768]\", arg200_1: \"f32[3072, 768]\", arg201_1: \"f32[3072]\", arg202_1: \"f32[768, 3072]\", arg203_1: \"f32[768]\", arg204_1: \"f32[768]\", arg205_1: \"f32[768]\", arg206_1: \"f32[768, 768]\", arg207_1: \"f32[768]\", arg208_1: \"f32[768, 768]\", arg209_1: \"f32[768]\", arg210_1: \"f32[768, 768]\", arg211_1: \"f32[768]\", arg212_1: \"f32[768, 768]\", arg213_1: \"f32[768]\", arg214_1: \"f32[768]\", arg215_1: \"f32[768]\", arg216_1: \"f32[3072, 768]\", arg217_1: \"f32[3072]\", arg218_1: \"f32[768, 3072]\", arg219_1: \"f32[768]\", arg220_1: \"f32[768]\", arg221_1: \"f32[768]\", arg222_1: \"f32[768, 768]\", arg223_1: \"f32[768]\", arg224_1: \"f32[768, 768]\", arg225_1: \"f32[768]\", arg226_1: \"f32[768, 768]\", arg227_1: \"f32[768]\", arg228_1: \"f32[768, 768]\", arg229_1: \"f32[768]\", arg230_1: \"f32[768]\", arg231_1: \"f32[768]\", arg232_1: \"f32[3072, 768]\", arg233_1: \"f32[3072]\", arg234_1: \"f32[768, 3072]\", arg235_1: \"f32[768]\", arg236_1: \"f32[768]\", arg237_1: \"f32[768]\", arg238_1: \"f32[768, 768]\", arg239_1: \"f32[768]\", arg240_1: \"f32[768, 768]\", arg241_1: \"f32[768]\", arg242_1: \"f32[768, 768]\", arg243_1: \"f32[768]\", arg244_1: \"f32[768, 768]\", arg245_1: \"f32[768]\", arg246_1: \"f32[768]\", arg247_1: \"f32[768]\", arg248_1: \"f32[3072, 768]\", arg249_1: \"f32[3072]\", arg250_1: \"f32[768, 3072]\", arg251_1: \"f32[768]\", arg252_1: \"f32[768]\", arg253_1: \"f32[768]\", arg254_1: \"f32[768, 768]\", arg255_1: \"f32[768]\", arg256_1: \"f32[768, 768]\", arg257_1: \"f32[768]\", arg258_1: \"f32[768, 768]\", arg259_1: \"f32[768]\", arg260_1: \"f32[768, 768]\", arg261_1: \"f32[768]\", arg262_1: \"f32[768]\", arg263_1: \"f32[768]\", arg264_1: \"f32[3072, 768]\", arg265_1: \"f32[3072]\", arg266_1: \"f32[768, 3072]\", arg267_1: \"f32[768]\", arg268_1: \"f32[768]\", arg269_1: \"f32[768]\", arg270_1: \"f32[768, 768]\", arg271_1: \"f32[768]\", arg272_1: \"f32[768, 768]\", arg273_1: \"f32[768]\", arg274_1: \"f32[768, 768]\", arg275_1: \"f32[768]\", arg276_1: \"f32[768, 768]\", arg277_1: \"f32[768]\", arg278_1: \"f32[768]\", arg279_1: \"f32[768]\", arg280_1: \"f32[3072, 768]\", arg281_1: \"f32[3072]\", arg282_1: \"f32[768, 3072]\", arg283_1: \"f32[768]\", arg284_1: \"f32[768]\", arg285_1: \"f32[768]\", arg286_1: \"f32[768, 768]\", arg287_1: \"f32[768]\", arg288_1: \"f32[768, 768]\", arg289_1: \"f32[768]\", arg290_1: \"f32[768, 768]\", arg291_1: \"f32[768]\", arg292_1: \"f32[768, 768]\", arg293_1: \"f32[768]\", arg294_1: \"f32[768]\", arg295_1: \"f32[768]\", arg296_1: \"f32[3072, 768]\", arg297_1: \"f32[3072]\", arg298_1: \"f32[768, 3072]\", arg299_1: \"f32[768]\", arg300_1: \"f32[768]\", arg301_1: \"f32[768]\", arg302_1: \"f32[768, 768]\", arg303_1: \"f32[768]\", arg304_1: \"f32[768, 768]\", arg305_1: \"f32[768]\", arg306_1: \"f32[768, 768]\", arg307_1: \"f32[768]\", arg308_1: \"f32[768, 768]\", arg309_1: \"f32[768]\", arg310_1: \"f32[768]\", arg311_1: \"f32[768]\", arg312_1: \"f32[3072, 768]\", arg313_1: \"f32[3072]\", arg314_1: \"f32[768, 3072]\", arg315_1: \"f32[768]\", arg316_1: \"f32[768]\", arg317_1: \"f32[768]\", arg318_1: \"f32[768, 768]\", arg319_1: \"f32[768]\", arg320_1: \"f32[768, 768]\", arg321_1: \"f32[768]\", arg322_1: \"f32[768, 768]\", arg323_1: \"f32[768]\", arg324_1: \"f32[768, 768]\", arg325_1: \"f32[768]\", arg326_1: \"f32[768]\", arg327_1: \"f32[768]\", arg328_1: \"f32[3072, 768]\", arg329_1: \"f32[3072]\", arg330_1: \"f32[768, 3072]\", arg331_1: \"f32[768]\", arg332_1: \"f32[768]\", arg333_1: \"f32[768]\", arg334_1: \"f32[768, 768]\", arg335_1: \"f32[768]\", arg336_1: \"f32[768, 768]\", arg337_1: \"f32[768]\", arg338_1: \"f32[768, 768]\", arg339_1: \"f32[768]\", arg340_1: \"f32[768, 768]\", arg341_1: \"f32[768]\", arg342_1: \"f32[768]\", arg343_1: \"f32[768]\", arg344_1: \"f32[3072, 768]\", arg345_1: \"f32[3072]\", arg346_1: \"f32[768, 3072]\", arg347_1: \"f32[768]\", arg348_1: \"f32[768]\", arg349_1: \"f32[768]\", arg350_1: \"f32[768, 768]\", arg351_1: \"f32[768]\", arg352_1: \"f32[2304, 768]\", arg353_1: \"f32[2304]\", arg354_1: \"f32[768, 768]\", arg355_1: \"f32[768]\", arg356_1: \"f32[768]\", arg357_1: \"f32[768]\", arg358_1: \"f32[3072, 768]\", arg359_1: \"f32[3072]\", arg360_1: \"f32[768, 3072]\", arg361_1: \"f32[768]\", arg362_1: \"f32[768]\", arg363_1: \"f32[768]\", arg364_1: \"f32[3072, 768]\", arg365_1: \"f32[3072]\", arg366_1: \"f32[768, 3072]\", arg367_1: \"f32[768]\", arg368_1: \"f32[768]\", arg369_1: \"f32[768]\", arg370_1: \"f32[2304, 768]\", arg371_1: \"f32[2304]\", arg372_1: \"f32[768, 768]\", arg373_1: \"f32[768]\", arg374_1: \"f32[768]\", arg375_1: \"f32[768]\", arg376_1: \"f32[2304, 768]\", arg377_1: \"f32[2304]\", arg378_1: \"f32[768, 768]\", arg379_1: \"f32[768]\", arg380_1: \"f32[768]\", arg381_1: \"f32[768]\", arg382_1: \"f32[3072, 768]\", arg383_1: \"f32[3072]\", arg384_1: \"f32[768, 3072]\", arg385_1: \"f32[768]\", arg386_1: \"f32[768]\", arg387_1: \"f32[768]\", arg388_1: \"f32[3072, 768]\", arg389_1: \"f32[3072]\", arg390_1: \"f32[768, 3072]\", arg391_1: \"f32[768]\", arg392_1: \"f32[768]\", arg393_1: \"f32[768]\", arg394_1: \"f32[2304, 768]\", arg395_1: \"f32[2304]\", arg396_1: \"f32[768, 768]\", arg397_1: \"f32[768]\", arg398_1: \"f32[768]\", arg399_1: \"f32[768]\", arg400_1: \"f32[3072, 768]\", arg401_1: \"f32[3072]\", arg402_1: \"f32[768, 3072]\", arg403_1: \"f32[768]\", arg404_1: \"f32[768]\", arg405_1: \"f32[768]\", arg406_1: \"f32[3072, 768]\", arg407_1: \"f32[3072]\", arg408_1: \"f32[768, 3072]\", arg409_1: \"f32[768]\", arg410_1: \"f32[768]\", arg411_1: \"f32[768]\", arg412_1: \"f32[2304, 768]\", arg413_1: \"f32[2304]\", arg414_1: \"f32[768, 768]\", arg415_1: \"f32[768]\", arg416_1: \"f32[768]\", arg417_1: \"f32[768]\", arg418_1: \"f32[2304, 768]\", arg419_1: \"f32[2304]\", arg420_1: \"f32[768, 768]\", arg421_1: \"f32[768]\", arg422_1: \"f32[768]\", arg423_1: \"f32[768]\", arg424_1: \"f32[3072, 768]\", arg425_1: \"f32[3072]\", arg426_1: \"f32[768, 3072]\", arg427_1: \"f32[768]\", arg428_1: \"f32[768]\", arg429_1: \"f32[768]\", arg430_1: \"f32[3072, 768]\", arg431_1: \"f32[3072]\", arg432_1: \"f32[768, 3072]\", arg433_1: \"f32[768]\", arg434_1: \"f32[768]\", arg435_1: \"f32[768]\", arg436_1: \"f32[2304, 768]\", arg437_1: \"f32[2304]\", arg438_1: \"f32[768, 768]\", arg439_1: \"f32[768]\", arg440_1: \"f32[768]\", arg441_1: \"f32[768]\", arg442_1: \"f32[3072, 768]\", arg443_1: \"f32[3072]\", arg444_1: \"f32[768, 3072]\", arg445_1: \"f32[768]\", arg446_1: \"f32[768]\", arg447_1: \"f32[768]\", arg448_1: \"f32[3072, 768]\", arg449_1: \"f32[3072]\", arg450_1: \"f32[768, 3072]\", arg451_1: \"f32[768]\", arg452_1: \"f32[768]\", arg453_1: \"f32[768]\", arg454_1: \"f32[2304, 768]\", arg455_1: \"f32[2304]\", arg456_1: \"f32[768, 768]\", arg457_1: \"f32[768]\", arg458_1: \"f32[768]\", arg459_1: \"f32[768]\", arg460_1: \"f32[2304, 768]\", arg461_1: \"f32[2304]\", arg462_1: \"f32[768, 768]\", arg463_1: \"f32[768]\", arg464_1: \"f32[768]\", arg465_1: \"f32[768]\", arg466_1: \"f32[3072, 768]\", arg467_1: \"f32[3072]\", arg468_1: \"f32[768, 3072]\", arg469_1: \"f32[768]\", arg470_1: \"f32[768]\", arg471_1: \"f32[768]\", arg472_1: \"f32[3072, 768]\", arg473_1: \"f32[3072]\", arg474_1: \"f32[768, 3072]\", arg475_1: \"f32[768]\", arg476_1: \"f32[768]\", arg477_1: \"f32[768]\", arg478_1: \"f32[2304, 768]\", arg479_1: \"f32[2304]\", arg480_1: \"f32[768, 768]\", arg481_1: \"f32[768]\", arg482_1: \"f32[768]\", arg483_1: \"f32[768]\", arg484_1: \"f32[3072, 768]\", arg485_1: \"f32[3072]\", arg486_1: \"f32[768, 3072]\", arg487_1: \"f32[768]\", arg488_1: \"f32[768]\", arg489_1: \"f32[768]\", arg490_1: \"f32[3072, 768]\", arg491_1: \"f32[3072]\", arg492_1: \"f32[768, 3072]\", arg493_1: \"f32[768]\", arg494_1: \"f32[768]\", arg495_1: \"f32[768]\", arg496_1: \"f32[2304, 768]\", arg497_1: \"f32[2304]\", arg498_1: \"f32[768, 768]\", arg499_1: \"f32[768]\", arg500_1: \"f32[768]\", arg501_1: \"f32[768]\", arg502_1: \"f32[30522, 768]\", arg503_1: \"f32[77, 768]\", arg504_1: \"f32[512, 768]\", arg505_1: \"f32[512]\", arg506_1: \"f32[32128, 512]\", arg507_1: \"f32[32128, 512]\", arg508_1: \"f32[384, 512]\", arg509_1: \"f32[384, 512]\", arg510_1: \"f32[384, 512]\", arg511_1: \"f32[512, 384]\", arg512_1: \"f32[32, 6]\", arg513_1: \"f32[512]\", arg514_1: \"f32[1024, 512]\", arg515_1: \"f32[1024, 512]\", arg516_1: \"f32[512, 1024]\", arg517_1: \"f32[512]\", arg518_1: \"f32[384, 512]\", arg519_1: \"f32[384, 512]\", arg520_1: \"f32[384, 512]\", arg521_1: \"f32[512, 384]\", arg522_1: \"f32[512]\", arg523_1: \"f32[1024, 512]\", arg524_1: \"f32[1024, 512]\", arg525_1: \"f32[512, 1024]\", arg526_1: \"f32[512]\", arg527_1: \"f32[384, 512]\", arg528_1: \"f32[384, 512]\", arg529_1: \"f32[384, 512]\", arg530_1: \"f32[512, 384]\", arg531_1: \"f32[512]\", arg532_1: \"f32[1024, 512]\", arg533_1: \"f32[1024, 512]\", arg534_1: \"f32[512, 1024]\", arg535_1: \"f32[512]\", arg536_1: \"f32[384, 512]\", arg537_1: \"f32[384, 512]\", arg538_1: \"f32[384, 512]\", arg539_1: \"f32[512, 384]\", arg540_1: \"f32[512]\", arg541_1: \"f32[1024, 512]\", arg542_1: \"f32[1024, 512]\", arg543_1: \"f32[512, 1024]\", arg544_1: \"f32[512]\", arg545_1: \"f32[384, 512]\", arg546_1: \"f32[384, 512]\", arg547_1: \"f32[384, 512]\", arg548_1: \"f32[512, 384]\", arg549_1: \"f32[512]\", arg550_1: \"f32[1024, 512]\", arg551_1: \"f32[1024, 512]\", arg552_1: \"f32[512, 1024]\", arg553_1: \"f32[512]\", arg554_1: \"f32[384, 512]\", arg555_1: \"f32[384, 512]\", arg556_1: \"f32[384, 512]\", arg557_1: \"f32[512, 384]\", arg558_1: \"f32[512]\", arg559_1: \"f32[1024, 512]\", arg560_1: \"f32[1024, 512]\", arg561_1: \"f32[512, 1024]\", arg562_1: \"f32[512]\", arg563_1: \"f32[384, 512]\", arg564_1: \"f32[384, 512]\", arg565_1: \"f32[384, 512]\", arg566_1: \"f32[512, 384]\", arg567_1: \"f32[512]\", arg568_1: \"f32[1024, 512]\", arg569_1: \"f32[1024, 512]\", arg570_1: \"f32[512, 1024]\", arg571_1: \"f32[512]\", arg572_1: \"f32[384, 512]\", arg573_1: \"f32[384, 512]\", arg574_1: \"f32[384, 512]\", arg575_1: \"f32[512, 384]\", arg576_1: \"f32[512]\", arg577_1: \"f32[1024, 512]\", arg578_1: \"f32[1024, 512]\", arg579_1: \"f32[512, 1024]\", arg580_1: \"f32[512]\", arg581_1: \"f32[512]\", arg582_1: \"f32[32128, 512]\", arg583_1: \"f32[384, 512]\", arg584_1: \"f32[384, 512]\", arg585_1: \"f32[384, 512]\", arg586_1: \"f32[512, 384]\", arg587_1: \"f32[32, 6]\", arg588_1: \"f32[512]\", arg589_1: \"f32[384, 512]\", arg590_1: \"f32[384, 512]\", arg591_1: \"f32[384, 512]\", arg592_1: \"f32[512, 384]\", arg593_1: \"f32[512]\", arg594_1: \"f32[1024, 512]\", arg595_1: \"f32[1024, 512]\", arg596_1: \"f32[512, 1024]\", arg597_1: \"f32[512]\", arg598_1: \"f32[384, 512]\", arg599_1: \"f32[384, 512]\", arg600_1: \"f32[384, 512]\", arg601_1: \"f32[512, 384]\", arg602_1: \"f32[512]\", arg603_1: \"f32[384, 512]\", arg604_1: \"f32[384, 512]\", arg605_1: \"f32[384, 512]\", arg606_1: \"f32[512, 384]\", arg607_1: \"f32[512]\", arg608_1: \"f32[1024, 512]\", arg609_1: \"f32[1024, 512]\", arg610_1: \"f32[512, 1024]\", arg611_1: \"f32[512]\", arg612_1: \"f32[384, 512]\", arg613_1: \"f32[384, 512]\", arg614_1: \"f32[384, 512]\", arg615_1: \"f32[512, 384]\", arg616_1: \"f32[512]\", arg617_1: \"f32[384, 512]\", arg618_1: \"f32[384, 512]\", arg619_1: \"f32[384, 512]\", arg620_1: \"f32[512, 384]\", arg621_1: \"f32[512]\", arg622_1: \"f32[1024, 512]\", arg623_1: \"f32[1024, 512]\", arg624_1: \"f32[512, 1024]\", arg625_1: \"f32[512]\", arg626_1: \"f32[384, 512]\", arg627_1: \"f32[384, 512]\", arg628_1: \"f32[384, 512]\", arg629_1: \"f32[512, 384]\", arg630_1: \"f32[512]\", arg631_1: \"f32[384, 512]\", arg632_1: \"f32[384, 512]\", arg633_1: \"f32[384, 512]\", arg634_1: \"f32[512, 384]\", arg635_1: \"f32[512]\", arg636_1: \"f32[1024, 512]\", arg637_1: \"f32[1024, 512]\", arg638_1: \"f32[512, 1024]\", arg639_1: \"f32[512]\", arg640_1: \"f32[384, 512]\", arg641_1: \"f32[384, 512]\", arg642_1: \"f32[384, 512]\", arg643_1: \"f32[512, 384]\", arg644_1: \"f32[512]\", arg645_1: \"f32[384, 512]\", arg646_1: \"f32[384, 512]\", arg647_1: \"f32[384, 512]\", arg648_1: \"f32[512, 384]\", arg649_1: \"f32[512]\", arg650_1: \"f32[1024, 512]\", arg651_1: \"f32[1024, 512]\", arg652_1: \"f32[512, 1024]\", arg653_1: \"f32[512]\", arg654_1: \"f32[384, 512]\", arg655_1: \"f32[384, 512]\", arg656_1: \"f32[384, 512]\", arg657_1: \"f32[512, 384]\", arg658_1: \"f32[512]\", arg659_1: \"f32[384, 512]\", arg660_1: \"f32[384, 512]\", arg661_1: \"f32[384, 512]\", arg662_1: \"f32[512, 384]\", arg663_1: \"f32[512]\", arg664_1: \"f32[1024, 512]\", arg665_1: \"f32[1024, 512]\", arg666_1: \"f32[512, 1024]\", arg667_1: \"f32[512]\", arg668_1: \"f32[384, 512]\", arg669_1: \"f32[384, 512]\", arg670_1: \"f32[384, 512]\", arg671_1: \"f32[512, 384]\", arg672_1: \"f32[512]\", arg673_1: \"f32[384, 512]\", arg674_1: \"f32[384, 512]\", arg675_1: \"f32[384, 512]\", arg676_1: \"f32[512, 384]\", arg677_1: \"f32[512]\", arg678_1: \"f32[1024, 512]\", arg679_1: \"f32[1024, 512]\", arg680_1: \"f32[512, 1024]\", arg681_1: \"f32[512]\", arg682_1: \"f32[384, 512]\", arg683_1: \"f32[384, 512]\", arg684_1: \"f32[384, 512]\", arg685_1: \"f32[512, 384]\", arg686_1: \"f32[512]\", arg687_1: \"f32[384, 512]\", arg688_1: \"f32[384, 512]\", arg689_1: \"f32[384, 512]\", arg690_1: \"f32[512, 384]\", arg691_1: \"f32[512]\", arg692_1: \"f32[1024, 512]\", arg693_1: \"f32[1024, 512]\", arg694_1: \"f32[512, 1024]\", arg695_1: \"f32[512]\", arg696_1: \"f32[512]\", arg697_1: \"f32[32128, 512]\", arg698_1: \"i64[1, 77]\", arg699_1: \"f32[109, 109]\", arg700_1: \"f32[109, 109]\", arg701_1: \"f32[109, 109]\", arg702_1: \"i64[1, 512]\", arg703_1: \"i64[1, 512]\", arg704_1: \"f32[1, 3, 224, 224]\", arg705_1: \"i64[1, 77]\", arg706_1: \"i64[1, 77]\"):\n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:554 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
      "    conv2d: \"f32[1, 192, 14, 14]\" = torch.ops.aten.conv2d.default(arg704_1, arg2_1, arg3_1, [16, 16]);  arg704_1 = arg2_1 = arg3_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/patch_embed.py:133 in forward, code: x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC\n",
      "    flatten: \"f32[1, 192, 196]\" = torch.ops.aten.flatten.using_ints(conv2d, 2);  conv2d = None\n",
      "    transpose: \"f32[1, 196, 192]\" = torch.ops.aten.transpose.int(flatten, 1, 2);  flatten = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:260 in forward, code: image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\n",
      "    expand: \"f32[1, 1, 192]\" = torch.ops.aten.expand.default(arg0_1, [1, -1, -1]);  arg0_1 = None\n",
      "    cat: \"f32[1, 197, 192]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
      "    add: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(cat, arg1_1);  cat = arg1_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(add, 0.0, False);  add = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(dropout, [192], arg4_1, arg5_1, 1e-06);  arg4_1 = arg5_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm, arg6_1, arg7_1);  layer_norm = arg6_1 = arg7_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear, [1, 197, 3, 3, 64]);  linear = None\n",
      "    permute: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape, [2, 0, 3, 1, 4]);  reshape = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind = torch.ops.aten.unbind.int(permute);  permute = None\n",
      "    getitem: \"f32[1, 3, 197, 64]\" = unbind[0]\n",
      "    getitem_1: \"f32[1, 3, 197, 64]\" = unbind[1]\n",
      "    getitem_2: \"f32[1, 3, 197, 64]\" = unbind[2];  unbind = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem, getitem_1, getitem_2);  getitem = getitem_1 = getitem_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_1: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention, 1, 2);  scaled_dot_product_attention = None\n",
      "    reshape_1: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_1, [1, 197, 192]);  transpose_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_1: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_1, arg8_1, arg9_1);  reshape_1 = arg8_1 = arg9_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_1: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_1, 0.0, False);  linear_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_1: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(dropout, dropout_1);  dropout = dropout_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_1: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_1, [192], arg10_1, arg11_1, 1e-06);  arg10_1 = arg11_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_2: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_1, arg12_1, arg13_1);  layer_norm_1 = arg12_1 = arg13_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_2: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu, 0.0, False);  gelu = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_3: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_2, arg14_1, arg15_1);  dropout_2 = arg14_1 = arg15_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_3: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_3, 0.0, False);  linear_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_2: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_1, dropout_3);  add_1 = dropout_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_2: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_2, [192], arg16_1, arg17_1, 1e-06);  arg16_1 = arg17_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_4: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_2, arg18_1, arg19_1);  layer_norm_2 = arg18_1 = arg19_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_2: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_4, [1, 197, 3, 3, 64]);  linear_4 = None\n",
      "    permute_1: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_2, [2, 0, 3, 1, 4]);  reshape_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_1 = torch.ops.aten.unbind.int(permute_1);  permute_1 = None\n",
      "    getitem_3: \"f32[1, 3, 197, 64]\" = unbind_1[0]\n",
      "    getitem_4: \"f32[1, 3, 197, 64]\" = unbind_1[1]\n",
      "    getitem_5: \"f32[1, 3, 197, 64]\" = unbind_1[2];  unbind_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_1: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_3, getitem_4, getitem_5);  getitem_3 = getitem_4 = getitem_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_2: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_1, 1, 2);  scaled_dot_product_attention_1 = None\n",
      "    reshape_3: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_2, [1, 197, 192]);  transpose_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_5: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_3, arg20_1, arg21_1);  reshape_3 = arg20_1 = arg21_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_4: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_3: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_2, dropout_4);  add_2 = dropout_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_3: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_3, [192], arg22_1, arg23_1, 1e-06);  arg22_1 = arg23_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_6: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_3, arg24_1, arg25_1);  layer_norm_3 = arg24_1 = arg25_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_1: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_5: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_1, 0.0, False);  gelu_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_7: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_5, arg26_1, arg27_1);  dropout_5 = arg26_1 = arg27_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_6: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_7, 0.0, False);  linear_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_4: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_3, dropout_6);  add_3 = dropout_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_4: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_4, [192], arg28_1, arg29_1, 1e-06);  arg28_1 = arg29_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_8: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_4, arg30_1, arg31_1);  layer_norm_4 = arg30_1 = arg31_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_4: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_8, [1, 197, 3, 3, 64]);  linear_8 = None\n",
      "    permute_2: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_4, [2, 0, 3, 1, 4]);  reshape_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_2 = torch.ops.aten.unbind.int(permute_2);  permute_2 = None\n",
      "    getitem_6: \"f32[1, 3, 197, 64]\" = unbind_2[0]\n",
      "    getitem_7: \"f32[1, 3, 197, 64]\" = unbind_2[1]\n",
      "    getitem_8: \"f32[1, 3, 197, 64]\" = unbind_2[2];  unbind_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_2: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_6, getitem_7, getitem_8);  getitem_6 = getitem_7 = getitem_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_3: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_2, 1, 2);  scaled_dot_product_attention_2 = None\n",
      "    reshape_5: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_3, [1, 197, 192]);  transpose_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_9: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_5, arg32_1, arg33_1);  reshape_5 = arg32_1 = arg33_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_7: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_9, 0.0, False);  linear_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_5: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_4, dropout_7);  add_4 = dropout_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_5: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_5, [192], arg34_1, arg35_1, 1e-06);  arg34_1 = arg35_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_10: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_5, arg36_1, arg37_1);  layer_norm_5 = arg36_1 = arg37_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_2: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_8: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_2, 0.0, False);  gelu_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_11: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_8, arg38_1, arg39_1);  dropout_8 = arg38_1 = arg39_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_9: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_6: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_5, dropout_9);  add_5 = dropout_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_6: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_6, [192], arg40_1, arg41_1, 1e-06);  arg40_1 = arg41_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_12: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_6, arg42_1, arg43_1);  layer_norm_6 = arg42_1 = arg43_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_6: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_12, [1, 197, 3, 3, 64]);  linear_12 = None\n",
      "    permute_3: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_6, [2, 0, 3, 1, 4]);  reshape_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_3 = torch.ops.aten.unbind.int(permute_3);  permute_3 = None\n",
      "    getitem_9: \"f32[1, 3, 197, 64]\" = unbind_3[0]\n",
      "    getitem_10: \"f32[1, 3, 197, 64]\" = unbind_3[1]\n",
      "    getitem_11: \"f32[1, 3, 197, 64]\" = unbind_3[2];  unbind_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_3: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_9, getitem_10, getitem_11);  getitem_9 = getitem_10 = getitem_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_4: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_3, 1, 2);  scaled_dot_product_attention_3 = None\n",
      "    reshape_7: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_4, [1, 197, 192]);  transpose_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_13: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_7, arg44_1, arg45_1);  reshape_7 = arg44_1 = arg45_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_10: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_13, 0.0, False);  linear_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_7: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_6, dropout_10);  add_6 = dropout_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_7: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_7, [192], arg46_1, arg47_1, 1e-06);  arg46_1 = arg47_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_14: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_7, arg48_1, arg49_1);  layer_norm_7 = arg48_1 = arg49_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_3: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_11: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_3, 0.0, False);  gelu_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_15: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_11, arg50_1, arg51_1);  dropout_11 = arg50_1 = arg51_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_12: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_15, 0.0, False);  linear_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_8: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_7, dropout_12);  add_7 = dropout_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_8: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_8, [192], arg52_1, arg53_1, 1e-06);  arg52_1 = arg53_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_16: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_8, arg54_1, arg55_1);  layer_norm_8 = arg54_1 = arg55_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_8: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_16, [1, 197, 3, 3, 64]);  linear_16 = None\n",
      "    permute_4: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_8, [2, 0, 3, 1, 4]);  reshape_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_4 = torch.ops.aten.unbind.int(permute_4);  permute_4 = None\n",
      "    getitem_12: \"f32[1, 3, 197, 64]\" = unbind_4[0]\n",
      "    getitem_13: \"f32[1, 3, 197, 64]\" = unbind_4[1]\n",
      "    getitem_14: \"f32[1, 3, 197, 64]\" = unbind_4[2];  unbind_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_4: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_12, getitem_13, getitem_14);  getitem_12 = getitem_13 = getitem_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_5: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_4, 1, 2);  scaled_dot_product_attention_4 = None\n",
      "    reshape_9: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_5, [1, 197, 192]);  transpose_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_17: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_9, arg56_1, arg57_1);  reshape_9 = arg56_1 = arg57_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_13: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_9: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_8, dropout_13);  add_8 = dropout_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_9: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_9, [192], arg58_1, arg59_1, 1e-06);  arg58_1 = arg59_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_18: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_9, arg60_1, arg61_1);  layer_norm_9 = arg60_1 = arg61_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_4: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_14: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_4, 0.0, False);  gelu_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_19: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_14, arg62_1, arg63_1);  dropout_14 = arg62_1 = arg63_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_15: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_19, 0.0, False);  linear_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_10: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_9, dropout_15);  add_9 = dropout_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_10: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_10, [192], arg64_1, arg65_1, 1e-06);  arg64_1 = arg65_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_20: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_10, arg66_1, arg67_1);  layer_norm_10 = arg66_1 = arg67_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_10: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_20, [1, 197, 3, 3, 64]);  linear_20 = None\n",
      "    permute_5: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_10, [2, 0, 3, 1, 4]);  reshape_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_5 = torch.ops.aten.unbind.int(permute_5);  permute_5 = None\n",
      "    getitem_15: \"f32[1, 3, 197, 64]\" = unbind_5[0]\n",
      "    getitem_16: \"f32[1, 3, 197, 64]\" = unbind_5[1]\n",
      "    getitem_17: \"f32[1, 3, 197, 64]\" = unbind_5[2];  unbind_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_5: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_15, getitem_16, getitem_17);  getitem_15 = getitem_16 = getitem_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_6: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_5, 1, 2);  scaled_dot_product_attention_5 = None\n",
      "    reshape_11: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_6, [1, 197, 192]);  transpose_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_21: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_11, arg68_1, arg69_1);  reshape_11 = arg68_1 = arg69_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_16: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_21, 0.0, False);  linear_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_11: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_10, dropout_16);  add_10 = dropout_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_11: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_11, [192], arg70_1, arg71_1, 1e-06);  arg70_1 = arg71_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_22: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_11, arg72_1, arg73_1);  layer_norm_11 = arg72_1 = arg73_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_5: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_17: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_5, 0.0, False);  gelu_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_23: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_17, arg74_1, arg75_1);  dropout_17 = arg74_1 = arg75_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_18: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_12: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_11, dropout_18);  add_11 = dropout_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_12: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_12, [192], arg76_1, arg77_1, 1e-06);  arg76_1 = arg77_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_24: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_12, arg78_1, arg79_1);  layer_norm_12 = arg78_1 = arg79_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_12: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_24, [1, 197, 3, 3, 64]);  linear_24 = None\n",
      "    permute_6: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_12, [2, 0, 3, 1, 4]);  reshape_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_6 = torch.ops.aten.unbind.int(permute_6);  permute_6 = None\n",
      "    getitem_18: \"f32[1, 3, 197, 64]\" = unbind_6[0]\n",
      "    getitem_19: \"f32[1, 3, 197, 64]\" = unbind_6[1]\n",
      "    getitem_20: \"f32[1, 3, 197, 64]\" = unbind_6[2];  unbind_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_6: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_18, getitem_19, getitem_20);  getitem_18 = getitem_19 = getitem_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_7: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_6, 1, 2);  scaled_dot_product_attention_6 = None\n",
      "    reshape_13: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_7, [1, 197, 192]);  transpose_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_25: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_13, arg80_1, arg81_1);  reshape_13 = arg80_1 = arg81_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_19: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_25, 0.0, False);  linear_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_13: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_12, dropout_19);  add_12 = dropout_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_13: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_13, [192], arg82_1, arg83_1, 1e-06);  arg82_1 = arg83_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_26: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_13, arg84_1, arg85_1);  layer_norm_13 = arg84_1 = arg85_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_6: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_20: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_6, 0.0, False);  gelu_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_27: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_20, arg86_1, arg87_1);  dropout_20 = arg86_1 = arg87_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_21: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_27, 0.0, False);  linear_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_14: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_13, dropout_21);  add_13 = dropout_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_14: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_14, [192], arg88_1, arg89_1, 1e-06);  arg88_1 = arg89_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_28: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_14, arg90_1, arg91_1);  layer_norm_14 = arg90_1 = arg91_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_14: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_28, [1, 197, 3, 3, 64]);  linear_28 = None\n",
      "    permute_7: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_14, [2, 0, 3, 1, 4]);  reshape_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_7 = torch.ops.aten.unbind.int(permute_7);  permute_7 = None\n",
      "    getitem_21: \"f32[1, 3, 197, 64]\" = unbind_7[0]\n",
      "    getitem_22: \"f32[1, 3, 197, 64]\" = unbind_7[1]\n",
      "    getitem_23: \"f32[1, 3, 197, 64]\" = unbind_7[2];  unbind_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_7: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_21, getitem_22, getitem_23);  getitem_21 = getitem_22 = getitem_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_8: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_7, 1, 2);  scaled_dot_product_attention_7 = None\n",
      "    reshape_15: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_8, [1, 197, 192]);  transpose_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_29: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_15, arg92_1, arg93_1);  reshape_15 = arg92_1 = arg93_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_22: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_29, 0.0, False);  linear_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_15: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_14, dropout_22);  add_14 = dropout_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_15: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_15, [192], arg94_1, arg95_1, 1e-06);  arg94_1 = arg95_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_30: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_15, arg96_1, arg97_1);  layer_norm_15 = arg96_1 = arg97_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_7: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_23: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_7, 0.0, False);  gelu_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_31: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_23, arg98_1, arg99_1);  dropout_23 = arg98_1 = arg99_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_24: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_31, 0.0, False);  linear_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_16: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_15, dropout_24);  add_15 = dropout_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_16: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_16, [192], arg100_1, arg101_1, 1e-06);  arg100_1 = arg101_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_32: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_16, arg102_1, arg103_1);  layer_norm_16 = arg102_1 = arg103_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_16: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_32, [1, 197, 3, 3, 64]);  linear_32 = None\n",
      "    permute_8: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_16, [2, 0, 3, 1, 4]);  reshape_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_8 = torch.ops.aten.unbind.int(permute_8);  permute_8 = None\n",
      "    getitem_24: \"f32[1, 3, 197, 64]\" = unbind_8[0]\n",
      "    getitem_25: \"f32[1, 3, 197, 64]\" = unbind_8[1]\n",
      "    getitem_26: \"f32[1, 3, 197, 64]\" = unbind_8[2];  unbind_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_8: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_24, getitem_25, getitem_26);  getitem_24 = getitem_25 = getitem_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_9: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_8, 1, 2);  scaled_dot_product_attention_8 = None\n",
      "    reshape_17: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_9, [1, 197, 192]);  transpose_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_33: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_17, arg104_1, arg105_1);  reshape_17 = arg104_1 = arg105_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_25: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_33, 0.0, False);  linear_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_17: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_16, dropout_25);  add_16 = dropout_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_17: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_17, [192], arg106_1, arg107_1, 1e-06);  arg106_1 = arg107_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_34: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_17, arg108_1, arg109_1);  layer_norm_17 = arg108_1 = arg109_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_8: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_26: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_8, 0.0, False);  gelu_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_35: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_26, arg110_1, arg111_1);  dropout_26 = arg110_1 = arg111_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_27: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_35, 0.0, False);  linear_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_18: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_17, dropout_27);  add_17 = dropout_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_18: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_18, [192], arg112_1, arg113_1, 1e-06);  arg112_1 = arg113_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_36: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_18, arg114_1, arg115_1);  layer_norm_18 = arg114_1 = arg115_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_18: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_36, [1, 197, 3, 3, 64]);  linear_36 = None\n",
      "    permute_9: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_18, [2, 0, 3, 1, 4]);  reshape_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_9 = torch.ops.aten.unbind.int(permute_9);  permute_9 = None\n",
      "    getitem_27: \"f32[1, 3, 197, 64]\" = unbind_9[0]\n",
      "    getitem_28: \"f32[1, 3, 197, 64]\" = unbind_9[1]\n",
      "    getitem_29: \"f32[1, 3, 197, 64]\" = unbind_9[2];  unbind_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_9: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_27, getitem_28, getitem_29);  getitem_27 = getitem_28 = getitem_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_10: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_9, 1, 2);  scaled_dot_product_attention_9 = None\n",
      "    reshape_19: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_10, [1, 197, 192]);  transpose_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_37: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_19, arg116_1, arg117_1);  reshape_19 = arg116_1 = arg117_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_28: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_37, 0.0, False);  linear_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_19: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_18, dropout_28);  add_18 = dropout_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_19: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_19, [192], arg118_1, arg119_1, 1e-06);  arg118_1 = arg119_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_38: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_19, arg120_1, arg121_1);  layer_norm_19 = arg120_1 = arg121_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_9: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_29: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_9, 0.0, False);  gelu_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_39: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_29, arg122_1, arg123_1);  dropout_29 = arg122_1 = arg123_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_30: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_39, 0.0, False);  linear_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_20: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_19, dropout_30);  add_19 = dropout_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_20: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_20, [192], arg124_1, arg125_1, 1e-06);  arg124_1 = arg125_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_40: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_20, arg126_1, arg127_1);  layer_norm_20 = arg126_1 = arg127_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_20: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_40, [1, 197, 3, 3, 64]);  linear_40 = None\n",
      "    permute_10: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_20, [2, 0, 3, 1, 4]);  reshape_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_10 = torch.ops.aten.unbind.int(permute_10);  permute_10 = None\n",
      "    getitem_30: \"f32[1, 3, 197, 64]\" = unbind_10[0]\n",
      "    getitem_31: \"f32[1, 3, 197, 64]\" = unbind_10[1]\n",
      "    getitem_32: \"f32[1, 3, 197, 64]\" = unbind_10[2];  unbind_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_10: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_30, getitem_31, getitem_32);  getitem_30 = getitem_31 = getitem_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_11: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_10, 1, 2);  scaled_dot_product_attention_10 = None\n",
      "    reshape_21: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_11, [1, 197, 192]);  transpose_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_41: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_21, arg128_1, arg129_1);  reshape_21 = arg128_1 = arg129_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_31: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_41, 0.0, False);  linear_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_21: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_20, dropout_31);  add_20 = dropout_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_21: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_21, [192], arg130_1, arg131_1, 1e-06);  arg130_1 = arg131_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_42: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_21, arg132_1, arg133_1);  layer_norm_21 = arg132_1 = arg133_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_10: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_32: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_10, 0.0, False);  gelu_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_43: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_32, arg134_1, arg135_1);  dropout_32 = arg134_1 = arg135_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_33: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_43, 0.0, False);  linear_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_22: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_21, dropout_33);  add_21 = dropout_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_22: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_22, [192], arg136_1, arg137_1, 1e-06);  arg136_1 = arg137_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_44: \"f32[1, 197, 576]\" = torch.ops.aten.linear.default(layer_norm_22, arg138_1, arg139_1);  layer_norm_22 = arg138_1 = arg139_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:75 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
      "    reshape_22: \"f32[1, 197, 3, 3, 64]\" = torch.ops.aten.reshape.default(linear_44, [1, 197, 3, 3, 64]);  linear_44 = None\n",
      "    permute_11: \"f32[3, 1, 3, 197, 64]\" = torch.ops.aten.permute.default(reshape_22, [2, 0, 3, 1, 4]);  reshape_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:76 in forward, code: q, k, v = qkv.unbind(0)\n",
      "    unbind_11 = torch.ops.aten.unbind.int(permute_11);  permute_11 = None\n",
      "    getitem_33: \"f32[1, 3, 197, 64]\" = unbind_11[0]\n",
      "    getitem_34: \"f32[1, 3, 197, 64]\" = unbind_11[1]\n",
      "    getitem_35: \"f32[1, 3, 197, 64]\" = unbind_11[2];  unbind_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:80 in forward, code: x = F.scaled_dot_product_attention(\n",
      "    scaled_dot_product_attention_11: \"f32[1, 3, 197, 64]\" = torch.ops.aten.scaled_dot_product_attention.default(getitem_33, getitem_34, getitem_35);  getitem_33 = getitem_34 = getitem_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/attention.py:93 in forward, code: x = x.transpose(1, 2).reshape(B, N, C)\n",
      "    transpose_12: \"f32[1, 197, 3, 64]\" = torch.ops.aten.transpose.int(scaled_dot_product_attention_11, 1, 2);  scaled_dot_product_attention_11 = None\n",
      "    reshape_23: \"f32[1, 197, 192]\" = torch.ops.aten.reshape.default(transpose_12, [1, 197, 192]);  transpose_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_45: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(reshape_23, arg140_1, arg141_1);  reshape_23 = arg140_1 = arg141_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_34: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_45, 0.0, False);  linear_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:176 in forward, code: x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\n",
      "    add_23: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_22, dropout_34);  add_22 = dropout_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_23: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_23, [192], arg142_1, arg143_1, 1e-06);  arg142_1 = arg143_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_46: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_23, arg144_1, arg145_1);  layer_norm_23 = arg144_1 = arg145_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:738 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
      "    gelu_11: \"f32[1, 197, 768]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_35: \"f32[1, 197, 768]\" = torch.ops.aten.dropout.default(gelu_11, 0.0, False);  gelu_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_47: \"f32[1, 197, 192]\" = torch.ops.aten.linear.default(dropout_35, arg146_1, arg147_1);  dropout_35 = arg146_1 = arg147_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_36: \"f32[1, 197, 192]\" = torch.ops.aten.dropout.default(linear_47, 0.0, False);  linear_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py:177 in forward, code: x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
      "    add_24: \"f32[1, 197, 192]\" = torch.ops.aten.add.Tensor(add_23, dropout_36);  add_23 = dropout_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/timm/layers/norm.py:89 in forward, code: x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "    layer_norm_24: \"f32[1, 197, 192]\" = torch.ops.aten.layer_norm.default(add_24, [192], arg148_1, arg149_1, 1e-06);  add_24 = arg148_1 = arg149_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_48: \"f32[1, 197, 768]\" = torch.ops.aten.linear.default(layer_norm_24, arg150_1, arg151_1);  layer_norm_24 = arg150_1 = arg151_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:263 in forward, code: cls_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    zeros: \"i64[77]\" = torch.ops.aten.zeros.default([77], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:264 in forward, code: dec_caption_dummy = torch.zeros(dummy_input_size, dtype=torch.long, device = image.device)\n",
      "    zeros_1: \"i64[77]\" = torch.ops.aten.zeros.default([77], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:144 in forward, code: learned_query = self.learned_query.unsqueeze(0).expand(B, -1, -1)\n",
      "    unsqueeze: \"f32[1, 32, 768]\" = torch.ops.aten.unsqueeze.default(arg152_1, 0);  arg152_1 = None\n",
      "    expand_1: \"f32[1, 32, 768]\" = torch.ops.aten.expand.default(unsqueeze, [1, -1, -1]);  unsqueeze = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros);  zeros = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_2: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1])\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_1: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_2);  expand_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:147 in forward, code: cls_text_embeddings = cls_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_25: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding, embedding_1);  embedding = embedding_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_2: \"f32[77, 768]\" = torch.ops.aten.embedding.default(arg502_1, zeros_1);  zeros_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    expand_3: \"i64[1, 77]\" = torch.ops.aten.expand.default(arg698_1, [1, -1]);  arg698_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_3: \"f32[1, 77, 768]\" = torch.ops.aten.embedding.default(arg503_1, expand_3);  arg503_1 = expand_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:149 in forward, code: dec_text_embeddings = dec_text_embeddings + self.position_embedding(self.position_ids.expand(B, -1))\n",
      "    add_26: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(embedding_2, embedding_3);  embedding_2 = embedding_3 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:105 in forward, code: itc_query_embds = query_embds.clone()\n",
      "    clone: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:106 in forward, code: itm_query_embds = query_embds.clone()\n",
      "    clone_1: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  clone_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:107 in forward, code: itg_query_embds = query_embds.clone()\n",
      "    clone_2: \"f32[1, 32, 768]\" = torch.ops.aten.clone.default(expand_1);  expand_1 = clone_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:109 in forward, code: itc_text_embds = cls_text_embds.clone()\n",
      "    clone_3: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25)\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:110 in forward, code: itm_text_embds = cls_text_embds.clone()\n",
      "    clone_4: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_25);  add_25 = clone_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:111 in forward, code: itg_text_embds = dec_text_embds.clone()\n",
      "    clone_5: \"f32[1, 77, 768]\" = torch.ops.aten.clone.default(add_26);  add_26 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:117 in forward, code: itc_add = to_additive_mask(self.itc_attn_mask, device=device, dtype=dtype)\n",
      "    to: \"f32[109, 109]\" = torch.ops.aten.to.device(arg699_1, device(type='cpu'), torch.float32);  arg699_1 = None\n",
      "    gt: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to, 0)\n",
      "    zeros_like: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to, pin_memory = False)\n",
      "    full_like: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to, -inf, pin_memory = False);  to = None\n",
      "    where: \"f32[109, 109]\" = torch.ops.aten.where.self(gt, zeros_like, full_like);  gt = zeros_like = full_like = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:118 in forward, code: itm_add = to_additive_mask(self.itm_attn_mask, device=device, dtype=dtype)\n",
      "    to_1: \"f32[109, 109]\" = torch.ops.aten.to.device(arg700_1, device(type='cpu'), torch.float32);  arg700_1 = None\n",
      "    gt_1: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_1, 0)\n",
      "    zeros_like_1: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_1, pin_memory = False)\n",
      "    full_like_1: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_1, -inf, pin_memory = False);  to_1 = None\n",
      "    where_1: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_1, zeros_like_1, full_like_1);  gt_1 = zeros_like_1 = full_like_1 = where_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:119 in forward, code: itg_add = to_additive_mask(self.itg_attn_mask, device=device, dtype=dtype)\n",
      "    to_2: \"f32[109, 109]\" = torch.ops.aten.to.device(arg701_1, device(type='cpu'), torch.float32);  arg701_1 = None\n",
      "    gt_2: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(to_2, 0)\n",
      "    zeros_like_2: \"f32[109, 109]\" = torch.ops.aten.zeros_like.default(to_2, pin_memory = False)\n",
      "    full_like_2: \"f32[109, 109]\" = torch.ops.aten.full_like.default(to_2, -inf, pin_memory = False);  to_2 = None\n",
      "    where_2: \"f32[109, 109]\" = torch.ops.aten.where.self(gt_2, zeros_like_2, full_like_2);  gt_2 = zeros_like_2 = full_like_2 = where_2 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([clone, clone_3], 1);  clone = clone_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_13: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_49: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_13, arg352_1, arg353_1);  transpose_13 = arg352_1 = arg353_1 = None\n",
      "    unflatten: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_49, -1, [3, 768]);  linear_49 = None\n",
      "    unsqueeze_1: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten, 0);  unflatten = None\n",
      "    transpose_14: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_1, 0, -2);  unsqueeze_1 = None\n",
      "    squeeze: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_14, -2);  transpose_14 = None\n",
      "    contiguous: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze);  squeeze = None\n",
      "    select: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 0)\n",
      "    select_1: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 1)\n",
      "    select_2: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous, 0, 2);  contiguous = None\n",
      "    unsqueeze_2: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select, [109, 12, 64]);  select = None\n",
      "    transpose_15: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view, 0, 1);  view = None\n",
      "    view_1: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_1, [109, 12, 64]);  select_1 = None\n",
      "    transpose_16: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_1, 0, 1);  view_1 = None\n",
      "    view_2: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_2, [109, 12, 64]);  select_2 = None\n",
      "    transpose_17: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_2, 0, 1);  view_2 = None\n",
      "    mul: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_15, 0.125);  transpose_15 = None\n",
      "    transpose_18: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_16, -2, -1);  transpose_16 = None\n",
      "    baddbmm: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_2, mul, transpose_18);  unsqueeze_2 = mul = transpose_18 = None\n",
      "    softmax: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm, -1);  baddbmm = None\n",
      "    bmm: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax, transpose_17);  transpose_17 = None\n",
      "    transpose_19: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm, 0, 1);  bmm = None\n",
      "    contiguous_1: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_19);  transpose_19 = None\n",
      "    view_3: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_1, [109, 768]);  contiguous_1 = None\n",
      "    linear_50: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_3, arg354_1, arg355_1);  view_3 = arg354_1 = arg355_1 = None\n",
      "    view_4: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_50, [109, 1, 768]);  linear_50 = None\n",
      "    view_5: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax, [1, 12, 109, 109]);  softmax = None\n",
      "    mean: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_5, [1]);  view_5 = mean = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_20: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_4, 1, 0);  view_4 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_27: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat, transpose_20);  concat = transpose_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_25: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_27, [768], arg356_1, arg357_1);  add_27 = arg356_1 = arg357_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_1: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25)\n",
      "    slice_2: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_1, 1, None, 32);  slice_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_3: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_25);  layer_norm_25 = None\n",
      "    slice_4: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_3, 1, 32);  slice_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_21: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_2, 1, 0)\n",
      "    transpose_22: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes = torch.ops.aten.split_with_sizes.default(arg370_1, [768, 1536]);  arg370_1 = None\n",
      "    getitem_36: \"f32[768, 768]\" = split_with_sizes[0]\n",
      "    getitem_37: \"f32[1536, 768]\" = split_with_sizes[1];  split_with_sizes = None\n",
      "    split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(arg371_1, [768, 1536]);  arg371_1 = None\n",
      "    getitem_38: \"f32[768]\" = split_with_sizes_1[0]\n",
      "    getitem_39: \"f32[1536]\" = split_with_sizes_1[1];  split_with_sizes_1 = None\n",
      "    linear_51: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_21, getitem_36, getitem_38);  transpose_21 = getitem_36 = getitem_38 = None\n",
      "    linear_52: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_22, getitem_37, getitem_39);  transpose_22 = getitem_37 = getitem_39 = None\n",
      "    unflatten_1: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_52, -1, [2, 768]);  linear_52 = None\n",
      "    unsqueeze_3: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_1, 0);  unflatten_1 = None\n",
      "    transpose_23: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_3, 0, -2);  unsqueeze_3 = None\n",
      "    squeeze_1: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_23, -2);  transpose_23 = None\n",
      "    contiguous_2: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_1);  squeeze_1 = None\n",
      "    select_3: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 0)\n",
      "    select_4: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_2, 0, 1);  contiguous_2 = None\n",
      "    view_6: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_51, [32, 12, 64]);  linear_51 = None\n",
      "    transpose_24: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_6, 0, 1);  view_6 = None\n",
      "    view_7: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_3, [197, 12, 64]);  select_3 = None\n",
      "    transpose_25: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_7, 0, 1);  view_7 = None\n",
      "    view_8: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_4, [197, 12, 64]);  select_4 = None\n",
      "    transpose_26: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_8, 0, 1);  view_8 = None\n",
      "    mul_1: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_24, 0.125);  transpose_24 = None\n",
      "    transpose_27: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_25, -2, -1);  transpose_25 = None\n",
      "    bmm_1: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_1, transpose_27);  mul_1 = transpose_27 = None\n",
      "    softmax_1: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_1, -1);  bmm_1 = None\n",
      "    bmm_2: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_1, transpose_26);  transpose_26 = None\n",
      "    transpose_28: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_2, 0, 1);  bmm_2 = None\n",
      "    contiguous_3: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_28);  transpose_28 = None\n",
      "    view_9: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_3, [32, 768]);  contiguous_3 = None\n",
      "    linear_53: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_9, arg372_1, arg373_1);  view_9 = arg372_1 = arg373_1 = None\n",
      "    view_10: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_53, [32, 1, 768]);  linear_53 = None\n",
      "    view_11: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_1, [1, 12, 32, 197]);  softmax_1 = None\n",
      "    mean_1: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_11, [1]);  view_11 = mean_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_29: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_10, 1, 0);  view_10 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_28: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_2, transpose_29);  slice_2 = transpose_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_26: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_28, [768], arg374_1, arg375_1);  add_28 = arg374_1 = arg375_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_54: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_26, arg358_1, arg359_1);  arg358_1 = arg359_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_12: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_54);  linear_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_55: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_12, arg360_1, arg361_1);  gelu_12 = arg360_1 = arg361_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_37: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_55, 0.1, False);  linear_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_29: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_37, layer_norm_26);  dropout_37 = layer_norm_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_27: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_29, [768], arg362_1, arg363_1, 1e-12);  add_29 = arg362_1 = arg363_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_56: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_4, arg364_1, arg365_1);  arg364_1 = arg365_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_13: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_56);  linear_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_57: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_13, arg366_1, arg367_1);  gelu_13 = arg366_1 = arg367_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_38: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_57, 0.1, False);  linear_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_30: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_38, slice_4);  dropout_38 = slice_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_28: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_30, [768], arg368_1, arg369_1, 1e-12);  add_30 = arg368_1 = arg369_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_1: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_27, layer_norm_28], 1);  layer_norm_27 = layer_norm_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_30: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_1, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_58: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_30, arg376_1, arg377_1);  transpose_30 = arg376_1 = arg377_1 = None\n",
      "    unflatten_2: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_58, -1, [3, 768]);  linear_58 = None\n",
      "    unsqueeze_4: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_2, 0);  unflatten_2 = None\n",
      "    transpose_31: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_4, 0, -2);  unsqueeze_4 = None\n",
      "    squeeze_2: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_31, -2);  transpose_31 = None\n",
      "    contiguous_4: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_2);  squeeze_2 = None\n",
      "    select_5: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 0)\n",
      "    select_6: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 1)\n",
      "    select_7: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_4, 0, 2);  contiguous_4 = None\n",
      "    unsqueeze_5: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_12: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_5, [109, 12, 64]);  select_5 = None\n",
      "    transpose_32: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_12, 0, 1);  view_12 = None\n",
      "    view_13: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_6, [109, 12, 64]);  select_6 = None\n",
      "    transpose_33: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_13, 0, 1);  view_13 = None\n",
      "    view_14: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_7, [109, 12, 64]);  select_7 = None\n",
      "    transpose_34: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_14, 0, 1);  view_14 = None\n",
      "    mul_2: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_32, 0.125);  transpose_32 = None\n",
      "    transpose_35: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_33, -2, -1);  transpose_33 = None\n",
      "    baddbmm_1: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_5, mul_2, transpose_35);  unsqueeze_5 = mul_2 = transpose_35 = None\n",
      "    softmax_2: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_1, -1);  baddbmm_1 = None\n",
      "    bmm_3: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_2, transpose_34);  transpose_34 = None\n",
      "    transpose_36: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_3, 0, 1);  bmm_3 = None\n",
      "    contiguous_5: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_36);  transpose_36 = None\n",
      "    view_15: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_5, [109, 768]);  contiguous_5 = None\n",
      "    linear_59: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_15, arg378_1, arg379_1);  view_15 = arg378_1 = arg379_1 = None\n",
      "    view_16: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_59, [109, 1, 768]);  linear_59 = None\n",
      "    view_17: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_2, [1, 12, 109, 109]);  softmax_2 = None\n",
      "    mean_2: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_17, [1]);  view_17 = mean_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_37: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_16, 1, 0);  view_16 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_31: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_1, transpose_37);  concat_1 = transpose_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_29: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_31, [768], arg380_1, arg381_1);  add_31 = arg380_1 = arg381_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_5: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29)\n",
      "    slice_6: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_5, 1, None, 32);  slice_5 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_7: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_29);  layer_norm_29 = None\n",
      "    slice_8: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_7, 1, 32);  slice_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_60: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_6, arg382_1, arg383_1);  arg382_1 = arg383_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_14: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_60);  linear_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_61: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_14, arg384_1, arg385_1);  gelu_14 = arg384_1 = arg385_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_39: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_61, 0.1, False);  linear_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_32: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_39, slice_6);  dropout_39 = slice_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_30: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_32, [768], arg386_1, arg387_1, 1e-12);  add_32 = arg386_1 = arg387_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_62: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_8, arg388_1, arg389_1);  arg388_1 = arg389_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_15: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_62);  linear_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_63: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_15, arg390_1, arg391_1);  gelu_15 = arg390_1 = arg391_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_40: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_63, 0.1, False);  linear_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_33: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_40, slice_8);  dropout_40 = slice_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_31: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_33, [768], arg392_1, arg393_1, 1e-12);  add_33 = arg392_1 = arg393_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_2: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_30, layer_norm_31], 1);  layer_norm_30 = layer_norm_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_38: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_2, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_64: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_38, arg394_1, arg395_1);  transpose_38 = arg394_1 = arg395_1 = None\n",
      "    unflatten_3: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_64, -1, [3, 768]);  linear_64 = None\n",
      "    unsqueeze_6: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_3, 0);  unflatten_3 = None\n",
      "    transpose_39: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_6, 0, -2);  unsqueeze_6 = None\n",
      "    squeeze_3: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_39, -2);  transpose_39 = None\n",
      "    contiguous_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_3);  squeeze_3 = None\n",
      "    select_8: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 0)\n",
      "    select_9: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 1)\n",
      "    select_10: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_6, 0, 2);  contiguous_6 = None\n",
      "    unsqueeze_7: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_18: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_8, [109, 12, 64]);  select_8 = None\n",
      "    transpose_40: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_18, 0, 1);  view_18 = None\n",
      "    view_19: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_9, [109, 12, 64]);  select_9 = None\n",
      "    transpose_41: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_19, 0, 1);  view_19 = None\n",
      "    view_20: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_10, [109, 12, 64]);  select_10 = None\n",
      "    transpose_42: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_20, 0, 1);  view_20 = None\n",
      "    mul_3: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_40, 0.125);  transpose_40 = None\n",
      "    transpose_43: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_41, -2, -1);  transpose_41 = None\n",
      "    baddbmm_2: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_7, mul_3, transpose_43);  unsqueeze_7 = mul_3 = transpose_43 = None\n",
      "    softmax_3: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_2, -1);  baddbmm_2 = None\n",
      "    bmm_4: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_3, transpose_42);  transpose_42 = None\n",
      "    transpose_44: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_4, 0, 1);  bmm_4 = None\n",
      "    contiguous_7: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_44);  transpose_44 = None\n",
      "    view_21: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_7, [109, 768]);  contiguous_7 = None\n",
      "    linear_65: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_21, arg396_1, arg397_1);  view_21 = arg396_1 = arg397_1 = None\n",
      "    view_22: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_65, [109, 1, 768]);  linear_65 = None\n",
      "    view_23: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_3, [1, 12, 109, 109]);  softmax_3 = None\n",
      "    mean_3: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_23, [1]);  view_23 = mean_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_45: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_22, 1, 0);  view_22 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_34: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_2, transpose_45);  concat_2 = transpose_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_32: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_34, [768], arg398_1, arg399_1);  add_34 = arg398_1 = arg399_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_9: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32)\n",
      "    slice_10: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_9, 1, None, 32);  slice_9 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_11: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_32);  layer_norm_32 = None\n",
      "    slice_12: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_11, 1, 32);  slice_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_46: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_10, 1, 0)\n",
      "    transpose_47: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(arg412_1, [768, 1536]);  arg412_1 = None\n",
      "    getitem_40: \"f32[768, 768]\" = split_with_sizes_2[0]\n",
      "    getitem_41: \"f32[1536, 768]\" = split_with_sizes_2[1];  split_with_sizes_2 = None\n",
      "    split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(arg413_1, [768, 1536]);  arg413_1 = None\n",
      "    getitem_42: \"f32[768]\" = split_with_sizes_3[0]\n",
      "    getitem_43: \"f32[1536]\" = split_with_sizes_3[1];  split_with_sizes_3 = None\n",
      "    linear_66: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_46, getitem_40, getitem_42);  transpose_46 = getitem_40 = getitem_42 = None\n",
      "    linear_67: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_47, getitem_41, getitem_43);  transpose_47 = getitem_41 = getitem_43 = None\n",
      "    unflatten_4: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_67, -1, [2, 768]);  linear_67 = None\n",
      "    unsqueeze_8: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_4, 0);  unflatten_4 = None\n",
      "    transpose_48: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_8, 0, -2);  unsqueeze_8 = None\n",
      "    squeeze_4: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_48, -2);  transpose_48 = None\n",
      "    contiguous_8: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_4);  squeeze_4 = None\n",
      "    select_11: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 0)\n",
      "    select_12: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_8, 0, 1);  contiguous_8 = None\n",
      "    view_24: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_66, [32, 12, 64]);  linear_66 = None\n",
      "    transpose_49: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_24, 0, 1);  view_24 = None\n",
      "    view_25: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_11, [197, 12, 64]);  select_11 = None\n",
      "    transpose_50: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_25, 0, 1);  view_25 = None\n",
      "    view_26: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_12, [197, 12, 64]);  select_12 = None\n",
      "    transpose_51: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_26, 0, 1);  view_26 = None\n",
      "    mul_4: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_49, 0.125);  transpose_49 = None\n",
      "    transpose_52: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_50, -2, -1);  transpose_50 = None\n",
      "    bmm_5: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_4, transpose_52);  mul_4 = transpose_52 = None\n",
      "    softmax_4: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_5, -1);  bmm_5 = None\n",
      "    bmm_6: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_4, transpose_51);  transpose_51 = None\n",
      "    transpose_53: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_6, 0, 1);  bmm_6 = None\n",
      "    contiguous_9: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_53);  transpose_53 = None\n",
      "    view_27: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_9, [32, 768]);  contiguous_9 = None\n",
      "    linear_68: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_27, arg414_1, arg415_1);  view_27 = arg414_1 = arg415_1 = None\n",
      "    view_28: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_68, [32, 1, 768]);  linear_68 = None\n",
      "    view_29: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_4, [1, 12, 32, 197]);  softmax_4 = None\n",
      "    mean_4: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_29, [1]);  view_29 = mean_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_54: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_28, 1, 0);  view_28 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_35: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_10, transpose_54);  slice_10 = transpose_54 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_33: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_35, [768], arg416_1, arg417_1);  add_35 = arg416_1 = arg417_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_69: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_33, arg400_1, arg401_1);  arg400_1 = arg401_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_16: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_69);  linear_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_70: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_16, arg402_1, arg403_1);  gelu_16 = arg402_1 = arg403_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_41: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_70, 0.1, False);  linear_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_36: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_41, layer_norm_33);  dropout_41 = layer_norm_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_34: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_36, [768], arg404_1, arg405_1, 1e-12);  add_36 = arg404_1 = arg405_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_71: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_12, arg406_1, arg407_1);  arg406_1 = arg407_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_17: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_71);  linear_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_72: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_17, arg408_1, arg409_1);  gelu_17 = arg408_1 = arg409_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_42: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_72, 0.1, False);  linear_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_37: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_42, slice_12);  dropout_42 = slice_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_35: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_37, [768], arg410_1, arg411_1, 1e-12);  add_37 = arg410_1 = arg411_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_3: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_34, layer_norm_35], 1);  layer_norm_34 = layer_norm_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_55: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_3, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_73: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_55, arg418_1, arg419_1);  transpose_55 = arg418_1 = arg419_1 = None\n",
      "    unflatten_5: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_73, -1, [3, 768]);  linear_73 = None\n",
      "    unsqueeze_9: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_5, 0);  unflatten_5 = None\n",
      "    transpose_56: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_9, 0, -2);  unsqueeze_9 = None\n",
      "    squeeze_5: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_56, -2);  transpose_56 = None\n",
      "    contiguous_10: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_5);  squeeze_5 = None\n",
      "    select_13: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 0)\n",
      "    select_14: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 1)\n",
      "    select_15: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_10, 0, 2);  contiguous_10 = None\n",
      "    unsqueeze_10: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_30: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_13, [109, 12, 64]);  select_13 = None\n",
      "    transpose_57: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None\n",
      "    view_31: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_14, [109, 12, 64]);  select_14 = None\n",
      "    transpose_58: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_31, 0, 1);  view_31 = None\n",
      "    view_32: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_15, [109, 12, 64]);  select_15 = None\n",
      "    transpose_59: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_32, 0, 1);  view_32 = None\n",
      "    mul_5: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_57, 0.125);  transpose_57 = None\n",
      "    transpose_60: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_58, -2, -1);  transpose_58 = None\n",
      "    baddbmm_3: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_10, mul_5, transpose_60);  unsqueeze_10 = mul_5 = transpose_60 = None\n",
      "    softmax_5: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_3, -1);  baddbmm_3 = None\n",
      "    bmm_7: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_5, transpose_59);  transpose_59 = None\n",
      "    transpose_61: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_7, 0, 1);  bmm_7 = None\n",
      "    contiguous_11: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_61);  transpose_61 = None\n",
      "    view_33: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_11, [109, 768]);  contiguous_11 = None\n",
      "    linear_74: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_33, arg420_1, arg421_1);  view_33 = arg420_1 = arg421_1 = None\n",
      "    view_34: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_74, [109, 1, 768]);  linear_74 = None\n",
      "    view_35: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_5, [1, 12, 109, 109]);  softmax_5 = None\n",
      "    mean_5: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_35, [1]);  view_35 = mean_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_62: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_34, 1, 0);  view_34 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_38: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_3, transpose_62);  concat_3 = transpose_62 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_36: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_38, [768], arg422_1, arg423_1);  add_38 = arg422_1 = arg423_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_13: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36)\n",
      "    slice_14: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_13, 1, None, 32);  slice_13 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_15: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_36);  layer_norm_36 = None\n",
      "    slice_16: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_15, 1, 32);  slice_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_75: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_14, arg424_1, arg425_1);  arg424_1 = arg425_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_18: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_75);  linear_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_76: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_18, arg426_1, arg427_1);  gelu_18 = arg426_1 = arg427_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_43: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_76, 0.1, False);  linear_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_39: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_43, slice_14);  dropout_43 = slice_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_37: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_39, [768], arg428_1, arg429_1, 1e-12);  add_39 = arg428_1 = arg429_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_77: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_16, arg430_1, arg431_1);  arg430_1 = arg431_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_19: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_77);  linear_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_78: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_19, arg432_1, arg433_1);  gelu_19 = arg432_1 = arg433_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_44: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_78, 0.1, False);  linear_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_40: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_44, slice_16);  dropout_44 = slice_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_38: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_40, [768], arg434_1, arg435_1, 1e-12);  add_40 = arg434_1 = arg435_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_4: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_37, layer_norm_38], 1);  layer_norm_37 = layer_norm_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_63: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_4, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_79: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_63, arg436_1, arg437_1);  transpose_63 = arg436_1 = arg437_1 = None\n",
      "    unflatten_6: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_79, -1, [3, 768]);  linear_79 = None\n",
      "    unsqueeze_11: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_6, 0);  unflatten_6 = None\n",
      "    transpose_64: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_11, 0, -2);  unsqueeze_11 = None\n",
      "    squeeze_6: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_64, -2);  transpose_64 = None\n",
      "    contiguous_12: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_6);  squeeze_6 = None\n",
      "    select_16: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 0)\n",
      "    select_17: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 1)\n",
      "    select_18: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_12, 0, 2);  contiguous_12 = None\n",
      "    unsqueeze_12: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_36: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_16, [109, 12, 64]);  select_16 = None\n",
      "    transpose_65: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_36, 0, 1);  view_36 = None\n",
      "    view_37: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_17, [109, 12, 64]);  select_17 = None\n",
      "    transpose_66: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_37, 0, 1);  view_37 = None\n",
      "    view_38: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_18, [109, 12, 64]);  select_18 = None\n",
      "    transpose_67: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_38, 0, 1);  view_38 = None\n",
      "    mul_6: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_65, 0.125);  transpose_65 = None\n",
      "    transpose_68: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_66, -2, -1);  transpose_66 = None\n",
      "    baddbmm_4: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_12, mul_6, transpose_68);  unsqueeze_12 = mul_6 = transpose_68 = None\n",
      "    softmax_6: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_4, -1);  baddbmm_4 = None\n",
      "    bmm_8: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_6, transpose_67);  transpose_67 = None\n",
      "    transpose_69: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_8, 0, 1);  bmm_8 = None\n",
      "    contiguous_13: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_69);  transpose_69 = None\n",
      "    view_39: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_13, [109, 768]);  contiguous_13 = None\n",
      "    linear_80: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_39, arg438_1, arg439_1);  view_39 = arg438_1 = arg439_1 = None\n",
      "    view_40: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_80, [109, 1, 768]);  linear_80 = None\n",
      "    view_41: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_6, [1, 12, 109, 109]);  softmax_6 = None\n",
      "    mean_6: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_41, [1]);  view_41 = mean_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_70: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_40, 1, 0);  view_40 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_41: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_4, transpose_70);  concat_4 = transpose_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_39: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_41, [768], arg440_1, arg441_1);  add_41 = arg440_1 = arg441_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_17: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39)\n",
      "    slice_18: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_17, 1, None, 32);  slice_17 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_19: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_39);  layer_norm_39 = None\n",
      "    slice_20: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_19, 1, 32);  slice_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_71: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_18, 1, 0)\n",
      "    transpose_72: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(arg454_1, [768, 1536]);  arg454_1 = None\n",
      "    getitem_44: \"f32[768, 768]\" = split_with_sizes_4[0]\n",
      "    getitem_45: \"f32[1536, 768]\" = split_with_sizes_4[1];  split_with_sizes_4 = None\n",
      "    split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(arg455_1, [768, 1536]);  arg455_1 = None\n",
      "    getitem_46: \"f32[768]\" = split_with_sizes_5[0]\n",
      "    getitem_47: \"f32[1536]\" = split_with_sizes_5[1];  split_with_sizes_5 = None\n",
      "    linear_81: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_71, getitem_44, getitem_46);  transpose_71 = getitem_44 = getitem_46 = None\n",
      "    linear_82: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_72, getitem_45, getitem_47);  transpose_72 = getitem_45 = getitem_47 = None\n",
      "    unflatten_7: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_82, -1, [2, 768]);  linear_82 = None\n",
      "    unsqueeze_13: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_7, 0);  unflatten_7 = None\n",
      "    transpose_73: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_13, 0, -2);  unsqueeze_13 = None\n",
      "    squeeze_7: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_73, -2);  transpose_73 = None\n",
      "    contiguous_14: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_7);  squeeze_7 = None\n",
      "    select_19: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 0)\n",
      "    select_20: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_14, 0, 1);  contiguous_14 = None\n",
      "    view_42: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_81, [32, 12, 64]);  linear_81 = None\n",
      "    transpose_74: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_42, 0, 1);  view_42 = None\n",
      "    view_43: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_19, [197, 12, 64]);  select_19 = None\n",
      "    transpose_75: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_43, 0, 1);  view_43 = None\n",
      "    view_44: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_20, [197, 12, 64]);  select_20 = None\n",
      "    transpose_76: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_44, 0, 1);  view_44 = None\n",
      "    mul_7: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_74, 0.125);  transpose_74 = None\n",
      "    transpose_77: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_75, -2, -1);  transpose_75 = None\n",
      "    bmm_9: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_7, transpose_77);  mul_7 = transpose_77 = None\n",
      "    softmax_7: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_9, -1);  bmm_9 = None\n",
      "    bmm_10: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_7, transpose_76);  transpose_76 = None\n",
      "    transpose_78: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_10, 0, 1);  bmm_10 = None\n",
      "    contiguous_15: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_78);  transpose_78 = None\n",
      "    view_45: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_15, [32, 768]);  contiguous_15 = None\n",
      "    linear_83: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_45, arg456_1, arg457_1);  view_45 = arg456_1 = arg457_1 = None\n",
      "    view_46: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_83, [32, 1, 768]);  linear_83 = None\n",
      "    view_47: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_7, [1, 12, 32, 197]);  softmax_7 = None\n",
      "    mean_7: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_47, [1]);  view_47 = mean_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_79: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_46, 1, 0);  view_46 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_42: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_18, transpose_79);  slice_18 = transpose_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_40: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_42, [768], arg458_1, arg459_1);  add_42 = arg458_1 = arg459_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_84: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_40, arg442_1, arg443_1);  arg442_1 = arg443_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_20: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_84);  linear_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_85: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_20, arg444_1, arg445_1);  gelu_20 = arg444_1 = arg445_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_45: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_85, 0.1, False);  linear_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_43: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_45, layer_norm_40);  dropout_45 = layer_norm_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_41: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_43, [768], arg446_1, arg447_1, 1e-12);  add_43 = arg446_1 = arg447_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_86: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_20, arg448_1, arg449_1);  arg448_1 = arg449_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_21: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_86);  linear_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_87: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_21, arg450_1, arg451_1);  gelu_21 = arg450_1 = arg451_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_46: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_87, 0.1, False);  linear_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_44: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_46, slice_20);  dropout_46 = slice_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_42: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_44, [768], arg452_1, arg453_1, 1e-12);  add_44 = arg452_1 = arg453_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_5: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_41, layer_norm_42], 1);  layer_norm_41 = layer_norm_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_80: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_5, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_88: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_80, arg460_1, arg461_1);  transpose_80 = arg460_1 = arg461_1 = None\n",
      "    unflatten_8: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_88, -1, [3, 768]);  linear_88 = None\n",
      "    unsqueeze_14: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_8, 0);  unflatten_8 = None\n",
      "    transpose_81: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_14, 0, -2);  unsqueeze_14 = None\n",
      "    squeeze_8: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_81, -2);  transpose_81 = None\n",
      "    contiguous_16: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_8);  squeeze_8 = None\n",
      "    select_21: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 0)\n",
      "    select_22: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 1)\n",
      "    select_23: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_16, 0, 2);  contiguous_16 = None\n",
      "    unsqueeze_15: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0)\n",
      "    view_48: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_21, [109, 12, 64]);  select_21 = None\n",
      "    transpose_82: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_48, 0, 1);  view_48 = None\n",
      "    view_49: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_22, [109, 12, 64]);  select_22 = None\n",
      "    transpose_83: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_49, 0, 1);  view_49 = None\n",
      "    view_50: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_23, [109, 12, 64]);  select_23 = None\n",
      "    transpose_84: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_50, 0, 1);  view_50 = None\n",
      "    mul_8: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_82, 0.125);  transpose_82 = None\n",
      "    transpose_85: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_83, -2, -1);  transpose_83 = None\n",
      "    baddbmm_5: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_15, mul_8, transpose_85);  unsqueeze_15 = mul_8 = transpose_85 = None\n",
      "    softmax_8: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_5, -1);  baddbmm_5 = None\n",
      "    bmm_11: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_8, transpose_84);  transpose_84 = None\n",
      "    transpose_86: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_11, 0, 1);  bmm_11 = None\n",
      "    contiguous_17: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_86);  transpose_86 = None\n",
      "    view_51: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_17, [109, 768]);  contiguous_17 = None\n",
      "    linear_89: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_51, arg462_1, arg463_1);  view_51 = arg462_1 = arg463_1 = None\n",
      "    view_52: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_89, [109, 1, 768]);  linear_89 = None\n",
      "    view_53: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_8, [1, 12, 109, 109]);  softmax_8 = None\n",
      "    mean_8: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_53, [1]);  view_53 = mean_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_87: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_52, 1, 0);  view_52 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_45: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_5, transpose_87);  concat_5 = transpose_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_43: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_45, [768], arg464_1, arg465_1);  add_45 = arg464_1 = arg465_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_21: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43)\n",
      "    slice_22: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_21, 1, None, 32);  slice_21 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_23: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_43);  layer_norm_43 = None\n",
      "    slice_24: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_23, 1, 32);  slice_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_90: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(slice_22, arg466_1, arg467_1);  arg466_1 = arg467_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_22: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_90);  linear_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_91: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_22, arg468_1, arg469_1);  gelu_22 = arg468_1 = arg469_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_47: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_91, 0.1, False);  linear_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_46: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_47, slice_22);  dropout_47 = slice_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_44: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_46, [768], arg470_1, arg471_1, 1e-12);  add_46 = arg470_1 = arg471_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_92: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_24, arg472_1, arg473_1);  arg472_1 = arg473_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_23: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_92);  linear_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_93: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_23, arg474_1, arg475_1);  gelu_23 = arg474_1 = arg475_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_48: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_93, 0.1, False);  linear_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_47: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_48, slice_24);  dropout_48 = slice_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_45: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_47, [768], arg476_1, arg477_1, 1e-12);  add_47 = arg476_1 = arg477_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:42 in forward, code: combined_embds = torch.concat((query_embds, text_embds), dim=1) # B, Qs + Ts, D\n",
      "    concat_6: \"f32[1, 109, 768]\" = torch.ops.aten.concat.default([layer_norm_44, layer_norm_45], 1);  layer_norm_44 = layer_norm_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1343 in forward, code: query = key = value = query.transpose(1, 0)\n",
      "    transpose_88: \"f32[109, 1, 768]\" = torch.ops.aten.transpose.int(concat_6, 1, 0)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    linear_94: \"f32[109, 1, 2304]\" = torch.ops.aten.linear.default(transpose_88, arg478_1, arg479_1);  transpose_88 = arg478_1 = arg479_1 = None\n",
      "    unflatten_9: \"f32[109, 1, 3, 768]\" = torch.ops.aten.unflatten.int(linear_94, -1, [3, 768]);  linear_94 = None\n",
      "    unsqueeze_16: \"f32[1, 109, 1, 3, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_9, 0);  unflatten_9 = None\n",
      "    transpose_89: \"f32[3, 109, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_16, 0, -2);  unsqueeze_16 = None\n",
      "    squeeze_9: \"f32[3, 109, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_89, -2);  transpose_89 = None\n",
      "    contiguous_18: \"f32[3, 109, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_9);  squeeze_9 = None\n",
      "    select_24: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 0)\n",
      "    select_25: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 1)\n",
      "    select_26: \"f32[109, 1, 768]\" = torch.ops.aten.select.int(contiguous_18, 0, 2);  contiguous_18 = None\n",
      "    unsqueeze_17: \"f32[1, 109, 109]\" = torch.ops.aten.unsqueeze.default(where, 0);  where = None\n",
      "    view_54: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_24, [109, 12, 64]);  select_24 = None\n",
      "    transpose_90: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_54, 0, 1);  view_54 = None\n",
      "    view_55: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_25, [109, 12, 64]);  select_25 = None\n",
      "    transpose_91: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_55, 0, 1);  view_55 = None\n",
      "    view_56: \"f32[109, 12, 64]\" = torch.ops.aten.view.default(select_26, [109, 12, 64]);  select_26 = None\n",
      "    transpose_92: \"f32[12, 109, 64]\" = torch.ops.aten.transpose.int(view_56, 0, 1);  view_56 = None\n",
      "    mul_9: \"f32[12, 109, 64]\" = torch.ops.aten.mul.Tensor(transpose_90, 0.125);  transpose_90 = None\n",
      "    transpose_93: \"f32[12, 64, 109]\" = torch.ops.aten.transpose.int(transpose_91, -2, -1);  transpose_91 = None\n",
      "    baddbmm_6: \"f32[12, 109, 109]\" = torch.ops.aten.baddbmm.default(unsqueeze_17, mul_9, transpose_93);  unsqueeze_17 = mul_9 = transpose_93 = None\n",
      "    softmax_9: \"f32[12, 109, 109]\" = torch.ops.aten.softmax.int(baddbmm_6, -1);  baddbmm_6 = None\n",
      "    bmm_12: \"f32[12, 109, 64]\" = torch.ops.aten.bmm.default(softmax_9, transpose_92);  transpose_92 = None\n",
      "    transpose_94: \"f32[109, 12, 64]\" = torch.ops.aten.transpose.int(bmm_12, 0, 1);  bmm_12 = None\n",
      "    contiguous_19: \"f32[109, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_94);  transpose_94 = None\n",
      "    view_57: \"f32[109, 768]\" = torch.ops.aten.view.default(contiguous_19, [109, 768]);  contiguous_19 = None\n",
      "    linear_95: \"f32[109, 768]\" = torch.ops.aten.linear.default(view_57, arg480_1, arg481_1);  view_57 = arg480_1 = arg481_1 = None\n",
      "    view_58: \"f32[109, 1, 768]\" = torch.ops.aten.view.default(linear_95, [109, 1, 768]);  linear_95 = None\n",
      "    view_59: \"f32[1, 12, 109, 109]\" = torch.ops.aten.view.default(softmax_9, [1, 12, 109, 109]);  softmax_9 = None\n",
      "    mean_9: \"f32[1, 109, 109]\" = torch.ops.aten.mean.dim(view_59, [1]);  view_59 = mean_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_95: \"f32[1, 109, 768]\" = torch.ops.aten.transpose.int(view_58, 1, 0);  view_58 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:49 in forward, code: combined_embds = self.self_ln(combined_embds+ attn_out)\n",
      "    add_48: \"f32[1, 109, 768]\" = torch.ops.aten.add.Tensor(concat_6, transpose_95);  concat_6 = transpose_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_46: \"f32[1, 109, 768]\" = torch.ops.aten.layer_norm.default(add_48, [768], arg482_1, arg483_1);  add_48 = arg482_1 = arg483_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:51 in forward, code: query_embds = combined_embds[:, :Qs]\n",
      "    slice_25: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46)\n",
      "    slice_26: \"f32[1, 32, 768]\" = torch.ops.aten.slice.Tensor(slice_25, 1, None, 32);  slice_25 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:52 in forward, code: text_embds= combined_embds[:, Qs:]\n",
      "    slice_27: \"f32[1, 109, 768]\" = torch.ops.aten.slice.Tensor(layer_norm_46);  layer_norm_46 = None\n",
      "    slice_28: \"f32[1, 77, 768]\" = torch.ops.aten.slice.Tensor(slice_27, 1, 32);  slice_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1345 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))\n",
      "    transpose_96: \"f32[32, 1, 768]\" = torch.ops.aten.transpose.int(slice_26, 1, 0)\n",
      "    transpose_97: \"f32[197, 1, 768]\" = torch.ops.aten.transpose.int(linear_48, 1, 0);  linear_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1377 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "    split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(arg496_1, [768, 1536]);  arg496_1 = None\n",
      "    getitem_48: \"f32[768, 768]\" = split_with_sizes_6[0]\n",
      "    getitem_49: \"f32[1536, 768]\" = split_with_sizes_6[1];  split_with_sizes_6 = None\n",
      "    split_with_sizes_7 = torch.ops.aten.split_with_sizes.default(arg497_1, [768, 1536]);  arg497_1 = None\n",
      "    getitem_50: \"f32[768]\" = split_with_sizes_7[0]\n",
      "    getitem_51: \"f32[1536]\" = split_with_sizes_7[1];  split_with_sizes_7 = None\n",
      "    linear_96: \"f32[32, 1, 768]\" = torch.ops.aten.linear.default(transpose_96, getitem_48, getitem_50);  transpose_96 = getitem_48 = getitem_50 = None\n",
      "    linear_97: \"f32[197, 1, 1536]\" = torch.ops.aten.linear.default(transpose_97, getitem_49, getitem_51);  transpose_97 = getitem_49 = getitem_51 = None\n",
      "    unflatten_10: \"f32[197, 1, 2, 768]\" = torch.ops.aten.unflatten.int(linear_97, -1, [2, 768]);  linear_97 = None\n",
      "    unsqueeze_18: \"f32[1, 197, 1, 2, 768]\" = torch.ops.aten.unsqueeze.default(unflatten_10, 0);  unflatten_10 = None\n",
      "    transpose_98: \"f32[2, 197, 1, 1, 768]\" = torch.ops.aten.transpose.int(unsqueeze_18, 0, -2);  unsqueeze_18 = None\n",
      "    squeeze_10: \"f32[2, 197, 1, 768]\" = torch.ops.aten.squeeze.dim(transpose_98, -2);  transpose_98 = None\n",
      "    contiguous_20: \"f32[2, 197, 1, 768]\" = torch.ops.aten.contiguous.default(squeeze_10);  squeeze_10 = None\n",
      "    select_27: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 0)\n",
      "    select_28: \"f32[197, 1, 768]\" = torch.ops.aten.select.int(contiguous_20, 0, 1);  contiguous_20 = None\n",
      "    view_60: \"f32[32, 12, 64]\" = torch.ops.aten.view.default(linear_96, [32, 12, 64]);  linear_96 = None\n",
      "    transpose_99: \"f32[12, 32, 64]\" = torch.ops.aten.transpose.int(view_60, 0, 1);  view_60 = None\n",
      "    view_61: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_27, [197, 12, 64]);  select_27 = None\n",
      "    transpose_100: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_61, 0, 1);  view_61 = None\n",
      "    view_62: \"f32[197, 12, 64]\" = torch.ops.aten.view.default(select_28, [197, 12, 64]);  select_28 = None\n",
      "    transpose_101: \"f32[12, 197, 64]\" = torch.ops.aten.transpose.int(view_62, 0, 1);  view_62 = None\n",
      "    mul_10: \"f32[12, 32, 64]\" = torch.ops.aten.mul.Tensor(transpose_99, 0.125);  transpose_99 = None\n",
      "    transpose_102: \"f32[12, 64, 197]\" = torch.ops.aten.transpose.int(transpose_100, -2, -1);  transpose_100 = None\n",
      "    bmm_13: \"f32[12, 32, 197]\" = torch.ops.aten.bmm.default(mul_10, transpose_102);  mul_10 = transpose_102 = None\n",
      "    softmax_10: \"f32[12, 32, 197]\" = torch.ops.aten.softmax.int(bmm_13, -1);  bmm_13 = None\n",
      "    bmm_14: \"f32[12, 32, 64]\" = torch.ops.aten.bmm.default(softmax_10, transpose_101);  transpose_101 = None\n",
      "    transpose_103: \"f32[32, 12, 64]\" = torch.ops.aten.transpose.int(bmm_14, 0, 1);  bmm_14 = None\n",
      "    contiguous_21: \"f32[32, 12, 64]\" = torch.ops.aten.contiguous.default(transpose_103);  transpose_103 = None\n",
      "    view_63: \"f32[32, 768]\" = torch.ops.aten.view.default(contiguous_21, [32, 768]);  contiguous_21 = None\n",
      "    linear_98: \"f32[32, 768]\" = torch.ops.aten.linear.default(view_63, arg498_1, arg499_1);  view_63 = arg498_1 = arg499_1 = None\n",
      "    view_64: \"f32[32, 1, 768]\" = torch.ops.aten.view.default(linear_98, [32, 1, 768]);  linear_98 = None\n",
      "    view_65: \"f32[1, 12, 32, 197]\" = torch.ops.aten.view.default(softmax_10, [1, 12, 32, 197]);  softmax_10 = None\n",
      "    mean_10: \"f32[1, 32, 197]\" = torch.ops.aten.mean.dim(view_65, [1]);  view_65 = mean_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1399 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
      "    transpose_104: \"f32[1, 32, 768]\" = torch.ops.aten.transpose.int(view_64, 1, 0);  view_64 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:56 in forward, code: query_embds = self.cross_layer_norm(query_embds + hidden_states)\n",
      "    add_49: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(slice_26, transpose_104);  slice_26 = transpose_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_47: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_49, [768], arg500_1, arg501_1);  add_49 = arg500_1 = arg501_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_99: \"f32[1, 32, 3072]\" = torch.ops.aten.linear.default(layer_norm_47, arg484_1, arg485_1);  arg484_1 = arg485_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_24: \"f32[1, 32, 3072]\" = torch.ops.aten.gelu.default(linear_99);  linear_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_100: \"f32[1, 32, 768]\" = torch.ops.aten.linear.default(gelu_24, arg486_1, arg487_1);  gelu_24 = arg486_1 = arg487_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_49: \"f32[1, 32, 768]\" = torch.ops.aten.dropout.default(linear_100, 0.1, False);  linear_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_50: \"f32[1, 32, 768]\" = torch.ops.aten.add.Tensor(dropout_49, layer_norm_47);  dropout_49 = layer_norm_47 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_48: \"f32[1, 32, 768]\" = torch.ops.aten.layer_norm.default(add_50, [768], arg488_1, arg489_1, 1e-12);  add_50 = arg488_1 = arg489_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_101: \"f32[1, 77, 3072]\" = torch.ops.aten.linear.default(slice_28, arg490_1, arg491_1);  arg490_1 = arg491_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:70 in forward, code: return self.act(input)\n",
      "    gelu_25: \"f32[1, 77, 3072]\" = torch.ops.aten.gelu.default(linear_101);  linear_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_102: \"f32[1, 77, 768]\" = torch.ops.aten.linear.default(gelu_25, arg492_1, arg493_1);  gelu_25 = arg492_1 = arg493_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_50: \"f32[1, 77, 768]\" = torch.ops.aten.dropout.default(linear_102, 0.1, False);  linear_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py:526 in forward, code: hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
      "    add_51: \"f32[1, 77, 768]\" = torch.ops.aten.add.Tensor(dropout_50, slice_28);  dropout_50 = slice_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(\n",
      "    layer_norm_49: \"f32[1, 77, 768]\" = torch.ops.aten.layer_norm.default(add_51, [768], arg494_1, arg495_1, 1e-12);  add_51 = arg494_1 = arg495_1 = layer_norm_49 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:155 in forward, code: itg_logits = itg_text_embds @ self.output_embedding.weight.T # (S,Vocab_size)\n",
      "    numpy_t: \"f32[768, 30522]\" = torch.ops.aten.numpy_T.default(arg502_1);  arg502_1 = None\n",
      "    matmul: \"f32[1, 77, 30522]\" = torch.ops.aten.matmul.default(clone_5, numpy_t);  clone_5 = numpy_t = matmul = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_103: \"f32[1, 32, 512]\" = torch.ops.aten.linear.default(layer_norm_48, arg504_1, arg505_1);  layer_norm_48 = arg504_1 = arg505_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_4: \"f32[1, 77, 512]\" = torch.ops.aten.embedding.default(arg582_1, arg705_1);  arg705_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_1 = torch._C._set_grad_enabled(False);  _set_grad_enabled_1 = None\n",
      "    concat_7: \"f32[1, 109, 512]\" = torch.ops.aten.concat.default([linear_103, embedding_4], 1);  linear_103 = embedding_4 = None\n",
      "    ones: \"i64[1, 32]\" = torch.ops.aten.ones.default([1, 32], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    concat_8: \"i64[1, 109]\" = torch.ops.aten.concat.default([ones, arg706_1], 1);  ones = arg706_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1025 in forward, code: cache_position = torch.arange(\n",
      "    arange: \"i64[109]\" = torch.ops.aten.arange.start(0, 109, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1045 in forward, code: causal_mask = attention_mask[:, None, None, :]\n",
      "    slice_29: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_19: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_29, 1);  slice_29 = None\n",
      "    unsqueeze_20: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_19, 2);  unsqueeze_19 = None\n",
      "    slice_30: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_20, 3, 0, 9223372036854775807);  unsqueeze_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1046 in forward, code: causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)\n",
      "    to_3: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_30, torch.float32);  slice_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1047 in forward, code: causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min\n",
      "    rsub: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_3, 1.0);  to_3 = None\n",
      "    mul_11: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub, -3.4028234663852886e+38);  rsub = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_51: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(concat_7, 0.1, False);  concat_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_4: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(dropout_51, torch.float32);  dropout_51 = None\n",
      "    pow_1: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_4, 2)\n",
      "    mean_11: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None\n",
      "    add_52: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_11, 1e-06);  mean_11 = None\n",
      "    rsqrt: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_52);  add_52 = None\n",
      "    mul_12: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_4, rsqrt);  rsqrt = None\n",
      "    mul_13: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg513_1, mul_12);  arg513_1 = mul_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_104: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg508_1);  arg508_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_66: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_104, [1, -1, 6, 64]);  linear_104 = None\n",
      "    transpose_105: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_66, 1, 2);  view_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_105: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg509_1);  arg509_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_106: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_13, arg510_1);  mul_13 = arg510_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_67: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_105, [1, -1, 6, 64]);  linear_105 = None\n",
      "    transpose_106: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_67, 1, 2);  view_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_68: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_106, [1, -1, 6, 64]);  linear_106 = None\n",
      "    transpose_107: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_68, 1, 2);  view_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_108: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_106, 3, 2);  transpose_106 = None\n",
      "    matmul_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_105, transpose_108);  transpose_105 = transpose_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_29: \"i64[]\" = torch.ops.aten.select.int(arange, 0, -1)\n",
      "    add_53: \"i64[]\" = torch.ops.aten.add.Tensor(select_29, 1);  select_29 = add_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_31: \"i64[109]\" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807);  arange = None\n",
      "    unsqueeze_21: \"i64[109, 1]\" = torch.ops.aten.unsqueeze.default(slice_31, 1);  slice_31 = None\n",
      "    to_5: \"i64[109, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_21, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_21 = None\n",
      "    arange_1: \"i64[109]\" = torch.ops.aten.arange.default(109, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_22: \"i64[1, 109]\" = torch.ops.aten.unsqueeze.default(arange_1, 0);  arange_1 = None\n",
      "    slice_32: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_22, 1, 0, 9223372036854775807);  unsqueeze_22 = None\n",
      "    sub: \"i64[109, 109]\" = torch.ops.aten.sub.Tensor(slice_32, to_5);  slice_32 = to_5 = None\n",
      "    gt_3: \"b8[109, 109]\" = torch.ops.aten.gt.Scalar(sub, 0)\n",
      "    to_6: \"i64[109, 109]\" = torch.ops.aten.to.dtype(gt_3, torch.int64);  gt_3 = None\n",
      "    mul_14: \"i64[109, 109]\" = torch.ops.aten.mul.Tensor(to_6, 16);  to_6 = None\n",
      "    add_54: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(mul_14, 0);  mul_14 = None\n",
      "    abs_1: \"i64[109, 109]\" = torch.ops.aten.abs.default(sub);  sub = None\n",
      "    lt: \"b8[109, 109]\" = torch.ops.aten.lt.Scalar(abs_1, 8)\n",
      "    to_7: \"f32[109, 109]\" = torch.ops.aten.to.dtype(abs_1, torch.float32)\n",
      "    div: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(to_7, 8);  to_7 = None\n",
      "    log: \"f32[109, 109]\" = torch.ops.aten.log.default(div);  div = None\n",
      "    div_1: \"f32[109, 109]\" = torch.ops.aten.div.Tensor(log, 2.772588722239781);  log = None\n",
      "    mul_15: \"f32[109, 109]\" = torch.ops.aten.mul.Tensor(div_1, 8);  div_1 = None\n",
      "    to_8: \"i64[109, 109]\" = torch.ops.aten.to.dtype(mul_15, torch.int64);  mul_15 = None\n",
      "    add_55: \"i64[109, 109]\" = torch.ops.aten.add.Tensor(to_8, 8);  to_8 = None\n",
      "    full_like_3: \"i64[109, 109]\" = torch.ops.aten.full_like.default(add_55, 15, pin_memory = False)\n",
      "    min_1: \"i64[109, 109]\" = torch.ops.aten.min.other(add_55, full_like_3);  add_55 = full_like_3 = None\n",
      "    where_3: \"i64[109, 109]\" = torch.ops.aten.where.self(lt, abs_1, min_1);  lt = abs_1 = min_1 = None\n",
      "    add_: \"i64[109, 109]\" = torch.ops.aten.add_.Tensor(add_54, where_3);  add_54 = where_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_5: \"f32[109, 109, 6]\" = torch.ops.aten.embedding.default(arg512_1, add_);  arg512_1 = add_ = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_12: \"f32[6, 109, 109]\" = torch.ops.aten.permute.default(embedding_5, [2, 0, 1]);  embedding_5 = None\n",
      "    unsqueeze_23: \"f32[1, 6, 109, 109]\" = torch.ops.aten.unsqueeze.default(permute_12, 0);  permute_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_33: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_23);  unsqueeze_23 = None\n",
      "    slice_34: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_33, 1);  slice_33 = None\n",
      "    slice_35: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_34, 2, -109);  slice_34 = None\n",
      "    slice_36: \"f32[1, 6, 109, 109]\" = torch.ops.aten.slice.Tensor(slice_35, 3);  slice_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_37: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_11);  mul_11 = None\n",
      "    slice_38: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_37, 1);  slice_37 = None\n",
      "    slice_39: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_38, 2);  slice_38 = None\n",
      "    slice_40: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_39, 3, None, 109);  slice_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add.Tensor(slice_36, slice_40);  slice_36 = slice_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_1, add_56);  matmul_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__1, torch.float32);  add__1 = None\n",
      "    softmax_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_9, -1)\n",
      "    type_as: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_11, to_9);  softmax_11 = to_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_52: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as, 0.1, False);  type_as = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_2: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_52, transpose_107);  dropout_52 = transpose_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_109: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_2, 1, 2);  matmul_2 = None\n",
      "    contiguous_22: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_109);  transpose_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_69: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_22, [1, -1, 384]);  contiguous_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_107: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_69, arg511_1);  view_69 = arg511_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_53: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_107, 0.1, False);  linear_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_57: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_4, dropout_53);  to_4 = dropout_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_10: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_57, torch.float32);  add_57 = None\n",
      "    pow_2: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_10, 2)\n",
      "    mean_12: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None\n",
      "    add_58: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_12, 1e-06);  mean_12 = None\n",
      "    rsqrt_1: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_58);  add_58 = None\n",
      "    mul_16: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_10, rsqrt_1);  rsqrt_1 = None\n",
      "    mul_17: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg517_1, mul_16);  arg517_1 = mul_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_108: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg514_1);  arg514_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_18: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_108, 0.5)\n",
      "    pow_3: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_108, 3.0)\n",
      "    mul_19: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_3, 0.044715);  pow_3 = None\n",
      "    add_59: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_108, mul_19);  linear_108 = mul_19 = None\n",
      "    mul_20: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_59, 0.7978845608028654);  add_59 = None\n",
      "    tanh: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_20);  mul_20 = None\n",
      "    add_60: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh, 1.0);  tanh = None\n",
      "    mul_21: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_18, add_60);  mul_18 = add_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_109: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_17, arg515_1);  mul_17 = arg515_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_22: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_21, linear_109);  mul_21 = linear_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_54: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_22, 0.1, False);  mul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_110: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_54, arg516_1);  dropout_54 = arg516_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_55: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_110, 0.1, False);  linear_110 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_61: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_10, dropout_55);  to_10 = dropout_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_11: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_61, torch.float32);  add_61 = None\n",
      "    pow_4: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_11, 2)\n",
      "    mean_13: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None\n",
      "    add_62: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_13, 1e-06);  mean_13 = None\n",
      "    rsqrt_2: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_62);  add_62 = None\n",
      "    mul_23: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_11, rsqrt_2);  rsqrt_2 = None\n",
      "    mul_24: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg522_1, mul_23);  arg522_1 = mul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_111: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg518_1);  arg518_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_70: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_111, [1, -1, 6, 64]);  linear_111 = None\n",
      "    transpose_110: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_70, 1, 2);  view_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_112: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg519_1);  arg519_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_113: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_24, arg520_1);  mul_24 = arg520_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_71: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_112, [1, -1, 6, 64]);  linear_112 = None\n",
      "    transpose_111: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_72: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_113, [1, -1, 6, 64]);  linear_113 = None\n",
      "    transpose_112: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_113: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_111, 3, 2);  transpose_111 = None\n",
      "    matmul_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_110, transpose_113);  transpose_110 = transpose_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_3, add_56);  matmul_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__2, torch.float32);  add__2 = None\n",
      "    softmax_12: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_12, -1)\n",
      "    type_as_1: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_12, to_12);  softmax_12 = to_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_56: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_1, 0.1, False);  type_as_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_56, transpose_112);  dropout_56 = transpose_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_114: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_4, 1, 2);  matmul_4 = None\n",
      "    contiguous_23: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_114);  transpose_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_73: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_23, [1, -1, 384]);  contiguous_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_114: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_73, arg521_1);  view_73 = arg521_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_57: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_114, 0.1, False);  linear_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_63: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_11, dropout_57);  to_11 = dropout_57 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_13: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_63, torch.float32);  add_63 = None\n",
      "    pow_5: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_13, 2)\n",
      "    mean_14: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_5, [-1], True);  pow_5 = None\n",
      "    add_64: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_14, 1e-06);  mean_14 = None\n",
      "    rsqrt_3: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_64);  add_64 = None\n",
      "    mul_25: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_13, rsqrt_3);  rsqrt_3 = None\n",
      "    mul_26: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg526_1, mul_25);  arg526_1 = mul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_115: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg523_1);  arg523_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_27: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_115, 0.5)\n",
      "    pow_6: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_115, 3.0)\n",
      "    mul_28: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_6, 0.044715);  pow_6 = None\n",
      "    add_65: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_115, mul_28);  linear_115 = mul_28 = None\n",
      "    mul_29: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_65, 0.7978845608028654);  add_65 = None\n",
      "    tanh_1: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_29);  mul_29 = None\n",
      "    add_66: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_1, 1.0);  tanh_1 = None\n",
      "    mul_30: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_27, add_66);  mul_27 = add_66 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_116: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_26, arg524_1);  mul_26 = arg524_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_31: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_30, linear_116);  mul_30 = linear_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_58: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_31, 0.1, False);  mul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_117: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_58, arg525_1);  dropout_58 = arg525_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_59: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_117, 0.1, False);  linear_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_67: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_13, dropout_59);  to_13 = dropout_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_14: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_67, torch.float32);  add_67 = None\n",
      "    pow_7: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_14, 2)\n",
      "    mean_15: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_7, [-1], True);  pow_7 = None\n",
      "    add_68: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_15, 1e-06);  mean_15 = None\n",
      "    rsqrt_4: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_68);  add_68 = None\n",
      "    mul_32: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_14, rsqrt_4);  rsqrt_4 = None\n",
      "    mul_33: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg531_1, mul_32);  arg531_1 = mul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_118: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg527_1);  arg527_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_74: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_118, [1, -1, 6, 64]);  linear_118 = None\n",
      "    transpose_115: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_119: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg528_1);  arg528_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_120: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_33, arg529_1);  mul_33 = arg529_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_75: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_119, [1, -1, 6, 64]);  linear_119 = None\n",
      "    transpose_116: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_75, 1, 2);  view_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_76: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_120, [1, -1, 6, 64]);  linear_120 = None\n",
      "    transpose_117: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_76, 1, 2);  view_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_118: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_116, 3, 2);  transpose_116 = None\n",
      "    matmul_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_115, transpose_118);  transpose_115 = transpose_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_5, add_56);  matmul_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__3, torch.float32);  add__3 = None\n",
      "    softmax_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_15, -1)\n",
      "    type_as_2: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_13, to_15);  softmax_13 = to_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_60: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_2, 0.1, False);  type_as_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_6: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_60, transpose_117);  dropout_60 = transpose_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_119: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_6, 1, 2);  matmul_6 = None\n",
      "    contiguous_24: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_119);  transpose_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_77: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_24, [1, -1, 384]);  contiguous_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_121: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_77, arg530_1);  view_77 = arg530_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_61: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_121, 0.1, False);  linear_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_69: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_14, dropout_61);  to_14 = dropout_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_16: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_69, torch.float32);  add_69 = None\n",
      "    pow_8: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_16, 2)\n",
      "    mean_16: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_8, [-1], True);  pow_8 = None\n",
      "    add_70: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_16, 1e-06);  mean_16 = None\n",
      "    rsqrt_5: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_70);  add_70 = None\n",
      "    mul_34: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_16, rsqrt_5);  rsqrt_5 = None\n",
      "    mul_35: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg535_1, mul_34);  arg535_1 = mul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_122: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg532_1);  arg532_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_36: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_122, 0.5)\n",
      "    pow_9: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_122, 3.0)\n",
      "    mul_37: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_9, 0.044715);  pow_9 = None\n",
      "    add_71: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_122, mul_37);  linear_122 = mul_37 = None\n",
      "    mul_38: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_71, 0.7978845608028654);  add_71 = None\n",
      "    tanh_2: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_38);  mul_38 = None\n",
      "    add_72: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_2, 1.0);  tanh_2 = None\n",
      "    mul_39: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_36, add_72);  mul_36 = add_72 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_123: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_35, arg533_1);  mul_35 = arg533_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_40: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_39, linear_123);  mul_39 = linear_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_62: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_40, 0.1, False);  mul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_124: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_62, arg534_1);  dropout_62 = arg534_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_63: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_124, 0.1, False);  linear_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_73: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_16, dropout_63);  to_16 = dropout_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_17: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_73, torch.float32);  add_73 = None\n",
      "    pow_10: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_17, 2)\n",
      "    mean_17: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_10, [-1], True);  pow_10 = None\n",
      "    add_74: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_17, 1e-06);  mean_17 = None\n",
      "    rsqrt_6: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_74);  add_74 = None\n",
      "    mul_41: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_17, rsqrt_6);  rsqrt_6 = None\n",
      "    mul_42: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg540_1, mul_41);  arg540_1 = mul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_125: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg536_1);  arg536_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_78: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_125, [1, -1, 6, 64]);  linear_125 = None\n",
      "    transpose_120: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_126: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg537_1);  arg537_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_127: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_42, arg538_1);  mul_42 = arg538_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_79: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_126, [1, -1, 6, 64]);  linear_126 = None\n",
      "    transpose_121: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_80: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_127, [1, -1, 6, 64]);  linear_127 = None\n",
      "    transpose_122: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_123: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_121, 3, 2);  transpose_121 = None\n",
      "    matmul_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_120, transpose_123);  transpose_120 = transpose_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_7, add_56);  matmul_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__4, torch.float32);  add__4 = None\n",
      "    softmax_14: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_18, -1)\n",
      "    type_as_3: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_14, to_18);  softmax_14 = to_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_64: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_3, 0.1, False);  type_as_3 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_64, transpose_122);  dropout_64 = transpose_122 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_124: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_8, 1, 2);  matmul_8 = None\n",
      "    contiguous_25: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_124);  transpose_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_81: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_25, [1, -1, 384]);  contiguous_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_128: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_81, arg539_1);  view_81 = arg539_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_65: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_128, 0.1, False);  linear_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_75: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_17, dropout_65);  to_17 = dropout_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_19: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_75, torch.float32);  add_75 = None\n",
      "    pow_11: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_19, 2)\n",
      "    mean_18: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_11, [-1], True);  pow_11 = None\n",
      "    add_76: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_18, 1e-06);  mean_18 = None\n",
      "    rsqrt_7: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_76);  add_76 = None\n",
      "    mul_43: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_19, rsqrt_7);  rsqrt_7 = None\n",
      "    mul_44: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg544_1, mul_43);  arg544_1 = mul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_129: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg541_1);  arg541_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_45: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_129, 0.5)\n",
      "    pow_12: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_129, 3.0)\n",
      "    mul_46: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_12, 0.044715);  pow_12 = None\n",
      "    add_77: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_129, mul_46);  linear_129 = mul_46 = None\n",
      "    mul_47: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_77, 0.7978845608028654);  add_77 = None\n",
      "    tanh_3: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_47);  mul_47 = None\n",
      "    add_78: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_3, 1.0);  tanh_3 = None\n",
      "    mul_48: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_45, add_78);  mul_45 = add_78 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_130: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_44, arg542_1);  mul_44 = arg542_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_49: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_48, linear_130);  mul_48 = linear_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_66: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_49, 0.1, False);  mul_49 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_131: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_66, arg543_1);  dropout_66 = arg543_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_67: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_131, 0.1, False);  linear_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_79: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_19, dropout_67);  to_19 = dropout_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_20: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_79, torch.float32);  add_79 = None\n",
      "    pow_13: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_20, 2)\n",
      "    mean_19: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_13, [-1], True);  pow_13 = None\n",
      "    add_80: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_19, 1e-06);  mean_19 = None\n",
      "    rsqrt_8: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_80);  add_80 = None\n",
      "    mul_50: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_20, rsqrt_8);  rsqrt_8 = None\n",
      "    mul_51: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg549_1, mul_50);  arg549_1 = mul_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_132: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg545_1);  arg545_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_82: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_132, [1, -1, 6, 64]);  linear_132 = None\n",
      "    transpose_125: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_82, 1, 2);  view_82 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_133: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg546_1);  arg546_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_134: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_51, arg547_1);  mul_51 = arg547_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_83: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_133, [1, -1, 6, 64]);  linear_133 = None\n",
      "    transpose_126: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_84: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_134, [1, -1, 6, 64]);  linear_134 = None\n",
      "    transpose_127: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_84, 1, 2);  view_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_128: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_126, 3, 2);  transpose_126 = None\n",
      "    matmul_9: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_125, transpose_128);  transpose_125 = transpose_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_9, add_56);  matmul_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_21: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__5, torch.float32);  add__5 = None\n",
      "    softmax_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_21, -1)\n",
      "    type_as_4: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_15, to_21);  softmax_15 = to_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_68: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_4, 0.1, False);  type_as_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_10: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_68, transpose_127);  dropout_68 = transpose_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_10, 1, 2);  matmul_10 = None\n",
      "    contiguous_26: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_129);  transpose_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_85: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_26, [1, -1, 384]);  contiguous_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_135: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_85, arg548_1);  view_85 = arg548_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_69: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_135, 0.1, False);  linear_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_81: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_20, dropout_69);  to_20 = dropout_69 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_22: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_81, torch.float32);  add_81 = None\n",
      "    pow_14: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_22, 2)\n",
      "    mean_20: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_14, [-1], True);  pow_14 = None\n",
      "    add_82: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_20, 1e-06);  mean_20 = None\n",
      "    rsqrt_9: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_82);  add_82 = None\n",
      "    mul_52: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_22, rsqrt_9);  rsqrt_9 = None\n",
      "    mul_53: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg553_1, mul_52);  arg553_1 = mul_52 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_136: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg550_1);  arg550_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_54: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_136, 0.5)\n",
      "    pow_15: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_136, 3.0)\n",
      "    mul_55: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_15, 0.044715);  pow_15 = None\n",
      "    add_83: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_136, mul_55);  linear_136 = mul_55 = None\n",
      "    mul_56: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_83, 0.7978845608028654);  add_83 = None\n",
      "    tanh_4: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_56);  mul_56 = None\n",
      "    add_84: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_4, 1.0);  tanh_4 = None\n",
      "    mul_57: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_54, add_84);  mul_54 = add_84 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_137: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_53, arg551_1);  mul_53 = arg551_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_58: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_57, linear_137);  mul_57 = linear_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_70: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_58, 0.1, False);  mul_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_138: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_70, arg552_1);  dropout_70 = arg552_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_71: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_138, 0.1, False);  linear_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_85: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_22, dropout_71);  to_22 = dropout_71 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_23: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_85, torch.float32);  add_85 = None\n",
      "    pow_16: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_23, 2)\n",
      "    mean_21: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_16, [-1], True);  pow_16 = None\n",
      "    add_86: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_21, 1e-06);  mean_21 = None\n",
      "    rsqrt_10: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_86);  add_86 = None\n",
      "    mul_59: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_23, rsqrt_10);  rsqrt_10 = None\n",
      "    mul_60: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg558_1, mul_59);  arg558_1 = mul_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_139: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg554_1);  arg554_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_86: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_139, [1, -1, 6, 64]);  linear_139 = None\n",
      "    transpose_130: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_86, 1, 2);  view_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_140: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg555_1);  arg555_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_141: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_60, arg556_1);  mul_60 = arg556_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_87: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_140, [1, -1, 6, 64]);  linear_140 = None\n",
      "    transpose_131: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_87, 1, 2);  view_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_88: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_141, [1, -1, 6, 64]);  linear_141 = None\n",
      "    transpose_132: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_88, 1, 2);  view_88 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_133: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_131, 3, 2);  transpose_131 = None\n",
      "    matmul_11: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_130, transpose_133);  transpose_130 = transpose_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_11, add_56);  matmul_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_24: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__6, torch.float32);  add__6 = None\n",
      "    softmax_16: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_24, -1)\n",
      "    type_as_5: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_16, to_24);  softmax_16 = to_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_72: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_5, 0.1, False);  type_as_5 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_72, transpose_132);  dropout_72 = transpose_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_134: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_12, 1, 2);  matmul_12 = None\n",
      "    contiguous_27: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_134);  transpose_134 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_89: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_27, [1, -1, 384]);  contiguous_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_142: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_89, arg557_1);  view_89 = arg557_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_73: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_142, 0.1, False);  linear_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_87: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_23, dropout_73);  to_23 = dropout_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_25: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_87, torch.float32);  add_87 = None\n",
      "    pow_17: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_25, 2)\n",
      "    mean_22: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_17, [-1], True);  pow_17 = None\n",
      "    add_88: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_22, 1e-06);  mean_22 = None\n",
      "    rsqrt_11: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_88);  add_88 = None\n",
      "    mul_61: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_25, rsqrt_11);  rsqrt_11 = None\n",
      "    mul_62: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg562_1, mul_61);  arg562_1 = mul_61 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_143: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg559_1);  arg559_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_63: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_143, 0.5)\n",
      "    pow_18: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_143, 3.0)\n",
      "    mul_64: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_18, 0.044715);  pow_18 = None\n",
      "    add_89: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_143, mul_64);  linear_143 = mul_64 = None\n",
      "    mul_65: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_89, 0.7978845608028654);  add_89 = None\n",
      "    tanh_5: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_65);  mul_65 = None\n",
      "    add_90: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_5, 1.0);  tanh_5 = None\n",
      "    mul_66: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_63, add_90);  mul_63 = add_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_144: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_62, arg560_1);  mul_62 = arg560_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_67: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_66, linear_144);  mul_66 = linear_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_74: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_67, 0.1, False);  mul_67 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_145: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_74, arg561_1);  dropout_74 = arg561_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_75: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_145, 0.1, False);  linear_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_91: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_25, dropout_75);  to_25 = dropout_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_26: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_91, torch.float32);  add_91 = None\n",
      "    pow_19: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_26, 2)\n",
      "    mean_23: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_19, [-1], True);  pow_19 = None\n",
      "    add_92: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_23, 1e-06);  mean_23 = None\n",
      "    rsqrt_12: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_92);  add_92 = None\n",
      "    mul_68: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_26, rsqrt_12);  rsqrt_12 = None\n",
      "    mul_69: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg567_1, mul_68);  arg567_1 = mul_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_146: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg563_1);  arg563_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_90: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_146, [1, -1, 6, 64]);  linear_146 = None\n",
      "    transpose_135: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_90, 1, 2);  view_90 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_147: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg564_1);  arg564_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_148: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_69, arg565_1);  mul_69 = arg565_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_91: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_147, [1, -1, 6, 64]);  linear_147 = None\n",
      "    transpose_136: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_91, 1, 2);  view_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_92: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_148, [1, -1, 6, 64]);  linear_148 = None\n",
      "    transpose_137: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_92, 1, 2);  view_92 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_138: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_136, 3, 2);  transpose_136 = None\n",
      "    matmul_13: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_135, transpose_138);  transpose_135 = transpose_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_13, add_56);  matmul_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_27: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__7, torch.float32);  add__7 = None\n",
      "    softmax_17: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_27, -1)\n",
      "    type_as_6: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_17, to_27);  softmax_17 = to_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_76: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_6, 0.1, False);  type_as_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_14: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_76, transpose_137);  dropout_76 = transpose_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_139: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_14, 1, 2);  matmul_14 = None\n",
      "    contiguous_28: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_139);  transpose_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_93: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_28, [1, -1, 384]);  contiguous_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_149: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_93, arg566_1);  view_93 = arg566_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_77: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_149, 0.1, False);  linear_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_93: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_26, dropout_77);  to_26 = dropout_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_28: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_93, torch.float32);  add_93 = None\n",
      "    pow_20: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_28, 2)\n",
      "    mean_24: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_20, [-1], True);  pow_20 = None\n",
      "    add_94: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_24, 1e-06);  mean_24 = None\n",
      "    rsqrt_13: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_94);  add_94 = None\n",
      "    mul_70: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_28, rsqrt_13);  rsqrt_13 = None\n",
      "    mul_71: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg571_1, mul_70);  arg571_1 = mul_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_150: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg568_1);  arg568_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_72: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_150, 0.5)\n",
      "    pow_21: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_150, 3.0)\n",
      "    mul_73: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_21, 0.044715);  pow_21 = None\n",
      "    add_95: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_150, mul_73);  linear_150 = mul_73 = None\n",
      "    mul_74: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_95, 0.7978845608028654);  add_95 = None\n",
      "    tanh_6: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_74);  mul_74 = None\n",
      "    add_96: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_6, 1.0);  tanh_6 = None\n",
      "    mul_75: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_72, add_96);  mul_72 = add_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_151: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_71, arg569_1);  mul_71 = arg569_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_76: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_75, linear_151);  mul_75 = linear_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_78: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_76, 0.1, False);  mul_76 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_152: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_78, arg570_1);  dropout_78 = arg570_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_79: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_152, 0.1, False);  linear_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_97: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_28, dropout_79);  to_28 = dropout_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_29: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_97, torch.float32);  add_97 = None\n",
      "    pow_22: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_29, 2)\n",
      "    mean_25: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_22, [-1], True);  pow_22 = None\n",
      "    add_98: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_25, 1e-06);  mean_25 = None\n",
      "    rsqrt_14: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_98);  add_98 = None\n",
      "    mul_77: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_29, rsqrt_14);  rsqrt_14 = None\n",
      "    mul_78: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg576_1, mul_77);  arg576_1 = mul_77 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_153: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg572_1);  arg572_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_94: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_153, [1, -1, 6, 64]);  linear_153 = None\n",
      "    transpose_140: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_94, 1, 2);  view_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_154: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg573_1);  arg573_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_155: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(mul_78, arg574_1);  mul_78 = arg574_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_95: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_154, [1, -1, 6, 64]);  linear_154 = None\n",
      "    transpose_141: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_95, 1, 2);  view_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_96: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_155, [1, -1, 6, 64]);  linear_155 = None\n",
      "    transpose_142: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_96, 1, 2);  view_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_143: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(transpose_141, 3, 2);  transpose_141 = None\n",
      "    matmul_15: \"f32[1, 6, 109, 109]\" = torch.ops.aten.matmul.default(transpose_140, transpose_143);  transpose_140 = transpose_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__8: \"f32[1, 6, 109, 109]\" = torch.ops.aten.add_.Tensor(matmul_15, add_56);  matmul_15 = add_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_30: \"f32[1, 6, 109, 109]\" = torch.ops.aten.to.dtype(add__8, torch.float32);  add__8 = None\n",
      "    softmax_18: \"f32[1, 6, 109, 109]\" = torch.ops.aten.softmax.int(to_30, -1)\n",
      "    type_as_7: \"f32[1, 6, 109, 109]\" = torch.ops.aten.type_as.default(softmax_18, to_30);  softmax_18 = to_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_80: \"f32[1, 6, 109, 109]\" = torch.ops.aten.dropout.default(type_as_7, 0.1, False);  type_as_7 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.matmul.default(dropout_80, transpose_142);  dropout_80 = transpose_142 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.transpose.int(matmul_16, 1, 2);  matmul_16 = None\n",
      "    contiguous_29: \"f32[1, 109, 6, 64]\" = torch.ops.aten.contiguous.default(transpose_144);  transpose_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_97: \"f32[1, 109, 384]\" = torch.ops.aten.view.default(contiguous_29, [1, -1, 384]);  contiguous_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_156: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(view_97, arg575_1);  view_97 = arg575_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_81: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_156, 0.1, False);  linear_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_99: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_29, dropout_81);  to_29 = dropout_81 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_31: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_99, torch.float32);  add_99 = None\n",
      "    pow_23: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_31, 2)\n",
      "    mean_26: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_23, [-1], True);  pow_23 = None\n",
      "    add_100: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_26, 1e-06);  mean_26 = None\n",
      "    rsqrt_15: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_100);  add_100 = None\n",
      "    mul_79: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_31, rsqrt_15);  rsqrt_15 = None\n",
      "    mul_80: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg580_1, mul_79);  arg580_1 = mul_79 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_157: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg577_1);  arg577_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_81: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(linear_157, 0.5)\n",
      "    pow_24: \"f32[1, 109, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_157, 3.0)\n",
      "    mul_82: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(pow_24, 0.044715);  pow_24 = None\n",
      "    add_101: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(linear_157, mul_82);  linear_157 = mul_82 = None\n",
      "    mul_83: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(add_101, 0.7978845608028654);  add_101 = None\n",
      "    tanh_7: \"f32[1, 109, 1024]\" = torch.ops.aten.tanh.default(mul_83);  mul_83 = None\n",
      "    add_102: \"f32[1, 109, 1024]\" = torch.ops.aten.add.Tensor(tanh_7, 1.0);  tanh_7 = None\n",
      "    mul_84: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_81, add_102);  mul_81 = add_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_158: \"f32[1, 109, 1024]\" = torch.ops.aten.linear.default(mul_80, arg578_1);  mul_80 = arg578_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_85: \"f32[1, 109, 1024]\" = torch.ops.aten.mul.Tensor(mul_84, linear_158);  mul_84 = linear_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_82: \"f32[1, 109, 1024]\" = torch.ops.aten.dropout.default(mul_85, 0.1, False);  mul_85 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_159: \"f32[1, 109, 512]\" = torch.ops.aten.linear.default(dropout_82, arg579_1);  dropout_82 = arg579_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_83: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(linear_159, 0.1, False);  linear_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_103: \"f32[1, 109, 512]\" = torch.ops.aten.add.Tensor(to_31, dropout_83);  to_31 = dropout_83 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_32: \"f32[1, 109, 512]\" = torch.ops.aten.to.dtype(add_103, torch.float32);  add_103 = None\n",
      "    pow_25: \"f32[1, 109, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_32, 2)\n",
      "    mean_27: \"f32[1, 109, 1]\" = torch.ops.aten.mean.dim(pow_25, [-1], True);  pow_25 = None\n",
      "    add_104: \"f32[1, 109, 1]\" = torch.ops.aten.add.Tensor(mean_27, 1e-06);  mean_27 = None\n",
      "    rsqrt_16: \"f32[1, 109, 1]\" = torch.ops.aten.rsqrt.default(add_104);  add_104 = None\n",
      "    mul_86: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(to_32, rsqrt_16);  to_32 = rsqrt_16 = None\n",
      "    mul_87: \"f32[1, 109, 512]\" = torch.ops.aten.mul.Tensor(arg581_1, mul_86);  arg581_1 = mul_86 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_84: \"f32[1, 109, 512]\" = torch.ops.aten.dropout.default(mul_87, 0.1, False);  mul_87 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    _set_grad_enabled_2 = torch._C._set_grad_enabled(False);  _set_grad_enabled_2 = None\n",
      "    ones_1: \"i64[1, 109]\" = torch.ops.aten.ones.default([1, 109], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_88: \"i64[1, 109]\" = torch.ops.aten.mul.Tensor(ones_1, -100);  ones_1 = mul_88 = None\n",
      "    _tensor_constant0 = self._tensor_constant0\n",
      "    lift_fresh_copy: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None\n",
      "    detach_: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n",
      "    _tensor_constant1 = self._tensor_constant1\n",
      "    lift_fresh_copy_1: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None\n",
      "    detach__1: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_1);  lift_fresh_copy_1 = None\n",
      "    _tensor_constant2 = self._tensor_constant2\n",
      "    lift_fresh_copy_2: \"i64[]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None\n",
      "    detach__2: \"i64[]\" = torch.ops.aten.detach_.default(lift_fresh_copy_2);  lift_fresh_copy_2 = None\n",
      "    unsqueeze_24: \"i64[1]\" = torch.ops.aten.unsqueeze.default(detach_, 0);  detach_ = None\n",
      "    isin: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(unsqueeze_24, detach__1)\n",
      "    any_1: \"b8[]\" = torch.ops.aten.any.default(isin);  isin = None\n",
      "    ne: \"b8[]\" = torch.ops.aten.ne.Scalar(any_1, 0);  any_1 = ne = None\n",
      "    lt_1: \"b8[1]\" = torch.ops.aten.lt.Scalar(unsqueeze_24, 0)\n",
      "    any_2: \"b8[]\" = torch.ops.aten.any.default(lt_1);  lt_1 = None\n",
      "    ne_1: \"b8[]\" = torch.ops.aten.ne.Scalar(any_2, 0);  any_2 = ne_1 = None\n",
      "    ones_2: \"i64[1, 1]\" = torch.ops.aten.ones.default([1, 1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    mul_89: \"i64[1, 1]\" = torch.ops.aten.mul.Tensor(ones_2, detach__2);  ones_2 = detach__2 = None\n",
      "    ones_3: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    ones_4: \"i64[1]\" = torch.ops.aten.ones.default([1], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    cumsum: \"i64[1]\" = torch.ops.aten.cumsum.default(ones_4, 0);  ones_4 = None\n",
      "    sub_1: \"i64[1]\" = torch.ops.aten.sub.Tensor(cumsum, 1);  cumsum = None\n",
      "    slice_41: \"i64[1]\" = torch.ops.aten.slice.Tensor(sub_1, 0, 0);  sub_1 = None\n",
      "    slice_42: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(mul_89, 0, 0, 9223372036854775807)\n",
      "    index: \"i64[1, 1]\" = torch.ops.aten.index.Tensor(slice_42, [None, slice_41]);  slice_42 = None\n",
      "    clone_6: \"i64[1, 1]\" = torch.ops.aten.clone.default(index, memory_format = torch.contiguous_format);  index = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:985 in forward, code: input_ids = input_ids.view(-1, input_shape[-1])\n",
      "    view_98: \"i64[1, 1]\" = torch.ops.aten.view.default(clone_6, [-1, 1]);  clone_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_6: \"f32[1, 1, 512]\" = torch.ops.aten.embedding.default(arg582_1, view_98);  arg582_1 = view_98 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1035 in forward, code: causal_mask = self._update_causal_mask(\n",
      "    full: \"f32[1, 2]\" = torch.ops.aten.full.default([1, 2], -3.4028234663852886e+38, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    arange_2: \"i64[2]\" = torch.ops.aten.arange.default(2, device = device(type='cpu'), pin_memory = False)\n",
      "    reshape_24: \"i64[1, 1]\" = torch.ops.aten.reshape.default(slice_41, [-1, 1])\n",
      "    gt_4: \"b8[1, 2]\" = torch.ops.aten.gt.Tensor(arange_2, reshape_24);  arange_2 = reshape_24 = None\n",
      "    mul_: \"f32[1, 2]\" = torch.ops.aten.mul_.Tensor(full, gt_4);  full = gt_4 = None\n",
      "    unsqueeze_25: \"f32[1, 1, 2]\" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None\n",
      "    unsqueeze_26: \"f32[1, 1, 1, 2]\" = torch.ops.aten.unsqueeze.default(unsqueeze_25, 1);  unsqueeze_25 = None\n",
      "    slice_43: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(unsqueeze_26, 2, 0, 9223372036854775807);  unsqueeze_26 = None\n",
      "    slice_44: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_43, 3, 0, 9223372036854775807);  slice_43 = None\n",
      "    expand_4: \"f32[1, 1, 1, 2]\" = torch.ops.aten.expand.default(slice_44, [1, 1, -1, -1]);  slice_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:1060 in forward, code: encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
      "    slice_45: \"i64[1, 109]\" = torch.ops.aten.slice.Tensor(concat_8, 0, 0, 9223372036854775807);  concat_8 = None\n",
      "    unsqueeze_27: \"i64[1, 1, 109]\" = torch.ops.aten.unsqueeze.default(slice_45, 1);  slice_45 = None\n",
      "    unsqueeze_28: \"i64[1, 1, 1, 109]\" = torch.ops.aten.unsqueeze.default(unsqueeze_27, 2);  unsqueeze_27 = None\n",
      "    slice_46: \"i64[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(unsqueeze_28, 3, 0, 9223372036854775807);  unsqueeze_28 = None\n",
      "    to_33: \"f32[1, 1, 1, 109]\" = torch.ops.aten.to.dtype(slice_46, torch.float32);  slice_46 = None\n",
      "    rsub_1: \"f32[1, 1, 1, 109]\" = torch.ops.aten.rsub.Scalar(to_33, 1.0);  to_33 = None\n",
      "    mul_90: \"f32[1, 1, 1, 109]\" = torch.ops.aten.mul.Tensor(rsub_1, -3.4028234663852886e+38);  rsub_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_85: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(embedding_6, 0.1, False);  embedding_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_34: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(dropout_85, torch.float32);  dropout_85 = None\n",
      "    pow_26: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_34, 2)\n",
      "    mean_28: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_26, [-1], True);  pow_26 = None\n",
      "    add_105: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_28, 1e-06);  mean_28 = None\n",
      "    rsqrt_17: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_105);  add_105 = None\n",
      "    mul_91: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_34, rsqrt_17);  rsqrt_17 = None\n",
      "    mul_92: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg588_1, mul_91);  arg588_1 = mul_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_160: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg583_1);  arg583_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_99: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_160, [1, -1, 6, 64]);  linear_160 = None\n",
      "    transpose_145: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_99, 1, 2);  view_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_161: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg584_1);  arg584_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_162: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_92, arg585_1);  mul_92 = arg585_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_100: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_161, [1, -1, 6, 64]);  linear_161 = None\n",
      "    transpose_146: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_100, 1, 2);  view_100 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_101: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_162, [1, -1, 6, 64]);  linear_162 = None\n",
      "    transpose_147: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_101, 1, 2);  view_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant3 = self._tensor_constant3\n",
      "    lift_fresh_copy_3: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None\n",
      "    detach__3: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_3);  lift_fresh_copy_3 = None\n",
      "    _tensor_constant4 = self._tensor_constant4\n",
      "    lift_fresh_copy_4: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None\n",
      "    detach__4: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_4);  lift_fresh_copy_4 = None\n",
      "    cat_1: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__3, transpose_146], -2);  detach__3 = transpose_146 = None\n",
      "    cat_2: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__4, transpose_147], -2);  detach__4 = transpose_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_148: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_1, 3, 2);  cat_1 = None\n",
      "    matmul_17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_145, transpose_148);  transpose_145 = transpose_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:533 in forward, code: real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
      "    select_30: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_106: \"i64[]\" = torch.ops.aten.add.Tensor(select_30, 1);  select_30 = add_106 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    slice_47: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, 0, 9223372036854775807)\n",
      "    unsqueeze_29: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_47, 1);  slice_47 = None\n",
      "    to_35: \"i64[1, 1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_29, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_29 = None\n",
      "    arange_3: \"i64[1]\" = torch.ops.aten.arange.default(1, dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
      "    unsqueeze_30: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(arange_3, 0);  arange_3 = None\n",
      "    slice_48: \"i64[1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_30, 1, 0, 9223372036854775807);  unsqueeze_30 = None\n",
      "    sub_2: \"i64[1, 1]\" = torch.ops.aten.sub.Tensor(slice_48, to_35);  slice_48 = to_35 = None\n",
      "    zeros_like_3: \"i64[1, 1]\" = torch.ops.aten.zeros_like.default(sub_2, pin_memory = False)\n",
      "    min_2: \"i64[1, 1]\" = torch.ops.aten.min.other(sub_2, zeros_like_3);  sub_2 = zeros_like_3 = None\n",
      "    neg: \"i64[1, 1]\" = torch.ops.aten.neg.default(min_2);  min_2 = None\n",
      "    lt_2: \"b8[1, 1]\" = torch.ops.aten.lt.Scalar(neg, 16)\n",
      "    to_36: \"f32[1, 1]\" = torch.ops.aten.to.dtype(neg, torch.float32)\n",
      "    div_2: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(to_36, 16);  to_36 = None\n",
      "    log_1: \"f32[1, 1]\" = torch.ops.aten.log.default(div_2);  div_2 = None\n",
      "    div_3: \"f32[1, 1]\" = torch.ops.aten.div.Tensor(log_1, 2.0794415416798357);  log_1 = None\n",
      "    mul_93: \"f32[1, 1]\" = torch.ops.aten.mul.Tensor(div_3, 16);  div_3 = None\n",
      "    to_37: \"i64[1, 1]\" = torch.ops.aten.to.dtype(mul_93, torch.int64);  mul_93 = None\n",
      "    add_107: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(to_37, 16);  to_37 = None\n",
      "    full_like_4: \"i64[1, 1]\" = torch.ops.aten.full_like.default(add_107, 31, pin_memory = False)\n",
      "    min_3: \"i64[1, 1]\" = torch.ops.aten.min.other(add_107, full_like_4);  add_107 = full_like_4 = None\n",
      "    where_4: \"i64[1, 1]\" = torch.ops.aten.where.self(lt_2, neg, min_3);  lt_2 = neg = min_3 = None\n",
      "    add_108: \"i64[1, 1]\" = torch.ops.aten.add.Tensor(where_4, 0);  where_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(\n",
      "    embedding_7: \"f32[1, 1, 6]\" = torch.ops.aten.embedding.default(arg587_1, add_108);  arg587_1 = add_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:541 in forward, code: position_bias = self.compute_bias(\n",
      "    permute_13: \"f32[6, 1, 1]\" = torch.ops.aten.permute.default(embedding_7, [2, 0, 1]);  embedding_7 = None\n",
      "    unsqueeze_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.unsqueeze.default(permute_13, 0);  permute_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:544 in forward, code: position_bias = position_bias[:, :, -seq_length:, :]\n",
      "    slice_49: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(unsqueeze_31);  unsqueeze_31 = None\n",
      "    slice_50: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_49, 1);  slice_49 = None\n",
      "    slice_51: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_50, 2, -1);  slice_50 = None\n",
      "    slice_52: \"f32[1, 6, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_51, 3);  slice_51 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_53: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(expand_4);  expand_4 = None\n",
      "    slice_54: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_53, 1);  slice_53 = None\n",
      "    slice_55: \"f32[1, 1, 1, 2]\" = torch.ops.aten.slice.Tensor(slice_54, 2);  slice_54 = None\n",
      "    slice_56: \"f32[1, 1, 1, 1]\" = torch.ops.aten.slice.Tensor(slice_55, 3, None, 1);  slice_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_109: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add.Tensor(slice_52, slice_56);  slice_52 = slice_56 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__9: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_17, add_109);  matmul_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_38: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__9, torch.float32);  add__9 = None\n",
      "    softmax_19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_38, -1)\n",
      "    type_as_8: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_19, to_38);  softmax_19 = to_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_86: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_8, 0.1, False);  type_as_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_86, cat_2);  dropout_86 = cat_2 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_18, 1, 2);  matmul_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_102: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_149, [1, -1, 384]);  transpose_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_163: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_102, arg586_1);  view_102 = arg586_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_87: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_163, 0.1, False);  linear_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_110: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_34, dropout_87);  to_34 = dropout_87 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_31: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_111: \"i64[]\" = torch.ops.aten.add.Tensor(select_31, 1);  select_31 = add_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_39: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_110, torch.float32);  add_110 = None\n",
      "    pow_27: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_39, 2)\n",
      "    mean_29: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_27, [-1], True);  pow_27 = None\n",
      "    add_112: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_29, 1e-06);  mean_29 = None\n",
      "    rsqrt_18: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_112);  add_112 = None\n",
      "    mul_94: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_39, rsqrt_18);  rsqrt_18 = None\n",
      "    mul_95: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg593_1, mul_94);  arg593_1 = mul_94 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_164: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_95, arg589_1);  mul_95 = arg589_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_103: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_164, [1, -1, 6, 64]);  linear_164 = None\n",
      "    transpose_150: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_103, 1, 2);  view_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_165: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg590_1);  arg590_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_166: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg591_1);  arg591_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_104: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_165, [1, -1, 6, 64]);  linear_165 = None\n",
      "    transpose_151: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_104, 1, 2);  view_104 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_105: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_166, [1, -1, 6, 64]);  linear_166 = None\n",
      "    transpose_152: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_105, 1, 2);  view_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant5 = self._tensor_constant5\n",
      "    lift_fresh_copy_5: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None\n",
      "    detach__5: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_5);  lift_fresh_copy_5 = None\n",
      "    _tensor_constant6 = self._tensor_constant6\n",
      "    lift_fresh_copy_6: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None\n",
      "    detach__6: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_6);  lift_fresh_copy_6 = None\n",
      "    cat_3: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__5, transpose_151], -2);  detach__5 = transpose_151 = None\n",
      "    cat_4: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__6, transpose_152], -2);  detach__6 = transpose_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_153: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_3, 3, 2);  cat_3 = None\n",
      "    matmul_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_150, transpose_153);  transpose_150 = transpose_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:535 in forward, code: position_bias = torch.zeros(\n",
      "    zeros_2: \"f32[1, 6, 1, 109]\" = torch.ops.aten.zeros.default([1, 6, 1, 109], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:547 in forward, code: causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
      "    slice_57: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(mul_90);  mul_90 = None\n",
      "    slice_58: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_57, 1);  slice_57 = None\n",
      "    slice_59: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_58, 2);  slice_58 = None\n",
      "    slice_60: \"f32[1, 1, 1, 109]\" = torch.ops.aten.slice.Tensor(slice_59, 3, None, 109);  slice_59 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:548 in forward, code: position_bias = position_bias + causal_mask\n",
      "    add_113: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add.Tensor(zeros_2, slice_60);  zeros_2 = slice_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__10: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_19, add_113);  matmul_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_40: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__10, torch.float32);  add__10 = None\n",
      "    softmax_20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_40, -1)\n",
      "    type_as_9: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_20, to_40);  softmax_20 = to_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_88: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_9, 0.1, False);  type_as_9 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_20: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_88, cat_4);  dropout_88 = cat_4 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_154: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_20, 1, 2);  matmul_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_106: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_154, [1, -1, 384]);  transpose_154 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_167: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_106, arg592_1);  view_106 = arg592_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_89: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_167, 0.1, False);  linear_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_114: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_39, dropout_89);  to_39 = dropout_89 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_41: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_114, torch.float32);  add_114 = None\n",
      "    pow_28: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_41, 2)\n",
      "    mean_30: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_28, [-1], True);  pow_28 = None\n",
      "    add_115: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_30, 1e-06);  mean_30 = None\n",
      "    rsqrt_19: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_115);  add_115 = None\n",
      "    mul_96: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_41, rsqrt_19);  rsqrt_19 = None\n",
      "    mul_97: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg597_1, mul_96);  arg597_1 = mul_96 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_168: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg594_1);  arg594_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_98: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_168, 0.5)\n",
      "    pow_29: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_168, 3.0)\n",
      "    mul_99: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_29, 0.044715);  pow_29 = None\n",
      "    add_116: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_168, mul_99);  linear_168 = mul_99 = None\n",
      "    mul_100: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_116, 0.7978845608028654);  add_116 = None\n",
      "    tanh_8: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_100);  mul_100 = None\n",
      "    add_117: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_8, 1.0);  tanh_8 = None\n",
      "    mul_101: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_98, add_117);  mul_98 = add_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_169: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_97, arg595_1);  mul_97 = arg595_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_102: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_101, linear_169);  mul_101 = linear_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_90: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_102, 0.1, False);  mul_102 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_170: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_90, arg596_1);  dropout_90 = arg596_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_91: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_170, 0.1, False);  linear_170 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_118: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_41, dropout_91);  to_41 = dropout_91 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_42: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_118, torch.float32);  add_118 = None\n",
      "    pow_30: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_42, 2)\n",
      "    mean_31: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_30, [-1], True);  pow_30 = None\n",
      "    add_119: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_31, 1e-06);  mean_31 = None\n",
      "    rsqrt_20: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_119);  add_119 = None\n",
      "    mul_103: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_42, rsqrt_20);  rsqrt_20 = None\n",
      "    mul_104: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg602_1, mul_103);  arg602_1 = mul_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_171: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg598_1);  arg598_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_107: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_171, [1, -1, 6, 64]);  linear_171 = None\n",
      "    transpose_155: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_107, 1, 2);  view_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_172: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg599_1);  arg599_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_173: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_104, arg600_1);  mul_104 = arg600_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_108: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_172, [1, -1, 6, 64]);  linear_172 = None\n",
      "    transpose_156: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_108, 1, 2);  view_108 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_109: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_173, [1, -1, 6, 64]);  linear_173 = None\n",
      "    transpose_157: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_109, 1, 2);  view_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant7 = self._tensor_constant7\n",
      "    lift_fresh_copy_7: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None\n",
      "    detach__7: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_7);  lift_fresh_copy_7 = None\n",
      "    _tensor_constant8 = self._tensor_constant8\n",
      "    lift_fresh_copy_8: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None\n",
      "    detach__8: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_8);  lift_fresh_copy_8 = None\n",
      "    cat_5: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__7, transpose_156], -2);  detach__7 = transpose_156 = None\n",
      "    cat_6: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__8, transpose_157], -2);  detach__8 = transpose_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_158: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_5, 3, 2);  cat_5 = None\n",
      "    matmul_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_155, transpose_158);  transpose_155 = transpose_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__11: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_21, add_109);  matmul_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_43: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__11, torch.float32);  add__11 = None\n",
      "    softmax_21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_43, -1)\n",
      "    type_as_10: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_21, to_43);  softmax_21 = to_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_92: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_10, 0.1, False);  type_as_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_92, cat_6);  dropout_92 = cat_6 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_22, 1, 2);  matmul_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_110: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_159, [1, -1, 384]);  transpose_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_174: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_110, arg601_1);  view_110 = arg601_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_93: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_174, 0.1, False);  linear_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_120: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_42, dropout_93);  to_42 = dropout_93 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_32: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_121: \"i64[]\" = torch.ops.aten.add.Tensor(select_32, 1);  select_32 = add_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_44: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_120, torch.float32);  add_120 = None\n",
      "    pow_31: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_44, 2)\n",
      "    mean_32: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_31, [-1], True);  pow_31 = None\n",
      "    add_122: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_32, 1e-06);  mean_32 = None\n",
      "    rsqrt_21: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_122);  add_122 = None\n",
      "    mul_105: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_44, rsqrt_21);  rsqrt_21 = None\n",
      "    mul_106: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg607_1, mul_105);  arg607_1 = mul_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_175: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_106, arg603_1);  mul_106 = arg603_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_111: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_175, [1, -1, 6, 64]);  linear_175 = None\n",
      "    transpose_160: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_111, 1, 2);  view_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_176: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg604_1);  arg604_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_177: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg605_1);  arg605_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_112: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_176, [1, -1, 6, 64]);  linear_176 = None\n",
      "    transpose_161: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_112, 1, 2);  view_112 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_113: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_177, [1, -1, 6, 64]);  linear_177 = None\n",
      "    transpose_162: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_113, 1, 2);  view_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant9 = self._tensor_constant9\n",
      "    lift_fresh_copy_9: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None\n",
      "    detach__9: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_9);  lift_fresh_copy_9 = None\n",
      "    _tensor_constant10 = self._tensor_constant10\n",
      "    lift_fresh_copy_10: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant10);  _tensor_constant10 = None\n",
      "    detach__10: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_10);  lift_fresh_copy_10 = None\n",
      "    cat_7: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__9, transpose_161], -2);  detach__9 = transpose_161 = None\n",
      "    cat_8: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__10, transpose_162], -2);  detach__10 = transpose_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_163: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_7, 3, 2);  cat_7 = None\n",
      "    matmul_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_160, transpose_163);  transpose_160 = transpose_163 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__12: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_23, add_113);  matmul_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_45: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__12, torch.float32);  add__12 = None\n",
      "    softmax_22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_45, -1)\n",
      "    type_as_11: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_22, to_45);  softmax_22 = to_45 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_94: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_11, 0.1, False);  type_as_11 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_24: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_94, cat_8);  dropout_94 = cat_8 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_164: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_24, 1, 2);  matmul_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_114: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_164, [1, -1, 384]);  transpose_164 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_178: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_114, arg606_1);  view_114 = arg606_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_95: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_178, 0.1, False);  linear_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_123: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_44, dropout_95);  to_44 = dropout_95 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_46: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_123, torch.float32);  add_123 = None\n",
      "    pow_32: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_46, 2)\n",
      "    mean_33: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_32, [-1], True);  pow_32 = None\n",
      "    add_124: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_33, 1e-06);  mean_33 = None\n",
      "    rsqrt_22: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_124);  add_124 = None\n",
      "    mul_107: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_46, rsqrt_22);  rsqrt_22 = None\n",
      "    mul_108: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg611_1, mul_107);  arg611_1 = mul_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_179: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg608_1);  arg608_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_109: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_179, 0.5)\n",
      "    pow_33: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_179, 3.0)\n",
      "    mul_110: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_33, 0.044715);  pow_33 = None\n",
      "    add_125: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_179, mul_110);  linear_179 = mul_110 = None\n",
      "    mul_111: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_125, 0.7978845608028654);  add_125 = None\n",
      "    tanh_9: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_111);  mul_111 = None\n",
      "    add_126: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_9, 1.0);  tanh_9 = None\n",
      "    mul_112: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_109, add_126);  mul_109 = add_126 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_180: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_108, arg609_1);  mul_108 = arg609_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_113: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_112, linear_180);  mul_112 = linear_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_96: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_113, 0.1, False);  mul_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_181: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_96, arg610_1);  dropout_96 = arg610_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_97: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_181, 0.1, False);  linear_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_127: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_46, dropout_97);  to_46 = dropout_97 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_47: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_127, torch.float32);  add_127 = None\n",
      "    pow_34: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_47, 2)\n",
      "    mean_34: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_34, [-1], True);  pow_34 = None\n",
      "    add_128: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_34, 1e-06);  mean_34 = None\n",
      "    rsqrt_23: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_128);  add_128 = None\n",
      "    mul_114: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_47, rsqrt_23);  rsqrt_23 = None\n",
      "    mul_115: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg616_1, mul_114);  arg616_1 = mul_114 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_182: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg612_1);  arg612_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_115: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_182, [1, -1, 6, 64]);  linear_182 = None\n",
      "    transpose_165: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_115, 1, 2);  view_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_183: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg613_1);  arg613_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_184: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_115, arg614_1);  mul_115 = arg614_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_116: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_183, [1, -1, 6, 64]);  linear_183 = None\n",
      "    transpose_166: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_116, 1, 2);  view_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_117: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_184, [1, -1, 6, 64]);  linear_184 = None\n",
      "    transpose_167: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_117, 1, 2);  view_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant11 = self._tensor_constant11\n",
      "    lift_fresh_copy_11: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant11);  _tensor_constant11 = None\n",
      "    detach__11: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_11);  lift_fresh_copy_11 = None\n",
      "    _tensor_constant12 = self._tensor_constant12\n",
      "    lift_fresh_copy_12: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant12);  _tensor_constant12 = None\n",
      "    detach__12: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_12);  lift_fresh_copy_12 = None\n",
      "    cat_9: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__11, transpose_166], -2);  detach__11 = transpose_166 = None\n",
      "    cat_10: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__12, transpose_167], -2);  detach__12 = transpose_167 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_168: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_9, 3, 2);  cat_9 = None\n",
      "    matmul_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_165, transpose_168);  transpose_165 = transpose_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__13: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_25, add_109);  matmul_25 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_48: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__13, torch.float32);  add__13 = None\n",
      "    softmax_23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_48, -1)\n",
      "    type_as_12: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_23, to_48);  softmax_23 = to_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_98: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_12, 0.1, False);  type_as_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_98, cat_10);  dropout_98 = cat_10 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_169: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_26, 1, 2);  matmul_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_118: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_169, [1, -1, 384]);  transpose_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_185: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_118, arg615_1);  view_118 = arg615_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_99: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_185, 0.1, False);  linear_185 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_129: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_47, dropout_99);  to_47 = dropout_99 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_33: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_130: \"i64[]\" = torch.ops.aten.add.Tensor(select_33, 1);  select_33 = add_130 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_49: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_129, torch.float32);  add_129 = None\n",
      "    pow_35: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_49, 2)\n",
      "    mean_35: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_35, [-1], True);  pow_35 = None\n",
      "    add_131: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_35, 1e-06);  mean_35 = None\n",
      "    rsqrt_24: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_131);  add_131 = None\n",
      "    mul_116: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_49, rsqrt_24);  rsqrt_24 = None\n",
      "    mul_117: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg621_1, mul_116);  arg621_1 = mul_116 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_186: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_117, arg617_1);  mul_117 = arg617_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_119: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_186, [1, -1, 6, 64]);  linear_186 = None\n",
      "    transpose_170: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_119, 1, 2);  view_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_187: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg618_1);  arg618_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_188: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg619_1);  arg619_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_120: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_187, [1, -1, 6, 64]);  linear_187 = None\n",
      "    transpose_171: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_120, 1, 2);  view_120 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_121: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_188, [1, -1, 6, 64]);  linear_188 = None\n",
      "    transpose_172: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_121, 1, 2);  view_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant13 = self._tensor_constant13\n",
      "    lift_fresh_copy_13: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant13);  _tensor_constant13 = None\n",
      "    detach__13: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_13);  lift_fresh_copy_13 = None\n",
      "    _tensor_constant14 = self._tensor_constant14\n",
      "    lift_fresh_copy_14: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant14);  _tensor_constant14 = None\n",
      "    detach__14: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_14);  lift_fresh_copy_14 = None\n",
      "    cat_11: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__13, transpose_171], -2);  detach__13 = transpose_171 = None\n",
      "    cat_12: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__14, transpose_172], -2);  detach__14 = transpose_172 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_173: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_11, 3, 2);  cat_11 = None\n",
      "    matmul_27: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_170, transpose_173);  transpose_170 = transpose_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__14: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_27, add_113);  matmul_27 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_50: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__14, torch.float32);  add__14 = None\n",
      "    softmax_24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_50, -1)\n",
      "    type_as_13: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_24, to_50);  softmax_24 = to_50 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_100: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_13, 0.1, False);  type_as_13 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_28: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_100, cat_12);  dropout_100 = cat_12 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_174: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_28, 1, 2);  matmul_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_122: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_174, [1, -1, 384]);  transpose_174 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_189: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_122, arg620_1);  view_122 = arg620_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_101: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_189, 0.1, False);  linear_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_132: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_49, dropout_101);  to_49 = dropout_101 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_51: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_132, torch.float32);  add_132 = None\n",
      "    pow_36: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_51, 2)\n",
      "    mean_36: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_36, [-1], True);  pow_36 = None\n",
      "    add_133: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_36, 1e-06);  mean_36 = None\n",
      "    rsqrt_25: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_133);  add_133 = None\n",
      "    mul_118: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_51, rsqrt_25);  rsqrt_25 = None\n",
      "    mul_119: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg625_1, mul_118);  arg625_1 = mul_118 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_190: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg622_1);  arg622_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_120: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_190, 0.5)\n",
      "    pow_37: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_190, 3.0)\n",
      "    mul_121: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_37, 0.044715);  pow_37 = None\n",
      "    add_134: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_190, mul_121);  linear_190 = mul_121 = None\n",
      "    mul_122: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_134, 0.7978845608028654);  add_134 = None\n",
      "    tanh_10: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_122);  mul_122 = None\n",
      "    add_135: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_10, 1.0);  tanh_10 = None\n",
      "    mul_123: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_120, add_135);  mul_120 = add_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_191: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_119, arg623_1);  mul_119 = arg623_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_124: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_123, linear_191);  mul_123 = linear_191 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_102: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_124, 0.1, False);  mul_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_192: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_102, arg624_1);  dropout_102 = arg624_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_103: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_192, 0.1, False);  linear_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_136: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_51, dropout_103);  to_51 = dropout_103 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_52: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_136, torch.float32);  add_136 = None\n",
      "    pow_38: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_52, 2)\n",
      "    mean_37: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_38, [-1], True);  pow_38 = None\n",
      "    add_137: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_37, 1e-06);  mean_37 = None\n",
      "    rsqrt_26: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_137);  add_137 = None\n",
      "    mul_125: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_52, rsqrt_26);  rsqrt_26 = None\n",
      "    mul_126: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg630_1, mul_125);  arg630_1 = mul_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_193: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg626_1);  arg626_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_123: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_193, [1, -1, 6, 64]);  linear_193 = None\n",
      "    transpose_175: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_123, 1, 2);  view_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_194: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg627_1);  arg627_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_195: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_126, arg628_1);  mul_126 = arg628_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_124: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_194, [1, -1, 6, 64]);  linear_194 = None\n",
      "    transpose_176: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_124, 1, 2);  view_124 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_125: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_195, [1, -1, 6, 64]);  linear_195 = None\n",
      "    transpose_177: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_125, 1, 2);  view_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant15 = self._tensor_constant15\n",
      "    lift_fresh_copy_15: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant15);  _tensor_constant15 = None\n",
      "    detach__15: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_15);  lift_fresh_copy_15 = None\n",
      "    _tensor_constant16 = self._tensor_constant16\n",
      "    lift_fresh_copy_16: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant16);  _tensor_constant16 = None\n",
      "    detach__16: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_16);  lift_fresh_copy_16 = None\n",
      "    cat_13: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__15, transpose_176], -2);  detach__15 = transpose_176 = None\n",
      "    cat_14: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__16, transpose_177], -2);  detach__16 = transpose_177 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_178: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_13, 3, 2);  cat_13 = None\n",
      "    matmul_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_175, transpose_178);  transpose_175 = transpose_178 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__15: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_29, add_109);  matmul_29 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_53: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__15, torch.float32);  add__15 = None\n",
      "    softmax_25: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_53, -1)\n",
      "    type_as_14: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_25, to_53);  softmax_25 = to_53 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_104: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_14, 0.1, False);  type_as_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_104, cat_14);  dropout_104 = cat_14 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_179: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_30, 1, 2);  matmul_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_126: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_179, [1, -1, 384]);  transpose_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_196: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_126, arg629_1);  view_126 = arg629_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_105: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_196, 0.1, False);  linear_196 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_138: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_52, dropout_105);  to_52 = dropout_105 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_34: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_139: \"i64[]\" = torch.ops.aten.add.Tensor(select_34, 1);  select_34 = add_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_54: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_138, torch.float32);  add_138 = None\n",
      "    pow_39: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_54, 2)\n",
      "    mean_38: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_39, [-1], True);  pow_39 = None\n",
      "    add_140: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_38, 1e-06);  mean_38 = None\n",
      "    rsqrt_27: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_140);  add_140 = None\n",
      "    mul_127: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_54, rsqrt_27);  rsqrt_27 = None\n",
      "    mul_128: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg635_1, mul_127);  arg635_1 = mul_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_197: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_128, arg631_1);  mul_128 = arg631_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_127: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_197, [1, -1, 6, 64]);  linear_197 = None\n",
      "    transpose_180: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_127, 1, 2);  view_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_198: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg632_1);  arg632_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_199: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg633_1);  arg633_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_128: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_198, [1, -1, 6, 64]);  linear_198 = None\n",
      "    transpose_181: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_128, 1, 2);  view_128 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_129: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_199, [1, -1, 6, 64]);  linear_199 = None\n",
      "    transpose_182: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_129, 1, 2);  view_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant17 = self._tensor_constant17\n",
      "    lift_fresh_copy_17: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant17);  _tensor_constant17 = None\n",
      "    detach__17: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_17);  lift_fresh_copy_17 = None\n",
      "    _tensor_constant18 = self._tensor_constant18\n",
      "    lift_fresh_copy_18: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant18);  _tensor_constant18 = None\n",
      "    detach__18: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_18);  lift_fresh_copy_18 = None\n",
      "    cat_15: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__17, transpose_181], -2);  detach__17 = transpose_181 = None\n",
      "    cat_16: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__18, transpose_182], -2);  detach__18 = transpose_182 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_183: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_15, 3, 2);  cat_15 = None\n",
      "    matmul_31: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_180, transpose_183);  transpose_180 = transpose_183 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__16: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_31, add_113);  matmul_31 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_55: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__16, torch.float32);  add__16 = None\n",
      "    softmax_26: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_55, -1)\n",
      "    type_as_15: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_26, to_55);  softmax_26 = to_55 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_106: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_15, 0.1, False);  type_as_15 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_32: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_106, cat_16);  dropout_106 = cat_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_184: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_32, 1, 2);  matmul_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_130: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_184, [1, -1, 384]);  transpose_184 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_200: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_130, arg634_1);  view_130 = arg634_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_107: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_200, 0.1, False);  linear_200 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_141: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_54, dropout_107);  to_54 = dropout_107 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_56: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_141, torch.float32);  add_141 = None\n",
      "    pow_40: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_56, 2)\n",
      "    mean_39: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_40, [-1], True);  pow_40 = None\n",
      "    add_142: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_39, 1e-06);  mean_39 = None\n",
      "    rsqrt_28: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_142);  add_142 = None\n",
      "    mul_129: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_56, rsqrt_28);  rsqrt_28 = None\n",
      "    mul_130: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg639_1, mul_129);  arg639_1 = mul_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_201: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg636_1);  arg636_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_131: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_201, 0.5)\n",
      "    pow_41: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_201, 3.0)\n",
      "    mul_132: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_41, 0.044715);  pow_41 = None\n",
      "    add_143: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_201, mul_132);  linear_201 = mul_132 = None\n",
      "    mul_133: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_143, 0.7978845608028654);  add_143 = None\n",
      "    tanh_11: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_133);  mul_133 = None\n",
      "    add_144: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_11, 1.0);  tanh_11 = None\n",
      "    mul_134: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_131, add_144);  mul_131 = add_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_202: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_130, arg637_1);  mul_130 = arg637_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_135: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_134, linear_202);  mul_134 = linear_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_108: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_135, 0.1, False);  mul_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_203: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_108, arg638_1);  dropout_108 = arg638_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_109: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_203, 0.1, False);  linear_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_145: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_56, dropout_109);  to_56 = dropout_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_57: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_145, torch.float32);  add_145 = None\n",
      "    pow_42: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_57, 2)\n",
      "    mean_40: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_42, [-1], True);  pow_42 = None\n",
      "    add_146: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_40, 1e-06);  mean_40 = None\n",
      "    rsqrt_29: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_146);  add_146 = None\n",
      "    mul_136: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_57, rsqrt_29);  rsqrt_29 = None\n",
      "    mul_137: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg644_1, mul_136);  arg644_1 = mul_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_204: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg640_1);  arg640_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_131: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_204, [1, -1, 6, 64]);  linear_204 = None\n",
      "    transpose_185: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_131, 1, 2);  view_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_205: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg641_1);  arg641_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_206: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_137, arg642_1);  mul_137 = arg642_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_132: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_205, [1, -1, 6, 64]);  linear_205 = None\n",
      "    transpose_186: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_132, 1, 2);  view_132 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_133: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_206, [1, -1, 6, 64]);  linear_206 = None\n",
      "    transpose_187: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_133, 1, 2);  view_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant19 = self._tensor_constant19\n",
      "    lift_fresh_copy_19: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant19);  _tensor_constant19 = None\n",
      "    detach__19: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_19);  lift_fresh_copy_19 = None\n",
      "    _tensor_constant20 = self._tensor_constant20\n",
      "    lift_fresh_copy_20: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant20);  _tensor_constant20 = None\n",
      "    detach__20: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_20);  lift_fresh_copy_20 = None\n",
      "    cat_17: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__19, transpose_186], -2);  detach__19 = transpose_186 = None\n",
      "    cat_18: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__20, transpose_187], -2);  detach__20 = transpose_187 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_188: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_17, 3, 2);  cat_17 = None\n",
      "    matmul_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_185, transpose_188);  transpose_185 = transpose_188 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__17: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_33, add_109);  matmul_33 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_58: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__17, torch.float32);  add__17 = None\n",
      "    softmax_27: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_58, -1)\n",
      "    type_as_16: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_27, to_58);  softmax_27 = to_58 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_110: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_16, 0.1, False);  type_as_16 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_34: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_110, cat_18);  dropout_110 = cat_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_189: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_34, 1, 2);  matmul_34 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_134: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_189, [1, -1, 384]);  transpose_189 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_207: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_134, arg643_1);  view_134 = arg643_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_111: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_207, 0.1, False);  linear_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_147: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_57, dropout_111);  to_57 = dropout_111 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_35: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_148: \"i64[]\" = torch.ops.aten.add.Tensor(select_35, 1);  select_35 = add_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_59: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_147, torch.float32);  add_147 = None\n",
      "    pow_43: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_59, 2)\n",
      "    mean_41: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_43, [-1], True);  pow_43 = None\n",
      "    add_149: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_41, 1e-06);  mean_41 = None\n",
      "    rsqrt_30: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_149);  add_149 = None\n",
      "    mul_138: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_59, rsqrt_30);  rsqrt_30 = None\n",
      "    mul_139: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg649_1, mul_138);  arg649_1 = mul_138 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_208: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_139, arg645_1);  mul_139 = arg645_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_135: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_208, [1, -1, 6, 64]);  linear_208 = None\n",
      "    transpose_190: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_135, 1, 2);  view_135 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_209: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg646_1);  arg646_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_210: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg647_1);  arg647_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_136: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_209, [1, -1, 6, 64]);  linear_209 = None\n",
      "    transpose_191: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_136, 1, 2);  view_136 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_137: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_210, [1, -1, 6, 64]);  linear_210 = None\n",
      "    transpose_192: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_137, 1, 2);  view_137 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant21 = self._tensor_constant21\n",
      "    lift_fresh_copy_21: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant21);  _tensor_constant21 = None\n",
      "    detach__21: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_21);  lift_fresh_copy_21 = None\n",
      "    _tensor_constant22 = self._tensor_constant22\n",
      "    lift_fresh_copy_22: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant22);  _tensor_constant22 = None\n",
      "    detach__22: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_22);  lift_fresh_copy_22 = None\n",
      "    cat_19: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__21, transpose_191], -2);  detach__21 = transpose_191 = None\n",
      "    cat_20: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__22, transpose_192], -2);  detach__22 = transpose_192 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_193: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_19, 3, 2);  cat_19 = None\n",
      "    matmul_35: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_190, transpose_193);  transpose_190 = transpose_193 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__18: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_35, add_113);  matmul_35 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_60: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__18, torch.float32);  add__18 = None\n",
      "    softmax_28: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_60, -1)\n",
      "    type_as_17: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_28, to_60);  softmax_28 = to_60 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_112: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_17, 0.1, False);  type_as_17 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_36: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_112, cat_20);  dropout_112 = cat_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_194: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_36, 1, 2);  matmul_36 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_138: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_194, [1, -1, 384]);  transpose_194 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_211: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_138, arg648_1);  view_138 = arg648_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_113: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_211, 0.1, False);  linear_211 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_150: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_59, dropout_113);  to_59 = dropout_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_61: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_150, torch.float32);  add_150 = None\n",
      "    pow_44: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_61, 2)\n",
      "    mean_42: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_44, [-1], True);  pow_44 = None\n",
      "    add_151: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_42, 1e-06);  mean_42 = None\n",
      "    rsqrt_31: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_151);  add_151 = None\n",
      "    mul_140: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_61, rsqrt_31);  rsqrt_31 = None\n",
      "    mul_141: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg653_1, mul_140);  arg653_1 = mul_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_212: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg650_1);  arg650_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_142: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_212, 0.5)\n",
      "    pow_45: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_212, 3.0)\n",
      "    mul_143: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_45, 0.044715);  pow_45 = None\n",
      "    add_152: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_212, mul_143);  linear_212 = mul_143 = None\n",
      "    mul_144: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_152, 0.7978845608028654);  add_152 = None\n",
      "    tanh_12: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_144);  mul_144 = None\n",
      "    add_153: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_12, 1.0);  tanh_12 = None\n",
      "    mul_145: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_142, add_153);  mul_142 = add_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_213: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_141, arg651_1);  mul_141 = arg651_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_146: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_145, linear_213);  mul_145 = linear_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_114: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_146, 0.1, False);  mul_146 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_214: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_114, arg652_1);  dropout_114 = arg652_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_115: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_214, 0.1, False);  linear_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_154: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_61, dropout_115);  to_61 = dropout_115 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_62: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_154, torch.float32);  add_154 = None\n",
      "    pow_46: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_62, 2)\n",
      "    mean_43: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_46, [-1], True);  pow_46 = None\n",
      "    add_155: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_43, 1e-06);  mean_43 = None\n",
      "    rsqrt_32: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_155);  add_155 = None\n",
      "    mul_147: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_62, rsqrt_32);  rsqrt_32 = None\n",
      "    mul_148: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg658_1, mul_147);  arg658_1 = mul_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_215: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg654_1);  arg654_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_139: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_215, [1, -1, 6, 64]);  linear_215 = None\n",
      "    transpose_195: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_139, 1, 2);  view_139 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_216: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg655_1);  arg655_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_217: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_148, arg656_1);  mul_148 = arg656_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_140: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_216, [1, -1, 6, 64]);  linear_216 = None\n",
      "    transpose_196: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_140, 1, 2);  view_140 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_141: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_217, [1, -1, 6, 64]);  linear_217 = None\n",
      "    transpose_197: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_141, 1, 2);  view_141 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant23 = self._tensor_constant23\n",
      "    lift_fresh_copy_23: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant23);  _tensor_constant23 = None\n",
      "    detach__23: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_23);  lift_fresh_copy_23 = None\n",
      "    _tensor_constant24 = self._tensor_constant24\n",
      "    lift_fresh_copy_24: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant24);  _tensor_constant24 = None\n",
      "    detach__24: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_24);  lift_fresh_copy_24 = None\n",
      "    cat_21: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__23, transpose_196], -2);  detach__23 = transpose_196 = None\n",
      "    cat_22: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__24, transpose_197], -2);  detach__24 = transpose_197 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_198: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_21, 3, 2);  cat_21 = None\n",
      "    matmul_37: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_195, transpose_198);  transpose_195 = transpose_198 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__19: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_37, add_109);  matmul_37 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_63: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__19, torch.float32);  add__19 = None\n",
      "    softmax_29: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_63, -1)\n",
      "    type_as_18: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_29, to_63);  softmax_29 = to_63 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_116: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_18, 0.1, False);  type_as_18 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_38: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_116, cat_22);  dropout_116 = cat_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_199: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_38, 1, 2);  matmul_38 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_142: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_199, [1, -1, 384]);  transpose_199 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_218: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_142, arg657_1);  view_142 = arg657_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_117: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_218, 0.1, False);  linear_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_156: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_62, dropout_117);  to_62 = dropout_117 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_36: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_157: \"i64[]\" = torch.ops.aten.add.Tensor(select_36, 1);  select_36 = add_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_64: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_156, torch.float32);  add_156 = None\n",
      "    pow_47: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_64, 2)\n",
      "    mean_44: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_47, [-1], True);  pow_47 = None\n",
      "    add_158: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_44, 1e-06);  mean_44 = None\n",
      "    rsqrt_33: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_158);  add_158 = None\n",
      "    mul_149: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_64, rsqrt_33);  rsqrt_33 = None\n",
      "    mul_150: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg663_1, mul_149);  arg663_1 = mul_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_219: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_150, arg659_1);  mul_150 = arg659_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_143: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_219, [1, -1, 6, 64]);  linear_219 = None\n",
      "    transpose_200: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_143, 1, 2);  view_143 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_220: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg660_1);  arg660_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_221: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg661_1);  arg661_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_144: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_220, [1, -1, 6, 64]);  linear_220 = None\n",
      "    transpose_201: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_144, 1, 2);  view_144 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_145: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_221, [1, -1, 6, 64]);  linear_221 = None\n",
      "    transpose_202: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_145, 1, 2);  view_145 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant25 = self._tensor_constant25\n",
      "    lift_fresh_copy_25: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant25);  _tensor_constant25 = None\n",
      "    detach__25: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_25);  lift_fresh_copy_25 = None\n",
      "    _tensor_constant26 = self._tensor_constant26\n",
      "    lift_fresh_copy_26: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant26);  _tensor_constant26 = None\n",
      "    detach__26: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_26);  lift_fresh_copy_26 = None\n",
      "    cat_23: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__25, transpose_201], -2);  detach__25 = transpose_201 = None\n",
      "    cat_24: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__26, transpose_202], -2);  detach__26 = transpose_202 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_203: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_23, 3, 2);  cat_23 = None\n",
      "    matmul_39: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_200, transpose_203);  transpose_200 = transpose_203 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__20: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_39, add_113);  matmul_39 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_65: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__20, torch.float32);  add__20 = None\n",
      "    softmax_30: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_65, -1)\n",
      "    type_as_19: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_30, to_65);  softmax_30 = to_65 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_118: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_19, 0.1, False);  type_as_19 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_40: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_118, cat_24);  dropout_118 = cat_24 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_204: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_40, 1, 2);  matmul_40 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_146: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_204, [1, -1, 384]);  transpose_204 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_222: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_146, arg662_1);  view_146 = arg662_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_119: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_222, 0.1, False);  linear_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_159: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_64, dropout_119);  to_64 = dropout_119 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_66: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_159, torch.float32);  add_159 = None\n",
      "    pow_48: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_66, 2)\n",
      "    mean_45: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_48, [-1], True);  pow_48 = None\n",
      "    add_160: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_45, 1e-06);  mean_45 = None\n",
      "    rsqrt_34: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_160);  add_160 = None\n",
      "    mul_151: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_66, rsqrt_34);  rsqrt_34 = None\n",
      "    mul_152: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg667_1, mul_151);  arg667_1 = mul_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_223: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg664_1);  arg664_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_153: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_223, 0.5)\n",
      "    pow_49: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_223, 3.0)\n",
      "    mul_154: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_49, 0.044715);  pow_49 = None\n",
      "    add_161: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_223, mul_154);  linear_223 = mul_154 = None\n",
      "    mul_155: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_161, 0.7978845608028654);  add_161 = None\n",
      "    tanh_13: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_155);  mul_155 = None\n",
      "    add_162: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_13, 1.0);  tanh_13 = None\n",
      "    mul_156: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_153, add_162);  mul_153 = add_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_224: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_152, arg665_1);  mul_152 = arg665_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_157: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_156, linear_224);  mul_156 = linear_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_120: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_157, 0.1, False);  mul_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_225: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_120, arg666_1);  dropout_120 = arg666_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_121: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_225, 0.1, False);  linear_225 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_163: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_66, dropout_121);  to_66 = dropout_121 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_67: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_163, torch.float32);  add_163 = None\n",
      "    pow_50: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_67, 2)\n",
      "    mean_46: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_50, [-1], True);  pow_50 = None\n",
      "    add_164: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_46, 1e-06);  mean_46 = None\n",
      "    rsqrt_35: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_164);  add_164 = None\n",
      "    mul_158: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_67, rsqrt_35);  rsqrt_35 = None\n",
      "    mul_159: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg672_1, mul_158);  arg672_1 = mul_158 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_226: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg668_1);  arg668_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_147: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_226, [1, -1, 6, 64]);  linear_226 = None\n",
      "    transpose_205: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_147, 1, 2);  view_147 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_227: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg669_1);  arg669_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_228: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_159, arg670_1);  mul_159 = arg670_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_148: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_227, [1, -1, 6, 64]);  linear_227 = None\n",
      "    transpose_206: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_148, 1, 2);  view_148 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_149: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_228, [1, -1, 6, 64]);  linear_228 = None\n",
      "    transpose_207: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_149, 1, 2);  view_149 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant27 = self._tensor_constant27\n",
      "    lift_fresh_copy_27: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant27);  _tensor_constant27 = None\n",
      "    detach__27: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_27);  lift_fresh_copy_27 = None\n",
      "    _tensor_constant28 = self._tensor_constant28\n",
      "    lift_fresh_copy_28: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant28);  _tensor_constant28 = None\n",
      "    detach__28: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_28);  lift_fresh_copy_28 = None\n",
      "    cat_25: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__27, transpose_206], -2);  detach__27 = transpose_206 = None\n",
      "    cat_26: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__28, transpose_207], -2);  detach__28 = transpose_207 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_208: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_25, 3, 2);  cat_25 = None\n",
      "    matmul_41: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_205, transpose_208);  transpose_205 = transpose_208 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__21: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_41, add_109);  matmul_41 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_68: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__21, torch.float32);  add__21 = None\n",
      "    softmax_31: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_68, -1)\n",
      "    type_as_20: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_31, to_68);  softmax_31 = to_68 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_122: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_20, 0.1, False);  type_as_20 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_42: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_122, cat_26);  dropout_122 = cat_26 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_209: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_42, 1, 2);  matmul_42 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_150: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_209, [1, -1, 384]);  transpose_209 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_229: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_150, arg671_1);  view_150 = arg671_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_123: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_229, 0.1, False);  linear_229 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_165: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_67, dropout_123);  to_67 = dropout_123 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_37: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_166: \"i64[]\" = torch.ops.aten.add.Tensor(select_37, 1);  select_37 = add_166 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_69: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_165, torch.float32);  add_165 = None\n",
      "    pow_51: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_69, 2)\n",
      "    mean_47: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_51, [-1], True);  pow_51 = None\n",
      "    add_167: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_47, 1e-06);  mean_47 = None\n",
      "    rsqrt_36: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_167);  add_167 = None\n",
      "    mul_160: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_69, rsqrt_36);  rsqrt_36 = None\n",
      "    mul_161: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg677_1, mul_160);  arg677_1 = mul_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_230: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_161, arg673_1);  mul_161 = arg673_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_151: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_230, [1, -1, 6, 64]);  linear_230 = None\n",
      "    transpose_210: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_151, 1, 2);  view_151 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_231: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg674_1);  arg674_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_232: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg675_1);  arg675_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_152: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_231, [1, -1, 6, 64]);  linear_231 = None\n",
      "    transpose_211: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_152, 1, 2);  view_152 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_153: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_232, [1, -1, 6, 64]);  linear_232 = None\n",
      "    transpose_212: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_153, 1, 2);  view_153 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant29 = self._tensor_constant29\n",
      "    lift_fresh_copy_29: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant29);  _tensor_constant29 = None\n",
      "    detach__29: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_29);  lift_fresh_copy_29 = None\n",
      "    _tensor_constant30 = self._tensor_constant30\n",
      "    lift_fresh_copy_30: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant30);  _tensor_constant30 = None\n",
      "    detach__30: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_30);  lift_fresh_copy_30 = None\n",
      "    cat_27: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__29, transpose_211], -2);  detach__29 = transpose_211 = None\n",
      "    cat_28: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__30, transpose_212], -2);  detach__30 = transpose_212 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_213: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_27, 3, 2);  cat_27 = None\n",
      "    matmul_43: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_210, transpose_213);  transpose_210 = transpose_213 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__22: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_43, add_113);  matmul_43 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_70: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__22, torch.float32);  add__22 = None\n",
      "    softmax_32: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_70, -1)\n",
      "    type_as_21: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_32, to_70);  softmax_32 = to_70 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_124: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_21, 0.1, False);  type_as_21 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_44: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_124, cat_28);  dropout_124 = cat_28 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_214: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_44, 1, 2);  matmul_44 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_154: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_214, [1, -1, 384]);  transpose_214 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_233: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_154, arg676_1);  view_154 = arg676_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_125: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_233, 0.1, False);  linear_233 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_168: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_69, dropout_125);  to_69 = dropout_125 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_71: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_168, torch.float32);  add_168 = None\n",
      "    pow_52: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_71, 2)\n",
      "    mean_48: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_52, [-1], True);  pow_52 = None\n",
      "    add_169: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_48, 1e-06);  mean_48 = None\n",
      "    rsqrt_37: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_169);  add_169 = None\n",
      "    mul_162: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_71, rsqrt_37);  rsqrt_37 = None\n",
      "    mul_163: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg681_1, mul_162);  arg681_1 = mul_162 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_234: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg678_1);  arg678_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_164: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_234, 0.5)\n",
      "    pow_53: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_234, 3.0)\n",
      "    mul_165: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_53, 0.044715);  pow_53 = None\n",
      "    add_170: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_234, mul_165);  linear_234 = mul_165 = None\n",
      "    mul_166: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_170, 0.7978845608028654);  add_170 = None\n",
      "    tanh_14: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_166);  mul_166 = None\n",
      "    add_171: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_14, 1.0);  tanh_14 = None\n",
      "    mul_167: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_164, add_171);  mul_164 = add_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_235: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_163, arg679_1);  mul_163 = arg679_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_168: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_167, linear_235);  mul_167 = linear_235 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_126: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_168, 0.1, False);  mul_168 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_236: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_126, arg680_1);  dropout_126 = arg680_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_127: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_236, 0.1, False);  linear_236 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_172: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_71, dropout_127);  to_71 = dropout_127 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_72: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_172, torch.float32);  add_172 = None\n",
      "    pow_54: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_72, 2)\n",
      "    mean_49: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_54, [-1], True);  pow_54 = None\n",
      "    add_173: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_49, 1e-06);  mean_49 = None\n",
      "    rsqrt_38: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_173);  add_173 = None\n",
      "    mul_169: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_72, rsqrt_38);  rsqrt_38 = None\n",
      "    mul_170: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg686_1, mul_169);  arg686_1 = mul_169 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_237: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg682_1);  arg682_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_155: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_237, [1, -1, 6, 64]);  linear_237 = None\n",
      "    transpose_215: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_155, 1, 2);  view_155 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_238: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg683_1);  arg683_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_239: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_170, arg684_1);  mul_170 = arg684_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_156: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_238, [1, -1, 6, 64]);  linear_238 = None\n",
      "    transpose_216: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_156, 1, 2);  view_156 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_157: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_239, [1, -1, 6, 64]);  linear_239 = None\n",
      "    transpose_217: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_157, 1, 2);  view_157 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant31 = self._tensor_constant31\n",
      "    lift_fresh_copy_31: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant31);  _tensor_constant31 = None\n",
      "    detach__31: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_31);  lift_fresh_copy_31 = None\n",
      "    _tensor_constant32 = self._tensor_constant32\n",
      "    lift_fresh_copy_32: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant32);  _tensor_constant32 = None\n",
      "    detach__32: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_32);  lift_fresh_copy_32 = None\n",
      "    cat_29: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__31, transpose_216], -2);  detach__31 = transpose_216 = None\n",
      "    cat_30: \"f32[1, 6, 1, 64]\" = torch.ops.aten.cat.default([detach__32, transpose_217], -2);  detach__32 = transpose_217 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_218: \"f32[1, 6, 64, 1]\" = torch.ops.aten.transpose.int(cat_29, 3, 2);  cat_29 = None\n",
      "    matmul_45: \"f32[1, 6, 1, 1]\" = torch.ops.aten.matmul.default(transpose_215, transpose_218);  transpose_215 = transpose_218 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__23: \"f32[1, 6, 1, 1]\" = torch.ops.aten.add_.Tensor(matmul_45, add_109);  matmul_45 = add_109 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_73: \"f32[1, 6, 1, 1]\" = torch.ops.aten.to.dtype(add__23, torch.float32);  add__23 = None\n",
      "    softmax_33: \"f32[1, 6, 1, 1]\" = torch.ops.aten.softmax.int(to_73, -1)\n",
      "    type_as_22: \"f32[1, 6, 1, 1]\" = torch.ops.aten.type_as.default(softmax_33, to_73);  softmax_33 = to_73 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_128: \"f32[1, 6, 1, 1]\" = torch.ops.aten.dropout.default(type_as_22, 0.1, False);  type_as_22 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_46: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_128, cat_30);  dropout_128 = cat_30 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_219: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_46, 1, 2);  matmul_46 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_158: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_219, [1, -1, 384]);  transpose_219 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_240: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_158, arg685_1);  view_158 = arg685_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_129: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_240, 0.1, False);  linear_240 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:612 in forward, code: hidden_states = hidden_states + self.dropout(attention_output[0])\n",
      "    add_174: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_72, dropout_129);  to_72 = dropout_129 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:717 in forward, code: query_length=cache_position[-1] + 1,\n",
      "    select_38: \"i64[]\" = torch.ops.aten.select.int(slice_41, 0, -1)\n",
      "    add_175: \"i64[]\" = torch.ops.aten.add.Tensor(select_38, 1);  select_38 = add_175 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_74: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_174, torch.float32);  add_174 = None\n",
      "    pow_55: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_74, 2)\n",
      "    mean_50: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_55, [-1], True);  pow_55 = None\n",
      "    add_176: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_50, 1e-06);  mean_50 = None\n",
      "    rsqrt_39: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_176);  add_176 = None\n",
      "    mul_171: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_74, rsqrt_39);  rsqrt_39 = None\n",
      "    mul_172: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg691_1, mul_171);  arg691_1 = mul_171 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_241: \"f32[1, 1, 384]\" = torch.ops.aten.linear.default(mul_172, arg687_1);  mul_172 = arg687_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:493 in forward, code: query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_159: \"f32[1, 1, 6, 64]\" = torch.ops.aten.view.default(linear_241, [1, -1, 6, 64]);  linear_241 = None\n",
      "    transpose_220: \"f32[1, 6, 1, 64]\" = torch.ops.aten.transpose.int(view_159, 1, 2);  view_159 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_242: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg688_1);  arg688_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_243: \"f32[1, 109, 384]\" = torch.ops.aten.linear.default(dropout_84, arg689_1);  dropout_84 = arg689_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:514 in forward, code: key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_160: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_242, [1, -1, 6, 64]);  linear_242 = None\n",
      "    transpose_221: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_160, 1, 2);  view_160 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:515 in forward, code: value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
      "    view_161: \"f32[1, 109, 6, 64]\" = torch.ops.aten.view.default(linear_243, [1, -1, 6, 64]);  linear_243 = None\n",
      "    transpose_222: \"f32[1, 6, 109, 64]\" = torch.ops.aten.transpose.int(view_161, 1, 2);  view_161 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:520 in forward, code: key_states, value_states = curr_past_key_value.update(\n",
      "    _tensor_constant33 = self._tensor_constant33\n",
      "    lift_fresh_copy_33: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant33);  _tensor_constant33 = None\n",
      "    detach__33: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_33);  lift_fresh_copy_33 = None\n",
      "    _tensor_constant34 = self._tensor_constant34\n",
      "    lift_fresh_copy_34: \"f32[0]\" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant34);  _tensor_constant34 = None\n",
      "    detach__34: \"f32[0]\" = torch.ops.aten.detach_.default(lift_fresh_copy_34);  lift_fresh_copy_34 = None\n",
      "    cat_31: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__33, transpose_221], -2);  detach__33 = transpose_221 = None\n",
      "    cat_32: \"f32[1, 6, 109, 64]\" = torch.ops.aten.cat.default([detach__34, transpose_222], -2);  detach__34 = transpose_222 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:528 in forward, code: scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
      "    transpose_223: \"f32[1, 6, 64, 109]\" = torch.ops.aten.transpose.int(cat_31, 3, 2);  cat_31 = None\n",
      "    matmul_47: \"f32[1, 6, 1, 109]\" = torch.ops.aten.matmul.default(transpose_220, transpose_223);  transpose_220 = transpose_223 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:557 in forward, code: scores += position_bias_masked\n",
      "    add__24: \"f32[1, 6, 1, 109]\" = torch.ops.aten.add_.Tensor(matmul_47, add_113);  matmul_47 = add_113 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:560 in forward, code: attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
      "    to_75: \"f32[1, 6, 1, 109]\" = torch.ops.aten.to.dtype(add__24, torch.float32);  add__24 = None\n",
      "    softmax_34: \"f32[1, 6, 1, 109]\" = torch.ops.aten.softmax.int(to_75, -1)\n",
      "    type_as_23: \"f32[1, 6, 1, 109]\" = torch.ops.aten.type_as.default(softmax_34, to_75);  softmax_34 = to_75 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:561 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
      "    dropout_130: \"f32[1, 6, 1, 109]\" = torch.ops.aten.dropout.default(type_as_23, 0.1, False);  type_as_23 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:567 in forward, code: attn_output = torch.matmul(attn_weights, value_states)\n",
      "    matmul_48: \"f32[1, 6, 1, 64]\" = torch.ops.aten.matmul.default(dropout_130, cat_32);  dropout_130 = cat_32 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:569 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    transpose_224: \"f32[1, 1, 6, 64]\" = torch.ops.aten.transpose.int(matmul_48, 1, 2);  matmul_48 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:570 in forward, code: attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
      "    view_162: \"f32[1, 1, 384]\" = torch.ops.aten.view.default(transpose_224, [1, -1, 384]);  transpose_224 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_244: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(view_162, arg690_1);  view_162 = arg690_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_131: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_244, 0.1, False);  linear_244 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:651 in forward, code: layer_output = hidden_states + self.dropout(attention_output[0])\n",
      "    add_177: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_74, dropout_131);  to_74 = dropout_131 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_76: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_177, torch.float32);  add_177 = None\n",
      "    pow_56: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_76, 2)\n",
      "    mean_51: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_56, [-1], True);  pow_56 = None\n",
      "    add_178: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_51, 1e-06);  mean_51 = None\n",
      "    rsqrt_40: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_178);  add_178 = None\n",
      "    mul_173: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_76, rsqrt_40);  rsqrt_40 = None\n",
      "    mul_174: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg695_1, mul_173);  arg695_1 = mul_173 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_245: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg692_1);  arg692_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/activations.py:48 in forward, code: return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
      "    mul_175: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(linear_245, 0.5)\n",
      "    pow_57: \"f32[1, 1, 1024]\" = torch.ops.aten.pow.Tensor_Scalar(linear_245, 3.0)\n",
      "    mul_176: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(pow_57, 0.044715);  pow_57 = None\n",
      "    add_179: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(linear_245, mul_176);  linear_245 = mul_176 = None\n",
      "    mul_177: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(add_179, 0.7978845608028654);  add_179 = None\n",
      "    tanh_15: \"f32[1, 1, 1024]\" = torch.ops.aten.tanh.default(mul_177);  mul_177 = None\n",
      "    add_180: \"f32[1, 1, 1024]\" = torch.ops.aten.add.Tensor(tanh_15, 1.0);  tanh_15 = None\n",
      "    mul_178: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_175, add_180);  mul_175 = add_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_246: \"f32[1, 1, 1024]\" = torch.ops.aten.linear.default(mul_174, arg693_1);  mul_174 = arg693_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:313 in forward, code: hidden_states = hidden_gelu * hidden_linear\n",
      "    mul_179: \"f32[1, 1, 1024]\" = torch.ops.aten.mul.Tensor(mul_178, linear_246);  mul_178 = linear_246 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_132: \"f32[1, 1, 1024]\" = torch.ops.aten.dropout.default(mul_179, 0.1, False);  mul_179 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_247: \"f32[1, 1, 512]\" = torch.ops.aten.linear.default(dropout_132, arg694_1);  dropout_132 = arg694_1 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_133: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(linear_247, 0.1, False);  linear_247 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py:344 in forward, code: hidden_states = hidden_states + self.dropout(forwarded_states)\n",
      "    add_181: \"f32[1, 1, 512]\" = torch.ops.aten.add.Tensor(to_76, dropout_133);  to_76 = dropout_133 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/apex/normalization/fused_layer_norm.py:895 in forward, code: return manual_rms_norm(input, self.normalized_shape, self.weight, self.eps)\n",
      "    to_77: \"f32[1, 1, 512]\" = torch.ops.aten.to.dtype(add_181, torch.float32);  add_181 = None\n",
      "    pow_58: \"f32[1, 1, 512]\" = torch.ops.aten.pow.Tensor_Scalar(to_77, 2)\n",
      "    mean_52: \"f32[1, 1, 1]\" = torch.ops.aten.mean.dim(pow_58, [-1], True);  pow_58 = None\n",
      "    add_182: \"f32[1, 1, 1]\" = torch.ops.aten.add.Tensor(mean_52, 1e-06);  mean_52 = None\n",
      "    rsqrt_41: \"f32[1, 1, 1]\" = torch.ops.aten.rsqrt.default(add_182);  add_182 = None\n",
      "    mul_180: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(to_77, rsqrt_41);  to_77 = rsqrt_41 = None\n",
      "    mul_181: \"f32[1, 1, 512]\" = torch.ops.aten.mul.Tensor(arg696_1, mul_180);  arg696_1 = mul_180 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
      "    dropout_134: \"f32[1, 1, 512]\" = torch.ops.aten.dropout.default(mul_181, 0.1, False);  mul_181 = None\n",
      "    \n",
      "     # File: /usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
      "    linear_248: \"f32[1, 1, 32128]\" = torch.ops.aten.linear.default(dropout_134, arg697_1);  dropout_134 = arg697_1 = None\n",
      "    \n",
      "     # File: /tmp/ipykernel_1989/700978424.py:271 in forward, code: gen_ids = self.lm_model.predict(z, input_token, enc_mask)\n",
      "    slice_61: \"i64[1]\" = torch.ops.aten.slice.Tensor(slice_41, 0, -1);  slice_41 = None\n",
      "    add_183: \"i64[1]\" = torch.ops.aten.add.Tensor(slice_61, 1);  slice_61 = add_183 = None\n",
      "    slice_62: \"f32[1, 1, 32128]\" = torch.ops.aten.slice.Tensor(linear_248);  linear_248 = None\n",
      "    select_39: \"f32[1, 32128]\" = torch.ops.aten.select.int(slice_62, 1, -1);  slice_62 = None\n",
      "    slice_63: \"f32[1, 32128]\" = torch.ops.aten.slice.Tensor(select_39, 1);  select_39 = None\n",
      "    to_78: \"f32[1, 32128]\" = torch.ops.aten.to.device(slice_63, device(type='cpu'), torch.float32, False, True);  slice_63 = None\n",
      "    argmax: \"i64[1]\" = torch.ops.aten.argmax.default(to_78, -1);  to_78 = None\n",
      "    mul_182: \"i64[1]\" = torch.ops.aten.mul.Tensor(argmax, ones_3);  argmax = None\n",
      "    rsub_2: \"i64[1]\" = torch.ops.aten.rsub.Scalar(ones_3, 1)\n",
      "    mul_183: \"i64[1]\" = torch.ops.aten.mul.Tensor(detach__1, rsub_2);  detach__1 = rsub_2 = None\n",
      "    add_184: \"i64[1]\" = torch.ops.aten.add.Tensor(mul_182, mul_183);  mul_182 = mul_183 = None\n",
      "    slice_64: \"i64[1]\" = torch.ops.aten.slice.Tensor(add_184, 0, 0, 9223372036854775807);  add_184 = None\n",
      "    unsqueeze_32: \"i64[1, 1]\" = torch.ops.aten.unsqueeze.default(slice_64, 1);  slice_64 = None\n",
      "    cat_33: \"i64[1, 2]\" = torch.ops.aten.cat.default([mul_89, unsqueeze_32], -1);  mul_89 = unsqueeze_32 = None\n",
      "    full_1: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    full_2: \"b8[1]\" = torch.ops.aten.full.default([1], False, dtype = torch.bool, device = device(type='cpu'), pin_memory = False)\n",
      "    or_1: \"b8[1]\" = torch.ops.aten.__or__.Tensor(full_1, full_2);  full_1 = full_2 = None\n",
      "    to_79: \"i64[1]\" = torch.ops.aten.to.dtype_layout(unsqueeze_24, dtype = torch.int64, layout = torch.strided, device = device(type='cpu'));  unsqueeze_24 = None\n",
      "    slice_65: \"i64[1, 2]\" = torch.ops.aten.slice.Tensor(cat_33);  cat_33 = None\n",
      "    select_40: \"i64[1]\" = torch.ops.aten.select.int(slice_65, 1, -1);  slice_65 = None\n",
      "    isin_1: \"b8[1]\" = torch.ops.aten.isin.Tensor_Tensor(select_40, to_79);  select_40 = to_79 = None\n",
      "    or_2: \"b8[1]\" = torch.ops.aten.__or__.Tensor(or_1, isin_1);  or_1 = isin_1 = None\n",
      "    bitwise_not: \"b8[1]\" = torch.ops.aten.bitwise_not.default(or_2);  or_2 = None\n",
      "    and_1: \"i64[1]\" = torch.ops.aten.__and__.Tensor(ones_3, bitwise_not);  ones_3 = bitwise_not = None\n",
      "    max_1: \"i64[]\" = torch.ops.aten.max.default(and_1);  and_1 = None\n",
      "    eq: \"b8[]\" = torch.ops.aten.eq.Scalar(max_1, 0);  max_1 = None\n",
      "    ne_2: \"b8[]\" = torch.ops.aten.ne.Scalar(eq, 0);  eq = None\n",
      "    item: \"Sym(Eq(u0, 1))\" = torch.ops.aten.item.default(ne_2);  ne_2 = item = None\n",
      "    _set_grad_enabled_3 = torch._C._set_grad_enabled(False);  _set_grad_enabled_3 = None\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `ExportWrapper([...]` with `torch.export.export(..., strict=False)`... \n",
      "[torch.onnx] Obtain model graph for `ExportWrapper([...]` with `torch.export.export(..., strict=True)`...\n",
      "[torch.onnx] Obtain model graph for `ExportWrapper([...]` with `torch.export.export(..., strict=True)`... \n",
      "[torch.onnx] Obtain model graph for `ExportWrapper([...]` with `torch.export draft_export`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0914 13:25:52.641000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u0, 1)) -> 0\n",
      "W0914 13:25:53.788000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u1, 1)) -> 0\n",
      "W0914 13:25:54.900000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u2, 1)) -> 0\n",
      "W0914 13:25:56.035000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u3, 1)) -> 0\n",
      "W0914 13:25:57.139000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u4, 1)) -> 0\n",
      "W0914 13:25:58.264000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u5, 1)) -> 0\n",
      "W0914 13:25:59.407000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u6, 1)) -> 0\n",
      "W0914 13:26:00.535000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u7, 1)) -> 0\n",
      "W0914 13:26:01.655000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u8, 1)) -> 0\n",
      "W0914 13:26:02.792000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u9, 1)) -> 0\n",
      "W0914 13:26:03.936000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u10, 1)) -> 0\n",
      "W0914 13:26:05.060000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u11, 1)) -> 0\n",
      "W0914 13:26:06.187000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u12, 1)) -> 0\n",
      "W0914 13:26:07.305000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u13, 1)) -> 0\n",
      "W0914 13:26:08.412000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u14, 1)) -> 0\n",
      "W0914 13:26:09.556000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u15, 1)) -> 0\n",
      "W0914 13:26:10.665000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u16, 1)) -> 0\n",
      "W0914 13:26:11.780000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u17, 1)) -> 0\n",
      "W0914 13:26:12.903000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u18, 1)) -> 0\n",
      "W0914 13:26:14.037000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u19, 1)) -> 0\n",
      "W0914 13:26:15.171000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u20, 1)) -> 0\n",
      "W0914 13:26:16.297000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u21, 1)) -> 0\n",
      "W0914 13:26:17.430000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u22, 1)) -> 0\n",
      "W0914 13:26:18.543000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u23, 1)) -> 0\n",
      "W0914 13:26:19.649000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u24, 1)) -> 0\n",
      "W0914 13:26:20.759000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u25, 1)) -> 0\n",
      "W0914 13:26:21.873000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u26, 1)) -> 0\n",
      "W0914 13:26:22.988000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u27, 1)) -> 0\n",
      "W0914 13:26:24.135000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u28, 1)) -> 0\n",
      "W0914 13:26:25.255000 1989 torch/fx/experimental/symbolic_shapes.py:7242] propagate_real_tensors evaluate_expr(Eq(u29, 1)) -> 1\n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497] \n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497] ###################################################################################################\n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497] WARNING: 1 issue(s) found during export, and it was not able to soundly produce a graph.\n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497] To view the report of failures in an html page, please run the command:\n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497]     `tlparse /tmp/export_ubuntu/dedicated_log_torch_trace_wwea53_l.log --export`\n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497] Or, you can view the errors in python by inspecting `print(ep._report)`.\n",
      "W0914 13:26:39.396000 1989 torch/export/_draft_export.py:497] #################################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Draft Export report:\n",
      "\u001b[93m\n",
      "###################################################################################################\n",
      "WARNING: 1 issue(s) found during export, and it was not able to soundly produce a graph.\n",
      "Please follow the instructions to fix the errors.\n",
      "###################################################################################################\n",
      "\n",
      "1. Data dependent error.\n",
      "    When exporting, we were unable to evaluate the value of `Eq(u0, 1)`.\n",
      "    This was encountered 30 times.\n",
      "    This occurred at the following user stacktrace: \n",
      "            elif this_peer_finished:\n",
      "        \n",
      "        Locals:\n",
      "            this_peer_finished: ['Tensor(shape: torch.Size([]), stride: (), storage_offset: 0)']\n",
      "\n",
      "    And the following framework stacktrace: \n",
      "            return func(*args, **kwargs)\n",
      "\n",
      "    As a result, it was specialized to a constant (e.g. `0` in the 1st occurrence), and asserts were inserted into the graph.\n",
      "\n",
      "    Please add `torch._check(...)` to the original code to assert this data-dependent assumption.\n",
      "    Please refer to https://docs.google.com/document/d/1kZ_BbB3JnoLbUZleDT6635dHs88ZVYId8jT-yTFgf3A/edit#heading=h.boi2xurpqa0o for more details.\n",
      "\n",
      "\u001b[0m\n",
      "[torch.onnx] Obtain model graph for `ExportWrapper([...]` with `torch.export draft_export`... \n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... \n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "/usr/local/lib/python3.12/dist-packages/onnx/reference/ops/op_log.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  return (np.log(x).astype(x.dtype),)\n",
      "/usr/local/lib/python3.12/dist-packages/onnx/reference/ops/op_cast.py:149: RuntimeWarning: invalid value encountered in cast\n",
      "  return x.astype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 404 of general pattern rewrite rules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n",
      "WARNING:onnxscript.optimizer._constant_folding:Skipping constant folding for op Split with multiple outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model.onnx validated\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Load model from model.onnx failed:Node (node_Concat_4455) Op (Concat) [ShapeInferenceError] axis must be in [-rank, rank-1].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFail\u001b[39m                                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m model.onnx validated\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m S = \u001b[32m77\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m sess = \u001b[43mort\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel.onnx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m outs = sess.run(\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     39\u001b[39m     {\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     },\n\u001b[32m     44\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mORT outputs:\u001b[39m\u001b[33m\"\u001b[39m, [np.asarray(o).shape \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outs])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:472\u001b[39m, in \u001b[36mInferenceSession.__init__\u001b[39m\u001b[34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m disabled_optimizers = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdisabled_optimizers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:550\u001b[39m, in \u001b[36mInferenceSession._create_inference_session\u001b[39m\u001b[34m(self, providers, provider_options, disabled_optimizers)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28mself\u001b[39m._register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_path:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     sess = \u001b[43mC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    552\u001b[39m     sess = C.InferenceSession(session_options, \u001b[38;5;28mself\u001b[39m._model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._read_config_from_model)\n",
      "\u001b[31mFail\u001b[39m: [ONNXRuntimeError] : 1 : FAIL : Load model from model.onnx failed:Node (node_Concat_4455) Op (Concat) [ShapeInferenceError] axis must be in [-rank, rank-1]."
     ]
    }
   ],
   "source": [
    "\n",
    "class ExportWrapper(nn.Module):\n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "    def forward(self, img, input_token, enc_mask):\n",
    "        # derive seq length from tokens to avoid scalar input in the graph\n",
    "        seq_len = input_token.size(1)\n",
    "        return self.m(img, input_token, enc_mask, seq_len)\n",
    "\n",
    "wrapped = ExportWrapper(model).cpu().eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    B, C, H, W, S = 1, 3, 224, 224, 77\n",
    "    img = torch.randn(B, C, H, W, dtype=torch.float32)\n",
    "    input_token = torch.zeros(B, S, dtype=torch.long)        # ids int64\n",
    "    enc_mask = torch.ones(B, S, dtype=torch.long)            # int64 or bool\n",
    "\n",
    "    # --- re-export with dynamo exporter ---\n",
    "    torch.onnx.export(\n",
    "        wrapped,\n",
    "        (img, input_token, enc_mask),\n",
    "        \"model.onnx\",\n",
    "        opset_version=18,\n",
    "        dynamo=True,                  # use the new exporter\n",
    "        export_params=True,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"img\",\"input_token\",\"enc_mask\"],\n",
    "        output_names=[\"output\"],      # add more if your model returns multiple\n",
    "        # (no dynamic_axes with dynamo=True)\n",
    "    )\n",
    "\n",
    "onnx.checker.check_model(\"model.onnx\")\n",
    "print(\" model.onnx validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m     graph.nodes.append(squeeze_node)\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Replace all uses of SequenceAt's output with squeezed tensor later\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43mtensor_replacements\u001b[49m\u001b[43m[\u001b[49m\u001b[43mout_tensor\u001b[49m\u001b[43m]\u001b[49m = squeezed\n\u001b[32m     75\u001b[39m     nodes_to_remove.append(at)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Mark the SplitToSequence node for removal\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'Variable'"
     ]
    }
   ],
   "source": [
    "# pip install onnx onnx_graphsurgeon\n",
    "import onnx\n",
    "import onnx_graphsurgeon as gs\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "\n",
    "ONNX_IN  = \"model.onnx\"\n",
    "ONNX_OUT = \"model_trt.onnx\"\n",
    "\n",
    "def uid(s): return f\"{s}_{uuid4().hex[:8]}\"\n",
    "\n",
    "def get_const_int(x):\n",
    "    if isinstance(x, gs.Constant):\n",
    "        arr = np.asarray(x.values)\n",
    "        if arr.size == 1:\n",
    "            return int(arr.reshape(()).item())\n",
    "    if isinstance(x, gs.Variable) and x.values is not None:\n",
    "        arr = np.asarray(x.values)\n",
    "        if arr.size == 1:\n",
    "            return int(arr.reshape(()).item())\n",
    "    return None\n",
    "\n",
    "model = onnx.load(ONNX_IN)\n",
    "graph = gs.import_onnx(model)\n",
    "\n",
    "changed_any = True\n",
    "while changed_any:\n",
    "    changed_any = False\n",
    "\n",
    "    # Collect replacements across the graph in this pass\n",
    "    tensor_replacements = {}\n",
    "    nodes_to_remove = []\n",
    "\n",
    "    for n in list(graph.nodes):\n",
    "        if n.op != \"SplitToSequence\":\n",
    "            continue\n",
    "\n",
    "        seq_out = n.outputs[0]\n",
    "        axis = int(n.attrs.get(\"axis\", 0))\n",
    "\n",
    "        # Find SequenceAt(seq, idx) users\n",
    "        seqat = []\n",
    "        for user in list(seq_out.outputs):\n",
    "            if user.op == \"SequenceAt\" and len(user.inputs) == 2:\n",
    "                idx = get_const_int(user.inputs[1])\n",
    "                if idx is not None and idx >= 0:\n",
    "                    seqat.append((user, idx))\n",
    "\n",
    "        if not seqat:\n",
    "            continue\n",
    "\n",
    "        # Build Split with enough outputs (0..max idx)\n",
    "        max_idx = max(idx for _, idx in seqat)\n",
    "        split_outs = [gs.Variable(name=uid(f\"{n.name}_split_{i}\")) for i in range(max_idx + 1)]\n",
    "        split_node = gs.Node(\n",
    "            op=\"Split\",\n",
    "            inputs=[n.inputs[0]],\n",
    "            outputs=split_outs,\n",
    "            attrs={\"axis\": axis}\n",
    "        )\n",
    "        graph.nodes.append(split_node)\n",
    "\n",
    "        # For each SequenceAt, create Squeeze and schedule replacement\n",
    "        for at, idx in seqat:\n",
    "            src = split_outs[idx]\n",
    "            out_tensor = at.outputs[0]\n",
    "\n",
    "            axes_const = gs.Constant(name=uid(f\"{at.name}_axes\"), values=np.array([axis], dtype=np.int64))\n",
    "            squeezed = gs.Variable(name=uid(f\"{at.name}_squeezed\"))\n",
    "            squeeze_node = gs.Node(op=\"Squeeze\", inputs=[src, axes_const], outputs=[squeezed])\n",
    "            graph.nodes.append(squeeze_node)\n",
    "\n",
    "            # Replace all uses of SequenceAt's output with squeezed tensor later\n",
    "            tensor_replacements[out_tensor] = squeezed\n",
    "            nodes_to_remove.append(at)\n",
    "\n",
    "        # Mark the SplitToSequence node for removal\n",
    "        nodes_to_remove.append(n)\n",
    "        changed_any = True\n",
    "\n",
    "    # Apply all tensor replacements at once (prevents partial rewires/cycles)\n",
    "    if tensor_replacements:\n",
    "        for node in graph.nodes:\n",
    "            for i, inp in enumerate(node.inputs):\n",
    "                if inp in tensor_replacements:\n",
    "                    node.inputs[i] = tensor_replacements[inp]\n",
    "        # Also replace in graph outputs if needed\n",
    "        for i, gout in enumerate(graph.outputs):\n",
    "            if gout in tensor_replacements:\n",
    "                graph.outputs[i] = tensor_replacements[gout]\n",
    "\n",
    "    # Now remove the obsolete nodes\n",
    "    for n in set(nodes_to_remove):\n",
    "        if n in graph.nodes:\n",
    "            graph.nodes.remove(n)\n",
    "\n",
    "    # Clean between passes\n",
    "    graph.cleanup().toposort()\n",
    "\n",
    "# Cast int64 inputs to int32 at the graph boundary\n",
    "for inp in list(graph.inputs):\n",
    "    if inp.name in {\"input_token\", \"enc_mask\"} and inp.dtype == np.int64:\n",
    "        cast_out = gs.Variable(name=uid(f\"{inp.name}_i32\"), dtype=np.int32, shape=inp.shape)\n",
    "        cast_node = gs.Node(op=\"Cast\", inputs=[inp], outputs=[cast_out],\n",
    "                            attrs={\"to\": onnx.TensorProto.INT32})\n",
    "        graph.nodes.append(cast_node)\n",
    "        for user in list(inp.outputs):\n",
    "            user.inputs = [cast_out if x is inp else x for x in user.inputs]\n",
    "\n",
    "graph.cleanup().toposort()\n",
    "onnx.save(gs.export_onnx(graph), ONNX_OUT)\n",
    "print(f\"Saved patched ONNX to {ONNX_OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT version: 10.13.2.6\n",
      "[09/14/2025-13:32:28] [TRT] [W] WARNING The logger passed into createInferBuilder differs from one already registered for an existing builder, runtime, or refitter. So the current new logger is ignored, and TensorRT will use the existing one which is returned by nvinfer1::getLogger() instead.\n",
      "[09/14/2025-13:32:29] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -989, GPU +9, now: CPU 19112, GPU 4316 (MiB)\n",
      "[09/14/2025-13:32:29] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/14/2025-13:32:29] [TRT] [I] ONNX IR version:  0.0.10\n",
      "[09/14/2025-13:32:29] [TRT] [I] Opset version:    18\n",
      "[09/14/2025-13:32:29] [TRT] [I] Producer name:    pytorch\n",
      "[09/14/2025-13:32:29] [TRT] [I] Producer version: 2.8.0a0+34c6371d24.nv25.08\n",
      "[09/14/2025-13:32:29] [TRT] [I] Domain:           \n",
      "[09/14/2025-13:32:29] [TRT] [I] Model version:    0\n",
      "[09/14/2025-13:32:29] [TRT] [I] Doc string:       \n",
      "[09/14/2025-13:32:29] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/14/2025-13:32:29] [TRT] [W] ModelImporter.cpp:653: Make sure input input_token has Int64 binding.\n",
      "[09/14/2025-13:32:29] [TRT] [W] ModelImporter.cpp:653: Make sure input enc_mask has Int64 binding.\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 10 [SplitToSequence -> \"unbind\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute\"\n",
      "output: \"unbind\"\n",
      "name: \"node_SplitToSequence_30\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.0: timm.models.vision_transformer.Block/m.image_encoder.blocks.0.attn: timm.layers.attention.Attention/unbind: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.0\\', \\'m.image_encoder.blocks.0.attn\\', \\'unbind\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 11 [SequenceAt -> \"getitem\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem\"\n",
      "name: \"n0\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.0: timm.models.vision_transformer.Block/m.image_encoder.blocks.0.attn: timm.layers.attention.Attention/getitem: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.0\\', \\'m.image_encoder.blocks.0.attn\\', \\'getitem\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 12 [SequenceAt -> \"getitem_1\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_1\"\n",
      "name: \"n0_2\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.0: timm.models.vision_transformer.Block/m.image_encoder.blocks.0.attn: timm.layers.attention.Attention/getitem_1: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_1 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.0\\', \\'m.image_encoder.blocks.0.attn\\', \\'getitem_1\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 13 [SequenceAt -> \"getitem_2\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_2\"\n",
      "name: \"n0_3\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.0: timm.models.vision_transformer.Block/m.image_encoder.blocks.0.attn: timm.layers.attention.Attention/getitem_2: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_2 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.0\\', \\'m.image_encoder.blocks.0.attn\\', \\'getitem_2\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 43 [SplitToSequence -> \"unbind_1\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_1\"\n",
      "output: \"unbind_1\"\n",
      "name: \"node_SplitToSequence_111\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.1: timm.models.vision_transformer.Block/m.image_encoder.blocks.1.attn: timm.layers.attention.Attention/unbind_1: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_1 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_1,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.1\\', \\'m.image_encoder.blocks.1.attn\\', \\'unbind_1\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 44 [SequenceAt -> \"getitem_3\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_1\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_3\"\n",
      "name: \"n0_4\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.1: timm.models.vision_transformer.Block/m.image_encoder.blocks.1.attn: timm.layers.attention.Attention/getitem_3: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_3 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_1, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.1\\', \\'m.image_encoder.blocks.1.attn\\', \\'getitem_3\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 45 [SequenceAt -> \"getitem_4\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_1\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_4\"\n",
      "name: \"n0_5\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.1: timm.models.vision_transformer.Block/m.image_encoder.blocks.1.attn: timm.layers.attention.Attention/getitem_4: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_4 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_1, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.1\\', \\'m.image_encoder.blocks.1.attn\\', \\'getitem_4\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 46 [SequenceAt -> \"getitem_5\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_1\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_5\"\n",
      "name: \"n0_6\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.1: timm.models.vision_transformer.Block/m.image_encoder.blocks.1.attn: timm.layers.attention.Attention/getitem_5: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_5 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_1, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.1\\', \\'m.image_encoder.blocks.1.attn\\', \\'getitem_5\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 76 [SplitToSequence -> \"unbind_2\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_2\"\n",
      "output: \"unbind_2\"\n",
      "name: \"node_SplitToSequence_187\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.2: timm.models.vision_transformer.Block/m.image_encoder.blocks.2.attn: timm.layers.attention.Attention/unbind_2: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_2 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_2,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.2\\', \\'m.image_encoder.blocks.2.attn\\', \\'unbind_2\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 77 [SequenceAt -> \"getitem_6\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_2\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_6\"\n",
      "name: \"n0_7\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.2: timm.models.vision_transformer.Block/m.image_encoder.blocks.2.attn: timm.layers.attention.Attention/getitem_6: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_6 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_2, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.2\\', \\'m.image_encoder.blocks.2.attn\\', \\'getitem_6\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 78 [SequenceAt -> \"getitem_7\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_2\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_7\"\n",
      "name: \"n0_8\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.2: timm.models.vision_transformer.Block/m.image_encoder.blocks.2.attn: timm.layers.attention.Attention/getitem_7: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_7 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_2, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.2\\', \\'m.image_encoder.blocks.2.attn\\', \\'getitem_7\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 79 [SequenceAt -> \"getitem_8\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_2\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_8\"\n",
      "name: \"n0_9\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.2: timm.models.vision_transformer.Block/m.image_encoder.blocks.2.attn: timm.layers.attention.Attention/getitem_8: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_8 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_2, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.2\\', \\'m.image_encoder.blocks.2.attn\\', \\'getitem_8\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 109 [SplitToSequence -> \"unbind_3\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_3\"\n",
      "output: \"unbind_3\"\n",
      "name: \"node_SplitToSequence_263\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.3: timm.models.vision_transformer.Block/m.image_encoder.blocks.3.attn: timm.layers.attention.Attention/unbind_3: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_3 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_3,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.3\\', \\'m.image_encoder.blocks.3.attn\\', \\'unbind_3\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 110 [SequenceAt -> \"getitem_9\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_3\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_9\"\n",
      "name: \"n0_10\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.3: timm.models.vision_transformer.Block/m.image_encoder.blocks.3.attn: timm.layers.attention.Attention/getitem_9: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_9 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_3, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.3\\', \\'m.image_encoder.blocks.3.attn\\', \\'getitem_9\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 111 [SequenceAt -> \"getitem_10\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_3\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_10\"\n",
      "name: \"n0_11\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.3: timm.models.vision_transformer.Block/m.image_encoder.blocks.3.attn: timm.layers.attention.Attention/getitem_10: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_10 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_3, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.3\\', \\'m.image_encoder.blocks.3.attn\\', \\'getitem_10\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 112 [SequenceAt -> \"getitem_11\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_3\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_11\"\n",
      "name: \"n0_12\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.3: timm.models.vision_transformer.Block/m.image_encoder.blocks.3.attn: timm.layers.attention.Attention/getitem_11: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_11 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_3, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.3\\', \\'m.image_encoder.blocks.3.attn\\', \\'getitem_11\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 142 [SplitToSequence -> \"unbind_4\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_4\"\n",
      "output: \"unbind_4\"\n",
      "name: \"node_SplitToSequence_339\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.4: timm.models.vision_transformer.Block/m.image_encoder.blocks.4.attn: timm.layers.attention.Attention/unbind_4: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_4 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_4,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.4\\', \\'m.image_encoder.blocks.4.attn\\', \\'unbind_4\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 143 [SequenceAt -> \"getitem_12\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_4\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_12\"\n",
      "name: \"n0_13\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.4: timm.models.vision_transformer.Block/m.image_encoder.blocks.4.attn: timm.layers.attention.Attention/getitem_12: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_12 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_4, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.4\\', \\'m.image_encoder.blocks.4.attn\\', \\'getitem_12\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 144 [SequenceAt -> \"getitem_13\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_4\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_13\"\n",
      "name: \"n0_14\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.4: timm.models.vision_transformer.Block/m.image_encoder.blocks.4.attn: timm.layers.attention.Attention/getitem_13: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_13 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_4, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.4\\', \\'m.image_encoder.blocks.4.attn\\', \\'getitem_13\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 145 [SequenceAt -> \"getitem_14\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_4\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_14\"\n",
      "name: \"n0_15\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.4: timm.models.vision_transformer.Block/m.image_encoder.blocks.4.attn: timm.layers.attention.Attention/getitem_14: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_14 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_4, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.4\\', \\'m.image_encoder.blocks.4.attn\\', \\'getitem_14\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 175 [SplitToSequence -> \"unbind_5\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_5\"\n",
      "output: \"unbind_5\"\n",
      "name: \"node_SplitToSequence_415\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.5: timm.models.vision_transformer.Block/m.image_encoder.blocks.5.attn: timm.layers.attention.Attention/unbind_5: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_5 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_5,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.5\\', \\'m.image_encoder.blocks.5.attn\\', \\'unbind_5\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 176 [SequenceAt -> \"getitem_15\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_5\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_15\"\n",
      "name: \"n0_16\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.5: timm.models.vision_transformer.Block/m.image_encoder.blocks.5.attn: timm.layers.attention.Attention/getitem_15: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_15 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_5, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.5\\', \\'m.image_encoder.blocks.5.attn\\', \\'getitem_15\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 177 [SequenceAt -> \"getitem_16\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_5\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_16\"\n",
      "name: \"n0_17\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.5: timm.models.vision_transformer.Block/m.image_encoder.blocks.5.attn: timm.layers.attention.Attention/getitem_16: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_16 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_5, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.5\\', \\'m.image_encoder.blocks.5.attn\\', \\'getitem_16\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 178 [SequenceAt -> \"getitem_17\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_5\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_17\"\n",
      "name: \"n0_18\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.5: timm.models.vision_transformer.Block/m.image_encoder.blocks.5.attn: timm.layers.attention.Attention/getitem_17: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_17 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_5, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.5\\', \\'m.image_encoder.blocks.5.attn\\', \\'getitem_17\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 208 [SplitToSequence -> \"unbind_6\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_6\"\n",
      "output: \"unbind_6\"\n",
      "name: \"node_SplitToSequence_491\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.6: timm.models.vision_transformer.Block/m.image_encoder.blocks.6.attn: timm.layers.attention.Attention/unbind_6: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_6 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_6,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.6\\', \\'m.image_encoder.blocks.6.attn\\', \\'unbind_6\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 209 [SequenceAt -> \"getitem_18\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_6\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_18\"\n",
      "name: \"n0_19\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.6: timm.models.vision_transformer.Block/m.image_encoder.blocks.6.attn: timm.layers.attention.Attention/getitem_18: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_18 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_6, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.6\\', \\'m.image_encoder.blocks.6.attn\\', \\'getitem_18\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 210 [SequenceAt -> \"getitem_19\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_6\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_19\"\n",
      "name: \"n0_20\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.6: timm.models.vision_transformer.Block/m.image_encoder.blocks.6.attn: timm.layers.attention.Attention/getitem_19: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_19 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_6, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.6\\', \\'m.image_encoder.blocks.6.attn\\', \\'getitem_19\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 211 [SequenceAt -> \"getitem_20\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_6\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_20\"\n",
      "name: \"n0_21\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.6: timm.models.vision_transformer.Block/m.image_encoder.blocks.6.attn: timm.layers.attention.Attention/getitem_20: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_20 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_6, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.6\\', \\'m.image_encoder.blocks.6.attn\\', \\'getitem_20\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 241 [SplitToSequence -> \"unbind_7\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_7\"\n",
      "output: \"unbind_7\"\n",
      "name: \"node_SplitToSequence_567\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.7: timm.models.vision_transformer.Block/m.image_encoder.blocks.7.attn: timm.layers.attention.Attention/unbind_7: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_7 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_7,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.7\\', \\'m.image_encoder.blocks.7.attn\\', \\'unbind_7\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 242 [SequenceAt -> \"getitem_21\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_7\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_21\"\n",
      "name: \"n0_22\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.7: timm.models.vision_transformer.Block/m.image_encoder.blocks.7.attn: timm.layers.attention.Attention/getitem_21: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_21 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_7, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.7\\', \\'m.image_encoder.blocks.7.attn\\', \\'getitem_21\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 243 [SequenceAt -> \"getitem_22\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_7\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_22\"\n",
      "name: \"n0_23\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.7: timm.models.vision_transformer.Block/m.image_encoder.blocks.7.attn: timm.layers.attention.Attention/getitem_22: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_22 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_7, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.7\\', \\'m.image_encoder.blocks.7.attn\\', \\'getitem_22\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 244 [SequenceAt -> \"getitem_23\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_7\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_23\"\n",
      "name: \"n0_24\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.7: timm.models.vision_transformer.Block/m.image_encoder.blocks.7.attn: timm.layers.attention.Attention/getitem_23: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_23 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_7, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.7\\', \\'m.image_encoder.blocks.7.attn\\', \\'getitem_23\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 274 [SplitToSequence -> \"unbind_8\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_8\"\n",
      "output: \"unbind_8\"\n",
      "name: \"node_SplitToSequence_643\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.8: timm.models.vision_transformer.Block/m.image_encoder.blocks.8.attn: timm.layers.attention.Attention/unbind_8: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_8 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_8,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.8\\', \\'m.image_encoder.blocks.8.attn\\', \\'unbind_8\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 275 [SequenceAt -> \"getitem_24\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_8\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_24\"\n",
      "name: \"n0_25\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.8: timm.models.vision_transformer.Block/m.image_encoder.blocks.8.attn: timm.layers.attention.Attention/getitem_24: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_24 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_8, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.8\\', \\'m.image_encoder.blocks.8.attn\\', \\'getitem_24\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 276 [SequenceAt -> \"getitem_25\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_8\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_25\"\n",
      "name: \"n0_26\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.8: timm.models.vision_transformer.Block/m.image_encoder.blocks.8.attn: timm.layers.attention.Attention/getitem_25: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_25 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_8, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.8\\', \\'m.image_encoder.blocks.8.attn\\', \\'getitem_25\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 277 [SequenceAt -> \"getitem_26\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_8\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_26\"\n",
      "name: \"n0_27\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.8: timm.models.vision_transformer.Block/m.image_encoder.blocks.8.attn: timm.layers.attention.Attention/getitem_26: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_26 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_8, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.8\\', \\'m.image_encoder.blocks.8.attn\\', \\'getitem_26\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 307 [SplitToSequence -> \"unbind_9\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_9\"\n",
      "output: \"unbind_9\"\n",
      "name: \"node_SplitToSequence_719\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.9: timm.models.vision_transformer.Block/m.image_encoder.blocks.9.attn: timm.layers.attention.Attention/unbind_9: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_9 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_9,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.9\\', \\'m.image_encoder.blocks.9.attn\\', \\'unbind_9\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 308 [SequenceAt -> \"getitem_27\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_9\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_27\"\n",
      "name: \"n0_28\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.9: timm.models.vision_transformer.Block/m.image_encoder.blocks.9.attn: timm.layers.attention.Attention/getitem_27: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_27 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_9, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.9\\', \\'m.image_encoder.blocks.9.attn\\', \\'getitem_27\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 309 [SequenceAt -> \"getitem_28\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_9\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_28\"\n",
      "name: \"n0_29\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.9: timm.models.vision_transformer.Block/m.image_encoder.blocks.9.attn: timm.layers.attention.Attention/getitem_28: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_28 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_9, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.9\\', \\'m.image_encoder.blocks.9.attn\\', \\'getitem_28\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 310 [SequenceAt -> \"getitem_29\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_9\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_29\"\n",
      "name: \"n0_30\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.9: timm.models.vision_transformer.Block/m.image_encoder.blocks.9.attn: timm.layers.attention.Attention/getitem_29: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_29 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_9, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.9\\', \\'m.image_encoder.blocks.9.attn\\', \\'getitem_29\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 340 [SplitToSequence -> \"unbind_10\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_10\"\n",
      "output: \"unbind_10\"\n",
      "name: \"node_SplitToSequence_795\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.10: timm.models.vision_transformer.Block/m.image_encoder.blocks.10.attn: timm.layers.attention.Attention/unbind_10: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_10 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_10,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.10\\', \\'m.image_encoder.blocks.10.attn\\', \\'unbind_10\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 341 [SequenceAt -> \"getitem_30\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_10\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_30\"\n",
      "name: \"n0_31\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.10: timm.models.vision_transformer.Block/m.image_encoder.blocks.10.attn: timm.layers.attention.Attention/getitem_30: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_30 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_10, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.10\\', \\'m.image_encoder.blocks.10.attn\\', \\'getitem_30\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 342 [SequenceAt -> \"getitem_31\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_10\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_31\"\n",
      "name: \"n0_32\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.10: timm.models.vision_transformer.Block/m.image_encoder.blocks.10.attn: timm.layers.attention.Attention/getitem_31: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_31 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_10, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.10\\', \\'m.image_encoder.blocks.10.attn\\', \\'getitem_31\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 343 [SequenceAt -> \"getitem_32\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_10\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_32\"\n",
      "name: \"n0_33\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.10: timm.models.vision_transformer.Block/m.image_encoder.blocks.10.attn: timm.layers.attention.Attention/getitem_32: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_32 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_10, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.10\\', \\'m.image_encoder.blocks.10.attn\\', \\'getitem_32\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 373 [SplitToSequence -> \"unbind_11\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"permute_11\"\n",
      "output: \"unbind_11\"\n",
      "name: \"node_SplitToSequence_871\"\n",
      "op_type: \"SplitToSequence\"\n",
      "attribute {\n",
      "  name: \"keepdims\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "attribute {\n",
      "  name: \"axis\"\n",
      "  i: 0\n",
      "  type: INT\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.11: timm.models.vision_transformer.Block/m.image_encoder.blocks.11.attn: timm.layers.attention.Attention/unbind_11: aten.unbind.int\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'aten.unbind.int\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%unbind_11 : [num_users=3] = call_function[target=torch.ops.aten.unbind.int](args = (%permute_11,), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.11\\', \\'m.image_encoder.blocks.11.attn\\', \\'unbind_11\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1072 In function checkSplitToSequence:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 374 [SequenceAt -> \"getitem_33\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_11\"\n",
      "input: \"val_21\"\n",
      "output: \"getitem_33\"\n",
      "name: \"n0_34\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.11: timm.models.vision_transformer.Block/m.image_encoder.blocks.11.attn: timm.layers.attention.Attention/getitem_33: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_33 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_11, 0), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.11\\', \\'m.image_encoder.blocks.11.attn\\', \\'getitem_33\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 375 [SequenceAt -> \"getitem_34\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_11\"\n",
      "input: \"val_22\"\n",
      "output: \"getitem_34\"\n",
      "name: \"n0_35\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.11: timm.models.vision_transformer.Block/m.image_encoder.blocks.11.attn: timm.layers.attention.Attention/getitem_34: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_34 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_11, 1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.11\\', \\'m.image_encoder.blocks.11.attn\\', \\'getitem_34\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 376 [SequenceAt -> \"getitem_35\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"unbind_11\"\n",
      "input: \"val_23\"\n",
      "output: \"getitem_35\"\n",
      "name: \"n0_36\"\n",
      "op_type: \"SequenceAt\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/m.image_encoder.blocks: torch.nn.modules.container.Sequential/m.image_encoder.blocks.11: timm.models.vision_transformer.Block/m.image_encoder.blocks.11.attn: timm.layers.attention.Attention/getitem_35: <built-in function getitem>\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'torch.nn.modules.container.Sequential\\', \\'timm.models.vision_transformer.Block\\', \\'timm.layers.attention.Attention\\', \\'<built-in function getitem>\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%getitem_35 : [num_users=1] = call_function[target=operator.getitem](args = (%unbind_11, 2), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'m.image_encoder.blocks\\', \\'m.image_encoder.blocks.11\\', \\'m.image_encoder.blocks.11.attn\\', \\'getitem_35\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 260, in forward\\n    image_embedding = self.image_encoder.forward_features(image)  # [B, C, F]\\n  File \\\"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\\\", line 245, in forward\\n    input = module(input)\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/models/vision_transformer.py\\\", line 176, in forward\\n    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))\\n  File \\\"/usr/local/lib/python3.12/dist-packages/timm/layers/attention.py\\\", line 76, in forward\\n    q, k, v = qkv.unbind(0)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:1042 In function checkSequenceAt:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 1976 [BitwiseAnd -> \"bitwise_and\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"val_795\"\n",
      "input: \"convert_element_type_default_1\"\n",
      "output: \"bitwise_and\"\n",
      "name: \"node_BitwiseAnd_4592\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%ones_3, %convert_element_type_default_1), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 2528 [BitwiseAnd -> \"bitwise_and_1\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and\"\n",
      "input: \"convert_element_type_default_3\"\n",
      "output: \"bitwise_and_1\"\n",
      "name: \"node_BitwiseAnd_5934\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_1: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_1 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and, %convert_element_type_default_3), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_1\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 3080 [BitwiseAnd -> \"bitwise_and_2\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_1\"\n",
      "input: \"convert_element_type_default_5\"\n",
      "output: \"bitwise_and_2\"\n",
      "name: \"node_BitwiseAnd_7277\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_2: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_2 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_1, %convert_element_type_default_5), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_2\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 3632 [BitwiseAnd -> \"bitwise_and_3\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_2\"\n",
      "input: \"convert_element_type_default_7\"\n",
      "output: \"bitwise_and_3\"\n",
      "name: \"node_BitwiseAnd_8620\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_3: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_3 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_2, %convert_element_type_default_7), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_3\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 4184 [BitwiseAnd -> \"bitwise_and_4\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_3\"\n",
      "input: \"convert_element_type_default_9\"\n",
      "output: \"bitwise_and_4\"\n",
      "name: \"node_BitwiseAnd_9963\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_4: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_4 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_3, %convert_element_type_default_9), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_4\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 4736 [BitwiseAnd -> \"bitwise_and_5\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_4\"\n",
      "input: \"convert_element_type_default_11\"\n",
      "output: \"bitwise_and_5\"\n",
      "name: \"node_BitwiseAnd_11306\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_5: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_5 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_4, %convert_element_type_default_11), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_5\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 5288 [BitwiseAnd -> \"bitwise_and_6\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_5\"\n",
      "input: \"convert_element_type_default_13\"\n",
      "output: \"bitwise_and_6\"\n",
      "name: \"node_BitwiseAnd_12648\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_6: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_6 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_5, %convert_element_type_default_13), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_6\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 5840 [BitwiseAnd -> \"bitwise_and_7\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_6\"\n",
      "input: \"convert_element_type_default_15\"\n",
      "output: \"bitwise_and_7\"\n",
      "name: \"node_BitwiseAnd_13991\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_7: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_7 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_6, %convert_element_type_default_15), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_7\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 6392 [BitwiseAnd -> \"bitwise_and_8\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_7\"\n",
      "input: \"convert_element_type_default_17\"\n",
      "output: \"bitwise_and_8\"\n",
      "name: \"node_BitwiseAnd_15334\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_8: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_8 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_7, %convert_element_type_default_17), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_8\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 6944 [BitwiseAnd -> \"bitwise_and_9\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_8\"\n",
      "input: \"convert_element_type_default_19\"\n",
      "output: \"bitwise_and_9\"\n",
      "name: \"node_BitwiseAnd_16677\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_9: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_9 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_8, %convert_element_type_default_19), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_9\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 7496 [BitwiseAnd -> \"bitwise_and_10\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_9\"\n",
      "input: \"convert_element_type_default_21\"\n",
      "output: \"bitwise_and_10\"\n",
      "name: \"node_BitwiseAnd_18020\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_10: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_10 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_9, %convert_element_type_default_21), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_10\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 8048 [BitwiseAnd -> \"bitwise_and_11\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_10\"\n",
      "input: \"convert_element_type_default_23\"\n",
      "output: \"bitwise_and_11\"\n",
      "name: \"node_BitwiseAnd_19363\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_11: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_11 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_10, %convert_element_type_default_23), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_11\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 8600 [BitwiseAnd -> \"bitwise_and_12\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_11\"\n",
      "input: \"convert_element_type_default_25\"\n",
      "output: \"bitwise_and_12\"\n",
      "name: \"node_BitwiseAnd_20706\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_12: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_12 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_11, %convert_element_type_default_25), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_12\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 9152 [BitwiseAnd -> \"bitwise_and_13\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_12\"\n",
      "input: \"convert_element_type_default_27\"\n",
      "output: \"bitwise_and_13\"\n",
      "name: \"node_BitwiseAnd_22048\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_13: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_13 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_12, %convert_element_type_default_27), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_13\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 9704 [BitwiseAnd -> \"bitwise_and_14\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_13\"\n",
      "input: \"convert_element_type_default_29\"\n",
      "output: \"bitwise_and_14\"\n",
      "name: \"node_BitwiseAnd_23390\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_14: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_14 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_13, %convert_element_type_default_29), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_14\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 10256 [BitwiseAnd -> \"bitwise_and_15\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_14\"\n",
      "input: \"convert_element_type_default_31\"\n",
      "output: \"bitwise_and_15\"\n",
      "name: \"node_BitwiseAnd_24733\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_15: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_15 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_14, %convert_element_type_default_31), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_15\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 10808 [BitwiseAnd -> \"bitwise_and_16\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_15\"\n",
      "input: \"convert_element_type_default_33\"\n",
      "output: \"bitwise_and_16\"\n",
      "name: \"node_BitwiseAnd_26076\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_16: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_16 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_15, %convert_element_type_default_33), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_16\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 11360 [BitwiseAnd -> \"bitwise_and_17\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_16\"\n",
      "input: \"convert_element_type_default_35\"\n",
      "output: \"bitwise_and_17\"\n",
      "name: \"node_BitwiseAnd_27419\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_17: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_17 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_16, %convert_element_type_default_35), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_17\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 11912 [BitwiseAnd -> \"bitwise_and_18\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_17\"\n",
      "input: \"convert_element_type_default_37\"\n",
      "output: \"bitwise_and_18\"\n",
      "name: \"node_BitwiseAnd_28762\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_18: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_18 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_17, %convert_element_type_default_37), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_18\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 12464 [BitwiseAnd -> \"bitwise_and_19\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_18\"\n",
      "input: \"convert_element_type_default_39\"\n",
      "output: \"bitwise_and_19\"\n",
      "name: \"node_BitwiseAnd_30105\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_19: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_19 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_18, %convert_element_type_default_39), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_19\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 13016 [BitwiseAnd -> \"bitwise_and_20\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_19\"\n",
      "input: \"convert_element_type_default_41\"\n",
      "output: \"bitwise_and_20\"\n",
      "name: \"node_BitwiseAnd_31448\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_20: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_20 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_19, %convert_element_type_default_41), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_20\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 13568 [BitwiseAnd -> \"bitwise_and_21\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_20\"\n",
      "input: \"convert_element_type_default_43\"\n",
      "output: \"bitwise_and_21\"\n",
      "name: \"node_BitwiseAnd_32791\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_21: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_21 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_20, %convert_element_type_default_43), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_21\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 14120 [BitwiseAnd -> \"bitwise_and_22\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_21\"\n",
      "input: \"convert_element_type_default_45\"\n",
      "output: \"bitwise_and_22\"\n",
      "name: \"node_BitwiseAnd_34134\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_22: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_22 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_21, %convert_element_type_default_45), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_22\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 14672 [BitwiseAnd -> \"bitwise_and_23\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_22\"\n",
      "input: \"convert_element_type_default_47\"\n",
      "output: \"bitwise_and_23\"\n",
      "name: \"node_BitwiseAnd_35477\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_23: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_23 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_22, %convert_element_type_default_47), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_23\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 15224 [BitwiseAnd -> \"bitwise_and_24\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_23\"\n",
      "input: \"convert_element_type_default_49\"\n",
      "output: \"bitwise_and_24\"\n",
      "name: \"node_BitwiseAnd_36820\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_24: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_24 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_23, %convert_element_type_default_49), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_24\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 15776 [BitwiseAnd -> \"bitwise_and_25\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_24\"\n",
      "input: \"convert_element_type_default_51\"\n",
      "output: \"bitwise_and_25\"\n",
      "name: \"node_BitwiseAnd_38163\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_25: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_25 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_24, %convert_element_type_default_51), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_25\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 16328 [BitwiseAnd -> \"bitwise_and_26\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_25\"\n",
      "input: \"convert_element_type_default_53\"\n",
      "output: \"bitwise_and_26\"\n",
      "name: \"node_BitwiseAnd_39506\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_26: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_26 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_25, %convert_element_type_default_53), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_26\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 16880 [BitwiseAnd -> \"bitwise_and_27\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_26\"\n",
      "input: \"convert_element_type_default_55\"\n",
      "output: \"bitwise_and_27\"\n",
      "name: \"node_BitwiseAnd_40849\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_27: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_27 : [num_users=3] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_26, %convert_element_type_default_55), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_27\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:135: While parsing node number 17432 [BitwiseAnd -> \"bitwise_and_28\"]:\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:138: --- Begin node ---\n",
      "input: \"bitwise_and_27\"\n",
      "input: \"convert_element_type_default_57\"\n",
      "output: \"bitwise_and_28\"\n",
      "name: \"node_BitwiseAnd_42192\"\n",
      "op_type: \"BitwiseAnd\"\n",
      "metadata_props {\n",
      "  key: \"namespace\"\n",
      "  value: \": __main__.ExportWrapper/m: __main__.Blip2Model/bitwise_and_28: aten.bitwise_and.Tensor\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.class_hierarchy\"\n",
      "  value: \"[\\'__main__.ExportWrapper\\', \\'__main__.Blip2Model\\', \\'aten.bitwise_and.Tensor\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.fx_node\"\n",
      "  value: \"%bitwise_and_28 : [num_users=2] = call_function[target=torch.ops.aten.bitwise_and.Tensor](args = (%bitwise_and_27, %convert_element_type_default_57), kwargs = {})\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.name_scopes\"\n",
      "  value: \"[\\'\\', \\'m\\', \\'bitwise_and_28\\']\"\n",
      "}\n",
      "metadata_props {\n",
      "  key: \"pkg.torch.onnx.stack_trace\"\n",
      "  value: \"File \\\"/tmp/ipykernel_1989/3808401606.py\\\", line 8, in forward\\n    return self.m(img, input_token, enc_mask, seq_len)\\n  File \\\"/tmp/ipykernel_1989/700978424.py\\\", line 271, in forward\\n    gen_ids = self.lm_model.predict(z, input_token, enc_mask)\"\n",
      "}\n",
      "\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:139: --- End node ---\n",
      "[09/14/2025-13:32:29] [TRT] [E] ModelImporter.cpp:141: ERROR: onnxOpCheckers.cpp:930 In function checkBitwiseAnd:\n",
      "[8] false\n",
      "Parser error: In node 10 with name: node_SplitToSequence_30 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 11 with name: n0 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 12 with name: n0_2 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 13 with name: n0_3 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 43 with name: node_SplitToSequence_111 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 44 with name: n0_4 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 45 with name: n0_5 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 46 with name: n0_6 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 76 with name: node_SplitToSequence_187 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 77 with name: n0_7 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 78 with name: n0_8 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 79 with name: n0_9 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 109 with name: node_SplitToSequence_263 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 110 with name: n0_10 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 111 with name: n0_11 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 112 with name: n0_12 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 142 with name: node_SplitToSequence_339 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 143 with name: n0_13 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 144 with name: n0_14 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 145 with name: n0_15 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 175 with name: node_SplitToSequence_415 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 176 with name: n0_16 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 177 with name: n0_17 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 178 with name: n0_18 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 208 with name: node_SplitToSequence_491 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 209 with name: n0_19 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 210 with name: n0_20 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 211 with name: n0_21 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 241 with name: node_SplitToSequence_567 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 242 with name: n0_22 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 243 with name: n0_23 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 244 with name: n0_24 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 274 with name: node_SplitToSequence_643 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 275 with name: n0_25 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 276 with name: n0_26 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 277 with name: n0_27 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 307 with name: node_SplitToSequence_719 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 308 with name: n0_28 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 309 with name: n0_29 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 310 with name: n0_30 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 340 with name: node_SplitToSequence_795 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 341 with name: n0_31 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 342 with name: n0_32 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 343 with name: n0_33 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 373 with name: node_SplitToSequence_871 and operator: SplitToSequence (checkSplitToSequence): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 374 with name: n0_34 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 375 with name: n0_35 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 376 with name: n0_36 and operator: SequenceAt (checkSequenceAt): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 1976 with name: node_BitwiseAnd_4592 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 2528 with name: node_BitwiseAnd_5934 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 3080 with name: node_BitwiseAnd_7277 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 3632 with name: node_BitwiseAnd_8620 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 4184 with name: node_BitwiseAnd_9963 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 4736 with name: node_BitwiseAnd_11306 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 5288 with name: node_BitwiseAnd_12648 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 5840 with name: node_BitwiseAnd_13991 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 6392 with name: node_BitwiseAnd_15334 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 6944 with name: node_BitwiseAnd_16677 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 7496 with name: node_BitwiseAnd_18020 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 8048 with name: node_BitwiseAnd_19363 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 8600 with name: node_BitwiseAnd_20706 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 9152 with name: node_BitwiseAnd_22048 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 9704 with name: node_BitwiseAnd_23390 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 10256 with name: node_BitwiseAnd_24733 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 10808 with name: node_BitwiseAnd_26076 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 11360 with name: node_BitwiseAnd_27419 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 11912 with name: node_BitwiseAnd_28762 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 12464 with name: node_BitwiseAnd_30105 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 13016 with name: node_BitwiseAnd_31448 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 13568 with name: node_BitwiseAnd_32791 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 14120 with name: node_BitwiseAnd_34134 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 14672 with name: node_BitwiseAnd_35477 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 15224 with name: node_BitwiseAnd_36820 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 15776 with name: node_BitwiseAnd_38163 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 16328 with name: node_BitwiseAnd_39506 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 16880 with name: node_BitwiseAnd_40849 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n",
      "Parser error: In node 17432 with name: node_BitwiseAnd_42192 and operator: BitwiseAnd (checkBitwiseAnd): UNSUPPORTED_NODE: false\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ONNX parse failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Build & run\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m engine = \u001b[43mbuild_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mONNX_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m outs = infer(engine)\n\u001b[32m    126\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutputs:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mbuild_engine\u001b[39m\u001b[34m(onnx_path, fp16, workspace_bytes)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parser.num_errors):\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mParser error:\u001b[39m\u001b[33m\"\u001b[39m, parser.get_error(i))\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mONNX parse failed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m config = builder.create_builder_config()\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# TRT 9+/10 API:\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: ONNX parse failed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # noqa: F401\n",
    "\n",
    "ONNX_PATH = \"model.onnx\"\n",
    "ENGINE_PATH = \"model_fp16.engine\"\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "print(\"TensorRT version:\", trt.__version__)\n",
    "\n",
    "# --- 1) Build engine from ONNX ---\n",
    "def build_engine(onnx_path, fp16=True, workspace_bytes=1<<30):\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "\n",
    "    # Create network with EXPLICIT_BATCH\n",
    "    try:\n",
    "        flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    except AttributeError:\n",
    "        flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    network = builder.create_network(flags)\n",
    "\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "    with open(onnx_path, \"rb\") as f:\n",
    "        ok = parser.parse(f.read())\n",
    "    if not ok:\n",
    "        for i in range(parser.num_errors):\n",
    "            print(\"Parser error:\", parser.get_error(i))\n",
    "        raise RuntimeError(\"ONNX parse failed\")\n",
    "\n",
    "    config = builder.create_builder_config()\n",
    "    # TRT 9+/10 API:\n",
    "    if hasattr(config, \"set_memory_pool_limit\"):\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_bytes)\n",
    "    else:\n",
    "        config.max_workspace_size = workspace_bytes  # older TRT\n",
    "\n",
    "    # Enable FP16 if platform supports it\n",
    "    if fp16 and builder.platform_has_fast_fp16:\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "    # TRT 9+/10 returns serialized engine bytes\n",
    "    if hasattr(builder, \"build_serialized_network\"):\n",
    "        plan = builder.build_serialized_network(network, config)\n",
    "        if plan is None:\n",
    "            raise RuntimeError(\"Engine build failed\")\n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "        engine = runtime.deserialize_cuda_engine(plan)\n",
    "    else:\n",
    "        engine = builder.build_engine(network, config)\n",
    "        if engine is None:\n",
    "            raise RuntimeError(\"Engine build failed\")\n",
    "\n",
    "    # Save engine\n",
    "    with open(ENGINE_PATH, \"wb\") as f:\n",
    "        f.write(engine.serialize())\n",
    "    print(f\"Saved engine to {ENGINE_PATH}\")\n",
    "    return engine\n",
    "\n",
    "# --- 2) Allocate buffers & run one inference ---\n",
    "def infer(engine):\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Prepare host inputs matching your ONNX (adjust shapes/dtypes!)\n",
    "    # Example for your model: img [1,3,224,224] float32, token/mask [1,77] int32\n",
    "    img = np.random.randn(1,3,224,224).astype(np.float32)\n",
    "    input_token = np.zeros((1,77), dtype=np.int32)   # prefer int32 (see note below)\n",
    "    enc_mask = np.ones((1,77), dtype=np.int32)\n",
    "\n",
    "    # Map binding names to host arrays (order must match engine bindings)\n",
    "    host_inputs_by_name = {\n",
    "        \"img\": img,\n",
    "        \"input_token\": input_token,\n",
    "        \"enc_mask\": enc_mask,\n",
    "        # If your engine still has \"dummy_input_size\", remove it at export time as advised earlier\n",
    "    }\n",
    "\n",
    "    bindings = [None] * engine.num_bindings\n",
    "    d_ptrs = [None] * engine.num_bindings\n",
    "    h_buffers = [None] * engine.num_bindings\n",
    "\n",
    "    # Set shapes if dynamic (optional, depends on network)\n",
    "    for idx in range(engine.num_bindings):\n",
    "        name = engine.get_binding_name(idx)\n",
    "        is_input = engine.binding_is_input(idx)\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(idx))\n",
    "        if is_input:\n",
    "            host = host_inputs_by_name[name].astype(dtype, copy=False)\n",
    "            # If network is dynamic, set the shape before allocating\n",
    "            if hasattr(context, \"set_binding_shape\"):\n",
    "                context.set_binding_shape(idx, host.shape)\n",
    "            size = host.size\n",
    "        else:\n",
    "            # Get output shape after shapes are set\n",
    "            out_shape = tuple(context.get_binding_shape(idx))\n",
    "            size = int(np.prod(out_shape))\n",
    "            host = np.empty(size, dtype=dtype)\n",
    "\n",
    "        dmem = cuda.mem_alloc(host.nbytes)\n",
    "        d_ptrs[idx] = int(dmem)\n",
    "        h_buffers[idx] = host\n",
    "        bindings[idx] = d_ptrs[idx]\n",
    "\n",
    "    # Copy inputs H2D\n",
    "    for idx in range(engine.num_bindings):\n",
    "        if engine.binding_is_input(idx):\n",
    "            cuda.memcpy_htod(d_ptrs[idx], h_buffers[idx])\n",
    "\n",
    "    # Execute\n",
    "    context.execute_v2(bindings)\n",
    "\n",
    "    # Copy outputs D2H\n",
    "    outputs = {}\n",
    "    for idx in range(engine.num_bindings):\n",
    "        if not engine.binding_is_input(idx):\n",
    "            name = engine.get_binding_name(idx)\n",
    "            shape = tuple(context.get_binding_shape(idx))\n",
    "            host = h_buffers[idx]\n",
    "            cuda.memcpy_dtoh(host, d_ptrs[idx])\n",
    "            outputs[name] = host.reshape(shape)\n",
    "    return outputs\n",
    "\n",
    "# Build & run\n",
    "engine = build_engine(ONNX_PATH, fp16=True)\n",
    "outs = infer(engine)\n",
    "print(\"Outputs:\")\n",
    "for k, v in outs.items():\n",
    "    print(k, v.shape, v.dtype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
